{
    "page_1": {
        "page_number": 1,
        "text": "CHAPTER 5\n\nDATA COMPRESSION\n\nWe now put content in the definition of entropy by establishing the funda-\nmental limit for the compression of information. Data compression can be\nachieved by assigning short descriptions to the most frequent outcomes\nof the data source, and necessarily longer descriptions to the less fre-\nquent outcomes. For example, in Morse code, the most frequent symbol\nis represented by a single dot. In this chapter we find the shortest average\ndescription length of a random variable.\n\nWe first define the notion of an instantaneous code and then prove the\nimportant Kraft inequality, which asserts that the exponentiated codeword\nlength assignments must look like a probability mass function. Elemen-\ntary calculus then shows that the expected description length must be\ngreater than or equal to the entropy, the first main result. Then Shan-\nnon’s simple construction shows that the expected description length can\nachieve this bound asymptotically for repeated descriptions. This estab-\nlishes the entropy as a natural measure of efficient description length.\nThe famous Huffman coding procedure for finding minimum expected\ndescription length assignments is provided. Finally, we show that Huff-\nman codes are competitively optimal and that it requires roughly H fair\ncoin flips to generate a sample of a random variable having entropy H .\nThus, the entropy is the data compression limit as well as the number of\nbits needed in random number generation, and codes achieving H turn\nout to be optimal from many points of view.\n\n5.1 EXAMPLES OF CODES\n\nDefinition A source code C for a random variable X is a mapping from\nX, the range of X, to D∗, the set of finite-length strings of symbols from\na D-ary alphabet. Let C(x) denote the codeword corresponding to x and\nlet l(x) denote the length of C(x).\n\nElements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas\nCopyright © 2006 John Wiley & Sons, Inc.\n\n103\n\n\n\n104 DATA COMPRESSION\n\nFor example, C(red) = 00, C(blue) = 11 is a source code for X = {red,\nblue} with alphabet D = {0, 1}.\n\nDefinition The expected length L(C) of a source code C(x) for a ran-\ndom variable X with probability mass function p(x) is given by\n\nL(C) =\n∑\n\nx∈X\n\np(x)l(x), (5.1)\n\nwhere l(x) is the length of the codeword associated with x.\nWithout loss of generality, we can assume that the D-ary alphabet is\n\nD = {0, 1, . . . , D − 1}.\nSome examples of codes follow.\n\nExample 5.1.1 Let X be a random variable with the following distri-\nbution and codeword assignment:\n\nPr(X = 1) = 1\n2 , codeword C(1) = 0\n\nPr(X = 2) = 1\n4 , codeword C(2) = 10\n\nPr(X = 3) = 1\n8 , codeword C(3) = 110\n\nPr(X = 4) = 1\n8 , codeword C(4) = 111.\n\n(5.2)\n\nThe entropy H(X) of X is 1.75 bits, and the expected length L(C) =\nEl(X) of this code is also 1.75 bits. Here we have a code that has the\nsame average length as the entropy. We note that any sequence of bits\ncan be uniquely decoded into a sequence of symbols of X. For example,\nthe bit string 0110111100110 is decoded as 134213.\n\nExample 5.1.2 Consider another simple example of a code for a random\nvariable:\n\nPr(X = 1) = 1\n3 , codeword C(1) = 0\n\nPr(X = 2) = 1\n3 , codeword C(2) = 10\n\nPr(X = 3) = 1\n3 , codeword C(3) = 11.\n\n(5.3)\n\nJust as in Example 5.1.1, the code is uniquely decodable. However, in\nthis case the entropy is log 3 = 1.58 bits and the average length of the\nencoding is 1.66 bits. Here El(X) > H(X).\n\nExample 5.1.3 (Morse code) The Morse code is a reasonably efficient\ncode for the English alphabet using an alphabet of four symbols: a dot,\nCHAPTER 5\nDATA COMPRESSION\nWe now put content in the deﬁnition of entropy by establishing the funda-\nmental limit for the compression of information. Data compression can be\nachieved by assigning short descriptions to the most frequent outcomes\nof the data source, and necessarily longer descriptions to the less fre-\nquent outcomes. For example, in Morse code, the most frequent symbol\nis represented by a single dot. In this chapter we ﬁnd the shortest average\ndescription length of a random variable.\nWe ﬁrst deﬁne the notion of an instantaneous code and then prove the\nimportant Kraft inequality, which asserts that the exponentiated codeword\nlength assignments must look like a probability mass function. Elemen-\ntary calculus then shows that the expected description length must be\ngreater than or equal to the entropy, the ﬁrst main result. Then Shan-\nnon’s simple construction shows that the expected description length can\nachieve this bound asymptotically for repeated descriptions. This estab-\nlishes the entropy as a natural measure of efﬁcient description length.\nThe famous Huffman coding procedure for ﬁnding minimum expected\ndescription length assignments is provided. Finally, we show that Huff-\nman codes are competitively optimal and that it requires roughly H fair\ncoin ﬂips to generate a sample of a random variable having entropy H.\nThus, the entropy is the data compression limit as well as the number of\nbits needed in random number generation, and codes achieving H turn\nout to be optimal from many points of view.\n5.1\nEXAMPLES OF CODES\nDeﬁnition\nA source code C for a random variable X is a mapping from\nX, the range of X, to D∗, the set of ﬁnite-length strings of symbols from\na D-ary alphabet. Let C(x) denote the codeword corresponding to x and\nlet l(x) denote the length of C(x).\nElements of Information Theory, Second Edition,\nBy Thomas M. Cover and Joy A. Thomas\nCopyright © 2006 John Wiley & Sons, Inc.\n103\n",
        "tables": [],
        "images": [
            "[IMAGE 1]"
        ],
        "math_formulas": []
    },
    "page_2": {
        "page_number": 2,
        "text": "CHAPTER 5\n\nDATA COMPRESSION\n\nWe now put content in the definition of entropy by establishing the funda-\nmental limit for the compression of information. Data compression can be\nachieved by assigning short descriptions to the most frequent outcomes\nof the data source, and necessarily longer descriptions to the less fre-\nquent outcomes. For example, in Morse code, the most frequent symbol\nis represented by a single dot. In this chapter we find the shortest average\ndescription length of a random variable.\n\nWe first define the notion of an instantaneous code and then prove the\nimportant Kraft inequality, which asserts that the exponentiated codeword\nlength assignments must look like a probability mass function. Elemen-\ntary calculus then shows that the expected description length must be\ngreater than or equal to the entropy, the first main result. Then Shan-\nnon’s simple construction shows that the expected description length can\nachieve this bound asymptotically for repeated descriptions. This estab-\nlishes the entropy as a natural measure of efficient description length.\nThe famous Huffman coding procedure for finding minimum expected\ndescription length assignments is provided. Finally, we show that Huff-\nman codes are competitively optimal and that it requires roughly H fair\ncoin flips to generate a sample of a random variable having entropy H .\nThus, the entropy is the data compression limit as well as the number of\nbits needed in random number generation, and codes achieving H turn\nout to be optimal from many points of view.\n\n5.1 EXAMPLES OF CODES\n\nDefinition A source code C for a random variable X is a mapping from\nX, the range of X, to D∗, the set of finite-length strings of symbols from\na D-ary alphabet. Let C(x) denote the codeword corresponding to x and\nlet l(x) denote the length of C(x).\n\nElements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas\nCopyright © 2006 John Wiley & Sons, Inc.\n\n103\n\n\n\n104 DATA COMPRESSION\n\nFor example, C(red) = 00, C(blue) = 11 is a source code for X = {red,\nblue} with alphabet D = {0, 1}.\n\nDefinition The expected length L(C) of a source code C(x) for a ran-\ndom variable X with probability mass function p(x) is given by\n\nL(C) =\n∑\n\nx∈X\n\np(x)l(x), (5.1)\n\nwhere l(x) is the length of the codeword associated with x.\nWithout loss of generality, we can assume that the D-ary alphabet is\n\nD = {0, 1, . . . , D − 1}.\nSome examples of codes follow.\n\nExample 5.1.1 Let X be a random variable with the following distri-\nbution and codeword assignment:\n\nPr(X = 1) = 1\n2 , codeword C(1) = 0\n\nPr(X = 2) = 1\n4 , codeword C(2) = 10\n\nPr(X = 3) = 1\n8 , codeword C(3) = 110\n\nPr(X = 4) = 1\n8 , codeword C(4) = 111.\n\n(5.2)\n\nThe entropy H(X) of X is 1.75 bits, and the expected length L(C) =\nEl(X) of this code is also 1.75 bits. Here we have a code that has the\nsame average length as the entropy. We note that any sequence of bits\ncan be uniquely decoded into a sequence of symbols of X. For example,\nthe bit string 0110111100110 is decoded as 134213.\n\nExample 5.1.2 Consider another simple example of a code for a random\nvariable:\n\nPr(X = 1) = 1\n3 , codeword C(1) = 0\n\nPr(X = 2) = 1\n3 , codeword C(2) = 10\n\nPr(X = 3) = 1\n3 , codeword C(3) = 11.\n\n(5.3)\n\nJust as in Example 5.1.1, the code is uniquely decodable. However, in\nthis case the entropy is log 3 = 1.58 bits and the average length of the\nencoding is 1.66 bits. Here El(X) > H(X).\n\nExample 5.1.3 (Morse code) The Morse code is a reasonably efficient\ncode for the English alphabet using an alphabet of four symbols: a dot,\n104\nDATA COMPRESSION\nFor example, C(red) = 00, C(blue) = 11 is a source code for X = {red,\nblue} with alphabet D = {0, 1}.\nDeﬁnition\nThe expected length L(C) of a source code C(x) for a ran-\ndom variable X with probability mass function p(x) is given by\nL(C) =\n!\nx∈X\np(x)l(x),\n(5.1)\nwhere l(x) is the length of the codeword associated with x.\nWithout loss of generality, we can assume that the D-ary alphabet is\nD = {0, 1, . . . , D −1}.\nSome examples of codes follow.\nExample 5.1.1\nLet X be a random variable with the following distri-\nbution and codeword assignment:\nPr(X = 1) = 1\n2,\ncodeword C(1) = 0\nPr(X = 2) = 1\n4,\ncodeword C(2) = 10\nPr(X = 3) = 1\n8,\ncodeword C(3) = 110\nPr(X = 4) = 1\n8,\ncodeword C(4) = 111.\n(5.2)\nThe entropy H(X) of X is 1.75 bits, and the expected length L(C) =\nEl(X) of this code is also 1.75 bits. Here we have a code that has the\nsame average length as the entropy. We note that any sequence of bits\ncan be uniquely decoded into a sequence of symbols of X. For example,\nthe bit string 0110111100110 is decoded as 134213.\nExample 5.1.2\nConsider another simple example of a code for a random\nvariable:\nPr(X = 1) = 1\n3,\ncodeword C(1) = 0\nPr(X = 2) = 1\n3,\ncodeword C(2) = 10\nPr(X = 3) = 1\n3,\ncodeword C(3) = 11.\n(5.3)\nJust as in Example 5.1.1, the code is uniquely decodable. However, in\nthis case the entropy is log 3 = 1.58 bits and the average length of the\nencoding is 1.66 bits. Here El(X) > H(X).\nExample 5.1.3\n(Morse code) The Morse code is a reasonably efﬁcient\ncode for the English alphabet using an alphabet of four symbols: a dot,\n",
        "tables": [],
        "images": [
            "[IMAGE 1]"
        ],
        "math_formulas": []
    }
}