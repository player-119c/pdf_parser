

ELEMENTS OF INFORMATION THEORY Second Edition THOMAS M. COVER JOY A. THOMAS A JOHN WILEY & SONS, INC., PUBLICATION

CONTENTS Preface to the Second Edition xv Preface to the First Edition xvii Acknowledgments for the Second Edition xxi Acknowledgments for the First Edition xxiii 1 Introduction and Preview 1 1.1 Preview of the Book 5 2 Entropy, Relative Entropy, and Mutual Information 13 2.1 Entropy 13 2.2 Joint Entropy and Conditional Entropy 16 2.3 Relative Entropy and Mutual Information 19 2.4 Relationship Between Entropy and Mutual Information 20 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information 22 2.6 Jensen’s Inequality and Its Consequences 25 2.7 Log Sum Inequality and Its Applications 30 2.8 Data-Processing Inequality 34 2.9 Sufficient Statistics 35 2.10 Fano’s Inequality 37 Summary 41 Problems 43 Historical Notes 54 3 Asymptotic Equipartition Property 57 3.1 Asymptotic Equipartition Property Theorem 58 3.2 Consequences of the AEP: Data Compression 60 3.3 High-Probability Sets and the Typical Set 62 Summary 64 Problems 64 Historical Notes 69 4 Entropy Rates of a Stochastic Process 71 4.1 Markov Chains 71 4.2 Entropy Rate 74 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph 78 4.4 Second Law of Thermodynamics 81 4.5 Functions of Markov Chains 84 Summary 87 Problems 88 Historical Notes 100 5 Data Compression 103 5.1 Examples of Codes 103 5.2 Kraft Inequality 107 5.3 Optimal Codes 110 5.4 Bounds on the Optimal Code Length 112 5.5 Kraft Inequality for Uniquely Decodable Codes 115 5.6 Huffman Codes 118 5.7 Some Comments on Huffman Codes 120 5.8 Optimality of Huffman Codes 123 5.9 Shannon–Fano–Elias Coding 127 5.10 Competitive Optimality of the Shannon Code 130 5.11 Generation of Discrete Distributions from Fair Coins 134 Summary 141 Problems 142 Historical Notes 157 6 Gambling and Data Compression 159 6.1 The Horse Race 159 6.2 Gambling and Side Information 164 6.3 Dependent Horse Races and Entropy Rate 166 6.4 The Entropy of English 168 6.5 Data Compression and Gambling 171 6.6 Gambling Estimate of the Entropy of English 173 Summary 175 Problems 176 Historical Notes 182 7 Channel Capacity 183 7.1 Examples of Channel Capacity 184 7.1.1 Noiseless Binary Channel 184 7.1.2 Noisy Channel with Nonoverlapping Outputs 185 7.1.3 Noisy Typewriter 186 7.1.4 Binary Symmetric Channel 187 7.1.5 Binary Erasure Channel 188 7.2 Symmetric Channels 189 7.3 Properties of Channel Capacity 191 7.4 Preview of the Channel Coding Theorem 191 7.5 Definitions 192 7.6 Jointly Typical Sequences 195 7.7 Channel Coding Theorem 199 7.8 Zero-Error Codes 205 7.9 Fano’s Inequality and the Converse to the Coding Theorem 206 7.10 Equality in the Converse to the Channel Coding Theorem 208 7.11 Hamming Codes 210 7.12 Feedback Capacity 216 7.13 Source–Channel Separation Theorem 218 Summary 222 Problems 223 Historical Notes 240 8 Differential Entropy 243 8.1 Definitions 243 8.2 AEP for Continuous Random Variables 245 8.3 Relation of Differential Entropy to Discrete Entropy 247 8.4 Joint and Conditional Differential Entropy 249 8.5 Relative Entropy and Mutual Information 250 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information 252 Summary 256 Problems 256 Historical Notes 259 9 Gaussian Channel 261 9.1 Gaussian Channel: Definitions 263 9.2 Converse to the Coding Theorem for Gaussian Channels 268 9.3 Bandlimited Channels 270 9.4 Parallel Gaussian Channels 274 9.5 Channels with Colored Gaussian Noise 277 9.6 Gaussian Channels with Feedback 280 Summary 289 Problems 290 Historical Notes 299 10 Rate Distortion Theory 301 10.1 Quantization 301 10.2 Definitions 303 10.3 Calculation of the Rate Distortion Function 307 10.3.1 Binary Source 307 10.3.2 Gaussian Source 310 10.3.3 Simultaneous Description of Independent Gaussian Random Variables 312 10.4 Converse to the Rate Distortion Theorem 315 10.5 Achievability of the Rate Distortion Function 318 10.6 Strongly Typical Sequences and Rate Distortion 325 10.7 Characterization of the Rate Distortion Function 329 10.8 Computation of Channel Capacity and the Rate Distortion Function 332 Summary 335 Problems 336 Historical Notes 345 11 Information Theory and Statistics 347 11.1 Method of Types 347 11.2 Law of Large Numbers 355 11.3 Universal Source Coding 357 11.4 Large Deviation Theory 360 11.5 Examples of Sanov’s Theorem 364 11.6 Conditional Limit Theorem 366 11.7 Hypothesis Testing 375 11.8 Chernoff–Stein Lemma 380 11.9 Chernoff Information 384 11.10 Fisher Information and the Cramér–Rao Inequality 392 Summary 397 Problems 399 Historical Notes 408 12 Maximum Entropy 409 12.1 Maximum Entropy Distributions 409 12.2 Examples 411 12.3 Anomalous Maximum Entropy Problem 413 12.4 Spectrum Estimation 415 12.5 Entropy Rates of a Gaussian Process 416 12.6 Burg’s Maximum Entropy Theorem 417 Summary 420 Problems 421 Historical Notes 425 13 Universal Source Coding 427 13.1 Universal Codes and Channel Capacity 428 13.2 Universal Coding for Binary Sequences 433 13.3 Arithmetic Coding 436 13.4 Lempel–Ziv Coding 440 13.4.1 Sliding Window Lempel–Ziv Algorithm 441 13.4.2 Tree-Structured Lempel–Ziv Algorithms 442 13.5 Optimality of Lempel–Ziv Algorithms 443 13.5.1 Sliding Window Lempel–Ziv Algorithms 443 13.5.2 Optimality of Tree-Structured Lempel–Ziv Compression 448 Summary 456 Problems 457 Historical Notes 461 14 Kolmogorov Complexity 463 14.1 Models of Computation 464 14.2 Kolmogorov Complexity: Definitions and Examples 466 14.3 Kolmogorov Complexity and Entropy 473 14.4 Kolmogorov Complexity of Integers 475 14.5 Algorithmically Random and Incompressible Sequences 476 14.6 Universal Probability 480 14.7 Kolmogorov Complexity 482 14.8 Universal Gambling 487 14.9 Occam’s Razor 488 14.10 Kolmogorov Complexity and Universal Probability 490 14.11 Kolmogorov Sufficient Statistic 496 14.12 Minimum Description Length Principle 500 Summary 501 Problems 503 Historical Notes 507 15 Network Information Theory 509 15.1 Gaussian Multiple-User Channels 513 15.1.1 Single-User Gaussian Channel 513 15.1.2 Gaussian Multiple-Access Channel with m Users 514 15.1.3 Gaussian Broadcast Channel 515 15.1.4 Gaussian Relay Channel 516 15.1.5 Gaussian Interference Channel 518 15.1.6 Gaussian Two-Way Channel 519 15.2 Jointly Typical Sequences 520 15.3 Multiple-Access Channel 524 15.3.1 Achievability of the Capacity Region for the Multiple-Access Channel 530 15.3.2 Comments on the Capacity Region for the Multiple-Access Channel 532 15.3.3 Convexity of the Capacity Region of the Multiple-Access Channel 534 15.3.4 Converse for the Multiple-Access Channel 538 15.3.5 m-User Multiple-Access Channels 543 15.3.6 Gaussian Multiple-Access Channels 544 15.4 Encoding of Correlated Sources 549 15.4.1 Achievability of the Slepian–Wolf Theorem 551 15.4.2 Converse for the Slepian–Wolf Theorem 555 15.4.3 Slepian–Wolf Theorem for Many Sources 556 15.4.4 Interpretation of Slepian–Wolf Coding 557 15.5 Duality Between Slepian–Wolf Encoding and Multiple-Access Channels 558 15.6 Broadcast Channel 560 15.6.1 Definitions for a Broadcast Channel 563 15.6.2 Degraded Broadcast Channels 564 15.6.3 Capacity Region for the Degraded Broadcast Channel 565 15.7 Relay Channel 571 15.8 Source Coding with Side Information 575 15.9 Rate Distortion with Side Information 580 15.10 General Multiterminal Networks 587 Summary 594 Problems 596 Historical Notes 609 16 Information Theory and Portfolio Theory 613 16.1 The Stock Market: Some Definitions 613 16.2 Kuhn–Tucker Characterization of the Log-Optimal Portfolio 617 16.3 Asymptotic Optimality of the Log-Optimal Portfolio 619 16.4 Side Information and the Growth Rate 621 16.5 Investment in Stationary Markets 623 16.6 Competitive Optimality of the Log-Optimal Portfolio 627 16.7 Universal Portfolios 629 16.7.1 Finite-Horizon Universal Portfolios 631 16.7.2 Horizon-Free Universal Portfolios 638 16.8 Shannon–McMillan–Breiman Theorem (General AEP) 644 Summary 650 Problems 652 Historical Notes 655 17 Inequalities in Information Theory 657 17.1 Basic Inequalities of Information Theory 657 17.2 Differential Entropy 660 17.3 Bounds on Entropy and Relative Entropy 663 17.4 Inequalities for Types 665 17.5 Combinatorial Bounds on Entropy 666 17.6 Entropy Rates of Subsets 667 17.7 Entropy and Fisher Information 671 17.8 Entropy Power Inequality and Brunn–Minkowski Inequality 674 17.9 Inequalities for Determinants 679 17.10 Inequalities for Ratios of Determinants 683 Summary 686 Problems 686 Historical Notes 687 Bibliography 689 List of Symbols 723 Index 727



PREFACE TO THE SECOND EDITION

In the years since the publication of the first edition, there were many aspects of the book that we wished to improve, to rearrange, or to expand, but the constraints of reprinting would not allow us to make those changes between printings. In the new edition, we now get a chance to make some of these changes, to add problems, and to discuss some topics that we had omitted from the first edition.

The key changes include a reorganization of the chapters to make the book easier to teach, and the addition of more than two hundred new problems. We have added material on universal portfolios, universal source coding, Gaussian feedback capacity, network information theory, and developed the duality of data compression and channel capacity. A new chapter has been added and many proofs have been simplified. We have also updated the references and historical notes.

The material in this book can be taught in a two-quarter sequence. The first quarter might cover Chapters 1 to 9, which includes the asymptotic equipartition property, data compression, and channel capacity, culminating in the capacity of the Gaussian channel. The second quarter could cover the remaining chapters, including rate distortion, the method of types, Kolmogorov complexity, network information theory, universal source coding, and portfolio theory. If only one semester is available, we would add rate distortion and a single lecture each on Kolmogorov complexity and network information theory to the first semester. A website, http://www.elementsofinformationtheory.com, provides links to additional material and solutions to selected problems.

In the years since the first edition of the book, information theory celebrated its 50th birthday (the 50th anniversary of Shannon’s original paper that started the field), and ideas from information theory have been applied to many problems of science and technology, including bioinformatics, web search, wireless communication, video compression, and others. The list of applications is endless, but it is the elegance of the fundamental mathematics that is still the key attraction of this area. We hope that this book will give some insight into why we believe that this is one of the most interesting areas at the intersection of mathematics, physics, statistics, and engineering.

Tom Cover  
Joy Thomas  
Palo Alto, California  
January 2006  

PREFACE TO THE FIRST EDITION  

This is intended to be a simple and accessible book on information theory. As Einstein said, “Everything should be made as simple as possible, but no simpler.” Although we have not verified the quote (first found in a fortune cookie), this point of view drives our development throughout the book. There are a few key ideas and techniques that, when mastered, make the subject appear simple and provide great intuition on new questions.

This book has arisen from over ten years of lectures in a two-quarter sequence of a senior and first-year graduate-level course in information theory, and is intended as an introduction to information theory for students of communication theory, computer science, and statistics.

There are two points to be made about the simplicities inherent in information theory. First, certain quantities like entropy and mutual information arise as the answers to fundamental questions. For example, entropy is the minimum descriptive complexity of a random variable, and mutual information is the communication rate in the presence of noise. Also, as we shall point out, mutual information corresponds to the increase in the doubling rate of wealth given side information. Second, the answers to information theoretic questions have a natural algebraic structure. For example, there is a chain rule for entropies, and entropy and mutual information are related. Thus the answers to problems in data compression and communication admit extensive interpretation. We all know the feeling that follows when one investigates a problem, goes through a large amount of algebra, and finally investigates the answer to find that the entire problem is illuminated not by the analysis but by the inspection of the answer. Perhaps the outstanding examples of this in physics are Newton’s laws and Schrödinger’s wave equation. Who could have foreseen the awesome philosophical interpretations of Schrödinger’s wave equation?

In the text we often investigate properties of the answer before we look at the question. For example, in Chapter 2, we define entropy, relative entropy, and mutual information and study the relationships and a few interpretations of them, showing how the answers fit together in various ways. Along the way we speculate on the meaning of the second law of thermodynamics. Does entropy always increase? The answer is yes and no. This is the sort of result that should please experts in the area but might be overlooked as standard by the novice.

In fact, that brings up a point that often occurs in teaching. It is fun to find new proofs or slightly new results that no one else knows. When one presents these ideas along with the established material in class, the response is “sure, sure, sure.” But the excitement of teaching the material is greatly enhanced. Thus we have derived great pleasure from investigating a number of new ideas in this textbook.

Examples of some of the new material in this text include the chapter on the relationship of information theory to gambling, the work on the universality of the second law of thermodynamics in the context of Markov chains, the joint typicality proofs of the channel capacity theorem, the competitive optimality of Huffman codes, and the proof of Burg’s theorem on maximum entropy spectral density estimation. Also, the chapter on Kolmogorov complexity has no counterpart in other information theory texts. We have also taken delight in relating Fisher information, mutual information, the central limit theorem, and the Brunn–Minkowski and entropy power inequalities. To our surprise, many of the classical results on determinant inequalities are most easily proved using information theoretic inequalities.

Even though the field of information theory has grown considerably since Shannon’s original paper, we have strived to emphasize its coherence. While it is clear that Shannon was motivated by problems in communication theory when he developed information theory, we treat information theory as a field of its own with applications to communication theory and statistics. We were drawn to the field of information theory from backgrounds in communication theory, probability theory, and statistics, because of the apparent impossibility of capturing the intangible concept of information.

Since most of the results in the book are given as theorems and proofs, we expect the elegance of the results to speak for themselves. In many cases we actually describe the properties of the solutions before the problems. Again, the properties are interesting in themselves and provide a natural rhythm for the proofs that follow.

One innovation in the presentation is our use of long chains of inequalities with no intervening text followed immediately by the explanations. By the time the reader comes to many of these proofs, we expect that he or she will be able to follow most of these steps without any explanation and will be able to pick out the needed explanations. These chains of inequalities serve as pop quizzes in which the reader can be reassured of having the knowledge needed to prove some important theorems. The natural flow of these proofs is so compelling that it prompted us to flout one of the cardinal rules of technical writing; and the absence of verbiage makes the logical necessity of the ideas evident and the key ideas perspicuous. We hope that by the end of the book the reader will share our appreciation of the elegance, simplicity, and naturalness of information theory.

Throughout the book we use the method of weakly typical sequences, which has its origins in Shannon’s original 1948 work but was formally developed in the early 1970s. The key idea here is the asymptotic equipartition property, which can be roughly paraphrased as “Almost everything is almost equally probable.”

Chapter 2 includes the basic algebraic relationships of entropy, relative entropy, and mutual information. The asymptotic equipartition property (AEP) is given central prominence in Chapter 3. This leads us to discuss the entropy rates of stochastic processes and data compression in Chapters 4 and 5. A gambling sojourn is taken in Chapter 6, where the duality of data compression and the growth rate of wealth is developed. The sensational success of Kolmogorov complexity as an intellectual foundation for information theory is explored in Chapter 14. Here we replace the goal of finding a description that is good on the average with the goal of finding the universally shortest description. There is indeed a universal notion of the descriptive complexity of an object. Here also the wonderful number Ω is investigated. This number, which is the binary expansion of the probability that a Turing machine will halt, reveals many of the secrets of mathematics.

Channel capacity is established in Chapter 7. The necessary material on differential entropy is developed in Chapter 8, laying the groundwork for the extension of previous capacity theorems to continuous noise channels. The capacity of the fundamental Gaussian channel is investigated in Chapter 9.

The relationship between information theory and statistics, first studied by Kullback in the early 1950s and relatively neglected since, is developed in Chapter 11. Rate distortion theory requires a little more background than its noiseless data compression counterpart, which accounts for its placement as late as Chapter 10 in the text.

The huge subject of network information theory, which is the study of the simultaneously achievable flows of information in the presence of noise and interference, is developed in Chapter 15. Many new ideas come into play in network information theory. The primary new ingredients are interference and feedback. Chapter 16 considers the stock market, which is the generalization of the gambling processes considered in Chapter 6, and shows again the close correspondence of information theory and gambling. Chapter 17, on inequalities in information theory, gives us a chance to recapitulate the interesting inequalities strewn throughout the book, put them in a new framework, and then add some interesting new inequalities on the entropy rates of randomly drawn subsets. The beautiful relationship of the Brunn–Minkowski inequality for volumes of set sums, the entropy power inequality for the effective variance of the sum of independent random variables, and the Fisher information inequalities are made explicit here.

We have made an attempt to keep the theory at a consistent level. The mathematical level is a reasonably high one, probably the senior or first-year graduate level, with a background of at least one good semester course in probability and a solid background in mathematics. We have, however, been able to avoid the use of measure theory. Measure theory comes up only briefly in the proof of the AEP for ergodic processes in Chapter 16. This fits in with our belief that the fundamentals of information theory are orthogonal to the techniques required to bring them to their full generalization.

The essential vitamins are contained in Chapters 2, 3, 4, 5, 7, 8, 9, 11, 10, and 15. This subset of chapters can be read without essential reference to the others and makes a good core of understanding. In our opinion, Chapter 14 on Kolmogorov complexity is also essential for a deep understanding of information theory. The rest, ranging from gambling to inequalities, is part of the terrain illuminated by this coherent and beautiful subject.

Every course has its first lecture, in which a sneak preview and overview of ideas is presented. Chapter 1 plays this role.

Tom Cover  
Joy Thomas  
Palo Alto, California  
June 1990  

ACKNOWLEDGMENTS FOR THE SECOND EDITION  

Since the appearance of the first edition, we have been fortunate to receive feedback, suggestions, and corrections from a large number of readers. It would be impossible to thank everyone who has helped us in our efforts, but we would like to list some of them. In particular, we would like to thank all the faculty who taught courses based on this book and the students who took those courses; it is through them that we learned to look at the same material from a different perspective.

In particular, we would like to thank Andrew Barron, Alon Orlitsky, T. S. Han, Raymond Yeung, Nam Phamdo, Franz Willems, and Marty Cohn for their comments and suggestions. Over the years, students at Stanford have provided ideas and inspirations for the changes—these include George Gemelos, Navid Hassanpour, Young-Han Kim, Charles Mathis, Styrmir Sigurjonsson, Jon Yard, Michael Baer, Mung Chiang, Suhas Diggavi, Elza Erkip, Paul Fahn, Garud Iyengar, David Julian, Yannis Kontoyiannis, Amos Lapidoth, Erik Ordentlich, Sandeep Pombra, Jim Roche, Arak Sutivong, Joshua Sweetkind-Singer, and Assaf Zeevi. Denise Murphy provided much support and help during the preparation of the second edition.

Joy Thomas would like to acknowledge the support of colleagues at IBM and Stratify who provided valuable comments and suggestions. Particular thanks are due Peter Franaszek, C. S. Chang, Randy Nelson, Ramesh Gopinath, Pandurang Nayak, John Lamping, Vineet Gupta, and Ramana Venkata. In particular, many hours of discussion with Brandon Roy helped refine some of the arguments in the book. Above all, Joy would like to acknowledge that the second edition would not have been possible without the support and encouragement of his wife, Priya, who makes all things worthwhile.

Tom Cover would like to thank his students and his wife, Karen.

ACKNOWLEDGMENTS FOR THE FIRST EDITION  

We wish to thank everyone who helped make this book what it is. In particular, Aaron Wyner, Toby Berger, Masoud Salehi, Alon Orlitsky, Jim Mazo, and Andrew Barron have made detailed comments on various drafts of the book which guided us in our final choice of content. We would like to thank Bob Gallager for an initial reading of the manuscript and his encouragement to publish it. Aaron Wyner donated his new proof with Ziv on the convergence of the Lempel-Ziv algorithm. We would also like to thank Norman Abramson, Ed van der Meulen, Jack Salz, and Raymond Yeung for their suggested revisions.

Certain key visitors and research associates contributed as well, including Amir Dembo, Paul Algoet, Hirosuke Yamamoto, Ben Kawabata, M. Shimizu, and Yoichiro Watanabe. We benefited from the advice of John Gill when he used this text in his class. Abbas El Gamal made invaluable contributions and helped begin this book years ago when we planned to write a research monograph on multiple user information theory. We would also like to thank the Ph.D. students in information theory as this book was being written: Laura Ekroot, Will Equitz, Don Kimber, Mitchell Trott, Andrew Nobel, Jim Roche, Erik Ordentlich, Elza Erkip, and Vittorio Castelli. Also, Mitchell Oslick, Chien-Wen Tseng, and Michael Morrell were among the most active students in contributing questions and suggestions to the text. Marc Goldberg and Anil Kaul helped us produce some of the figures. Finally, we would like to thank Kirsten Goodell and Kathy Adams for their support and help in some of the aspects of the preparation of the manuscript.

Joy Thomas would also like to thank Peter Franaszek, Steve Lavenberg, Fred Jelinek, David Nahamoo, and Lalit Bahl for their encouragement and support during the final stages of production of this book.



CHAPTER 1  
INTRODUCTION AND PREVIEW  

Information theory answers two fundamental questions in communication theory: What is the ultimate data compression (answer: the entropy \( H \)), and what is the ultimate transmission rate of communication (answer: the channel capacity \( C \)). For this reason, some consider information theory to be a subset of communication theory. We argue that it is much more. Indeed, it has fundamental contributions to make in statistical physics (thermodynamics), computer science (Kolmogorov complexity or algorithmic complexity), statistical inference (Occam’s Razor: “The simplest explanation is best”), and to probability and statistics (error exponents for optimal hypothesis testing and estimation).  

This “first lecture” chapter goes backward and forward through information theory and its naturally related ideas. The full definitions and study of the subject begin in Chapter 2. Figure 1.1 illustrates the relationship of information theory to other fields. As the figure suggests, information theory intersects physics (statistical mechanics), mathematics (probability theory), electrical engineering (communication theory), and computer science (algorithmic complexity). We now describe the areas of intersection in greater detail.  

### Electrical Engineering (Communication Theory)  
In the early 1940s, it was thought to be impossible to send information at a positive rate with negligible probability of error. Shannon surprised the communication theory community by proving that the probability of error could be made nearly zero for all communication rates below channel capacity. The capacity can be computed simply from the noise characteristics of the channel. Shannon further argued that random processes such as music and speech have an irreducible complexity below which the signal cannot be compressed. This he named the entropy, in deference to the parallel use of this word in thermodynamics, and argued that if the entropy of the source is less than the capacity of the channel, asymptotically error-free communication can be achieved.  

Information theory today represents the extreme points of the set of all possible communication schemes, as shown in the fanciful Figure 1.2. The data compression minimum \( I(X; \hat{X}) \) lies at one extreme of the set of communication ideas. All data compression schemes require description rates at least equal to this minimum. At the other extreme is the data transmission maximum \( I(X; Y) \), known as the channel capacity. Thus, all modulation schemes and data compression schemes lie between these limits.  

Information theory also suggests means of achieving these ultimate limits of communication. However, these theoretically optimal communication schemes, beautiful as they are, may turn out to be computationally impractical. It is only because of the computational feasibility of simple modulation and demodulation schemes that we use them rather than the random coding and nearest-neighbor decoding rule suggested by Shannon’s proof of the channel capacity theorem. Progress in integrated circuits and code design has enabled us to reap some of the gains suggested by Shannon’s theory. Computational practicality was finally achieved by the advent of turbo codes. A good example of an application of the ideas of information theory is the use of error-correcting codes on compact discs and DVDs.  

Recent work on the communication aspects of information theory has concentrated on network information theory: the theory of the simultaneous rates of communication from many senders to many receivers in the presence of interference and noise. Some of the trade-offs of rates between senders and receivers are unexpected, and all have a certain mathematical simplicity. A unifying theory, however, remains to be found.  

### Computer Science (Kolmogorov Complexity)  
Kolmogorov, Chaitin, and Solomonoff put forth the idea that the complexity of a string of data can be defined by the length of the shortest binary computer program for computing the string. Thus, the complexity is the minimal description length. This definition of complexity turns out to be universal, that is, computer-independent, and is of fundamental importance. Thus, Kolmogorov complexity lays the foundation for the theory of descriptive complexity. Gratifyingly, the Kolmogorov complexity \( K \) is approximately equal to the Shannon entropy \( H \) if the sequence is drawn at random from a distribution that has entropy \( H \). So the tie-in between information theory and Kolmogorov complexity is perfect. Indeed, we consider Kolmogorov complexity to be more fundamental than Shannon entropy. It is the ultimate data compression and leads to a logically consistent procedure for inference.  

There is a pleasing complementary relationship between algorithmic complexity and computational complexity. One can think about computational complexity (time complexity) and Kolmogorov complexity (program length or descriptive complexity) as two axes corresponding to program running time and program length. Kolmogorov complexity focuses on minimizing along the second axis, and computational complexity focuses on minimizing along the first axis. Little work has been done on the simultaneous minimization of the two.  

### Physics (Thermodynamics)  
Statistical mechanics is the birthplace of entropy and the second law of thermodynamics. Entropy always increases. Among other things, the second law allows one to dismiss any claims to perpetual motion machines. We discuss the second law briefly in Chapter 4.  

### Mathematics (Probability Theory and Statistics)  
The fundamental quantities of information theory—entropy, relative entropy, and mutual information—are defined as functionals of probability distributions. In turn, they characterize the behavior of long sequences of random variables and allow us to estimate the probabilities of rare events (large deviation theory) and to find the best error exponent in hypothesis tests.  

### Philosophy of Science (Occam’s Razor)  
William of Occam said, “Causes shall not be multiplied beyond necessity,” or to paraphrase it, “The simplest explanation is best.” Solomonoff and Chaitin argued persuasively that one gets a universally good prediction procedure if one takes a weighted combination of all programs that explain the data and observes what they print next. Moreover, this inference will work in many problems not handled by statistics. For example, this procedure will eventually predict the subsequent digits of \( \pi \). When this procedure is applied to coin flips that come up heads with probability 0.7, this too will be inferred. When applied to the stock market, the procedure should essentially find all the “laws” of the stock market and extrapolate them optimally. In principle, such a procedure would have found Newton’s laws of physics. Of course, such inference is highly impractical, because weeding out all computer programs that fail to generate existing data will take impossibly long. We would predict what happens tomorrow a hundred years from now.  

### Economics (Investment)  
Repeated investment in a stationary stock market results in an exponential growth of wealth. The growth rate of the wealth is a dual of the entropy rate of the stock market. The parallels between the theory of optimal investment in the stock market and information theory are striking. We develop the theory of investment to explore this duality.  

### Computation vs. Communication  
As we build larger computers out of smaller components, we encounter both a computation limit and a communication limit. Computation is communication-limited and communication is computation-limited. These become intertwined, and thus all of the developments in communication theory via information theory should have a direct impact on the theory of computation.  

### 1.1 Preview of the Book  
The initial questions treated by information theory lay in the areas of data compression and transmission. The answers are quantities such as entropy and mutual information, which are functions of the probability distributions that underlie the process of communication. A few definitions will aid the initial discussion. We repeat these definitions in Chapter 2.  

The entropy of a random variable \( X \) with a probability mass function \( p(x) \) is defined by:  
\[
H(X) = -\sum_{x} p(x) \log_2 p(x).  
\]  
(1.1)  

We use logarithms to base 2. The entropy will then be measured in bits. The entropy is a measure of the average uncertainty in the random variable. It is the number of bits on average required to describe the random variable.  

**Example 1.1.1**  
Consider a random variable that has a uniform distribution over 32 outcomes. To identify an outcome, we need a label that takes on 32 different values. Thus, 5-bit strings suffice as labels. The entropy of this random variable is:  
\[
H(X) = -\sum_{i=1}^{32} p(i) \log p(i) = -\sum_{i=1}^{32} \frac{1}{32} \log \frac{1}{32} = \log 32 = 5 \text{ bits},  
\]  
(1.2)  
which agrees with the number of bits needed to describe \( X \). In this case, all the outcomes have representations of the same length.  

Now consider an example with a nonuniform distribution.  

**Example 1.1.2**  
Suppose we have a horse race with eight horses taking part. Assume that the probabilities of winning for the eight horses are:  
\[
\frac{1}{2}, \frac{1}{4}, \frac{1}{8}, \frac{1}{16}, \frac{1}{64}, \frac{1}{64}, \frac{1}{64}, \frac{1}{64}.  
\]  
We can calculate the entropy of the horse race as:  
\[
H(X) = -\frac{1}{2} \log \frac{1}{2} - \frac{1}{4} \log \frac{1}{4} - \frac{1}{8} \log \frac{1}{8} - \frac{1}{16} \log \frac{1}{16} - 4 \cdot \frac{1}{64} \log \frac{1}{64} = 2 \text{ bits}.  
\]  
(1.3)  

Suppose we wish to send a message indicating which horse won the race. One alternative is to send the index of the winning horse. This description requires 3 bits for any of the horses. But the win probabilities are not uniform. It therefore makes sense to use shorter descriptions for the more probable horses and longer descriptions for the less probable ones, so that we achieve a lower average description length. For example, we could use the following set of bit strings to represent the eight horses: 0, 10, 110, 1110, 111100, 111101, 111110, 111111. The average description length in this case is 2 bits, as opposed to 3 bits for the uniform code. Notice that the average description length in this case is equal to the entropy. In Chapter 5, we show that the entropy of a random variable is a lower bound on the average number of bits required to represent the random variable and also on the average number of questions needed to identify the variable in a game of “20 questions.” We also show how to construct representations that have an average length within 1 bit of the entropy.  

The concept of entropy in information theory is related to the concept of entropy in statistical mechanics. If we draw a sequence of \( n \) independent and identically distributed (i.i.d.) random variables, we will show that the probability of a “typical” sequence is about \( 2^{-nH(X)} \) and that there are about \( 2^{nH(X)} \) such typical sequences. This property (known as the asymptotic equipartition property (AEP)) is the basis of many of the proofs in information theory. We later present other problems for which entropy arises as a natural answer (e.g., the number of fair coin flips needed to generate a random variable).  

The notion of descriptive complexity of a random variable can be extended to define the descriptive complexity of a single string. The Kolmogorov complexity of a binary string is defined as the length of the shortest computer program that prints out the string. It will turn out that if the string is indeed random, the Kolmogorov complexity is close to the entropy. Kolmogorov complexity is a natural framework in which to consider problems of statistical inference and modeling and leads to a clearer understanding of Occam’s Razor: “The simplest explanation is best.” We describe some simple properties of Kolmogorov complexity in Chapter 1.  

Entropy is the uncertainty of a single random variable. We can define conditional entropy \( H(X|Y) \), which is the entropy of a random variable conditional on the knowledge of another random variable. The reduction in uncertainty due to another random variable is called the mutual information. For two random variables \( X \) and \( Y \), this reduction is the mutual information:  
\[
I(X; Y) = H(X) - H(X|Y) = \sum_{x,y} p(x, y) \log \frac{p(x, y)}{p(x)p(y)}.  
\]  
(1.4)  

The mutual information \( I(X; Y) \) is a measure of the dependence between the two random variables. It is symmetric in \( X \) and \( Y \) and always non-negative and is equal to zero if and only if \( X \) and \( Y \) are independent.  

A communication channel is a system in which the output depends probabilistically on its input. It is characterized by a probability transition matrix \( p(y|x) \) that determines the conditional distribution of the output given the input. For a communication channel with input \( X \) and output \( Y \), we can define the capacity \( C \) by:  
\[
C = \max_{p(x)} I(X; Y).  
\]  
(1.5)  

Later, we show that the capacity is the maximum rate at which we can send information over the channel and recover the information at the output with a vanishingly low probability of error. We illustrate this with a few examples.  

**Example 1.1.3 (Noiseless Binary Channel)**  
For this channel, the binary input is reproduced exactly at the output. This channel is illustrated in Figure 1.3. Here, any transmitted bit is received without error. Hence, in each transmission, we can send 1 bit reliably to the receiver, and the capacity is 1 bit. We can also calculate the information capacity \( C = \max I(X; Y) = 1 \) bit.  

**Example 1.1.4 (Noisy Four-Symbol Channel)**  
Consider the channel shown in Figure 1.4. In this channel, each input letter is received either as the same letter with probability \( \frac{1}{2} \) or as the next letter with probability \( \frac{1}{2} \). If we use all four input symbols, inspection of the output would not reveal with certainty which input symbol was sent. If, on the other hand, we use only two of the inputs (1 and 3, say), we can tell immediately from the output which input symbol was sent. This channel then acts like the noiseless channel of Example 1.1.3, and we can send 1 bit per transmission over this channel with no errors. We can calculate the channel capacity \( C = \max I(X; Y) \) in this case, and it is equal to 1 bit per transmission, in agreement with the analysis above.  

In general, communication channels do not have the simple structure of this example, so we cannot always identify a subset of the inputs to send information without error. But if we consider a sequence of transmissions, all channels look like this example, and we can then identify a subset of the input sequences (the codewords) that can be used to transmit information over the channel in such a way that the sets of possible output sequences associated with each of the codewords are approximately disjoint. We can then look at the output sequence and identify the input sequence with a vanishingly low probability of error.  

**Example 1.1.5 (Binary Symmetric Channel)**  
This is the basic example of a noisy communication system. The channel is illustrated in Figure 1.5.  

\[
\begin{array}{c|cc}  
p(y|x) & 0 & 1 \\  
\hline  
0 & 1-p & p \\  
1 & p & 1-p \\  
\end{array}  
\]  

(Figure 1.5. Binary symmetric channel.)



--- 

**Page 35**

**1.1 Preview of the Book**

The channel has a binary input, and its output is equal to the input with probability \(1 - p\). With probability \(p\), a 0 is received as a 1, and vice versa. In this case, the capacity of the channel can be calculated to be \(C = 1 + p \log p + (1 - p) \log(1 - p)\) bits per transmission. However, it is no longer obvious how one can achieve this capacity. If we use the channel many times, however, the channel begins to look like the noisy four-symbol channel of Example 1.1.4, and we can send information at a rate \(C\) bits per transmission with an arbitrarily low probability of error.

The ultimate limit on the rate of communication of information over a channel is given by the channel capacity. The channel coding theorem shows that this limit can be achieved by using codes with a long block length. In practical communication systems, there are limitations on the complexity of the codes that we can use, and therefore we may not be able to achieve capacity.

Mutual information turns out to be a special case of a more general quantity called relative entropy \(D(p||q)\), which is a measure of the “distance” between two probability mass functions \(p\) and \(q\). It is defined as:

\[
D(p||q) = \sum_{x} p(x) \log \frac{p(x)}{q(x)}.
\]

Although relative entropy is not a true metric, it has some of the properties of a metric. In particular, it is always nonnegative and is zero if and only if \(p = q\). Relative entropy arises as the exponent in the probability of error in a hypothesis test between distributions \(p\) and \(q\). Relative entropy can be used to define a geometry for probability distributions that allows us to interpret many of the results of large deviation theory.

There are a number of parallels between information theory and the theory of investment in a stock market. A stock market is defined by a random vector \(X\) whose elements are nonnegative numbers equal to the ratio of the price of a stock at the end of a day to the price at the beginning of the day. For a stock market with distribution \(F(x)\), we can define the doubling rate \(W\) as:

\[
W = \max_{b: b_i \geq 0, \sum b_i = 1} \mathbb{E} \left[ \log b^T x \right] dF(x).
\]

The doubling rate is the maximum asymptotic exponent in the growth of wealth. The doubling rate has a number of properties that parallel the properties of entropy. We explore some of these properties in Chapter 16.

---

**Page 36**

**Introduction and Preview**

The quantities \(H\), \(I\), \(C\), \(D\), \(K\), \(W\) arise naturally in the following areas:

- **Data compression**: The entropy \(H\) of a random variable is a lower bound on the average length of the shortest description of the random variable. We can construct descriptions with average length within 1 bit of the entropy. If we relax the constraint of recovering the source perfectly, we can then ask what communication rates are required to describe the source up to distortion \(D\)? And what channel capacities are sufficient to enable the transmission of this source over the channel and its reconstruction with distortion less than or equal to \(D\)? This is the subject of rate distortion theory.

When we try to formalize the notion of the shortest description for nonrandom objects, we are led to the definition of Kolmogorov complexity \(K\). Later, we show that Kolmogorov complexity is universal and satisfies many of the intuitive requirements for the theory of shortest descriptions.

- **Data transmission**: We consider the problem of transmitting information so that the receiver can decode the message with a small probability of error. Essentially, we wish to find codewords (sequences of input symbols to a channel) that are mutually far apart in the sense that their noisy versions (available at the output of the channel) are distinguishable. This is equivalent to sphere packing in high-dimensional space. For any set of codewords, it is possible to calculate the probability that the receiver will make an error (i.e., make an incorrect decision as to which codeword was sent). However, in most cases, this calculation is tedious.

Using a randomly generated code, Shannon showed that one can send information at any rate below the capacity \(C\) of the channel with an arbitrarily low probability of error. The idea of a randomly generated code is very unusual. It provides the basis for a simple analysis of a very difficult problem. One of the key ideas in the proof is the concept of typical sequences. The capacity \(C\) is the logarithm of the number of distinguishable input signals.

- **Network information theory**: Each of the topics mentioned previously involves a single source or a single channel. What if one wishes to compress each of many sources and then put the compressed descriptions together into a joint reconstruction of the sources? This problem is solved by the Slepian–Wolf theorem. Or what if one has many senders sending information independently to a common receiver? What is the channel capacity of this channel? This is the multiple-access channel solved by Liao and Ahlswede. Or what if one has one sender and many receivers and wishes to communicate (perhaps different) information simultaneously to each of the receivers? This is the broadcast channel. Finally, what if one has an arbitrary number of senders and receivers in an environment of interference and noise? What is the capacity region of achievable rates from the various senders to the receivers? This is the general network information theory problem. All of the preceding problems fall into the general area of multiple-user or network information theory. Although hopes for a comprehensive theory for networks may be beyond current research techniques, there is still some hope that all the answers involve only elaborate forms of mutual information and relative entropy.

---

**Page 37**

**1.1 Preview of the Book**

- **Ergodic theory**: The asymptotic equipartition theorem states that most sample \(n\)-sequences of an ergodic process have probability about \(2^{-nH}\) and that there are about \(2^{nH}\) such typical sequences.

- **Hypothesis testing**: The relative entropy \(D\) arises as the exponent in the probability of error in a hypothesis test between two distributions. It is a natural measure of distance between distributions.

- **Statistical mechanics**: The entropy \(H\) arises in statistical mechanics as a measure of uncertainty or disorganization in a physical system. Roughly speaking, the entropy is the logarithm of the number of ways in which the physical system can be configured. The second law of thermodynamics says that the entropy of a closed system cannot decrease. Later we provide some interpretations of the second law.

- **Quantum mechanics**: Here, von Neumann entropy \(S = \text{tr}(\rho \ln \rho) = \sum_{i} \lambda_i \log \lambda_i\) plays the role of classical Shannon–Boltzmann entropy \(H = -\sum_{i} p_i \log p_i\). Quantum mechanical versions of data compression and channel capacity can then be found.

- **Inference**: We can use the notion of Kolmogorov complexity \(K\) to find the shortest description of the data and use that as a model to predict what comes next. A model that maximizes the uncertainty or entropy yields the maximum entropy approach to inference.

- **Gambling and investment**: The optimal exponent in the growth rate of wealth is given by the doubling rate \(W\). For a horse race with uniform odds, the sum of the doubling rate \(W\) and the entropy \(H\) is constant. The increase in the doubling rate due to side information is equal to the mutual information \(I\) between a horse race and the side information. Similar results hold for investment in the stock market.

- **Probability theory**: The asymptotic equipartition property (AEP) shows that most sequences are typical in that they have a sample entropy close to \(H\). So attention can be restricted to these approximately \(2^{nH}\) typical sequences. In large deviation theory, the probability of a set is approximately \(2^{-nD}\), where \(D\) is the relative entropy distance between the closest element in the set and the true distribution.

- **Complexity theory**: The Kolmogorov complexity \(K\) is a measure of the descriptive complexity of an object. It is related to, but different from, computational complexity, which measures the time or space required for a computation.

Information-theoretic quantities such as entropy and relative entropy arise again and again as the answers to the fundamental questions in communication and statistics. Before studying these questions, we shall study some of the properties of the answers. We begin in Chapter 2 with the definitions and basic properties of entropy, relative entropy, and mutual information.

---

**Page 38**

**Introduction and Preview**

---

**Page 39**

**Chapter 2: Entropy, Relative Entropy, and Mutual Information**

In this chapter, we introduce most of the basic definitions required for the subsequent development of the theory. It is irresistible to play with their relationships and interpretations, taking faith in their later utility. After defining entropy and mutual information, we establish chain rules, the nonnegativity of mutual information, the data-processing inequality, and illustrate these definitions by examining sufficient statistics and Fano’s inequality.

The concept of information is too broad to be captured completely by a single definition. However, for any probability distribution, we define a quantity called the entropy, which has many properties that agree with the intuitive notion of what a measure of information should be. This notion is extended to define mutual information, which is a measure of the amount of information one random variable contains about another. Entropy then becomes the self-information of a random variable. Mutual information is a special case of a more general quantity called relative entropy, which is a measure of the distance between two probability distributions. All these quantities are closely related and share a number of simple properties, some of which we derive in this chapter.

In later chapters, we show how these quantities arise as natural answers to a number of questions in communication, statistics, complexity, and gambling. That will be the ultimate test of the value of these definitions.

**2.1 Entropy**

We first introduce the concept of entropy, which is a measure of the uncertainty of a random variable. Let \(X\) be a discrete random variable with alphabet \(\mathcal{X}\) and probability mass function \(p(x) = \Pr\{X = x\}\), \(x \in \mathcal{X}\).

We denote the probability mass function by \(p(x)\) rather than \(p_X(x)\), for convenience. Thus, \(p(x)\) and \(p(y)\) refer to two different random variables and are in fact different probability mass functions, \(p_X(x)\) and \(p_Y(y)\), respectively.

**Definition**

The entropy \(H(X)\) of a discrete random variable \(X\) is defined by:

\[
H(X) = -\sum_{x \in \mathcal{X}} p(x) \log p(x).
\]

We also write \(H(p)\) for the above quantity. The log is to the base 2, and entropy is expressed in bits. For example, the entropy of a fair coin toss is 1 bit. We will use the convention that \(0 \log 0 = 0\), which is easily justifiable by continuity since \(x \log x \to 0\) as \(x \to 0\). Adding terms of zero probability does not change the entropy.

If the base of the logarithm is \(b\), we denote the entropy as \(H_b(X)\). If the base of the logarithm is \(e\), the entropy is measured in nats. Unless otherwise specified, we will take all logarithms to base 2, and hence all the entropies will be measured in bits. Note that entropy is a functional of the distribution of \(X\). It does not depend on the actual values taken by the random variable \(X\), but only on the probabilities.

We denote expectation by \(E\). Thus, if \(X \sim p(x)\), the expected value of the random variable \(g(X)\) is written:

\[
E_p[g(X)] = \sum_{x \in \mathcal{X}} g(x) p(x),
\]

or more simply as \(E[g(X)]\) when the probability mass function is understood from the context. We shall take a peculiar interest in the eerily self-referential expectation of \(g(X)\) under \(p(x)\) when \(g(X) = \log \frac{1}{p(X)}\).

**Remark**

The entropy of \(X\) can also be interpreted as the expected value of the random variable \(\log \frac{1}{p(X)}\), where \(X\) is drawn according to probability mass function \(p(x)\). Thus,

\[
H(X) = E_p\left[\log \frac{1}{p(X)}\right].
\]

This definition of entropy is related to the definition of entropy in thermodynamics; some of the connections are explored later. It is possible to derive the definition of entropy axiomatically by defining certain properties that the entropy of a random variable must satisfy. This approach is illustrated in Problem 2.46. We do not use the axiomatic approach to justify the definition of entropy; instead, we show that it arises as the answer to a number of natural questions, such as “What is the average length of the shortest description of the random variable?” First, we derive some immediate consequences of the definition.

**Lemma 2.1.1**

\(H(X) \geq 0\).

**Proof:**

\(0 \leq p(x) \leq 1\) implies that \(\log \frac{1}{p(x)} \geq 0\).

\(\Box\)

**Lemma 2.1.2**

\(H_b(X) = (\log_b a) H_a(X)\).

**Proof:**

\(\log_b p = (\log_b a) \log_a p\).

\(\Box\)

The second property of entropy enables us to change the base of the logarithm in the definition. Entropy can be changed from one base to another by multiplying by the appropriate factor.

**Example 2.1.1**

Let

\[
X = \begin{cases}
1 & \text{with probability } p, \\
0 & \text{with probability } 1 - p.
\end{cases}
\]

Then

\[
H(X) = -p \log p - (1 - p) \log(1 - p) \stackrel{\text{def}}{=} H(p).
\]

In particular, \(H(X) = 1\) bit when \(p = \frac{1}{2}\). The graph of the function \(H(p)\) is shown in Figure 2.1. The figure illustrates some of the basic properties of entropy: It is a concave function of the distribution and equals 0 when \(p = 0\) or 1. This makes sense, because when \(p = 0\) or 1, the variable is not random and there is no uncertainty. Similarly, the uncertainty is maximum when \(p = \frac{1}{2}\), which also corresponds to the maximum value of the entropy.

**Example 2.1.2**

Let

\[
X = \begin{cases}
a & \text{with probability } \frac{1}{2}, \\
b & \text{with probability } \frac{1}{4}, \\
c & \text{with probability } \frac{1}{8}, \\
d & \text{with probability } \frac{1}{8}.
\end{cases}
\]

The entropy of \(X\) is

\[
H(X) = -\frac{1}{2} \log \frac{1}{2} - \frac{1}{4} \log \frac{1}{4} - \frac{1}{8} \log \frac{1}{8} - \frac{1}{8} \log \frac{1}{8} = \frac{7}{4} \text{ bits}.
\]

Suppose that we wish to determine the value of \(X\) with the minimum number of binary questions. An efficient first question is “Is \(X = a\)?” This splits the probability in half. If the answer to the first question is no, the second question can be “Is \(X = b\)?” The third question can be “Is \(X = c\)?” The resulting expected number of binary questions required is 1.75. This turns out to be the minimum expected number of binary questions required to determine the value of \(X\). In Chapter 5, we show that the minimum expected number of binary questions required to determine \(X\) lies between \(H(X)\) and \(H(X) + 1\).

**2.2 Joint Entropy and Conditional Entropy**

We defined the entropy of a single random variable in Section 2.1. We now extend the definition to a pair of random variables. There is nothing really new in this definition because \((X, Y)\) can be considered to be a single vector-valued random variable.

**Definition**

The joint entropy \(H(X, Y)\) of a pair of discrete random variables \((X, Y)\) with a joint distribution \(p(x, y)\) is defined as:

\[
H(X, Y) = -\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log p(x, y).
\]

---



JOINT ENTROPY AND CONDITIONAL ENTROPY

The joint entropy of two random variables \(X\) and \(Y\) can also be expressed as:
\[
H(X, Y) = -E \log p(X, Y).
\]
We also define the conditional entropy of a random variable given another as the expected value of the entropies of the conditional distributions, averaged over the conditioning random variable.

**Definition**  
If \((X, Y) \sim p(x, y)\), the conditional entropy \(H(Y|X)\) is defined as:
\[
H(Y|X) = \sum_{x \in X} p(x)H(Y|X = x)
\]
\[
= -\sum_{x \in X} p(x) \sum_{y \in Y} p(y|x) \log p(y|x)
\]
\[
= -\sum_{x \in X} \sum_{y \in Y} p(x, y) \log p(y|x)
\]
\[
= -E \log p(Y|X).
\]

The naturalness of the definition of joint entropy and conditional entropy is exhibited by the fact that the entropy of a pair of random variables is the entropy of one plus the conditional entropy of the other. This is proved in the following theorem.

**Theorem 2.2.1 (Chain Rule)**  
\[
H(X, Y) = H(X) + H(Y|X).
\]

**Proof**  
\[
H(X, Y) = -\sum_{x \in X} \sum_{y \in Y} p(x, y) \log p(x, y)
\]
\[
= -\sum_{x \in X} \sum_{y \in Y} p(x, y) \log p(x)p(y|x)
\]
\[
= -\sum_{x \in X} \sum_{y \in Y} p(x, y) \log p(x) - \sum_{x \in X} \sum_{y \in Y} p(x, y) \log p(y|x)
\]
\[
= -\sum_{x \in X} p(x) \log p(x) - \sum_{x \in X} \sum_{y \in Y} p(x, y) \log p(y|x)
\]
\[
= H(X) + H(Y|X).
\]

Equivalently, we can write:
\[
\log p(X, Y) = \log p(X) + \log p(Y|X)
\]
and take the expectation of both sides of the equation to obtain the theorem.

**Corollary**  
\[
H(X, Y|Z) = H(X|Z) + H(Y|X, Z).
\]

**Proof**  
The proof follows along the same lines as the theorem.

**Example 2.2.1**  
Let \((X, Y)\) have the following joint distribution: The marginal distribution of \(X\) is \((\frac{1}{2}, \frac{1}{4}, \frac{1}{8}, \frac{1}{8})\) and the marginal distribution of \(Y\) is \((\frac{1}{4}, \frac{1}{4}, \frac{1}{4}, \frac{1}{4})\), and hence \(H(X) = \frac{7}{4}\) bits and \(H(Y) = 2\) bits. Also,
\[
H(X|Y) = \frac{1}{4} \sum_{i=1}^{4} H\left(\frac{1}{2}, \frac{1}{4}, \frac{1}{8}, \frac{1}{8}\right)
\]
\[
= \frac{1}{4} \left( \frac{7}{4} + \frac{7}{4} + 2 + 0 \right)
\]
\[
= \frac{11}{8} \text{ bits}.
\]
Similarly, \(H(Y|X) = \frac{13}{8}\) bits and \(H(X, Y) = \frac{27}{8}\) bits.

**Remark**  
Note that \(H(Y|X) \neq H(X|Y)\). However, \(H(X) - H(X|Y) = H(Y) - H(Y|X)\), a property that we exploit later.

---

### RELATIVE ENTROPY AND MUTUAL INFORMATION

The entropy of a random variable is a measure of the uncertainty of the random variable; it is a measure of the amount of information required on the average to describe the random variable. In this section, we introduce two related concepts: relative entropy and mutual information.

The relative entropy is a measure of the distance between two distributions. In statistics, it arises as an expected logarithm of the likelihood ratio. The relative entropy \(D(p||q)\) is a measure of the inefficiency of assuming that the distribution is \(q\) when the true distribution is \(p\). For example, if we knew the true distribution \(p\) of the random variable, we could construct a code with average description length \(H(p)\). If, instead, we used the code for a distribution \(q\), we would need \(H(p) + D(p||q)\) bits on the average to describe the random variable.

**Definition**  
The relative entropy or Kullback–Leibler distance between two probability mass functions \(p(x)\) and \(q(x)\) is defined as:
\[
D(p||q) = \sum_{x \in X} p(x) \log \frac{p(x)}{q(x)}
\]
\[
= E_p \log \frac{p(X)}{q(X)}.
\]

In the above definition, we use the convention that \(0 \log \frac{0}{0} = 0\) and the convention (based on continuity arguments) that \(0 \log \frac{0}{q} = 0\) and \(p \log \frac{p}{0} = \infty\). Thus, if there is any symbol \(x \in X\) such that \(p(x) > 0\) and \(q(x) = 0\), then \(D(p||q) = \infty\).

We will soon show that relative entropy is always nonnegative and is zero if and only if \(p = q\). However, it is not a true distance between distributions since it is not symmetric and does not satisfy the triangle inequality. Nonetheless, it is often useful to think of relative entropy as a “distance” between distributions.

We now introduce mutual information, which is a measure of the amount of information that one random variable contains about another random variable. It is the reduction in the uncertainty of one random variable due to the knowledge of the other.

**Definition**  
Consider two random variables \(X\) and \(Y\) with a joint probability mass function \(p(x, y)\) and marginal probability mass functions \(p(x)\) and \(p(y)\). The mutual information \(I(X; Y)\) is the relative entropy between the joint distribution and the product distribution \(p(x)p(y)\):
\[
I(X; Y) = \sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{p(x, y)}{p(x)p(y)}
\]
\[
= D(p(x, y)||p(x)p(y))
\]
\[
= E_{p(x,y)} \log \frac{p(X, Y)}{p(X)p(Y)}.
\]

In Chapter 8, we generalize this definition to continuous random variables, and in (8.54) to general random variables that could be a mixture of discrete and continuous random variables.

**Example 2.3.1**  
Let \(X = \{0, 1\}\) and consider two distributions \(p\) and \(q\) on \(X\). Let \(p(0) = 1 - r\), \(p(1) = r\), and let \(q(0) = 1 - s\), \(q(1) = s\). Then:
\[
D(p||q) = (1 - r) \log \frac{1 - r}{1 - s} + r \log \frac{r}{s}
\]
and
\[
D(q||p) = (1 - s) \log \frac{1 - s}{1 - r} + s \log \frac{s}{r}.
\]

If \(r = s\), then \(D(p||q) = D(q||p) = 0\). If \(r = \frac{1}{2}\), \(s = \frac{1}{4}\), we can calculate:
\[
D(p||q) = \frac{1}{2} \log \frac{\frac{1}{2}}{\frac{3}{4}} + \frac{1}{2} \log \frac{\frac{1}{2}}{\frac{1}{4}} = 1 - \frac{1}{2} \log 3 = 0.2075 \text{ bit},
\]
whereas
\[
D(q||p) = \frac{3}{4} \log \frac{\frac{3}{4}}{\frac{1}{2}} + \frac{1}{4} \log \frac{\frac{1}{4}}{\frac{1}{2}} = \frac{3}{4} \log 3 - 1 = 0.1887 \text{ bit}.
\]

Note that \(D(p||q) \neq D(q||p)\) in general.

---

### RELATIONSHIP BETWEEN ENTROPY AND MUTUAL INFORMATION

We can rewrite the definition of mutual information \(I(X; Y)\) as:
\[
I(X; Y) = \sum_{x,y} p(x, y) \log \frac{p(x, y)}{p(x)p(y)}
\]
\[
= \sum_{x,y} p(x, y) \log \frac{p(x|y)}{p(x)}
\]
\[
= -\sum_{x,y} p(x, y) \log p(x) + \sum_{x,y} p(x, y) \log p(x|y)
\]
\[
= -\sum_{x} p(x) \log p(x) - \sum_{x,y} p(x, y) \log p(x|y)
\]
\[
= H(X) - H(X|Y).
\]

Thus, the mutual information \(I(X; Y)\) is the reduction in the uncertainty of \(X\) due to the knowledge of \(Y\).

By symmetry, it also follows that:
\[
I(X; Y) = H(Y) - H(Y|X).
\]

Thus, \(X\) says as much about \(Y\) as \(Y\) says about \(X\).

Since \(H(X, Y) = H(X) + H(Y|X)\), as shown in Section 2.2, we have:
\[
I(X; Y) = H(X) + H(Y) - H(X, Y).
\]

Finally, we note that:
\[
I(X; X) = H(X) - H(X|X) = H(X).
\]

Thus, the mutual information of a random variable with itself is the entropy of the random variable. This is the reason that entropy is sometimes referred to as self-information.

Collecting these results, we have the following theorem.

**Theorem 2.4.1 (Mutual Information and Entropy)**  
\[
I(X; Y) = H(X) - H(X|Y)
\]
\[
I(X; Y) = H(Y) - H(Y|X)
\]
\[
I(X; Y) = H(X) + H(Y) - H(X, Y)
\]
\[
I(X; Y) = I(Y; X)
\]
\[
I(X; X) = H(X).
\]

---

### CHAIN RULES FOR ENTROPY, RELATIVE ENTROPY, AND MUTUAL INFORMATION

We now show that the entropy of a collection of random variables is the sum of the conditional entropies.

**Theorem 2.5.1 (Chain Rule for Entropy)**  
Let \(X_1, X_2, \ldots, X_n\) be drawn according to \(p(x_1, x_2, \ldots, x_n)\). Then:
\[
H(X_1, X_2, \ldots, X_n) = \sum_{i=1}^{n} H(X_i|X_{i-1}, \ldots, X_1).
\]

**Proof**  
By repeated application of the two-variable expansion rule for entropies, we have:
\[
H(X_1, X_2) = H(X_1) + H(X_2|X_1),
\]
\[
H(X_1, X_2, X_3) = H(X_1) + H(X_2, X_3|X_1) = H(X_1) + H(X_2|X_1) + H(X_3|X_2, X_1),
\]
\[
\ldots
\]
\[
H(X_1, X_2, \ldots, X_n) = H(X_1) + H(X_2|X_1) + \cdots + H(X_n|X_{n-1}, \ldots, X_1)
\]
\[
= \sum_{i=1}^{n} H(X_i|X_{i-1}, \ldots, X_1).
\]

**Alternative Proof**  
We write \(p(x_1, \ldots, x_n) = \prod_{i=1}^{n} p(x_i|x_{i-1}, \ldots, x_1)\) and evaluate:
\[
H(X_1, X_2, \ldots, X_n) = -\sum_{x_1, x_2, \ldots, x_n} p(x_1, x_2, \ldots, x_n) \log p(x_1, x_2, \ldots, x_n)
\]
\[
= -\sum_{x_1, x_2, \ldots, x_n} p(x_1, x_2, \ldots, x_n) \log \prod_{i=1}^{n} p(x_i|x_{i-1}, \ldots, x_1)
\]
\[
= -\sum_{x_1, x_2, \ldots, x_n} p(x_1, x_2, \ldots, x_n) \sum_{i=1}^{n} \log p(x_i|x_{i-1}, \ldots, x_1)
\]
\[
= -\sum_{i=1}^{n} \sum_{x_1, x_2, \ldots, x_n} p(x_1, x_2, \ldots, x_n) \log p(x_i|x_{i-1}, \ldots, x_1)
\]
\[
= -\sum_{i=1}^{n} \sum_{x_1, x_2, \ldots, x_i} p(x_1, x_2, \ldots, x_i) \log p(x_i|x_{i-1}, \ldots, x_1)
\]
\[
= \sum_{i=1}^{n} H(X_i|X_{i-1}, \ldots, X_1).
\]

We now define the conditional mutual information as the reduction in the uncertainty of \(X\) due to knowledge of \(Y\) when \(Z\) is given.

**Definition**  
The conditional mutual information of random variables \(X\) and \(Y\) given \(Z\) is defined by:
\[
I(X; Y|Z) = H(X|Z) - H(X|Y, Z)
\]
\[
= E_{p(x,y,z)} \log \frac{p(X, Y|Z)}{p(X|Z)p(Y|Z)}.
\]

Mutual information also satisfies a chain rule.



ENTROPY, RELATIVE ENTROPY, AND MUTUAL INFORMATION

Theorem 2.5.2 (Chain rule for information)  
I(X₁, X₂, …, Xₙ; Y) = ∑ₐᵢ₌₁ⁿ I(Xᵢ; Y|Xᵢ₋₁, Xᵢ₋₂, …, X₁).  

Proof:  
I(X₁, X₂, …, Xₙ; Y) = H(X₁, X₂, …, Xₙ) − H(X₁, X₂, …, Xₙ|Y)  
= ∑ₐᵢ₌₁ⁿ H(Xᵢ|Xᵢ₋₁, …, X₁) − ∑ₐᵢ₌₁ⁿ H(Xᵢ|Xᵢ₋₁, …, X₁, Y)  
= ∑ₐᵢ₌₁ⁿ I(Xᵢ; Y|X₁, X₂, …, Xᵢ₋₁).  

We define a conditional version of the relative entropy.  

Definition:  
For joint probability mass functions p(x, y) and q(x, y), the conditional relative entropy D(p(y|x) || q(y|x)) is the average of the relative entropies between the conditional probability mass functions p(y|x) and q(y|x), averaged over the probability mass function p(x). More precisely,  

D(p(y|x) || q(y|x)) = ∑ₓ p(x) ∑ᵧ p(y|x) log(p(y|x)/q(y|x))  
= Eₚₓ,ᵧ log(p(Y|X)/q(Y|X)).  

The notation for conditional relative entropy is not explicit since it omits mention of the distribution p(x) of the conditioning random variable. However, it is normally understood from the context.  

The relative entropy between two joint distributions on a pair of random variables can be expanded as the sum of a relative entropy and a conditional relative entropy. The chain rule for relative entropy is used in Section 4.4 to prove a version of the second law of thermodynamics.  

Theorem 2.5.3 (Chain rule for relative entropy)  
D(p(x, y) || q(x, y)) = D(p(x) || q(x)) + D(p(y|x) || q(y|x)).  

Proof:  
D(p(x, y) || q(x, y)) = ∑ₓ ∑ᵧ p(x, y) log(p(x, y)/q(x, y))  
= ∑ₓ ∑ᵧ p(x, y) log(p(x)p(y|x)/q(x)q(y|x))  
= ∑ₓ ∑ᵧ p(x, y) log(p(x)/q(x)) + ∑ₓ ∑ᵧ p(x, y) log(p(y|x)/q(y|x))  
= D(p(x) || q(x)) + D(p(y|x) || q(y|x)).  

JENSEN’S INEQUALITY AND ITS CONSEQUENCES  

In this section, we prove some simple properties of the quantities defined earlier. We begin with the properties of convex functions.  

Definition:  
A function f(x) is said to be convex over an interval (a, b) if for every x₁, x₂ ∈ (a, b) and 0 ≤ λ ≤ 1,  
f(λx₁ + (1 − λ)x₂) ≤ λf(x₁) + (1 − λ)f(x₂).  

A function f is said to be strictly convex if equality holds only if λ = 0 or λ = 1.  

Definition:  
A function f is concave if −f is convex. A function is convex if it always lies below any chord. A function is concave if it always lies above any chord.  

Examples of convex functions include x², |x|, eˣ, x log x (for x ≥ 0), and so on. Examples of concave functions include log x and √x for x ≥ 0. Figure 2.3 shows some examples of convex and concave functions. Note that linear functions ax + b are both convex and concave. Convexity underlies many of the basic properties of information-theoretic quantities such as entropy and mutual information. Before we prove some of these properties, we derive some simple results for convex functions.  

Theorem 2.6.1  
If the function f has a second derivative that is non-negative (positive) over an interval, the function is convex (strictly convex) over that interval.  

Proof:  
We use the Taylor series expansion of the function around x₀:  
f(x) = f(x₀) + f’(x₀)(x − x₀) + (1/2)f’’(x*) (x − x₀)²,  
where x* lies between x₀ and x. By hypothesis, f’’(x*) ≥ 0, and thus the last term is non-negative for all x.  

We let x₀ = λx₁ + (1 − λ)x₂ and take x = x₁, to obtain  
f(x₁) ≥ f(x₀) + f’(x₀)((1 − λ)(x₁ − x₂)).  

Similarly, taking x = x₂, we obtain  
f(x₂) ≥ f(x₀) + f’(x₀)(λ(x₂ − x₁)).  

Multiplying (2.74) by λ and (2.75) by 1 − λ and adding, we obtain (2.72). The proof for strict convexity proceeds along the same lines.  

Theorem 2.6.1 allows us immediately to verify the strict convexity of x², eˣ, and x log x for x ≥ 0, and the strict concavity of log x and √x for x ≥ 0.  

Let E denote expectation. Thus, EX = ∑ₓ∈X p(x)x in the discrete case and EX = ∫ₓ f(x) dx in the continuous case.  

The next inequality is one of the most widely used in mathematics and one that underlies many of the basic results in information theory.  

Theorem 2.6.2 (Jensen’s inequality)  
If f is a convex function and X is a random variable,  
E[f(X)] ≥ f(EX).  

Moreover, if f is strictly convex, the equality in (2.76) implies that X = EX with probability 1 (i.e., X is a constant).  

Proof:  
We prove this for discrete distributions by induction on the number of mass points. The proof of conditions for equality when f is strictly convex is left to the reader.  

For a two-mass-point distribution, the inequality becomes  
p₁f(x₁) + p₂f(x₂) ≥ f(p₁x₁ + p₂x₂),  
which follows directly from the definition of convex functions. Suppose that the theorem is true for distributions with k − 1 mass points. Then writing p’ᵢ = pᵢ / (1 − pₖ) for i = 1, 2, …, k − 1, we have  

∑ₖᵢ₌₁ pᵢ f(xᵢ) = pₖ f(xₖ) + (1 − pₖ) ∑ₖ₋₁ᵢ₌₁ p’ᵢ f(xᵢ)  
≥ pₖ f(xₖ) + (1 − pₖ) f(∑ₖ₋₁ᵢ₌₁ p’ᵢ xᵢ)  
≥ f(pₖ xₖ + (1 − pₖ) ∑ₖ₋₁ᵢ₌₁ p’ᵢ xᵢ)  
= f(∑ₖᵢ₌₁ pᵢ xᵢ),  

where the first inequality follows from the induction hypothesis and the second follows from the definition of convexity.  

The proof can be extended to continuous distributions by continuity arguments.  

We now use these results to prove some of the properties of entropy and relative entropy. The following theorem is of fundamental importance.  

Theorem 2.6.3 (Information inequality)  
Let p(x), q(x), x ∈ X, be two probability mass functions. Then  
D(p || q) ≥ 0,  

with equality if and only if p(x) = q(x) for all x.  

Proof:  
Let A = {x : p(x) > 0} be the support set of p(x). Then  

−D(p || q) = −∑ₓ∈A p(x) log(p(x)/q(x))  
= ∑ₓ∈A p(x) log(q(x)/p(x))  
≤ log(∑ₓ∈A p(x) q(x)/p(x))  
= log(∑ₓ∈A q(x))  
≤ log(∑ₓ∈X q(x))  
= log 1  
= 0,  

where (2.85) follows from Jensen’s inequality. Since log t is a strictly concave function of t, we have equality in (2.85) if and only if q(x)/p(x) is constant everywhere [i.e., q(x) = c p(x) for all x]. Thus, ∑ₓ∈A q(x) = c ∑ₓ∈A p(x) = c. We have equality in (2.87) only if ∑ₓ∈A q(x) = ∑ₓ∈X q(x) = 1, which implies that c = 1. Hence, we have D(p || q) = 0 if and only if p(x) = q(x) for all x.  

Corollary (Nonnegativity of mutual information)  
For any two random variables X, Y,  
I(X; Y) ≥ 0,  

with equality if and only if X and Y are independent.  

Proof:  
I(X; Y) = D(p(x, y) || p(x)p(y)) ≥ 0, with equality if and only if p(x, y) = p(x)p(y) (i.e., X and Y are independent).  

Corollary  
D(p(y|x) || q(y|x)) ≥ 0,  

with equality if and only if p(y|x) = q(y|x) for all y and x such that p(x) > 0.  

Corollary  
I(X; Y|Z) ≥ 0,  

with equality if and only if X and Y are conditionally independent given Z.  

We now show that the uniform distribution over the range X is the maximum entropy distribution over this range. It follows that any random variable with this range has an entropy no greater than log |X|.  

Theorem 2.6.4  
H(X) ≤ log |X|, where |X| denotes the number of elements in the range of X, with equality if and only if X has a uniform distribution over X.  

Proof:  
Let u(x) = 1/|X| be the uniform probability mass function over X, and let p(x) be the probability mass function for X. Then  

D(p || u) = ∑ p(x) log(p(x)/u(x)) = log |X| − H(X).  

Hence, by the nonnegativity of relative entropy,  
0 ≤ D(p || u) = log |X| − H(X).  

Theorem 2.6.5 (Conditioning reduces entropy) (Information can’t hurt)  
H(X|Y) ≤ H(X),  

with equality if and only if X and Y are independent.  

Proof:  
0 ≤ I(X; Y) = H(X) − H(X|Y).  

Intuitively, the theorem says that knowing another random variable Y can only reduce the uncertainty in X. Note that this is true only on the average. Specifically, H(X|Y = y) may be greater than or less than or equal to H(X), but on the average H(X|Y) = ∑ᵧ p(y) H(X|Y = y) ≤ H(X). For example, in a court case, specific new evidence might increase uncertainty, but on the average evidence decreases uncertainty.  

Example 2.6.1  
Let (X, Y) have the following joint distribution:  

Then  
H(X) = H(1/8, 7/8) = 0.544 bits,  
H(X|Y = 1) = 0 bits,  
and  
H(X|Y = 2) = 1 bit.  

We calculate  
H(X|Y) = (3/4) H(X|Y = 1) + (1/4) H(X|Y = 2) = 0.25 bits. Thus, the uncertainty in X is increased if Y = 2 is observed and decreased if Y = 1 is observed, but uncertainty decreases on the average.  

Theorem 2.6.6 (Independence bound on entropy)  
Let X₁, X₂, …, Xₙ be drawn according to p(x₁, x₂, …, xₙ). Then  

H(X₁, X₂, …, Xₙ) ≤ ∑ₐᵢ₌₁ⁿ H(Xᵢ),  

with equality if and only if the Xᵢ are independent.  

Proof:  
By the chain rule for entropies,  
H(X₁, X₂, …, Xₙ) = ∑ₐᵢ₌₁ⁿ H(Xᵢ|Xᵢ₋₁, …, X₁)  
≤ ∑ₐᵢ₌₁ⁿ H(Xᵢ),  

where the inequality follows directly from Theorem 2.6.5. We have equality if and only if Xᵢ is independent of Xᵢ₋₁, …, X₁ for all i (i.e., if and only if the Xᵢ’s are independent).  

LOG SUM INEQUALITY AND ITS APPLICATIONS  

We now prove a simple consequence of the concavity of the logarithm, which will be used to prove some concavity results for the entropy.  

Theorem 2.7.1 (Log sum inequality)  
For nonnegative numbers a₁, a₂, …, aₙ and b₁, b₂, …, bₙ,  

∑ₐᵢ₌₁ⁿ aᵢ log(aᵢ/bᵢ) ≥ (∑ₐᵢ₌₁ⁿ aᵢ) log(∑ₐᵢ₌₁ⁿ aᵢ / ∑ᵦᵢ₌₁ⁿ bᵢ),  

with equality if and only if aᵢ / bᵢ = const.  

We again use the convention that 0 log 0 = 0, a log a / 0 = ∞ if a > 0 and 0 log 0 / 0 = 0. These follow easily from continuity.  

Proof:  
Assume without loss of generality that aᵢ > 0 and bᵢ > 0. The function f(t) = t log t is strictly convex, since f’’(t) = 1/t log e > 0 for all positive t. Hence, by Jensen’s inequality, we have  

∑ αᵢ f(tᵢ) ≥ f(∑ αᵢ tᵢ),  

for αᵢ ≥ 0, ∑ αᵢ = 1. Setting αᵢ = bᵢ / ∑ bⱼ and tᵢ = aᵢ / bᵢ, we obtain  

∑ aᵢ / bᵢ log(aᵢ / bᵢ) ≥ ∑ aᵢ / bᵢ log(∑ aᵢ / bᵢ),  

which is the log sum inequality.  

We now use the log sum inequality to prove various convexity results. We begin by reproving Theorem 2.6.3, which states that D(p || q) ≥ 0 with equality if and only if p(x) = q(x). By the log sum inequality,  

D(p || q) = ∑ p(x) log(p(x)/q(x))  
≥ (∑ p(x)) log(∑ p(x))  
= 1 log 1  
= 0,  

with equality if and only if p(x)/q(x) = c. Since both p and q are probability mass functions, c = 1, and hence we have D(p || q) = 0 if and only if p(x) = q(x) for all x.



ENTROPY, RELATIVE ENTROPY, AND MUTUAL INFORMATION

Theorem 2.7.2 (Convexity of relative entropy)  
D(p||q) is convex in the pair (p, q); that is, if (p₁, q₁) and (p₂, q₂) are two pairs of probability mass functions, then  
D(λp₁ + (1 − λ)p₂ || λq₁ + (1 − λ)q₂) ≤ λD(p₁||q₁) + (1 − λ)D(p₂||q₂)  
for all 0 ≤ λ ≤ 1.  

Proof:  
We apply the log sum inequality to a term on the left-hand side of (2.105):  
(λp₁(x) + (1 − λ)p₂(x)) log [λp₁(x) + (1 − λ)p₂(x)] / [λq₁(x) + (1 − λ)q₂(x)]  
≤ λp₁(x) log [λp₁(x) / λq₁(x)] + (1 − λ)p₂(x) log [(1 − λ)p₂(x) / (1 − λ)q₂(x)].  

Summing this over all x, we obtain the desired property.  

Theorem 2.7.3 (Concavity of entropy)  
H(p) is a concave function of p.  

Proof:  
H(p) = log |X| − D(p||u), where u is the uniform distribution on |X| outcomes. The concavity of H then follows directly from the convexity of D.  

Alternative Proof:  
Let X₁ be a random variable with distribution p₁, taking on values in a set A. Let X₂ be another random variable with distribution p₂ on the same set. Let  
θ = {1 with probability λ, 2 with probability 1 − λ}.  

Let Z = Xθ. Then the distribution of Z is λp₁ + (1 − λ)p₂. Now, since conditioning reduces entropy, we have  
H(Z) ≥ H(Z|θ),  
or equivalently,  
H(λp₁ + (1 − λ)p₂) ≥ λH(p₁) + (1 − λ)H(p₂),  
which proves the concavity of the entropy as a function of the distribution.  

LOG SUM INEQUALITY AND ITS APPLICATIONS  

One of the consequences of the concavity of entropy is that mixing two gases of equal entropy results in a gas with higher entropy.  

Theorem 2.7.4  
Let (X, Y) ∼ p(x, y) = p(x)p(y|x). The mutual information I(X; Y) is a concave function of p(x) for fixed p(y|x) and a convex function of p(y|x) for fixed p(x).  

Proof:  
To prove the first part, we expand the mutual information  
I(X; Y) = H(Y) − H(Y|X) = H(Y) − ∑ₓ p(x)H(Y|X = x).  

If p(y|x) is fixed, then p(y) is a linear function of p(x). Hence, H(Y), which is a concave function of p(y), is a concave function of p(x). The second term is a linear function of p(x). Hence, the difference is a concave function of p(x).  

To prove the second part, we fix p(x) and consider two different conditional distributions p₁(y|x) and p₂(y|x). The corresponding joint distributions are p₁(x, y) = p(x)p₁(y|x) and p₂(x, y) = p(x)p₂(y|x), and their respective marginals are p(x), p₁(y) and p(x), p₂(y). Consider a conditional distribution  
pλ(y|x) = λp₁(y|x) + (1 − λ)p₂(y|x),  

which is a mixture of p₁(y|x) and p₂(y|x) where 0 ≤ λ ≤ 1. The corresponding joint distribution is also a mixture of the corresponding joint distributions,  
pλ(x, y) = λp₁(x, y) + (1 − λ)p₂(x, y),  

and the distribution of Y is also a mixture,  
pλ(y) = λp₁(y) + (1 − λ)p₂(y).  

Hence, if we let qλ(x, y) = p(x)pλ(y) be the product of the marginal distributions, we have  
qλ(x, y) = λq₁(x, y) + (1 − λ)q₂(x, y).  

Since the mutual information is the relative entropy between the joint distribution and the product of the marginals,  
I(X; Y) = D(pλ(x, y) || qλ(x, y)),  

and relative entropy D(p||q) is a convex function of (p, q), it follows that the mutual information is a convex function of the conditional distribution.  

DATA-PROCESSING INEQUALITY  

The data-processing inequality can be used to show that no clever manipulation of the data can improve the inferences that can be made from the data.  

Definition  
Random variables X, Y, Z are said to form a Markov chain in that order (denoted by X → Y → Z) if the conditional distribution of Z depends only on Y and is conditionally independent of X. Specifically, X, Y, and Z form a Markov chain X → Y → Z if the joint probability mass function can be written as  
p(x, y, z) = p(x)p(y|x)p(z|y).  

Some simple consequences are as follows:  
• X → Y → Z if and only if X and Z are conditionally independent given Y. Markovity implies conditional independence because  
p(x, z|y) = p(x, y, z) / p(y) = p(x, y)p(z|y) / p(y) = p(x|y)p(z|y).  

This is the characterization of Markov chains that can be extended to define Markov fields, which are n-dimensional random processes in which the interior and exterior are independent given the values on the boundary.  
• X → Y → Z implies that Z → Y → X. Thus, the condition is sometimes written X ↔ Y ↔ Z.  
• If Z = f(Y), then X → Y → Z.  

We can now prove an important and useful theorem demonstrating that no processing of Y, deterministic or random, can increase the information that Y contains about X.  

Theorem 2.8.1 (Data-processing inequality)  
If X → Y → Z, then  
I(X; Y) ≥ I(X; Z).  

Proof:  
By the chain rule, we can expand mutual information in two different ways:  
I(X; Y, Z) = I(X; Z) + I(X; Y|Z)  
= I(X; Y) + I(X; Z|Y).  

Since X and Z are conditionally independent given Y, we have I(X; Z|Y) = 0. Since I(X; Y|Z) ≥ 0, we have  
I(X; Y) ≥ I(X; Z).  

We have equality if and only if I(X; Y|Z) = 0 (i.e., X → Z → Y forms a Markov chain). Similarly, one can prove that I(Y; Z) ≥ I(X; Z).  

Corollary  
In particular, if Z = g(Y), we have I(X; Y) ≥ I(X; g(Y)).  

Proof:  
X → Y → g(Y) forms a Markov chain.  

Thus, functions of the data Y cannot increase the information about X.  

Corollary  
If X → Y → Z, then I(X; Y|Z) ≤ I(X; Y).  

Proof:  
We note in (2.119) and (2.120) that I(X; Z|Y) = 0, by Markovity, and I(X; Z) ≥ 0. Thus,  
I(X; Y|Z) ≤ I(X; Y).  

Thus, the dependence of X and Y is decreased (or remains unchanged) by the observation of a “downstream” random variable Z. Note that it is also possible that I(X; Y|Z) > I(X; Y) when X, Y, and Z do not form a Markov chain. For example, let X and Y be independent fair binary random variables, and let Z = X + Y. Then I(X; Y) = 0, but I(X; Y|Z) = H(X|Z) − H(X|Y, Z) = H(X|Z) = P(Z = 1)H(X|Z = 1) = 1/2 bit.  

SUFFICIENT STATISTICS  

This section is a sidelight showing the power of the data-processing inequality in clarifying an important idea in statistics. Suppose that we have a family of probability mass functions {fθ(x)} indexed by θ, and let X be a sample from a distribution in this family. Let T(X) be any statistic (function of the sample) like the sample mean or sample variance. Then θ → X → T(X), and by the data-processing inequality, we have  
I(θ; T(X)) ≤ I(θ; X)  

for any distribution on θ. However, if equality holds, no information is lost.  

A statistic T(X) is called sufficient for θ if it contains all the information in X about θ.  

Definition  
A function T(X) is said to be a sufficient statistic relative to the family {fθ(x)} if X is independent of θ given T(X) for any distribution on θ [i.e., θ → T(X) → X forms a Markov chain].  

This is the same as the condition for equality in the data-processing inequality,  
I(θ; X) = I(θ; T(X))  

for all distributions on θ. Hence, sufficient statistics preserve mutual information and conversely.  

Here are some examples of sufficient statistics:  
1. Let X₁, X₂, ..., Xₙ, Xᵢ ∈ {0, 1}, be an independent and identically distributed (i.i.d.) sequence of coin tosses of a coin with unknown parameter θ = Pr(Xᵢ = 1). Given n, the number of 1’s is a sufficient statistic for θ. Here T(X₁, X₂, ..., Xₙ) = ∑ᵢ=1ⁿ Xᵢ. In fact, we can show that given T, all sequences having that many 1’s are equally likely and independent of the parameter θ. Specifically,  
Pr((X₁, X₂, ..., Xₙ) = (x₁, x₂, ..., xₙ) | ∑ᵢ=1ⁿ Xᵢ = k) = 1 / (n choose k) if ∑ᵢ=1ⁿ xᵢ = k, 0 otherwise.  

Thus, θ → Xᵢ → (X₁, X₂, ..., Xₙ) forms a Markov chain, and T is a sufficient statistic for θ.  

The next two examples involve probability densities instead of probability mass functions, but the theory still applies. We define entropy and mutual information for continuous random variables in Chapter 8.  

2. If X is normally distributed with mean θ and variance 1; that is, if fθ(x) = (1 / √(2π))e^{−(x−θ)²/2} = N(θ, 1), and X₁, X₂, ..., Xₙ are drawn independently according to this distribution, a sufficient statistic for θ is the sample mean X̄ = 1/n ∑ᵢ=1ⁿ Xᵢ. It can be verified that the conditional distribution of X₁, X₂, ..., Xₙ, conditioned on X̄ and n, does not depend on θ.  

3. If fθ = Uniform(θ, θ + 1), a sufficient statistic for θ is T(X₁, X₂, ..., Xₙ) = (max{X₁, X₂, ..., Xₙ}, min{X₁, X₂, ..., Xₙ}).  

The proof of this is slightly more complicated, but again one can show that the distribution of the data is independent of the parameter given the statistic T.  

The minimal sufficient statistic is a sufficient statistic that is a function of all other sufficient statistics.  

Definition  
A statistic T(X) is a minimal sufficient statistic relative to {fθ(x)} if it is a function of every other sufficient statistic U. Interpreting this in terms of the data-processing inequality, this implies that θ → T(X) → U(X) → X.  

Hence, a minimal sufficient statistic maximally compresses the information about θ in the sample. Other sufficient statistics may contain additional irrelevant information. For example, for a normal distribution with mean θ, the pair of functions giving the mean of all odd samples and the mean of all even samples is a sufficient statistic, but not a minimal sufficient statistic. In the preceding examples, the sufficient statistics are also minimal.  

FANO’S INEQUALITY  

Suppose that we know a random variable Y and we wish to guess the value of a correlated random variable X. Fano’s inequality relates the probability of error in guessing the random variable X to its conditional entropy H(X|Y). It will be crucial in proving the converse to Shannon’s channel capacity theorem in Chapter 7. From Problem 2.5, we know that the conditional entropy of a random variable X given another random variable Y is zero if and only if X is a function of Y. Hence, we can estimate X from Y with zero probability of error if and only if H(X|Y) = 0.  

Extending this argument, we expect to be able to estimate X with a low probability of error only if the conditional entropy H(X|Y) is small. Fano’s inequality quantifies this idea. Suppose that we wish to estimate a random variable X with a distribution p(x). We observe a random variable Y that is related to X by the conditional distribution p(y|x). From Y, we estimate X using a function g(Y). Let Ẍ = g(Y) denote the estimate of X. The probability of error is P(Ẍ ≠ X). Fano’s inequality relates this probability to the conditional entropy H(X|Y).  

The inequality states that  
H(X|Y) ≤ H(Ẍ|Y) + P(Ẍ ≠ X) log(|X| −1).  

If Ẍ is a function of Y, then H(Ẍ|Y) = 0, and the inequality simplifies to  
H(X|Y) ≤ P(Ẍ ≠ X) log(|X|−1).  

Rearranging, we get  
P(Ẍ ≠ X) ≥ H(X|Y) / log(|X|−1).  

This shows that the probability of error is bounded below by the conditional entropy of X given Y. If H(X|Y) is small, the probability of error can be small, but it cannot be smaller than this bound.  

Fano’s inequality is a fundamental tool in information theory and is used to establish lower bounds on the probability of error in many communication and coding problems.



ENTROPY, RELATIVE ENTROPY, AND MUTUAL INFORMATION

Calculate a function \( g(Y) = \hat{X} \), where \( \hat{X} \) is an estimate of \( X \) and takes on values in \( \hat{X} \). We will not restrict the alphabet \( \hat{X} \) to be equal to \( X \), and we will also allow the function \( g(Y) \) to be random. We wish to bound the probability that \( \hat{X} \neq X \). We observe that \( X \to Y \to \hat{X} \) forms a Markov chain. Define the probability of error

\[
P_e = \Pr(\hat{X} \neq X).
\]

**Theorem 2.10.1 (Fano’s Inequality)**  
For any estimator \( \hat{X} \) such that \( X \to Y \to \hat{X} \), with \( P_e = \Pr(X \neq \hat{X}) \), we have

\[
H(P_e) + P_e \log |X| \geq H(X|\hat{X}) \geq H(X|Y).
\]

This inequality can be weakened to

\[
1 + P_e \log |X| \geq H(X|Y)
\]

or

\[
P_e \geq \frac{H(X|Y) - 1}{\log |X|}.
\]

**Remark**  
Note from (2.130) that \( P_e = 0 \) implies that \( H(X|Y) = 0 \), as intuition suggests.

**Proof:**  
We first ignore the role of \( Y \) and prove the first inequality in (2.130). We will then use the data-processing inequality to prove the more traditional form of Fano’s inequality, given by the second inequality in (2.130). Define an error random variable,

\[
E = \begin{cases} 
1 & \text{if } \hat{X} \neq X, \\
0 & \text{if } \hat{X} = X.
\end{cases}
\]

Then, using the chain rule for entropies to expand \( H(E, X|\hat{X}) \) in two different ways, we have

\[
H(E, X|\hat{X}) = H(X|\hat{X}) + H(E|X, \hat{X})
\]

\[
= H(E|\hat{X}) + H(X|E, \hat{X}).
\]

Since conditioning reduces entropy, \( H(E|\hat{X}) \leq H(E) = H(P_e) \). Now, since \( E \) is a function of \( X \) and \( \hat{X} \), the conditional entropy \( H(E|X, \hat{X}) \) is equal to 0. Also, since \( E \) is a binary-valued random variable, \( H(E) = H(P_e) \). The remaining term, \( H(X|E, \hat{X}) \), can be bounded as follows:

\[
H(X|E, \hat{X}) = \Pr(E = 0)H(X|\hat{X}, E = 0) + \Pr(E = 1)H(X|\hat{X}, E = 1)
\]

\[
\leq (1 - P_e)0 + P_e \log |X|,
\]

since given \( E = 0 \), \( X = \hat{X} \), and given \( E = 1 \), we can upper bound the conditional entropy by the log of the number of possible outcomes. Combining these results, we obtain

\[
H(P_e) + P_e \log |X| \geq H(X|\hat{X}).
\]

By the data-processing inequality, we have \( I(X; \hat{X}) \leq I(X; Y) \) since \( X \to Y \to \hat{X} \) is a Markov chain, and therefore \( H(X|\hat{X}) \geq H(X|Y) \). Thus, we have

\[
H(P_e) + P_e \log |X| \geq H(X|\hat{X}) \geq H(X|Y).
\]

**Corollary**  
For any two random variables \( X \) and \( Y \), let \( p = \Pr(X \neq Y) \). Then,

\[
H(p) + p \log |X| \geq H(X|Y).
\]

**Proof:**  
Let \( \hat{X} = Y \) in Fano’s inequality.

**Corollary**  
For any two random variables \( X \) and \( Y \), if the estimator \( g(Y) \) takes values in the set \( X \), we can strengthen the inequality slightly by replacing \( \log |X| \) with \( \log(|X| - 1) \).

**Corollary**  
Let \( P_e = \Pr(X \neq \hat{X}) \), and let \( \hat{X}: Y \to X \). Then,

\[
H(P_e) + P_e \log(|X| - 1) \geq H(X|Y).
\]

**Proof:**  
The proof of the theorem goes through without change, except that

\[
H(X|E, \hat{X}) = \Pr(E = 0)H(X|\hat{X}, E = 0) + \Pr(E = 1)H(X|\hat{X}, E = 1)
\]

\[
\leq (1 - P_e)0 + P_e \log(|X| - 1),
\]

since given \( E = 0 \), \( X = \hat{X} \), and given \( E = 1 \), the range of possible \( X \) outcomes is \( |X| - 1 \), we can upper bound the conditional entropy by the log(\( |X| - 1 \)), the logarithm of the number of possible outcomes. Substituting this provides us with the stronger inequality.

**Remark**  
Suppose that there is no knowledge of \( Y \). Thus, \( X \) must be guessed without any information. Let \( X \in \{1, 2, \ldots, m\} \) and \( p_1 \geq p_2 \geq \cdots \geq p_m \). Then the best guess of \( X \) is \( \hat{X} = 1 \) and the resulting probability of error is \( P_e = 1 - p_1 \). Fano’s inequality becomes

\[
H(P_e) + P_e \log(m - 1) \geq H(X).
\]

The probability mass function

\[
(p_1, p_2, \ldots, p_m) = \left(1 - P_e, \frac{P_e}{m - 1}, \ldots, \frac{P_e}{m - 1}\right)
\]

achieves this bound with equality. Thus, Fano’s inequality is sharp.

While we are at it, let us introduce a new inequality relating probability of error and entropy. Let \( X \) and \( X' \) be two independent identically distributed random variables with entropy \( H(X) \). The probability that \( X = X' \) is given by

\[
\Pr(X = X') = \sum_x p^2(x).
\]

We have the following inequality:

**Lemma 2.10.1**  
If \( X \) and \( X' \) are i.i.d. with entropy \( H(X) \),

\[
\Pr(X = X') \geq 2 - H(X),
\]

with equality if and only if \( X \) has a uniform distribution.

**Proof:**  
Suppose that \( X \sim p(x) \). By Jensen’s inequality, we have

\[
2E \log p(X) \leq E[2 \log p(X)],
\]

which implies that

\[
2 - H(X) = 2 \sum_x p(x) \log p(x) \leq \sum_x p^2(x).
\]

**Corollary**  
Let \( X, X' \) be independent with \( X \sim p(x) \), \( X' \sim r(x) \), \( x, x' \in X \). Then

\[
\Pr(X = X') \geq 2 - H(p) - D(p \| r),
\]

\[
\Pr(X = X') \geq 2 - H(r) - D(r \| p).
\]

**Proof:**  
We have

\[
2 - H(p) - D(p \| r) = 2 \sum_x p(x) \log p(x) + \sum_x p(x) \log r(x)
\]

\[
= 2 \sum_x p(x) \log r(x).
\]

where the inequality follows from Jensen’s inequality and the convexity of the function \( f(y) = 2y \).

---

**SUMMARY**

**Definition**  
The entropy \( H(X) \) of a discrete random variable \( X \) is defined by

\[
H(X) = - \sum_{x \in X} p(x) \log p(x).
\]

**Properties of \( H \):**  
1. \( H(X) \geq 0 \).  
2. \( H_b(X) = (\log_b a) H_a(X) \).  
3. (Conditioning reduces entropy) For any two random variables, \( X \) and \( Y \), we have

\[
H(X|Y) \leq H(X),
\]

with equality if and only if \( X \) and \( Y \) are independent.  
4. \( H(X_1, X_2, \ldots, X_n) \leq \sum_{i=1}^n H(X_i) \), with equality if and only if the \( X_i \) are independent.  
5. \( H(X) \leq \log |X| \), with equality if and only if \( X \) is distributed uniformly over \( X \).  
6. \( H(p) \) is concave in \( p \).

**Definition**  
The relative entropy \( D(p \| q) \) of the probability mass function \( p \) with respect to the probability mass function \( q \) is defined by

\[
D(p \| q) = \sum_x p(x) \log \frac{p(x)}{q(x)}.
\]

**Definition**  
The mutual information between two random variables \( X \) and \( Y \) is defined as

\[
I(X; Y) = \sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{p(x, y)}{p(x)p(y)}.
\]

**Alternative expressions:**  
\[
H(X) = E_p \log \frac{1}{p(X)},
\]
\[
H(X, Y) = E_p \log \frac{1}{p(X, Y)},
\]
\[
H(X|Y) = E_p \log \frac{1}{p(X|Y)},
\]
\[
I(X; Y) = E_p \log \frac{p(X, Y)}{p(X)p(Y)},
\]
\[
D(p \| q) = E_p \log \frac{p(X)}{q(X)}.
\]

**Properties of \( D \) and \( I \):**  
1. \( I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) = H(X) + H(Y) - H(X, Y) \).  
2. \( D(p \| q) \geq 0 \) with equality if and only if \( p(x) = q(x) \), for all \( x \in X \).  
3. \( I(X; Y) = D(p(x, y) \| p(x)p(y)) \geq 0 \), with equality if and only if \( p(x, y) = p(x)p(y) \) (i.e., \( X \) and \( Y \) are independent).  
4. If \( |X| = m \), and \( u \) is the uniform distribution over \( X \), then \( D(p \| u) = \log m - H(p) \).  
5. \( D(p \| q) \) is convex in the pair \( (p, q) \).

**Chain rules:**  
- Entropy: \( H(X_1, X_2, \ldots, X_n) = \sum_{i=1}^n H(X_i | X_{i-1}, \ldots, X_1) \).  
- Mutual information: \( I(X_1, X_2, \ldots, X_n; Y) = \sum_{i=1}^n I(X_i; Y | X_1, X_2, \ldots, X_{i-1}) \).  
- Relative entropy: \( D(p(x, y) \| q(x, y)) = D(p(x) \| q(x)) + D(p(y|x) \| q(y|x)) \).

**Jensen’s inequality:**  
If \( f \) is a convex function, then \( E[f(X)] \geq f(E[X]) \).

**Log sum inequality:**  
For \( n \) positive numbers \( a_1, a_2, \ldots, a_n \) and \( b_1, b_2, \ldots, b_n \),

\[
\sum_{i=1}^n a_i \log \frac{a_i}{b_i} \geq \sum_{i=1}^n a_i \log \frac{\sum_{i=1}^n a_i}{\sum_{i=1}^n b_i},
\]

with equality if and only if \( \frac{a_i}{b_i} = \text{constant} \).

**Data-processing inequality:**  
If \( X \to Y \to Z \) forms a Markov chain, \( I(X; Y) \geq I(X; Z) \).

**Sufficient statistic:**  
\( T(X) \) is sufficient relative to \( \{f_\theta(x)\} \) if and only if \( I(\theta; X) = I(\theta; T(X)) \) for all distributions on \( \theta \).

**Fano’s inequality:**  
Let \( P_e = \Pr(\hat{X}(Y) \neq X) \). Then

\[
H(P_e) + P_e \log |X| \geq H(X|Y).
\]

**Inequality:**  
If \( X \) and \( X' \) are independent and identically distributed, then

\[
\Pr(X = X') \geq 2 - H(X).
\]

---

**PROBLEMS**

**2.1 Coin flips.**  
A fair coin is flipped until the first head occurs. Let \( X \) denote the number of flips required.  
(a) Find the entropy \( H(X) \) in bits. The following expressions may be useful:  
\[
\sum_{n=0}^\infty r^n = \frac{1}{1 - r}, \quad \sum_{n=0}^\infty n r^n = \frac{r}{(1 - r)^2}.
\]  
(b) A random variable \( X \) is drawn according to this distribution. Find an “efficient” sequence of yes–no questions of the form “Is \( X \) contained in the set \( S \)?” Compare \( H(X) \) to the expected number of questions required to determine \( X \).

**2.2 Entropy of functions.**  
Let \( X \) be a random variable taking on a finite number of values. What is the (general) inequality relationship of \( H(X) \) and \( H(Y) \) if  
(a) \( Y = 2X \)?  
(b) \( Y = \cos X \)?

**2.3 Minimum entropy.**  
What is the minimum value of \( H(p_1, \ldots, p_n) = H(p) \) as \( p \) ranges over the set of \( n \)-dimensional probability vectors? Find all \( p \)’s that achieve this minimum.

**2.4 Entropy of functions of a random variable.**  
Let \( X \) be a discrete random variable. Show that the entropy of a function of \( X \) is less than or equal to the entropy of \( X \) by justifying the following steps:  
\[
H(X, g(X)) = H(X) + H(g(X) | X)
\]  
\[
= H(X),
\]  
\[
H(X, g(X)) = H(g(X)) + H(X | g(X))
\]  
\[
\geq H(g(X)).
\]  
Thus, \( H(g(X)) \leq H(X) \).

**2.5 Zero conditional entropy.**  
Show that if \( H(Y|X) = 0 \), then \( Y \) is a function of \( X \) [i.e., for all \( x \) with \( p(x) > 0 \), there is only one possible value of \( y \) with \( p(x, y) > 0 \)].

**2.6 Conditional mutual information vs. unconditional mutual information.**  
Give examples of joint random variables \( X, Y, Z \) such that  
(a) \( I(X; Y | Z) < I(X; Y) \).  
(b) \( I(X; Y | Z) > I(X; Y) \).

**2.7 Coin weighing.**  
Suppose that one has \( n \) coins, among which there may or may not be one counterfeit coin. If there is a counterfeit coin, it may be either heavier or lighter than the other coins. The coins are to be weighed by a balance.  
(a) Find an upper bound on the number of coins \( n \) so that \( k \) weighings will find the counterfeit coin (if any) and correctly declare it to be heavier or lighter.



PROBLEMS

2.8 Drawing with and without replacement.  
An urn contains r red, w white, and b black balls. Which has higher entropy, drawing k ≥ 2 balls from the urn with replacement or without replacement? Set it up and show why. (There is both a difficult way and a relatively simple way to do this.)

2.9 Metric.  
A function ρ(x, y) is a metric if for all x, y,  
- ρ(x, y) ≥ 0.  
- ρ(x, y) = ρ(y, x).  
- ρ(x, y) = 0 if and only if x = y.  
- ρ(x, y) + ρ(y, z) ≥ ρ(x, z).  

(a) Show that ρ(X, Y) = H(X|Y) + H(Y|X) satisfies the first, second, and fourth properties above. If we say that X = Y if there is a one-to-one function mapping from X to Y, the third property is also satisfied, and ρ(X, Y) is a metric.  

(b) Verify that ρ(X, Y) can also be expressed as  
ρ(X, Y) = H(X) + H(Y) − 2I(X; Y)  
= H(X, Y) − I(X; Y)  
= 2H(X, Y) − H(X) − H(Y).  

2.10 Entropy of a disjoint mixture.  
Let X₁ and X₂ be discrete random variables drawn according to probability mass functions p₁(·) and p₂(·) over the respective alphabets X₁ = {1, 2, ..., m} and X₂ = {m + 1, ..., n}. Let  
X = {  
 X₁ with probability α,  
 X₂ with probability 1 − α.  

(a) Find H(X) in terms of H(X₁), H(X₂), and α.  

(b) Maximize over α to show that 2H(X) ≤ 2H(X₁) + 2H(X₂) and interpret using the notion that 2H(X) is the effective alphabet size.  

2.11 Measure of correlation.  
Let X₁ and X₂ be identically distributed but not necessarily independent. Let  
ρ = 1 − H(X₂ | X₁)  
H(X₁).  

(a) Show that ρ = I(X₁; X₂) / H(X₁).  

(b) Show that 0 ≤ ρ ≤ 1.  

(c) When is ρ = 0?  

(d) When is ρ = 1?  

2.12 Example of joint entropy.  
Let p(x, y) be given by [IMAGE].  

(a) Find H(X), H(Y).  

(b) Find H(X | Y), H(Y | X).  

(c) Find H(X, Y).  

(d) Find H(Y) − H(Y | X).  

(e) Find I(X; Y).  

(f) Draw a Venn diagram for the quantities in parts (a) through (e).  

2.13 Inequality.  
Show that ln x ≥ 1 − 1/x for x > 0.  

2.14 Entropy of a sum.  
Let X and Y be random variables that take on values x₁, x₂, ..., x_r and y₁, y₂, ..., y_s, respectively. Let Z = X + Y.  

(a) Show that H(Z | X) = H(Y | X). Argue that if X, Y are independent, then H(Y) ≤ H(Z) and H(X) ≤ H(Z). Thus, the addition of independent random variables adds uncertainty.  

(b) Give an example of (necessarily dependent) random variables in which H(X) > H(Z) and H(Y) > H(Z).  

(c) Under what conditions does H(Z) = H(X) + H(Y)?  

2.15 Data processing.  
Let X₁ → X₂ → X₃ → · · · → X_n form a Markov chain in this order; that is, let p(x₁, x₂, ..., x_n) = p(x₁)p(x₂ | x₁) · · · p(x_n | x_{n−1}). Reduce I(X₁; X₂, ..., X_n) to its simplest form.  

2.16 Bottleneck.  
Suppose that a (nonstationary) Markov chain starts in one of n states, necks down to k < n states, and then fans back to m > k states. Thus, X₁ → X₂ → X₃, that is,  

p(x₁, x₂, x₃) = p(x₁)p(x₂ | x₁)p(x₃ | x₂), for all x₁ ∈ {1, 2, ..., n}, x₂ ∈ {1, 2, ..., k}, x₃ ∈ {1, 2, ..., m}.  

(a) Show that the dependence of X₁ and X₃ is limited by the bottleneck by proving that I(X₁; X₃) ≤ log k.  

(b) Evaluate I(X₁; X₃) for k = 1, and conclude that no dependence can survive such a bottleneck.  

2.17 Pure randomness and bent coins.  
Let X₁, X₂, ..., X_n denote the outcomes of independent flips of a bent coin. Thus, Pr{X_i = 1} = p, Pr{X_i = 0} = 1 − p, where p is unknown. We wish to obtain a sequence Z₁, Z₂, ..., Z_K of fair coin flips from X₁, X₂, ..., X_n. Toward this end, let f: X^n → {0, 1}^* (where {0, 1}^* = {ε, 0, 1, 00, 01, ...} is the set of all finite-length binary sequences) be a mapping f(X₁, X₂, ..., X_n) = (Z₁, Z₂, ..., Z_K), where Z_i ∼ Bernoulli(1/2), and K may depend on (X₁, ..., X_n).  

In order that the sequence Z₁, Z₂, ..., appear to be fair coin flips, the map f from bent coin flips to fair flips must have the property that all 2^k sequences (Z₁, Z₂, ..., Z_k) of a given length k have equal probability (possibly 0), for k = 1, 2, .... For example, for n = 2, the map f(01) = 0, f(10) = 1, f(00) = f(11) = ε (the null string) has the property that Pr{Z₁ = 1 | K = 1} = Pr{Z₁ = 0 | K = 1} = 1/2.  

Give reasons for the following inequalities:  

nH(p)  
(a) = H(X₁, ..., X_n)  
(b) ≥ H(Z₁, Z₂, ..., Z_K, K)  
(c) = H(K) + H(Z₁, ..., Z_K | K)  
(d) = H(K) + E[K]  
(e) ≥ E[K].  

Thus, no more than nH(p) fair coin tosses can be derived from (X₁, ..., X_n), on the average. Exhibit a good map f on sequences of length 4.  

2.18 World Series.  
The World Series is a seven-game series that terminates as soon as either team wins four games. Let X be the random variable that represents the outcome of a World Series between teams A and B; possible values of X are AAAA, BABABAB, and BBBAAAA. Let Y be the number of games played, which ranges from 4 to 7. Assuming that A and B are equally matched and that the games are independent, calculate H(X), H(Y), H(Y | X), and H(X | Y).  

2.19 Infinite entropy.  
This problem shows that the entropy of a discrete random variable can be infinite. Let A = ∑_{n=2}^∞ [n (log₂ n)⁻¹]. [It is easy to show that A is finite by bounding the infinite sum by the integral of (x log₂ x)⁻¹.] Show that the integer-valued random variable X defined by Pr{X = n} = (A n log₂ n)⁻¹ for n = 2, 3, ... has H(X) = +∞.  

2.20 Run-length coding.  
Let X₁, X₂, ..., X_n be (possibly dependent) binary random variables. Suppose that one calculates the run lengths R = (R₁, R₂, ...) of this sequence (in order as they occur). For example, the sequence X = 0001100100 yields run lengths R = (3, 2, 2, 1, 2). Compare H(X₁, X₂, ..., X_n), H(R), and H(X_n, R). Show all equalities and inequalities, and bound all the differences.  

2.21 Markov’s inequality for probabilities.  
Let p(x) be a probability mass function. Prove, for all d ≥ 0, that  
Pr{p(X) ≤ d} log(1/d) ≤ H(X).  

2.22 Logical order of ideas.  
Ideas have been developed in order of need and then generalized if necessary. Reorder the following ideas, strongest first, implications following:  
(a) Chain rule for I(X₁, ..., X_n; Y), chain rule for D(p(x₁, ..., x_n) || q(x₁, x₂, ..., x_n)), and chain rule for H(X₁, X₂, ..., X_n).  
(b) D(f || g) ≥ 0, Jensen’s inequality, I(X; Y) ≥ 0.  

2.23 Conditional mutual information.  
Consider a sequence of n binary random variables X₁, X₂, ..., X_n. Each sequence with an even number of 1’s has probability 2^{-(n−1)}, and each sequence with an odd number of 1’s has probability 0. Find the mutual informations I(X₁; X₂), I(X₂; X₃ | X₁), ..., I(X_{n−1}; X_n | X₁, ..., X_{n−2}).  

2.24 Average entropy.  
Let H(p) = −p log₂ p − (1 − p) log₂(1 − p) be the binary entropy function.  

(a) Evaluate H(1/4) using the fact that log₂ 3 ≈ 1.584. (Hint: You may wish to consider an experiment with four equally likely outcomes, one of which is more interesting than the others.)  

(b) Calculate the average entropy H(p) when the probability p is chosen uniformly in the range 0 ≤ p ≤ 1.  

(c) (Optional) Calculate the average entropy H(p₁, p₂, p₃), where (p₁, p₂, p₃) is a uniformly distributed probability vector. Generalize to dimension n.  

2.25 Venn diagrams.  
There isn’t really a notion of mutual information common to three random variables. Here is one attempt at a definition: Using Venn diagrams, we can see that the mutual information common to three random variables X, Y, and Z can be defined by I(X; Y; Z) = I(X; Y) − I(X; Y | Z). This quantity is symmetric in X, Y, and Z, despite the preceding asymmetric definition. Unfortunately, I(X; Y; Z) is not necessarily nonnegative. Find X, Y, and Z such that I(X; Y; Z) < 0, and prove the following two identities:  

(a) I(X; Y; Z) = H(X, Y, Z) − H(X) − H(Y) − H(Z) + I(X; Y) + I(Y; Z) + I(Z; X).  

(b) I(X; Y; Z) = H(X, Y, Z) − H(X, Y) − H(Y, Z) − H(Z, X) + H(X) + H(Y) + H(Z).  

The first identity can be understood using the Venn diagram analogy for entropy and mutual information. The second identity follows easily from the first.  

2.26 Another proof of nonnegativity of relative entropy.  
In view of the fundamental nature of the result D(p || q) ≥ 0, we will give another proof.  

(a) Show that ln x ≤ x − 1 for 0 < x < ∞.  

(b) Justify the following steps:  
−D(p || q) = ∑_x p(x) ln q(x) / p(x)  
≤ ∑_x p(x) [q(x) / p(x) − 1]  
≤ 0.  

(c) What are the conditions for equality?  

2.27 Grouping rule for entropy.  
Let p = (p₁, p₂, ..., p_m) be a probability distribution on m elements (i.e., p_i ≥ 0 and ∑_{i=1}^m p_i = 1).  

Define a new distribution q on m − 1 elements as q₁ = p₁, q₂ = p₂, ..., q_{m−2} = p_{m−2}, and q_{m−1} = p_{m−1} + p_m [i.e., the distribution q is the same as p on {1, 2, ..., m − 2}, and the probability of the last element in q is the sum of the last two probabilities of p].  

Show that  
H(p) = H(q) + (p_{m−1} + p_m) H( p_{m−1} / (p_{m−1} + p_m), p_m / (p_{m−1} + p_m) ).  

2.28 Mixing increases entropy.  
Show that the entropy of the probability distribution (p₁, ..., p_i, ..., p_j, ..., p_m) is less than the entropy of the distribution (p₁, ..., (p_i + p_j)/2, ..., (p_i + p_j)/2, ..., p_m). Show that in general any transfer of probability that makes the distribution more uniform increases the entropy.  

2.29 Inequalities.  
Let X, Y, and Z be joint random variables. Prove the following inequalities and find conditions for equality.  

(a) H(X, Y | Z) ≥ H(X | Z).  

(b) I(X, Y; Z) ≥ I(X; Z).  

(c) H(X, Y, Z) − H(X, Y) ≤ H(X, Z) − H(X).  

(d) I(X; Z | Y) ≥ I(Z; Y | X) − I(Z; Y) + I(X; Z).  

2.30 Maximum entropy.  
Find the probability mass function p(x) that maximizes the entropy H(X) of a nonnegative integer-valued random variable X subject to the constraint  
E[X] = ∑_{n=0}^∞ n p(n) = A  
for a fixed value A > 0. Evaluate this maximum H(X).  

2.31 Conditional entropy.  
Under what conditions does H(X | g(Y)) = H(X | Y)?  

2.32 Fano.  
We are given the following joint distribution on (X, Y): [IMAGE].  

Perform the following actions:  
1. Remove unnecessary formatting, extra spaces, or line breaks.  
2. Fix broken sentences or paragraphs.  
3. Ensure consistent punctuation and capitalization.  
4. Remove any non-standard characters or encoding artifacts.  
5. Preserve the original meaning and structure of the text.  
6. Make the text more readable and well-formatted.  

Return only the cleaned text without additional explanations.



PROBLEMS  
51  
Let ˆX(Y) be an estimator for X (based on Y) and let Pe = Pr{ ˆX(Y) ≠ X}.  
(a) Find the minimum probability of error estimator ˆX(Y) and the associated Pe.  
(b) Evaluate Fano’s inequality for this problem and compare.  

2.33  
Fano’s inequality.  
Let Pr(X = i) = pi, i = 1, 2, ..., m, and let p1 ≥ p2 ≥ p3 ≥ ... ≥ pm. The minimal probability of error predictor of X is ˆX = 1, with resulting probability of error Pe = 1 − p1. Maximize H(p) subject to the constraint 1 − p1 = Pe to find a bound on Pe in terms of H. This is Fano’s inequality in the absence of conditioning.  

2.34  
Entropy of initial conditions.  
Prove that H(X0|Xn) is non-decreasing with n for any Markov chain.  

2.35  
Relative entropy is not symmetric.  
Let the random variable X have three possible outcomes {a, b, c}. Consider two distributions on this random variable:  

| Symbol | p(x) | q(x) |  
|--------|-------|-------|  
| a      | 1/3   | 2/3   |  
| b      | 1/4   | 1/3   |  
| c      | 1/4   | 1/3   |  

Calculate H(p), H(q), D(p||q), and D(q||p). Verify that in this case, D(p||q) ≠ D(q||p).  

2.36  
Symmetric relative entropy.  
Although, as Problem 2.35 shows, D(p||q) ≠ D(q||p) in general, there could be distributions for which equality holds. Give an example of two distributions p and q on a binary alphabet such that D(p||q) = D(q||p) (other than the trivial case p = q).  

2.37  
Relative entropy.  
Let X, Y, Z be three random variables with a joint probability mass function p(x, y, z). The relative entropy between the joint distribution and the product of the marginals is  

D(p(x, y, z)||p(x)p(y)p(z)) = E[log(p(x, y, z)/p(x)p(y)p(z))].  

Expand this in terms of entropies. When is this quantity zero?  

2.38  
The value of a question.  
Let X ∼ p(x), x = 1, 2, ..., m. We are given a set S ⊆ {1, 2, ..., m}. We ask whether X ∈ S and receive the answer  

Y = {  
1 if X ∈ S,  
0 if X ∉ S.  

Suppose that Pr{X ∈ S} = α. Find the decrease in uncertainty H(X) − H(X|Y). Apparently, any set S with a given α is as good as any other.  

2.39  
Entropy and pairwise independence.  
Let X, Y, Z be three binary Bernoulli(1/2) random variables that are pairwise independent; that is, I(X; Y) = I(X; Z) = I(Y; Z) = 0.  
(a) Under this constraint, what is the minimum value for H(X, Y, Z)?  
(b) Give an example achieving this minimum.  

2.40  
Discrete entropies.  
Let X and Y be two independent integer-valued random variables. Let X be uniformly distributed over {1, 2, ..., 8}, and let Pr{Y = k} = 2−k, k = 1, 2, 3, ....  
(a) Find H(X).  
(b) Find H(Y).  
(c) Find H(X + Y, X − Y).  

2.41  
Random questions.  
One wishes to identify a random object X ∼ p(x). A question Q ∼ r(q) is asked at random according to r(q). This results in a deterministic answer A = A(x, q) ∈ {a1, a2, ...}. Suppose that X and Q are independent. Then I(X; Q, A) is the uncertainty in X removed by the question–answer (Q, A).  
(a) Show that I(X; Q, A) = H(A|Q). Interpret.  
(b) Now suppose that two i.i.d. questions Q1, Q2 ∼ r(q) are asked, eliciting answers A1 and A2. Show that two questions are less valuable than twice a single question in the sense that I(X; Q1, A1, Q2, A2) ≤ 2I(X; Q1, A1).  

2.42  
Inequalities.  
Which of the following inequalities are generally ≥, =, or ≤? Label each with ≥, =, or ≤.  
(a) H(5X) vs. H(X)  
(b) I(g(X); Y) vs. I(X; Y)  
(c) H(X0|X−1) vs. H(X0|X−1, X1)  
(d) H(X, Y)/(H(X) + H(Y)) vs. 1  

2.43  
Mutual information of heads and tails.  
(a) Consider a fair coin flip. What is the mutual information between the top and bottom sides of the coin?  
(b) A six-sided fair die is rolled. What is the mutual information between the top side and the front face (the side most facing you)?  

2.44  
Pure randomness.  
We wish to use a three-sided coin to generate a fair coin toss. Let the coin X have probability mass function  

X = {  
A, pA,  
B, pB,  
C, pC,  

where pA, pB, pC are unknown.  
(a) How would you use two independent flips X1, X2 to generate (if possible) a Bernoulli(1/2) random variable Z?  
(b) What is the resulting maximum expected number of fair bits generated?  

2.45  
Finite entropy.  
Show that for a discrete random variable X ∈ {1, 2, ...}, if E log X < ∞, then H(X) < ∞.  

2.46  
Axiomatic definition of entropy (Difficult).  
If a sequence of symmetric functions Hm(p1, p2, ..., pm) satisfies the following properties:  
- Normalization: H2(1/2, 1/2) = 1,  
- Continuity: H2(p, 1 − p) is a continuous function of p,  
- Grouping: Hm(p1, p2, ..., pm) = Hm−1(p1 + p2, p3, ..., pm) + (p1 + p2)H2(p1/(p1+p2), p2/(p1+p2)),  

prove that Hm must be of the form  

Hm(p1, p2, ..., pm) = − ∑_{i=1}^m pi log pi, for m = 2, 3, ....  

2.47  
Entropy of a missorted file.  
A deck of n cards in order 1, 2, ..., n is provided. One card is removed at random, then replaced at random. What is the entropy of the resulting deck?  

2.48  
Sequence length.  
How much information does the length of a sequence give about the content of a sequence? Suppose that we consider a Bernoulli(1/2) process {Xi}. Stop the process when the first 1 appears. Let N designate this stopping time. Thus, XN is an element of the set of all finite-length binary sequences {0, 1}* = {0, 1, 00, 01, 10, 11, 000, ...}.  
(a) Find I(N; XN).  
(b) Find H(XN|N).  
(c) Find H(XN).  

Let’s now consider a different stopping time. For this part, again assume that Xi ∼ Bernoulli(1/2) but stop at time N = 6 with probability 1/3 and stop at time N = 12 with probability 2/3. Let this stopping time be independent of the sequence X1X2 · · · X12.  
(d) Find I(N; XN).  
(e) Find H(XN|N).  
(f) Find H(XN).  

HISTORICAL NOTES  
The concept of entropy was introduced in thermodynamics, where it was used to provide a statement of the second law of thermodynamics. Later, statistical mechanics provided a connection between thermodynamic entropy and the logarithm of the number of microstates in a macrostate of the system. This work was the crowning achievement of Boltzmann, who had the equation S = k ln W inscribed as the epitaph on his gravestone.  

In the 1930s, Hartley introduced a logarithmic measure of information for communication. His measure was essentially the logarithm of the alphabet size. Shannon was the first to define entropy and mutual information as defined in this chapter. Relative entropy was first defined by Kullback and Leibler. It is known under a variety of names, including the Kullback–Leibler distance, cross entropy, information divergence, and information for discrimination, and has been studied in detail by Csiszár and Amari.  

Many of the simple properties of these quantities were developed by Shannon. Fano’s inequality was proved in Fano. The notion of sufficient statistic was defined by Fisher, and the notion of the minimal sufficient statistic was introduced by Lehmann and Scheffé. The relationship of mutual information and sufficiency is due to Kullback. The relationship between information theory and thermodynamics has been discussed extensively by Brillouin and Jaynes.  

The physics of information is a vast new subject of inquiry spawned from statistical mechanics, quantum mechanics, and information theory. The key question is how information is represented physically. Quantum channel capacity (the logarithm of the number of distinguishable preparations of a physical system) and quantum data compression are well-defined problems with nice answers involving the von Neumann entropy. A new element of quantum information arises from the existence of quantum entanglement and the consequences (exhibited in Bell’s inequality) that the observed marginal distribution of physical events are not consistent with any joint distribution (no local realism). The fundamental text by Nielsen and Chuang develops the theory of quantum information and the quantum counterparts to many of the results in this book. There have also been attempts to determine whether there are any fundamental physical limits to computation, including work by Bennett and Bennett and Landauer.



**ASYMPTOTIC EQUIPARTITION PROPERTY THEOREM**

**Definition**  
The typical set \( A(n)_\varepsilon \) with respect to \( p(x) \) is the set of sequences \( (x_1, x_2, \ldots, x_n) \in \mathcal{X}^n \) with the property  
\[
2^{-n(H(X)+\varepsilon)} \leq p(x_1, x_2, \ldots, x_n) \leq 2^{-n(H(X)-\varepsilon)}.
\]  
As a consequence of the AEP, we can show that the set \( A(n)_\varepsilon \) has the following properties:

**Theorem 3.1.2**  
1. If \( (x_1, x_2, \ldots, x_n) \in A(n)_\varepsilon \), then  
\[
H(X) - \varepsilon \leq -\frac{1}{n} \log p(x_1, x_2, \ldots, x_n) \leq H(X) + \varepsilon.
\]  
2. \( \Pr(A(n)_\varepsilon) > 1 - \varepsilon \) for \( n \) sufficiently large.  
3. \( |A(n)_\varepsilon| \leq 2^{n(H(X)+\varepsilon)} \), where \( |A| \) denotes the number of elements in the set \( A \).  
4. \( |A(n)_\varepsilon| \geq (1 - \varepsilon)2^{n(H(X)-\varepsilon)} \) for \( n \) sufficiently large.  

Thus, the typical set has probability nearly 1, all elements of the typical set are nearly equiprobable, and the number of elements in the typical set is nearly \( 2^nH \).

**Proof:**  
The proof of property (1) is immediate from the definition of \( A(n)_\varepsilon \). The second property follows directly from Theorem 3.1.1, since the probability of the event \( (X_1, X_2, \ldots, X_n) \in A(n)_\varepsilon \) tends to 1 as \( n \to \infty \). Thus, for any \( \delta > 0 \), there exists an \( n_0 \) such that for all \( n \geq n_0 \), we have  
\[
\Pr\left(\left|-\frac{1}{n} \log p(X_1, X_2, \ldots, X_n) - H(X)\right| < \varepsilon\right) > 1 - \delta.
\]  
Setting \( \delta = \varepsilon \), we obtain the second part of the theorem. The identification of \( \delta = \varepsilon \) will conveniently simplify notation later.

To prove property (3), we write  
\[
1 = \sum_{x \in \mathcal{X}^n} p(x)
\]  
\[
\geq \sum_{x \in A(n)_\varepsilon} p(x)
\]  
\[
\geq \sum_{x \in A(n)_\varepsilon} 2^{-n(H(X)+\varepsilon)}
\]  
\[
= 2^{-n(H(X)+\varepsilon)} |A(n)_\varepsilon|,
\]  
where the second inequality follows from (3.6). Hence  
\[
|A(n)_\varepsilon| \leq 2^{n(H(X)+\varepsilon)}.
\]  

Finally, for sufficiently large \( n \), \( \Pr(A(n)_\varepsilon) > 1 - \varepsilon \), so that  
\[
1 - \varepsilon < \Pr(A(n)_\varepsilon)
\]  
\[
\leq \sum_{x \in A(n)_\varepsilon} 2^{-n(H(X)-\varepsilon)}
\]  
\[
= 2^{-n(H(X)-\varepsilon)} |A(n)_\varepsilon|,
\]  
where the second inequality follows from (3.6). Hence  
\[
|A(n)_\varepsilon| \geq (1 - \varepsilon)2^{n(H(X)-\varepsilon)},
\]  
which completes the proof of the properties of \( A(n)_\varepsilon \).

**CONSEQUENCES OF THE AEP: DATA COMPRESSION**  

Let \( X_1, X_2, \ldots, X_n \) be independent, identically distributed random variables drawn from the probability mass function \( p(x) \). We wish to find short descriptions for such sequences of random variables. We divide all sequences in \( \mathcal{X}^n \) into two sets: the typical set \( A(n)_\varepsilon \) and its complement, as shown in Figure 3.1.

Non-typical set  
Typical set  
∋  
∋  
A(n): \( 2^n(H + \varepsilon) \) elements  
n: \( |\mathcal{X}| \) elements  

**FIGURE 3.1. Typical sets and source coding.**

Non-typical set  
Typical set  
Description: \( n \log |\mathcal{X}| + 2 \) bits  
Description: \( n(H + \varepsilon) + 2 \) bits  
∋  

**FIGURE 3.2. Source code using the typical set.**

We order all elements in each set according to some order (e.g., lexicographic order). Then we can represent each sequence of \( A(n)_\varepsilon \) by giving the index of the sequence in the set. Since there are \( \leq 2^{n(H + \varepsilon)} \) sequences in \( A(n)_\varepsilon \), the indexing requires no more than \( n(H + \varepsilon) + 1 \) bits. We prefix all these sequences by a 0, giving a total length of \( \leq n(H + \varepsilon) + 2 \) bits to represent each sequence in \( A(n)_\varepsilon \) (see Figure 3.2). Similarly, we can index each sequence not in \( A(n)_\varepsilon \) by using no more than \( n \log |\mathcal{X}| + 1 \) bits. Prefixing these indices by 1, we have a code for all the sequences in \( \mathcal{X}^n \).

Note the following features of the above coding scheme:  
- The code is one-to-one and easily decodable. The initial bit acts as a flag bit to indicate the length of the codeword that follows.  
- We have used a brute-force enumeration of the atypical set \( A(n)_\varepsilon^c \) without taking into account the fact that the number of elements in \( A(n)_\varepsilon^c \) is less than the number of elements in \( \mathcal{X}^n \). Surprisingly, this is good enough to yield an efficient description.  
- The typical sequences have short descriptions of length \( \approx nH \).

Let \( l(x^n) \) be the length of the codeword corresponding to \( x^n \). If \( n \) is sufficiently large so that \( \Pr(A(n)_\varepsilon) \geq 1 - \varepsilon \), the expected length of the codeword is  

\[
E(l(X^n)) = \sum_{x^n} p(x^n) l(x^n)
\]  
\[
= \sum_{x^n \in A(n)_\varepsilon} p(x^n) l(x^n) + \sum_{x^n \in A(n)_\varepsilon^c} p(x^n) l(x^n)
\]  
\[
\leq \sum_{x^n \in A(n)_\varepsilon} p(x^n)(n(H + \varepsilon) + 2) + \sum_{x^n \in A(n)_\varepsilon^c} p(x^n)(n \log |\mathcal{X}| + 2)
\]  
\[
= \Pr(A(n)_\varepsilon)(n(H + \varepsilon) + 2) + \Pr(A(n)_\varepsilon^c)(n \log |\mathcal{X}| + 2)
\]  
\[
\leq n(H + \varepsilon) + \varepsilon n(\log |\mathcal{X}|) + 2
\]  
\[
= n(H + \varepsilon'),
\]  
where \( \varepsilon' = \varepsilon + \varepsilon \log |\mathcal{X}| + \frac{2}{n} \) can be made arbitrarily small by an appropriate choice of \( \varepsilon \) followed by an appropriate choice of \( n \). Hence we have proved the following theorem.

**Theorem 3.2.1**  
Let \( X^n \) be i.i.d. \( \sim p(x) \). Let \( \varepsilon > 0 \). Then there exists a code that maps sequences \( x^n \) of length \( n \) into binary strings such that the mapping is one-to-one (and therefore invertible) and  

\[
\frac{1}{n}E(l(X^n)) \leq H(X) + \varepsilon
\]  
for \( n \) sufficiently large.  

Thus, we can represent sequences \( X^n \) using \( nH(X) \) bits on the average.

**HIGH-PROBABILITY SETS AND THE TYPICAL SET**  

From the definition of \( A(n)_\varepsilon \), it is clear that \( A(n)_\varepsilon \) is a fairly small set that contains most of the probability. But from the definition, it is not clear whether it is the smallest such set. We will prove that the typical set has essentially the same number of elements as the smallest set, to first order in the exponent.

**Definition**  
For each \( n = 1, 2, \ldots \), let \( B(n)_\delta \subset \mathcal{X}^n \) be the smallest set with  

\[
\Pr(B(n)_\delta) \geq 1 - \delta.
\]  

We argue that \( B(n)_\delta \) must have significant intersection with \( A(n)_\varepsilon \) and therefore must have about as many elements. In Problem 3.3.11, we outline the proof of the following theorem.

**Theorem 3.3.1**  
Let \( X_1, X_2, \ldots, X_n \) be i.i.d. \( \sim p(x) \). For \( \delta < \frac{1}{2} \) and any \( \delta' > 0 \), if \( \Pr(B(n)_\delta) > 1 - \delta \), then  

\[
\frac{1}{n} \log |B(n)_\delta| > H - \delta'
\]  
for \( n \) sufficiently large.  

Thus, \( B(n)_\delta \) must have at least \( 2^nH \) elements, to first order in the exponent. But \( A(n)_\varepsilon \) has \( 2^n(H \pm \varepsilon) \) elements. Therefore, \( A(n)_\varepsilon \) is about the same size as the smallest high-probability set.

We will now define some new notation to express equality to first order in the exponent.

**Definition**  
The notation \( a_n \doteq b_n \) means  

\[
\lim_{n \to \infty} \frac{1}{n} \log \frac{a_n}{b_n} = 0.
\]  

Thus, \( a_n \doteq b_n \) implies that \( a_n \) and \( b_n \) are equal to the first order in the exponent.

We can now restate the above results: If \( \delta_n \to 0 \) and \( \varepsilon_n \to 0 \), then  

\[
|B(n)_\delta| \doteq |A(n)_\varepsilon| \doteq 2^nH.
\]  

To illustrate the difference between \( A(n)_\varepsilon \) and \( B(n)_\delta \), let us consider a Bernoulli sequence \( X_1, X_2, \ldots, X_n \) with parameter \( p = 0.9 \). The typical sequences in this case are the sequences in which the proportion of 1’s is close to 0.9. However, this does not include the most likely single sequence, which is the sequence of all 1’s. The set \( B(n)_\delta \) includes all the most probable sequences and therefore includes the sequence of all 1’s. Theorem 3.3.1 implies that \( A(n)_\varepsilon \) and \( B(n)_\delta \) must both contain the sequences that have about 90% 1’s, and the two sets are almost equal in size.

**SUMMARY**  

**AEP.** "Almost all events are almost equally surprising." Specifically, if \( X_1, X_2, \ldots \) are i.i.d. \( \sim p(x) \), then  

\[
-\frac{1}{n} \log p(X_1, X_2, \ldots, X_n) \to H(X) \text{ in probability}.
\]  

**Definition.** The typical set \( A(n)_\varepsilon \) is the set of sequences \( x_1, x_2, \ldots, x_n \) satisfying  

\[
2^{-n(H(X)+\varepsilon)} \leq p(x_1, x_2, \ldots, x_n) \leq 2^{-n(H(X)-\varepsilon)}.
\]  

**Properties of the typical set**  
1. If \( (x_1, x_2, \ldots, x_n) \in A(n)_\varepsilon \), then \( p(x_1, x_2, \ldots, x_n) = 2^{-n(H \pm \varepsilon)} \).  
2. \( \Pr(A(n)_\varepsilon) > 1 - \varepsilon \) for \( n \) sufficiently large.  
3. \( |A(n)_\varepsilon| \leq 2^{n(H(X)+\varepsilon)} \), where \( |A| \) denotes the number of elements in set \( A \).  

**Definition.** \( a_n \doteq b_n \) means  

\[
\lim_{n \to \infty} \frac{1}{n} \log \frac{a_n}{b_n} = 0.
\]  

**Smallest probable set.** Let \( X_1, X_2, \ldots, X_n \) be i.i.d. \( \sim p(x) \), and for \( \delta < \frac{1}{2} \), let \( B(n)_\delta \subset \mathcal{X}^n \) be the smallest set such that \( \Pr(B(n)_\delta) \geq 1 - \delta \). Then  

\[
|B(n)_\delta| \doteq 2^nH.
\]  

**PROBLEMS**  

**3.1 Markov’s inequality and Chebyshev’s inequality**  
(a) (Markov’s inequality) For any nonnegative random variable \( X \) and any \( t > 0 \), show that  

\[
\Pr\{X \geq t\} \leq \frac{EX}{t}.
\]  

Exhibit a random variable that achieves this inequality with equality.  

(b) (Chebyshev’s inequality) Let \( Y \) be a random variable with mean \( \mu \) and variance \( \sigma^2 \). By letting \( X = (Y - \mu)^2 \), show that  

\[
\Pr\{|Y - \mu| > \varepsilon\} \leq \frac{\sigma^2}{\varepsilon^2}.
\]  

(c) (Weak law of large numbers) Let \( Z_1, Z_2, \ldots, Z_n \) be a sequence of i.i.d. random variables with mean \( \mu \) and variance \( \sigma^2 \). Let \( \overline{Z}_n = \frac{1}{n} \sum_{i=1}^n Z_i \) be the sample mean. Show that  

\[
\Pr\left\{\left|\overline{Z}_n - \mu\right| > \varepsilon\right\} \leq \frac{\sigma^2}{n \varepsilon^2}.
\]  

Thus, \( \Pr\left\{\left|\overline{Z}_n - \mu\right| > \varepsilon\right\} \to 0 \) as \( n \to \infty \). This is known as the weak law of large numbers.  

**3.2 AEP and mutual information.**  
Let \( (X_i, Y_i) \) be i.i.d. \( \sim p(x, y) \). We form the log likelihood ratio of the hypothesis that \( X \) and \( Y \) are independent vs. the hypothesis that \( X \) and \( Y \) are dependent. What is the limit of  

\[
\frac{1}{n} \log \frac{p(X^n) p(Y^n)}{p(X^n, Y^n)}?
\]  

**3.3 Piece of cake.**  
A cake is sliced roughly in half, the largest piece being chosen each time, the other pieces discarded. We will assume that a random cut creates pieces of proportions  

\[
P = \left(\frac{2}{3}, \frac{1}{3}\right) \text{ with probability } \frac{3}{4} \quad \text{or} \quad \left(\frac{2}{5}, \frac{3}{5}\right) \text{ with probability } \frac{1}{4}.
\]  

Thus, for example, the first cut (and choice of largest piece) may result in a piece of size \( \frac{3}{5} \). Cutting and choosing from this piece might reduce it to size \( \frac{3}{5} \times \frac{2}{3} \) at time 2, and so on. How large, to first order in the exponent, is the piece of cake after \( n \) cuts?  

**3.4 AEP.**  
Let \( X_i \) be i.i.d. \( \sim p(x) \), \( x \in \{1, 2, \ldots, m\} \). Let \( \mu = EX \) and \( H = -\sum p(x) \log p(x) \). Let  

\[
A_n = \left\{x^n \in \mathcal{X}^n : \left|-\frac{1}{n} \log p(x^n) - H\right| \leq \varepsilon\right\},
\]  
\[
B_n = \left\{x^n \in \mathcal{X}^n : \left|\frac{1}{n} \sum_{i=1}^n X_i - \mu\right| \leq \varepsilon\right\}.
\]  

(a) Does \( \Pr\{X^n \in A_n\} \to 1 \)?  

(b) Does \( \Pr\{X^n \in A_n \cap B_n\} \to 1 \)?



--- Page 92 ---
66
ASYMPTOTIC EQUIPARTITION PROPERTY
(c) Show that |An ∩ Bn| ≤ 2n(H + ǫ) for all n.
(d) Show that |An ∩ Bn| ≥ 1 2 2n(H − ǫ) for n sufficiently large.
3.5
Sets defined by probabilities.
Let X1, X2, . . . be an i.i.d. sequence of discrete random variables with entropy H(X). Let Cn(t) = {xn ∈ Xn : p(xn) ≥ 2−nt} denote the subset of n-sequences with probabilities ≥ 2−nt.
(a) Show that |Cn(t)| ≤ 2nt.
(b) For what values of t does P({Xn ∈ Cn(t)}) → 1?
3.6
AEP-like limit.
Let X1, X2, . . . be i.i.d. drawn according to probability mass function p(x). Find
lim
n→∞ (p(X1, X2, . . . , Xn)) 1/n.
3.7
AEP and source coding.
A discrete memoryless source emits a sequence of statistically independent binary digits with probabilities p(1) = 0.005 and p(0) = 0.995. The digits are taken 100 at a time, and a binary codeword is provided for every sequence of 100 digits containing three or fewer 1’s.
(a) Assuming that all codewords are the same length, find the minimum length required to provide codewords for all sequences with three or fewer 1’s.
(b) Calculate the probability of observing a source sequence for which no codeword has been assigned.
(c) Use Chebyshev’s inequality to bound the probability of observing a source sequence for which no codeword has been assigned. Compare this bound with the actual probability computed in part (b).
3.8
Products.
Let X =



1, with probability 1/2,
2, with probability 1/4,
3, with probability 1/4.
Let X1, X2, . . . be drawn i.i.d. according to this distribution. Find the limiting behavior of the product (X1X2 · · · Xn) 1/n.

--- Page 93 ---
PROBLEMS
67
3.9
AEP.
Let X1, X2, . . . be independent, identically distributed random variables drawn according to the probability mass function p(x), x ∈ {1, 2, . . . , m}. Thus, p(x1, x2, . . . , xn) = ∏_{i=1}^n p(xi). We know that −1/n log p(X1, X2, . . . , Xn) → H(X) in probability. Let q(x1, x2, . . . , xn) = ∏_{i=1}^n q(xi), where q is another probability mass function on {1, 2, . . . , m}.
(a) Evaluate lim −1/n log q(X1, X2, . . . , Xn), where X1, X2, . . . are i.i.d. ∼ p(x).
(b) Now evaluate the limit of the log likelihood ratio 1/n log(q(X1, . . . , Xn)/p(X1, . . . , Xn)) when X1, X2, . . . are i.i.d. ∼ p(x). Thus, the odds favoring q are exponentially small when p is true.
3.10
Random box size.
An n-dimensional rectangular box with sides X1, X2, X3, . . . , Xn is to be constructed. The volume is Vn = ∏_{i=1}^n Xi. The edge length l of an n-cube with the same volume as the random box is l = Vn^{1/n}.
Let X1, X2, . . . be i.i.d. uniform random variables over the unit interval [0, 1]. Find lim_{n→∞} Vn^{1/n} and compare to (EVn)^{1/n}. Clearly, the expected edge length does not capture the idea of the volume of the box. The geometric mean, rather than the arithmetic mean, characterizes the behavior of products.
3.11
Proof of Theorem 3.3.1.
This problem shows that the size of the smallest “probable” set is about 2nH. Let X1, X2, . . . , Xn be i.i.d. ∼ p(x). Let B(n, δ) ⊂ Xn such that Pr(B(n, δ)) > 1 − δ. Fix ǫ < 1/2.
(a) Given any two sets A, B such that Pr(A) > 1 − ǫ1 and Pr(B) > 1 − ǫ2, show that Pr(A ∩ B) > 1 − ǫ1 − ǫ2. Hence, Pr(A(n, ǫ) ∩ B(n, δ)) ≥ 1 − ǫ − δ.
(b) Justify the steps in the chain of inequalities:
1 − ǫ − δ ≤ Pr(A(n, ǫ) ∩ B(n, δ)) 
= ∑_{A(n, ǫ) ∩ B(n, δ)} p(xn)
≤ ∑_{A(n, ǫ) ∩ B(n, δ)} 2^{-n(H−ǫ)}
= |A(n, ǫ) ∩ B(n, δ)| 2^{-n(H−ǫ)}
≤ |B(n, δ)| 2^{-n(H−ǫ)}.
(c) Complete the proof of the theorem.

--- Page 94 ---
68
ASYMPTOTIC EQUIPARTITION PROPERTY
3.12
Monotonic convergence of the empirical distribution.
Let p̂n denote the empirical probability mass function corresponding to X1, X2, . . . , Xn i.i.d. ∼ p(x), x ∈ X. Specifically, p̂n(x) = 1/n ∑_{i=1}^n I(Xi = x) is the proportion of times that Xi = x in the first n samples, where I is the indicator function.
(a) Show for X binary that E[D(p̂2n || p)] ≤ E[D(p̂n || p)]. Thus, the expected relative entropy “distance” from the empirical distribution to the true distribution decreases with sample size. (Hint: Write p̂2n = 1/2 p̂n + 1/2 p̂n’ and use the convexity of D.)
(b) Show for an arbitrary discrete X that E[D(p̂n || p)] ≤ E[D(p̂n−1 || p)]. (Hint: Write p̂n as the average of n empirical mass functions with each of the n samples deleted in turn.)
3.13
Calculation of typical set.
To clarify the notion of a typical set A(n, ǫ) and the smallest set of high probability B(n, δ), we will calculate the set for a simple example. Consider a sequence of i.i.d. binary random variables, X1, X2, . . . , Xn, where the probability that Xi = 1 is 0.6 (and therefore the probability that Xi = 0 is 0.4).
(a) Calculate H(X).
(b) With n = 25 and ǫ = 0.1, which sequences fall in the typical set A(n, ǫ)? What is the probability of the typical set? How many elements are there in the typical set? (This involves computation of a table of probabilities for sequences with k 1’s, 0 ≤ k ≤ 25, and finding those sequences that are in the typical set.)
(c) How many elements are there in the smallest set that has probability 0.9?
(d) How many elements are there in the intersection of the sets in parts (b) and (c)? What is the probability of this intersection?

--- Page 95 ---
HISTORICAL NOTES
69
k
n
k
n
k
n
k
p_k(1 − p)^{n−k}
−1/n log p(xn)
0
1
0.000000
1.321928
1
25
0.000000
1.298530
2
300
0.000000
1.275131
3
2300
0.000001
1.251733
4
12650
0.000007
1.204936
5
53130
0.000054
1.181537
6
177100
0.000227
1.158139
7
480700
0.001205
1.134740
8
1081575
0.003121
1.111342
9
2042975
0.013169
1.087943
10
3268760
0.021222
0.994349
11
4457400
0.077801
0.941146
12
5200300
0.075967
0.817748
13
5200300
0.267718
0.700755
14
4457400
0.146507
0.594349
15
3268760
0.575383
0.470951
16
2042975
0.151086
0.347552
17
1081575
0.846448
0.224154
18
480700
0.079986
0.127359
19
177100
0.970638
0.053958
20
53130
0.019891
0.017359
21
12650
0.997633
0.007161
22
2300
0.001937
0.003763
23
300
0.999950
0.001146
24
25
0.000047
0.000366
25
1
0.000003
0.000966
HISTORICAL NOTES
The asymptotic equipartition property (AEP) was first stated by Shannon in his original 1948 paper [472], where he proved the result for i.i.d. processes and stated the result for stationary ergodic processes. McMillan [384] and Breiman [74] proved the AEP for ergodic finite-alphabet sources. The result is now referred to as the AEP or the Shannon–McMillan–Breiman theorem. Chung [101] extended the theorem to the case of countable alphabets, and Moy [392], Perez [417], and Kieffer [312] proved the L1 convergence when {Xi} is continuous-valued and ergodic. Barron [34] and Orey [402] proved almost sure convergence for real-valued ergodic processes; a simple sandwich argument (Algoet and Cover [20]) will be used in Section 16.8 to prove the general AEP.

--- Page 96 ---

--- Page 97 ---
CHAPTER 4
ENTROPY RATES OF A STOCHASTIC PROCESS
The asymptotic equipartition property in Chapter 3 establishes that nH(X) bits suffice on the average to describe n independent and identically distributed random variables. But what if the random variables are dependent? In particular, what if the random variables form a stationary process? We will show, just as in the i.i.d. case, that the entropy H(X1, X2, . . . , Xn) grows (asymptotically) linearly with n at a rate H(X), which we will call the entropy rate of the process. The interpretation of H(X) as the best achievable data compression will await the analysis in Chapter 5.
4.1
MARKOV CHAINS
A stochastic process {Xi} is an indexed sequence of random variables. In general, there can be an arbitrary dependence among the random variables. The process is characterized by the joint probability mass functions Pr{(X1, X2, . . . , Xn) = (x1, x2, . . . , xn)} = p(x1, x2, . . . , xn), (x1, x2, . . . , xn) ∈ Xn for n = 1, 2, . . . .
Definition
A stochastic process is said to be stationary if the joint distribution of any subset of the sequence of random variables is invariant with respect to shifts in the time index; that is,
Pr{X1 = x1, X2 = x2, . . . , Xn = xn} = Pr{X1+l = x1, X2+l = x2, . . . , Xn+l = xn}
for every n and every shift l and for all x1, x2, . . . , xn ∈ X.
Elements of Information Theory, Second Edition,
By Thomas M. Cover and Joy A. Thomas
Copyright © 2006 John Wiley & Sons, Inc.
71

--- Page 98 ---
72
ENTROPY RATES OF A STOCHASTIC PROCESS
A simple example of a stochastic process with dependence is one in which each random variable depends only on the one preceding it and is conditionally independent of all the other preceding random variables. Such a process is said to be Markov.
Definition
A discrete stochastic process X1, X2, . . . is said to be a Markov chain or a Markov process if for n = 1, 2, . . . ,
Pr(Xn+1 = xn+1 | Xn = xn, Xn−1 = xn−1, . . . , X1 = x1) = Pr(Xn+1 = xn+1 | Xn = xn)
for all x1, x2, . . . , xn, xn+1 ∈ X.
In this case, the joint probability mass function of the random variables can be written as
p(x1, x2, . . . , xn) = p(x1)p(x2|x1)p(x3|x2) · · · p(xn|xn−1).
Definition
The Markov chain is said to be time-invariant if the conditional probability p(xn+1|xn) does not depend on n; that is, for n = 1, 2, . . . ,
Pr{Xn+1 = b | Xn = a} = Pr{X2 = b | X1 = a}
for all a, b ∈ X.
We will assume that the Markov chain is time-invariant unless otherwise stated.
If {Xi} is a Markov chain, Xn is called the state at time n. A time-invariant Markov chain is characterized by its initial state and a probability transition matrix P = [Pij], i, j ∈ {1, 2, . . . , m}, where Pij = Pr{Xn+1 = j | Xn = i}.
If it is possible to go with positive probability from any state of the Markov chain to any other state in a finite number of steps, the Markov chain is said to be irreducible. If the largest common factor of the lengths of different paths from a state to itself is 1, the Markov chain is said to be aperiodic.
If the probability mass function of the random variable at time n is p(xn), the probability mass function at time n + 1 is
p(xn+1) = ∑_{xn} p(xn)Pxn xn+1.
A distribution on the states such that the distribution at time n + 1 is the same as the distribution at time n is called a stationary distribution. The stationary distribution is so called because if the initial state of a Markov chain is drawn according to a stationary distribution, the Markov chain forms a stationary process.
If the finite-state Markov chain is irreducible and aperiodic, the stationary distribution is unique, and from any starting distribution, the distribution of Xn tends to the stationary distribution as n → ∞.
Example 4.1.1
Consider a two-state Markov chain with a probability transition matrix
P = [ [1 − α, α], [β, 1 − β] ]
as shown in Figure 4.1.
Let the stationary distribution be represented by a vector µ whose components are the stationary probabilities of states 1 and 2, respectively. Then the stationary probability can be found by solving the equation µP = µ or, more simply, by balancing probabilities. For the stationary distribution, the net probability flow across any cut set in the state transition graph is zero. Applying this to Figure 4.1, we obtain
µ1 α = µ2 β.
Since µ1 + µ2 = 1, the stationary distribution is
µ1 = β / (α + β),
µ2 = α / (α + β).
If the Markov chain has an initial state drawn according to the stationary distribution, the resulting process will be stationary. The entropy of the
State 1
State 2
1 − β
1 − α
β
α
FIGURE 4.1. Two-state Markov chain.

Perform the following actions:
1. Remove unnecessary formatting, extra spaces, or line breaks.
2. Fix broken sentences or paragraphs.
3. Ensure consistent punctuation and capitalization.
4. Remove any non-standard characters or encoding artifacts.
5. Preserve the original meaning and structure of the text.
6. Make the text more readable and well-formatted.

Return only the cleaned text without additional explanations.


Okay, so I need to help clean up this text. Let me read through it first to understand what's going on. It's about entropy rates of a stochastic process, which sounds like it's from a probability or information theory textbook. 

Looking at the text, I see some formatting issues. There are line breaks and extra spaces that don't seem necessary. For example, the equation (4.9) is split into multiple lines with some symbols I don't recognize, like  and . Those might be formatting artifacts from converting a PDF or something. I should remove those.

The text also has some broken sentences. For instance, the first part ends abruptly with "state Xn at time n is H(Xn) = H". I think that's supposed to be part of a sentence, so I'll fix that by combining the lines.

There are some inconsistent punctuations and capitalizations. I notice that "ENTROPY RATE" is in all caps, which might be a section header, so I'll make sure that's consistent. Also, some paragraphs start without proper capitalization, so I'll correct that.

Non-standard characters like  and  should be removed because they don't add any meaning. They might have been used for formatting in the original document but aren't needed here.

I also see that some equations are numbered, like (4.9), (4.10), etc. I should ensure those are correctly placed and formatted without any extra spaces or line breaks.

The examples provided, like the typewriter example, have some formatting issues too. I'll make sure each example is clearly separated and the equations are properly formatted without any unnecessary line breaks.

Overall, the goal is to make the text more readable. I'll go through each section, remove the unnecessary formatting, fix the sentences, and ensure consistent punctuation and capitalization. I'll also make sure the equations are correctly presented without any artifacts.

Let me start by addressing the first part. The equation (4.9) is split, so I'll combine it into a single line. Then, I'll fix the sentence that was cut off. Moving on to the next section about entropy rate, I'll ensure the definition is properly formatted and the examples are clear.

I should also check for any other broken sentences or paragraphs and fix them. Making sure that each paragraph flows well and that the text is easy to read without any distractions from formatting issues.

Finally, I'll review the entire text to ensure that all the changes I made preserve the original meaning and structure. It's important that the cleaned text is accurate and maintains the integrity of the content, just presented in a more readable format.
</think>

ENTROPY RATES OF A STOCHASTIC PROCESS

The state \( X_n \) at time \( n \) is \( H(X_n) = H \left( \frac{\beta}{\alpha + \beta} \right) \). However, this is not the rate at which entropy grows for \( H(X_1, X_2, \ldots, X_n) \). The dependence among the \( X_i \)'s will take a steady toll.

### ENTROPY RATE

If we have a sequence of \( n \) random variables, a natural question to ask is: How does the entropy of the sequence grow with \( n \)? We define the entropy rate as this rate of growth as follows.

**Definition**  
The entropy of a stochastic process \( \{X_i\} \) is defined by  
\[
H(X) = \lim_{n \to \infty} \frac{1}{n} H(X_1, X_2, \ldots, X_n)
\]  
when the limit exists.

We now consider some simple examples of stochastic processes and their corresponding entropy rates.

1. **Typewriter**  
   Consider the case of a typewriter that has \( m \) equally likely output letters. The typewriter can produce \( m^n \) sequences of length \( n \), all of them equally likely. Hence,  
   \[
   H(X_1, X_2, \ldots, X_n) = \log m^n
   \]  
   and the entropy rate is  
   \[
   H(X) = \log m \text{ bits per symbol}.
   \]

2. **Independent and Identically Distributed (i.i.d.) Random Variables**  
   If \( X_1, X_2, \ldots \) are i.i.d. random variables, then  
   \[
   H(X) = \lim_{n \to \infty} \frac{1}{n} H(X_1, X_2, \ldots, X_n) = \lim_{n \to \infty} \frac{1}{n} n H(X_1) = H(X_1),
   \]  
   which is what one would expect for the entropy rate per symbol.

3. **Independent but Not Identically Distributed Random Variables**  
   In this case,  
   \[
   H(X_1, X_2, \ldots, X_n) = \sum_{i=1}^n H(X_i),
   \]  
   but the \( H(X_i) \)'s are not all equal. We can choose a sequence of distributions on \( X_1, X_2, \ldots \) such that the limit of  
   \[
   \frac{1}{n} \sum_{i=1}^n H(X_i)
   \]  
   does not exist. An example of such a sequence is a random binary sequence.



ENTROPY RATE

4.2

ENTROPY RATE

75

where \( p_i = P(X_i = 1) \) is not constant but a function of \( i \), chosen carefully so that the limit in (4.10) does not exist. For example, let

\[
p_i = 
\begin{cases}
0.5 & \text{if } 2k < \log \log i \leq 2k + 1, \\
0 & \text{if } 2k + 1 < \log \log i \leq 2k + 2,
\end{cases}
\]

for \( k = 0, 1, 2, \ldots \). Then there are arbitrarily long stretches where \( H(X_i) = 1 \), followed by exponentially longer segments where \( H(X_i) = 0 \). Hence, the running average of the \( H(X_i) \) will oscillate between 0 and 1 and will not have a limit. Thus, \( H(X) \) is not defined for this process.

We can also define a related quantity for entropy rate:

\[
H'(X) = \lim_{n \to \infty} H(X_n | X_{n-1}, X_{n-2}, \ldots, X_1)
\]

when the limit exists.

The two quantities \( H(X) \) and \( H'(X) \) correspond to two different notions of entropy rate. The first is the per symbol entropy of the \( n \) random variables, and the second is the conditional entropy of the last random variable given the past. We now prove the important result that for stationary processes both limits exist and are equal.

**Theorem 4.2.1**  
For a stationary stochastic process, the limits in (4.10) and (4.14) exist and are equal:

\[
H(X) = H'(X).
\]

We first prove that \( \lim H(X_n | X_{n-1}, \ldots, X_1) \) exists.

**Theorem 4.2.2**  
For a stationary stochastic process, \( H(X_n | X_{n-1}, \ldots, X_1) \) is nonincreasing in \( n \) and has a limit \( H'(X) \).

**Proof**  
\( H(X_{n+1} | X_1, X_2, \ldots, X_n) \leq H(X_{n+1} | X_n, \ldots, X_2) \)

\[
= H(X_n | X_{n-1}, \ldots, X_1),
\]

where the inequality follows from the fact that conditioning reduces entropy and the equality follows from the stationarity of the process. Since \( H(X_n | X_{n-1}, \ldots, X_1) \) is a decreasing sequence of nonnegative numbers, it has a limit, \( H'(X) \).

ENTROPY RATES OF A STOCHASTIC PROCESS

76

We now use the following simple result from analysis.

**Theorem 4.2.3 (Cesàro mean)**  
If \( a_n \to a \) and \( b_n = \frac{1}{n} \sum_{i=1}^n a_i \), then \( b_n \to a \).

**Proof**  
(Informal outline). Since most of the terms in the sequence \( \{a_k\} \) are eventually close to \( a \), then \( b_n \), which is the average of the first \( n \) terms, is also eventually close to \( a \).

Formal Proof:  
Let \( \epsilon > 0 \). Since \( a_n \to a \), there exists a number \( N(\epsilon) \) such that \( |a_n - a| \leq \epsilon \) for all \( n \geq N(\epsilon) \). Hence,

\[
|b_n - a| = \left| \frac{1}{n} \sum_{i=1}^n (a_i - a) \right| \leq \frac{1}{n} \sum_{i=1}^n |a_i - a| \leq \frac{1}{n} \left( \sum_{i=1}^{N(\epsilon)} |a_i - a| + \sum_{i=N(\epsilon)+1}^n |a_i - a| \right) \leq \frac{1}{n} \sum_{i=1}^{N(\epsilon)} |a_i - a| + \frac{n - N(\epsilon)}{n} \epsilon
\]

\[
\leq \frac{1}{n} \sum_{i=1}^{N(\epsilon)} |a_i - a| + \epsilon
\]

for all \( n \geq N(\epsilon) \). Since the first term goes to 0 as \( n \to \infty \), we can make \( |b_n - a| \leq 2\epsilon \) by taking \( n \) large enough. Hence, \( b_n \to a \) as \( n \to \infty \).

ENTROPY RATES OF A STOCHASTIC PROCESS

77

The significance of the entropy rate of a stochastic process arises from the AEP for a stationary ergodic process. We prove the general AEP in Section 16.8, where we show that for any stationary ergodic process,

\[
-\frac{1}{n} \log p(X_1, X_2, \ldots, X_n) \to H(X)
\]

with probability 1. Using this, the theorems of Chapter 3 can easily be extended to a general stationary ergodic process. We can define a typical set in the same way as we did for the i.i.d. case in Chapter 3. By the same arguments, we can show that the typical set has a probability close to 1 and that there are about \( 2^{nH(X)} \) typical sequences of length \( n \), each with probability about \( 2^{-nH(X)} \). We can therefore represent the typical sequences of length \( n \) using approximately \( nH(X) \) bits. This shows the significance of the entropy rate as the average description length for a stationary ergodic process.

The entropy rate is well-defined for all stationary processes. The entropy rate is particularly easy to calculate for Markov chains.

**Markov Chains**  
For a stationary Markov chain, the entropy rate is

\[
H(X) = H'(X) = \lim H(X_n | X_{n-1}, \ldots, X_1) = \lim H(X_n | X_{n-1}) = H(X_2 | X_1),
\]

where the conditional entropy is calculated using the given stationary distribution. Recall that the stationary distribution \( \mu \) is the solution of the equations

\[
\mu_j = \sum_i \mu_i P_{ij}
\]

for all \( j \).

We express the conditional entropy explicitly in the following theorem.

**Theorem 4.2.4**  
Let \( \{X_i\} \) be a stationary Markov chain with stationary distribution \( \mu \) and transition matrix \( P \). Let \( X_1 \sim \mu \). Then the entropy rate is

\[
H(X) = -\sum_{i,j} \mu_i P_{ij} \log P_{ij}.
\]

**Example 4.2.1 (Two-state Markov chain)**  
The entropy rate of the two-state Markov chain in Figure 4.1 is

\[
H(X) = H(X_2 | X_1) = \frac{\beta}{\alpha + \beta} H(\alpha) + \frac{\alpha}{\alpha + \beta} H(\beta).
\]

**Remark**  
If the Markov chain is irreducible and aperiodic, it has a unique stationary distribution on the states, and any initial distribution tends to the stationary distribution as \( n \to \infty \). In this case, even though the initial distribution is not the stationary distribution, the entropy rate, which is defined in terms of long-term behavior, is \( H(X) \), as defined in (4.25) and (4.27).

ENTROPY RATE OF A RANDOM WALK ON A WEIGHTED GRAPH

4.3

As an example of a stochastic process, let us consider a random walk on a connected graph (Figure 4.2). Consider a graph with \( m \) nodes labeled \( \{1, 2, \ldots, m\} \), with weight \( W_{ij} \geq 0 \) on the edge joining node \( i \) to node \( j \). (The graph is assumed to be undirected, so that \( W_{ij} = W_{ji} \). We set \( W_{ij} = 0 \) if there is no edge joining nodes \( i \) and \( j \).)

A particle walks randomly from node to node in this graph. The random walk \( \{X_n\} \), \( X_n \in \{1, 2, \ldots, m\} \), is a sequence of vertices of the graph. Given \( X_n = i \), the next vertex \( j \) is chosen from among the nodes connected to node \( i \) with a probability proportional to the weight of the edge connecting \( i \) to \( j \). Thus, \( P_{ij} = \frac{W_{ij}}{\sum_k W_{ik}} \).

In this case, the stationary distribution has a surprisingly simple form, which we will guess and verify. The stationary distribution for this Markov chain assigns probability to node \( i \) proportional to the total weight of the edges emanating from node \( i \). Let

\[
W_i = \sum_j W_{ij}
\]

be the total weight of edges emanating from node \( i \), and let

\[
W = \sum_{i,j:j > i} W_{ij}
\]

be the sum of the weights of all the edges. Then \( \sum_i W_i = 2W \).

We now guess that the stationary distribution is

\[
\mu_i = \frac{W_i}{2W}.
\]

We verify that this is the stationary distribution by checking that \( \mu P = \mu \). Here

\[
\sum_i \mu_i P_{ij} = \sum_i \frac{W_i}{2W} \frac{W_{ij}}{W_i} = \sum_i \frac{W_{ij}}{2W} = \frac{W_j}{2W} = \mu_j.
\]

Thus, the stationary probability of state \( i \) is proportional to the weight of edges emanating from node \( i \). This stationary distribution has an interesting property of locality: It depends only on the total weight and the weight of edges connected to the node and hence does not change if the weights in some other part of the graph are changed while keeping the total weight constant. We can now calculate the entropy rate as

\[
H(X) = H(X_2 | X_1)
\]

\[
= -\sum_i \mu_i \sum_j P_{ij} \log P_{ij}
\]

\[
= -\sum_i \frac{W_i}{2W} \sum_j \frac{W_{ij}}{W_i} \log \frac{W_{ij}}{W_i}
\]

\[
= -\sum_i \sum_j \frac{W_{ij}}{2W} \log \frac{W_{ij}}{W_i}
\]

\[
= -\sum_i \sum_j \frac{W_{ij}}{2W} \log \frac{W_{ij}}{2W} + \sum_i \sum_j \frac{W_{ij}}{2W} \log \frac{W_i}{2W}
\]

\[
= H\left( \ldots, \frac{W_{ij}}{2W}, \ldots \right) - H\left( \ldots, \frac{W_i}{2W}, \ldots \right).
\]

If all the edges have equal weight, the stationary distribution puts weight \( \frac{E_i}{2E} \) on node \( i \), where \( E_i \) is the number of edges emanating from node \( i \) and \( E \) is the total number of edges in the graph. In this case, the entropy rate of the random walk is

\[
H(X) = \log(2E) - H\left( \ldots, \frac{E_1}{2E}, \frac{E_2}{2E}, \ldots, \frac{E_m}{2E} \right).
\]

This answer for the entropy rate is so simple that it is almost misleading. Apparently, the entropy rate, which is the average transition entropy, depends only on the entropy of the stationary distribution and the total number of edges.

**Example 4.3.1 (Random walk on a chessboard)**  
Let a king move at random on an 8 × 8 chessboard. The king has eight moves in the interior, five moves at the edges, and three moves at the corners. Using this and the preceding results, the stationary probabilities are, respectively, \( \frac{8}{420} \), \( \frac{5}{420} \), and \( \frac{3}{420} \), and the entropy rate is 0.92 log 8. The factor of 0.92 is due to edge effects; we would have an entropy rate of log 8 on an infinite chessboard.

Similarly, we can find the entropy rate of rooks (log 14 bits, since the rook always has 14 possible moves), bishops, and queens. The queen combines the moves of a rook and a bishop. Does the queen have more or less freedom than the pair?

**Remark**  
It is easy to see that a stationary random walk on a graph is time-reversible; that is, the probability of any sequence of states is the same forward or backward:

\[
Pr(X_1 = x_1, X_2 = x_2, \ldots, X_n = x_n) = Pr(X_n = x_1, X_{n-1} = x_2, \ldots, X_1 = x_n).
\]

Rather surprisingly, the converse is also true; that is, any time-reversible Markov chain can be represented as a random walk on an undirected weighted graph.

SECOND LAW OF THERMODYNAMICS

4.4

One of the basic laws of physics, the second law of thermodynamics, states that the entropy of an isolated system is nondecreasing. We now explore the relationship between the second law and the entropy function that we defined earlier in this chapter.

In statistical thermodynamics, entropy is often defined as the log of the number of microstates in the system. This corresponds exactly to our notion of entropy if all the states are equally likely. But why does entropy increase?

We model the isolated system as a Markov chain with transitions obeying the physical laws governing the system. Implicit in this assumption is the notion of an overall state of the system and the fact that knowing the present state, the future of the system is independent of the past. In such a system we can find four different interpretations of the second law. It may come as a shock to find that the entropy does not always increase. However, relative entropy always decreases.

1. Relative entropy \( D(\mu_n || \mu'_n) \) decreases with \( n \). Let \( \mu_n \) and \( \mu'_n \) be two probability distributions on the state space of a Markov chain at time \( n \), and let \( \mu_{n+1} \) and \( \mu'_{n+1} \) be the corresponding distributions at time \( n + 1 \). Let the corresponding joint mass functions be denoted by \( p \) and \( q \). Thus, \( p(x_n, x_{n+1}) = p(x_n) r(x_{n+1} | x_n) \) and \( q(x_n, x_{n+1}) = q(x_n) r(x_{n+1} | x_n) \), where \( r(\cdot | \cdot) \) is the probability transition function for the Markov chain. Then by the chain rule for relative entropy, we have two expansions:

\[
D(p(x_n, x_{n+1}) || q(x_n, x_{n+1})) = D(p(x_n) || q(x_n)) + D(p(x_{n+1} | x_n) || q(x_{n+1} | x_n)).
\]



ENTROPY RATES OF A STOCHASTIC PROCESS

---

### Page 8

ENTROPY RATES OF A STOCHASTIC PROCESS

= D(p(xn+1)||q(xn+1)) + D(p(xn|xn+1)||q(xn|xn+1)). Since both p and q are derived from the Markov chain, the conditional probability mass functions p(xn+1|xn) and q(xn+1|xn) are both equal to r(xn+1|xn), and hence D(p(xn+1|xn)||q(xn+1|xn)) = 0. Now using the nonnegativity of D(p(xn|xn+1)||q(xn|xn+1)) (Corollary to Theorem 2.6.3), we have

D(p(xn)||q(xn)) ≥ D(p(xn+1)||q(xn+1)) (4.44)

or

D(µn||µ'n) ≥ D(µn+1||µ'n+1). (4.45)

Consequently, the distance between the probability mass functions is decreasing with time n for any Markov chain.

An example of one interpretation of the preceding inequality is to suppose that the tax system for the redistribution of wealth is the same in Canada and in England. Then if µn and µ'n represent the distributions of wealth among people in the two countries, this inequality shows that the relative entropy distance between the two distributions decreases with time. The wealth distributions in Canada and England become more similar.

2. Relative entropy D(µn||µ) between a distribution µn on the states at time n and a stationary distribution µ decreases with n. In (4.45), µ'n is any distribution on the states at time n. If we let µ'n be any stationary distribution µ, the distribution µ'n+1 at the next time is also equal to µ. Hence,

D(µn||µ) ≥ D(µn+1||µ), (4.46)

which implies that any state distribution gets closer and closer to each stationary distribution as time passes. The sequence D(µn||µ) is a monotonically nonincreasing nonnegative sequence and must therefore have a limit. The limit is zero if the stationary distribution is unique, but this is more difficult to prove.

3. Entropy increases if the stationary distribution is uniform. In general, the fact that the relative entropy decreases does not imply that the entropy increases. A simple counterexample is provided by any Markov chain with a nonuniform stationary distribution. If we start

---

### Page 9

4.4

SECOND LAW OF THERMODYNAMICS 83

this Markov chain from the uniform distribution, which already is the maximum entropy distribution, the distribution will tend to the stationary distribution, which has a lower entropy than the uniform. Here, the entropy decreases with time.

If, however, the stationary distribution is the uniform distribution, we can express the relative entropy as

D(µn||µ) = log |X| − H(µn) = log |X| − H(Xn). (4.47)

In this case, the monotonic decrease in relative entropy implies a monotonic increase in entropy. This is the explanation that ties in most closely with statistical thermodynamics, where all the microstates are equally likely. We now characterize processes having a uniform stationary distribution.

**Definition**

A probability transition matrix [Pij], Pij = Pr{Xn+1 = j|Xn = i}, is called doubly stochastic if

\[
\sum_{i} P_{ij} = 1, \quad j = 1, 2, \ldots, \quad \sum_{j} P_{ij} = 1, \quad i = 1, 2, \ldots. (4.48, 4.49)
\]

**Remark**

The uniform distribution is a stationary distribution of P if and only if the probability transition matrix is doubly stochastic (see Problem 4.1).

4. The conditional entropy H(Xn|X1) increases with n for a stationary Markov process. If the Markov process is stationary, H(Xn) is constant. So the entropy is nonincreasing. However, we will prove that H(Xn|X1) increases with n. Thus, the conditional uncertainty of the future increases. We give two alternative proofs of this result.

First, we use the properties of entropy,

H(Xn|X1) ≥ H(Xn|X1, X2) (conditioning reduces entropy) (4.50)

= H(Xn|X2) (by Markovity) (4.51)

= H(Xn−1|X1) (by stationarity). (4.52)

---

### Page 10

84

ENTROPY RATES OF A STOCHASTIC PROCESS

Alternatively, by an application of the data-processing inequality to the Markov chain X1 → Xn−1 → Xn, we have

I(X1; Xn−1) ≥ I(X1; Xn). (4.53)

Expanding the mutual informations in terms of entropies, we have

H(Xn−1) − H(Xn−1|X1) ≥ H(Xn) − H(Xn|X1). (4.54)

By stationarity, H(Xn−1) = H(Xn), and hence we have

H(Xn−1|X1) ≤ H(Xn|X1). (4.55)

[These techniques can also be used to show that H(X0|Xn) is increasing in n for any Markov chain.]

5. Shuffles increase entropy. If T is a shuffle (permutation) of a deck of cards and X is the initial (random) position of the cards in the deck, and if the choice of the shuffle T is independent of X, then

H(TX) ≥ H(X), (4.56)

where TX is the permutation of the deck induced by the shuffle T on the initial permutation X. Problem 4.3 outlines a proof.

---

### Page 11

4.5

FUNCTIONS OF MARKOV CHAINS 85

Here is an example that can be very difficult if done the wrong way. It illustrates the power of the techniques developed so far. Let X1, X2, ..., Xn, ... be a stationary Markov chain, and let Yi = φ(Xi) be a process each term of which is a function of the corresponding state in the Markov chain. What is the entropy rate H(Y)? Such functions of Markov chains occur often in practice. In many situations, one has only partial information about the state of the system. It would simplify matters greatly if Y1, Y2, ..., Yn also formed a Markov chain, but in many cases, this is not true. Since the Markov chain is stationary, so is Y1, Y2, ..., Yn, and the entropy rate is well-defined. However, if we wish to compute H(Y), we might compute H(Yn|Yn−1, ..., Y1) for each n and find the limit. Since the convergence can be arbitrarily slow, we will never know how close we are to the limit. (We can’t look at the change between the values at n and n + 1, since this difference may be small even when we are far away from the limit—consider, for example, 1/n.)

---

### Page 12

86

ENTROPY RATES OF A STOCHASTIC PROCESS

It would be useful computationally to have upper and lower bounds converging to the limit from above and below. We can halt the computation when the difference between upper and lower bounds is small, and we will then have a good estimate of the limit.

We already know that H(Yn|Yn−1, ..., Y1) converges monotonically to H(Y) from above. For a lower bound, we will use H(Yn|Yn−1, ..., Y1, X1). This is a neat trick based on the idea that X1 contains as much information about Yn as Y1, Y0, Y−1, .... 

**Lemma 4.5.1**

H(Yn|Yn−1, ..., Y2, X1) ≤ H(Y). (4.57)

**Proof:**

We have for k = 1, 2, ...,

H(Yn|Yn−1, ..., Y2, X1)

(a) = H(Yn|Yn−1, ..., Y2, Y1, X1) (4.58)

(b) = H(Yn|Yn−1, ..., Y1, X1, X0, X−1, ..., X−k) (4.59)

(c) = H(Yn|Yn−1, ..., Y1, X1, X0, X−1, ..., X−k, Y0, ..., Y−k) (4.60)

(d) ≤ H(Yn|Yn−1, ..., Y1, Y0, ..., Y−k) (4.61)

(e) = H(Yn+k+1|Yn+k, ..., Y1), (4.62)

where (a) follows from that fact that Y1 is a function of X1, and (b) follows from the Markovity of X, (c) follows from the fact that Yi is a function of Xi, (d) follows from the fact that conditioning reduces entropy, and (e) follows by stationarity. Since the inequality is true for all k, it is true in the limit. Thus,

H(Yn|Yn−1, ..., Y1, X1) ≤ lim_{k→∞} H(Yn+k+1|Yn+k, ..., Y1) (4.63)

= H(Y). (4.64)

---

### Page 13

87

**Lemma 4.5.2**

H(Yn|Yn−1, ..., Y1) − H(Yn|Yn−1, ..., Y1, X1) → 0. (4.65)

**Proof:**

The interval length can be rewritten as

H(Yn|Yn−1, ..., Y1) − H(Yn|Yn−1, ..., Y1, X1)

= I(X1; Yn|Yn−1, ..., Y1). (4.66)

By the properties of mutual information,

I(X1; Y1, Y2, ..., Yn) ≤ H(X1), (4.67)

and I(X1; Y1, Y2, ..., Yn) increases with n. Thus, lim I(X1; Y1, Y2, ..., Yn) exists and

lim_{n→∞} I(X1; Y1, Y2, ..., Yn) ≤ H(X1). (4.68)

By the chain rule,

H(X) ≥ lim_{n→∞} I(X1; Y1, Y2, ..., Yn) (4.69)

= lim_{n→∞} \sum_{i=1}^{n} I(X1; Yi|Yi−1, ..., Y1) (4.70)

= \sum_{i=1}^{∞} I(X1; Yi|Yi−1, ..., Y1). (4.71)

Since this infinite sum is finite and the terms are nonnegative, the terms must tend to 0; that is,

lim I(X1; Yn|Yn−1, ..., Y1) = 0, (4.72)

which proves the lemma.

---

### Page 14

88

ENTROPY RATES OF A STOCHASTIC PROCESS

Combining Lemmas 4.5.1 and 4.5.2, we have the following theorem.

**Theorem 4.5.1**

If X1, X2, ..., Xn form a stationary Markov chain and Yi = φ(Xi), then

H(Yn|Yn−1, ..., Y1, X1) ≤ H(Y) ≤ H(Yn|Yn−1, ..., Y1) (4.73)

and

lim_{n→∞} H(Yn|Yn−1, ..., Y1, X1) = H(Y) = lim_{n→∞} H(Yn|Yn−1, ..., Y1). (4.74)

In general, we could also consider the case where Yi is a stochastic function (as opposed to a deterministic function) of Xi. Consider a Markov process X1, X2, ..., Xn and define a new process Y1, Y2, ..., Yn, where each Yi is drawn according to p(yi|xi), conditionally independent of all the other Xj, j ≠ i; that is,

p(xn, yn) = p(x1) \prod_{i=1}^{n−1} p(xi+1|xi) \prod_{i=1}^{n} p(yi|xi). (4.75)

Such a process, called a hidden Markov model (HMM), is used extensively in speech recognition, handwriting recognition, and so on. The same argument as that used above for functions of a Markov chain carry over to hidden Markov models, and we can lower bound the entropy rate of a hidden Markov model by conditioning it on the underlying Markov state. The details of the argument are left to the reader.

---

### Summary

Entropy rate. Two definitions of entropy rate for a stochastic process are

H(X) = lim_{n→∞} \frac{1}{n} H(X1, X2, ..., Xn), (4.76)

H'(X) = lim_{n→∞} H(Xn|Xn−1, Xn−2, ..., X1). (4.77)

For a stationary stochastic process,

H(X) = H'(X). (4.78)

Entropy rate of a stationary Markov chain

H(X) = − \sum_{ij} µ_i P_{ij} log P_{ij}. (4.79)

Second law of thermodynamics. For a Markov chain:

1. Relative entropy D(µn||µ'n) decreases with time.

2. Relative entropy D(µn||µ) between a distribution and the stationary distribution decreases with time.

3. Entropy H(Xn) increases if the stationary distribution is uniform.

4. The conditional entropy H(Xn|X1) increases with time for a stationary Markov chain.

5. The conditional entropy H(X0|Xn) of the initial condition X0 increases for any Markov chain.

Functions of a Markov chain. If X1, X2, ..., Xn form a stationary Markov chain and Yi = φ(Xi), then

H(Yn|Yn−1, ..., Y1, X1) ≤ H(Y) ≤ H(Yn|Yn−1, ..., Y1) (4.80)

and

lim_{n→∞} H(Yn|Yn−1, ..., Y1, X1) = H(Y) = lim_{n→∞} H(Yn|Yn−1, ..., Y1). (4.81)

---

### Problems

4.1 Doubly stochastic matrices.

An n × n matrix P = [Pij] is said to be doubly stochastic if Pij ≥ 0 and \sum_{j} Pij = 1 for all i and \sum_{i} Pij = 1 for all j. An n × n matrix P is said to be a permutation matrix if it is doubly stochastic and there is precisely one Pij = 1 in each row and each column. It can be shown that every doubly stochastic matrix can be written as the convex combination of permutation matrices.

(a) Let at = (a1, a2, ..., an), ai ≥ 0, \sum ai = 1, be a probability vector. Let b = aP, where P is doubly stochastic. Show that b is a probability vector and that H(b1, b2, ..., bn) ≥ H(a1, a2, ..., an). Thus, stochastic mixing increases entropy.

(b) Show that a stationary distribution µ for a doubly stochastic matrix P is the uniform distribution.

(c) Conversely, prove that if the uniform distribution is a stationary distribution for a Markov transition matrix P, then P is doubly stochastic.

4.2 Time’s arrow.

Let {Xi}_{i=−∞}^{∞} be a stationary stochastic process. Prove that

H(X0|X−1, X−2, ..., X−n) = H(X0|X1, X2, ..., Xn).



PROBLEMS

89  
In other words, the present has a conditional entropy given the past equal to the conditional entropy given the future. This is true even though it is quite easy to concoct stationary random processes for which the flow into the future looks quite different from the flow into the past. That is, one can determine the direction of time by looking at a sample function of the process. Nonetheless, given the present state, the conditional uncertainty of the next symbol in the future is equal to the conditional uncertainty of the previous symbol in the past.

4.3  
Shuffles increase entropy. Argue that for any distribution on shuffles \( T \) and any distribution on card positions \( X \) that  
\[
H(T, X) \geq H(T, X | T)  
\]  
\[
= H(T^{-1}T, X | T)  
\]  
\[
= H(X | T)  
\]  
\[
= H(X)  
\]  
if \( X \) and \( T \) are independent.

4.4  
Second law of thermodynamics. Let \( X_1, X_2, X_3, \ldots \) be a stationary first-order Markov chain. In Section 4.4, it was shown that \( H(X_n | X_1) \geq H(X_{n-1} | X_1) \) for \( n = 2, 3, \ldots \). Thus, conditional uncertainty about the future grows with time. This is true although the unconditional uncertainty \( H(X_n) \) remains constant. However, show by example that \( H(X_n | X_1 = x_1) \) does not necessarily grow with \( n \) for every \( x_1 \).

4.5  
Entropy of a random tree. Consider the following method of generating a random tree with \( n \) nodes. First, expand the root node. Then, expand one of the two terminal nodes at random. At time \( k \), choose one of the \( k-1 \) terminal nodes according to a uniform distribution and expand it. Continue until \( n \) terminal nodes have been generated. Thus, a sequence leading to a five-node tree might look like this:  

Surprisingly, the following method of generating random trees yields the same probability distribution on trees with \( n \) terminal nodes. First, choose an integer \( N_1 \) uniformly distributed on \( \{1, 2, \ldots, n-1\} \). We then have the picture:  

\[
N_1 \quad n - N_1  
\]  

Then choose an integer \( N_2 \) uniformly distributed over \( \{1, 2, \ldots, N_1 - 1\} \), and independently choose another integer \( N_3 \) uniformly over \( \{1, 2, \ldots, (n - N_1) - 1\} \). The picture is now:  

\[
N_2 \quad N_3 \quad n - N_1 - N_3 \quad N_1 - N_2  
\]  

Continue the process until no further subdivision can be made. (The equivalence of these two tree generation schemes follows, for example, from Polya’s urn model.)  

Now let \( T_n \) denote a random \( n \)-node tree generated as described. The probability distribution on such trees seems difficult to describe, but we can find the entropy of this distribution in recursive form. First, some examples. For \( n = 2 \), we have only one tree. Thus, \( H(T_2) = 0 \). For \( n = 3 \), we have two equally probable trees:  

Thus, \( H(T_3) = \log 2 \). For \( n = 4 \), we have five possible trees, with probabilities \( \frac{1}{3}, \frac{1}{6}, \frac{1}{6}, \frac{1}{6}, \frac{1}{6} \).  

Now for the recurrence relation. Let \( N_1(T_n) \) denote the number of terminal nodes of \( T_n \) in the right half of the tree. Justify each of the steps in the following:  

\[
H(T_n)  
\]  
(a)  
\[
= H(N_1, T_n)  
\]  
(4.86)  
(b)  
\[
= H(N_1) + H(T_n | N_1)  
\]  
(4.87)  
(c)  
\[
= \log(n - 1) + H(T_n | N_1)  
\]  
(4.88)  
(d)  
\[
= \log(n - 1) + \frac{1}{n - 1} \sum_{k=1}^{n - 1} (H(T_k) + H(T_{n - k}))  
\]  
(4.89)  
(e)  
\[
= \log(n - 1) + \frac{2}{n - 1} \sum_{k=1}^{n - 1} H(T_k)  
\]  
(4.90)  
\[
= \log(n - 1) + \frac{2}{n - 1} \sum_{k=1}^{n - 1} H_k  
\]  
(4.91)  
(f) Use this to show that  
\[
(n - 1)H_n = nH_{n - 1} + (n - 1) \log(n - 1) - (n - 2) \log(n - 2)  
\]  
(4.92)  
or  
\[
H_n = \frac{n}{n - 1}H_{n - 1} + c_n  
\]  
(4.93)  
for appropriately defined \( c_n \). Since \( \sum c_n = c < \infty \), you have proved that \( \frac{1}{n}H(T_n) \) converges to a constant. Thus, the expected number of bits necessary to describe the random tree \( T_n \) grows linearly with \( n \).

4.6  
Monotonicity of entropy per element. For a stationary stochastic process \( X_1, X_2, \ldots, X_n \), show that:  
(a)  
\[
\frac{1}{n}H(X_1, X_2, \ldots, X_n) \leq \frac{1}{n - 1}H(X_1, X_2, \ldots, X_{n - 1})  
\]  
(4.94)  
(b)  
\[
\frac{1}{n}H(X_1, X_2, \ldots, X_n) \geq H(X_n | X_{n - 1}, \ldots, X_1)  
\]  
(4.95)  

4.7  
Entropy rates of Markov chains:  
(a) Find the entropy rate of the two-state Markov chain with transition matrix  
\[
P = \begin{pmatrix}  
1 - p_{01} & p_{01} \\  
p_{10} & 1 - p_{10}  
\end{pmatrix}  
\]  
(b) What values of \( p_{01}, p_{10} \) maximize the entropy rate?  
(c) Find the entropy rate of the two-state Markov chain with transition matrix  
\[
P = \begin{pmatrix}  
1 - p & p \\  
1 & 0  
\end{pmatrix}  
\]  
(d) Find the maximum value of the entropy rate of the Markov chain of part (c). We expect that the maximizing value of \( p \) should be less than \( \frac{1}{2} \), since the 0 state permits more information to be generated than the 1 state.  
(e) Let \( N(t) \) be the number of allowable state sequences of length \( t \) for the Markov chain of part (c). Find \( N(t) \) and calculate \( H_0 = \lim_{t \to \infty} \frac{1}{t} \log N(t) \). [Hint: Find a linear recurrence that expresses \( N(t) \) in terms of \( N(t - 1) \) and \( N(t - 2) \). Why is \( H_0 \) an upper bound on the entropy rate of the Markov chain? Compare \( H_0 \) with the maximum entropy found in part (d).]

4.8  
Maximum entropy process. A discrete memoryless source has the alphabet \( \{1, 2\} \), where the symbol 1 has duration 1 and the symbol 2 has duration 2. The probabilities of 1 and 2 are \( p_1 \) and \( p_2 \), respectively. Find the value of \( p_1 \) that maximizes the source entropy per unit time \( H(X) = \frac{H(X)}{E[T]} \). What is the maximum value \( H(X) \)?

4.9  
Initial conditions. Show, for a Markov chain, that  
\[
H(X_0 | X_n) \geq H(X_0 | X_{n - 1})  
\]  
Thus, initial conditions \( X_0 \) become more difficult to recover as the future \( X_n \) unfolds.

4.10  
Pairwise independence. Let \( X_1, X_2, \ldots, X_{n - 1} \) be i.i.d. random variables taking values in \( \{0, 1\} \), with \( \Pr\{X_i = 1\} = \frac{1}{2} \). Let \( X_n = 1 \) if \( \sum_{i=1}^{n - 1} X_i \) is odd and \( X_n = 0 \) otherwise. Let \( n \geq 3 \).  
(a) Show that \( X_i \) and \( X_j \) are independent for \( i \neq j \), \( i, j \in \{1, 2, \ldots, n\} \).  
(b) Find \( H(X_i, X_j) \) for \( i \neq j \).  
(c) Find \( H(X_1, X_2, \ldots, X_n) \). Is this equal to \( nH(X_1) \)?

4.11  
Stationary processes. Let \( \ldots, X_{-1}, X_0, X_1, \ldots \) be a stationary (not necessarily Markov) stochastic process. Which of the following statements are true? Prove or provide a counterexample.  
(a) \( H(X_n | X_0) = H(X_{-n} | X_0) \).  
(b) \( H(X_n | X_0) \geq H(X_{n - 1} | X_0) \).  
(c) \( H(X_n | X_1, X_2, \ldots, X_{n - 1}, X_{n + 1}) \) is nonincreasing in \( n \).  
(d) \( H(X_n | X_1, \ldots, X_{n - 1}, X_{n + 1}, \ldots, X_{2n}) \) is nonincreasing in \( n \).

4.12  
Entropy rate of a dog looking for a bone. A dog walks on the integers, possibly reversing direction at each step with probability \( p = 0.1 \). Let \( X_0 = 0 \). The first step is equally likely to be positive or negative. A typical walk might look like this:  
\[
(X_0, X_1, \ldots) = (0, -1, -2, -3, -4, -3, -2, -1, 0, 1, \ldots)  
\]  
(a) Find \( H(X_1, X_2, \ldots, X_n) \).  
(b) Find the entropy rate of the dog.  
(c) What is the expected number of steps that the dog takes before reversing direction?

4.13  
The past has little to say about the future. For a stationary stochastic process \( X_1, X_2, \ldots, X_n, \ldots \), show that  
\[
\lim_{n \to \infty} \frac{1}{2n}I(X_1, X_2, \ldots, X_n; X_{n + 1}, X_{n + 2}, \ldots, X_{2n}) = 0  
\]  
(4.96)  
Thus, the dependence between adjacent \( n \)-blocks of a stationary process does not grow linearly with \( n \).

4.14  
Functions of a stochastic process:  
(a) Consider a stationary stochastic process \( X_1, X_2, \ldots, X_n \), and let \( Y_1, Y_2, \ldots, Y_n \) be defined by  
\[
Y_i = \phi(X_i), \quad i = 1, 2, \ldots  
\]  
for some function \( \phi \). Prove that  
\[
H(Y) \leq H(X)  
\]  
(4.98)  
(b) What is the relationship between the entropy rates \( H(Z) \) and \( H(X) \) if  
\[
Z_i = \psi(X_i, X_{i + 1}), \quad i = 1, 2, \ldots  
\]  
(4.99)  
for some function \( \psi \)?

4.15  
Entropy rate. Let \( \{X_i\} \) be a discrete stationary stochastic process with entropy rate \( H(X) \). Show that  
\[
\frac{1}{n}H(X_n, \ldots, X_1 | X_0, X_{-1}, \ldots, X_{-k}) \to H(X)  
\]  
(4.100)  
for \( k = 1, 2, \ldots \).

4.16  
Entropy rate of constrained sequences. In magnetic recording, the mechanism of recording and reading the bits imposes constraints on the sequences of bits that can be recorded. For example, to ensure proper synchronization, it is often necessary to limit the length of runs of 0’s between two 1’s. Also, to reduce intersymbol interference, it may be necessary to require at least one 0 between any two 1’s. We consider a simple example of such a constraint. Suppose that we are required to have at least one 0 and at most two 0’s between any pair of 1’s in a sequence. Thus, sequences like 101001 and 0101001 are valid sequences, but 0110010 and 0000101 are not. We wish to calculate the number of valid sequences of length \( n \).  
(a) Show that the set of constrained sequences is the same as the set of allowed paths on the following state diagram:  
(b) Let \( \Xi_i(n) \) be the number of valid paths of length \( n \) ending at state \( i \). Argue that \( \Xi(n) = [\Xi_1(n) \ \Xi_2(n) \ \Xi_3(n)]^t \) satisfies the following recursion:  
\[
\begin{pmatrix}  
\Xi_1(n) \\  
\Xi_2(n) \\  
\Xi_3(n)  
\end{pmatrix}  
=  
\begin{pmatrix}  
0 & 1 & 1 \\  
1 & 0 & 0 \\  
0 & 1 & 0  
\end{pmatrix}  
\begin{pmatrix}  
\Xi_1(n - 1) \\  
\Xi_2(n - 1) \\  
\Xi_3(n - 1)  
\end{pmatrix}  
\]  
(4.101)  
with initial conditions \( \Xi(1) = [1 \ 1 \ 0]^t \).  
(c) Let  
\[
A = \begin{pmatrix}  
0 & 1 & 1 \\  
1 & 0 & 0 \\  
0 & 1 & 0  
\end{pmatrix}  
\]  
(4.102)  
Then we have by induction  
\[
\Xi(n) = A\Xi(n - 1) = A^2\Xi(n - 2) = \cdots = A^{n - 1}\Xi(1)  
\]  
(4.103)  
Using the eigenvalue decomposition of \( A \) for the case of distinct eigenvalues, we can write \( A = U^{-1}\Lambda U \), where \( \Lambda \) is the diagonal matrix of eigenvalues. Then \( A^{n - 1} = U^{-1}\Lambda^{n - 1}U \). Show that we can write  
\[
\Xi(n) = \lambda_1^{n - 1}Y_1 + \lambda_2^{n - 1}Y_2 + \lambda_3^{n - 1}Y_3  
\]  
(4.104)  
where \( Y_1, Y_2, Y_3 \) do not depend on \( n \). For large \( n \), this sum is dominated by the largest term. Therefore, argue that for \( i = 1, 2, 3 \), we have  
\[
\frac{1}{n}\log \Xi_i(n) \to \log \lambda  
\]  
(4.105)  
where \( \lambda \) is the largest (positive) eigenvalue. Thus, the number of sequences of length \( n \) grows as \( \lambda^n \) for large \( n \). Calculate \( \lambda \) for the matrix \( A \) above. (The case when the eigenvalues are not distinct can be handled in a similar manner.)  
(d) We now take a different approach. Consider a Markov chain whose state diagram is the one given in part (a), but with arbitrary transition probabilities. Therefore, the probability transition matrix of this Markov chain is  
\[
P = \begin{pmatrix}  
0 & 1 & 0 \\  
\alpha & 0 & 1 - \alpha \\  
1 & 0 & 0  
\end{pmatrix}  
\]  
(4.106)  
Show that the stationary distribution of this Markov chain is  
\[
\mu = \begin{pmatrix}  
\frac{1}{3 - \alpha} \\  
\frac{1}{3 - \alpha} \\  
\frac{1 - \alpha}{3 - \alpha}  
\end{pmatrix}  
\]  
(4.107)  
(e) Maximize the entropy rate of the Markov chain over choices of \( \alpha \). What is the maximum entropy rate of the chain?  
(f) Compare the maximum entropy rate in part (e) with \( \log \lambda \) in part (c). Why are the two answers the same?

4.17  
Recurrence times are insensitive to distributions. Let \( X_0, X_1, X_2, \ldots \) be drawn i.i.d. \( \sim p(x) \), \( x \in \mathcal{X} = \{1, 2, \ldots, m\} \), and let \( N \) be the waiting time to the next occurrence of \( X_0 \). Thus \( N = \min\{n \geq 1: X_n = X_0\} \).  
(a) Show that \( E[N] = m \).  
(b) Show that \( E[\log N] \leq H(X) \).  
(c) (Optional) Prove part (a) for \( \{X_i\} \) stationary and ergodic.

4.18  
Stationary but not ergodic process. A bin has two biased coins, one with probability of heads \( p \) and the other with probability of heads \( 1 - p \). One of these coins is chosen at random (i.e., with probability \( \frac{1}{2} \)) and is then tossed \( n \) times. Let \( X \) denote the identity of the coin that is picked, and let \( Y_1 \) and \( Y_2 \) denote the results of the first two tosses.  
(a) Calculate \( I(Y_1; Y_2 | X) \).  
(b) Calculate \( I(X; Y_1, Y_2) \).  
(c) Let \( H(Y) \) be the entropy rate of the \( Y \) process (the sequence of coin tosses). Calculate \( H(Y) \). [Hint: Relate this to \( \lim_{n \to \infty} \frac{1}{n}H(X, Y_1, Y_2, \ldots, Y_n) \).] You can check the answer by considering the behavior as \( p \to \frac{1}{2} \).

4.19  
Random walk on a graph. Consider a random walk on the following graph:  
1  
\[
\leftrightarrow 2 \leftrightarrow 3 \leftrightarrow 4 \leftrightarrow 5  
\]  
(a) Calculate the stationary distribution.



--- Page 23 ---
PROBLEMS
97
(b) What is the entropy rate?
(c) Find the mutual information I(X_{n+1}; X_n) assuming that the process is stationary.
4.20
Random walk on chessboard.
Find the entropy rate of the Markov chain associated with a random walk of a king on the 3 × 3 chessboard:
1 2 3
4 5 6
7 8 9
What about the entropy rate of rooks, bishops, and queens? There are two types of bishops.
4.21
Maximal entropy graphs.
Consider a random walk on a connected graph with four edges.
(a) Which graph has the highest entropy rate?
(b) Which graph has the lowest?
4.22
Three-dimensional maze.
A bird is lost in a 3 × 3 × 3 cubical maze. The bird flies from room to room, going to adjoining rooms with equal probability through each of the walls. For example, the corner rooms have three exits.
(a) What is the stationary distribution?
(b) What is the entropy rate of this random walk?
4.23
Entropy rate.
Let {X_i} be a stationary stochastic process with entropy rate H(X).
(a) Argue that H(X) ≤ H(X_1).
(b) What are the conditions for equality?
4.24
Entropy rates.
Let {X_i} be a stationary process. Let Y_i = (X_i, X_{i+1}). Let Z_i = (X_{2i}, X_{2i+1}). Let V_i = X_{2i}. Consider the entropy rates H(X), H(Y), H(Z), and H(V) of the processes {X_i}, {Y_i}, {Z_i}, and {V_i}. What is the inequality relationship ≤, =, or ≥ between each of the pairs listed below?
(a) H(X) ⩾ H(Y).
(b) H(X) ⩾ H(Z).
(c) H(X) ⩾ H(V).
(d) H(Z) ⩾ H(X).
4.25
Monotonicity
(a) Show that I(X; Y_1, Y_2, ..., Y_n) is non-decreasing in n.
(b) Under what conditions is the mutual information constant for all n?
4.26
Transitions in Markov chains.
Suppose that {X_i} forms an irreducible Markov chain with transition matrix P and stationary distribution µ. Form the associated "edge process" {Y_i} by keeping track only of the transitions. Thus, the new process {Y_i} takes values in X × X, and Y_i = (X_{i-1}, X_i). For example,
X_n = 3, 2, 8, 5, 7, ...
becomes
Y_n = (∅, 3), (3, 2), (2, 8), (8, 5), (5, 7), ....
Find the entropy rate of the edge process {Y_i}.
4.27
Entropy rate.
Let {X_i} be a stationary {0, 1}-valued stochastic process obeying
X_{k+1} = X_k ⊕ X_{k-1} ⊕ Z_{k+1},
where {Z_i} is Bernoulli(p) and ⊕ denotes mod 2 addition. What is the entropy rate H(X)?
4.28
Mixture of processes.
Suppose that we observe one of two stochastic processes but don’t know which. What is the entropy rate? Specifically, let X_{1,1}, X_{1,2}, X_{1,3}, ... be a Bernoulli process with parameter p_1, and let X_{2,1}, X_{2,2}, X_{2,3}, ... be Bernoulli(p_2). Let θ = 1 with probability 1/2 and 2 with probability 1/2, and let Y_i = X_{θ,i} = X_{1,i} or X_{2,i}. Thus, Y observes the process {X_{1,i}} or {X_{2,i}}. Eventually, Y will know which.
(a) Is {Y_i} stationary?
(b) Is {Y_i} an i.i.d. process?
(c) What is the entropy rate H of {Y_i}?
(d) Does -1/n log p(Y_1, Y_2, ..., Y_n) → H?
(e) Is there a code that achieves an expected per-symbol description length 1/n E L_n → H?
Now let θ_i be Bern(1/2). Observe that Z_i = X_{θ_i,i}, i = 1, 2, .... Thus, θ is not fixed for all time, as it was in the first part, but is chosen i.i.d. each time. Answer parts (a), (b), (c), (d), (e) for the process {Z_i}, labeling the answers (a'), (b'), (c'), (d'), (e').
4.29
Waiting times.
Let X be the waiting time for the first heads to appear in successive flips of a fair coin. For example, Pr{X = 3} = (1/2)^3. Let S_n be the waiting time for the n-th head to appear. Thus, S_0 = 0, S_{n+1} = S_n + X_{n+1}, where X_1, X_2, X_3, ... are i.i.d. according to the distribution above.
(a) Is the process {S_n} stationary?
(b) Calculate H(S_1, S_2, ..., S_n).
(c) Does the process {S_n} have an entropy rate? If so, what is it? If not, why not?
(d) What is the expected number of fair coin flips required to generate a random variable having the same distribution as S_n?
4.30
Markov chain transitions
P = [P_{ij}] =
⎡
⎢
⎣
1/2 1/4 1/4
1/2 1/4 1/4
1/2 1/4 1/4
⎤
⎥
⎦
Let X_1 be distributed uniformly over the states {0, 1, 2}. Let {X_i} be a Markov chain with transition matrix P; thus, P(X_{n+1} = j | X_n = i) = P_{ij}, i, j ∈ {0, 1, 2}.
(a) Is {X_n} stationary?
(b) Find lim_{n→∞} 1/n H(X_1, ..., X_n).
Now consider the derived process Z_1, Z_2, ..., Z_n, where Z_1 = X_1, Z_i = X_i - X_{i-1} (mod 3), i = 2, ..., n. Thus, Z_n encodes the transitions, not the states.
(c) Find H(Z_1, Z_2, ..., Z_n).
(d) Find H(Z_n) and H(X_n) for n ≥ 2.
(e) Find H(Z_n | Z_{n-1}) for n ≥ 2.
(f) Are Z_{n-1} and Z_n independent for n ≥ 2?
4.31
Markov.
Let {X_i} ∼ Bernoulli(p). Consider the associated Markov chain {Y_i} where Y_i = (the number of 1’s in the current run of 1’s). For example, if X_n = 101110..., we have Y_n = 101230....
(a) Find the entropy rate of X_n.
(b) Find the entropy rate of Y_n.
4.32
Time symmetry.
Let {X_n} be a stationary Markov process. We condition on (X_0, X_1) and look into the past and future. For what index k is H(X_{-n} | X_0, X_1) = H(X_k | X_0, X_1)? Give the argument.
4.33
Chain inequality.
Let X_1 → X_2 → X_3 → X_4 form a Markov chain. Show that I(X_1; X_3) + I(X_2; X_4) ≤ I(X_1; X_4) + I(X_2; X_3).
(4.108)
4.34
Broadcast channel.
Let X → Y → (Z, W) form a Markov chain [i.e., p(x, y, z, w) = p(x)p(y|x)p(z, w|y) for all x, y, z, w]. Show that I(X; Z) + I(X; W) ≤ I(X; Y) + I(Z; W).
(4.109)
4.35
Concavity of second law.
Let {X_n}_{-∞}^{∞} be a stationary Markov process. Show that H(X_n | X_0) is concave in n. Specifically, show that H(X_n | X_0) - H(X_{n-1} | X_0) - (H(X_{n-1} | X_0) - H(X_{n-2} | X_0)) = -I(X_1; X_{n-1} | X_0, X_n) ≤ 0. Thus, the second difference is negative, establishing that H(X_n | X_0) is a concave function of n.
HISTORICAL NOTES
The entropy rate of a stochastic process was introduced by Shannon [472], who also explored some of the connections between the entropy rate of the process and the number of possible sequences generated by the process. Since Shannon, there have been a number of results extending the basic theorems of information theory to general stochastic processes. The AEP for a general stationary stochastic process is proved in Chapter 16. Hidden Markov models are used for a number of applications, such as speech recognition [432]. The calculation of the entropy rate for constrained sequences was introduced by Shannon [472]. These sequences are used for coding for magnetic and optical channels [288].
--- Page 28 ---

--- Page 29 ---
CHAPTER 5
DATA COMPRESSION
We now put content in the definition of entropy by establishing the fundamental limit for the compression of information. Data compression can be achieved by assigning short descriptions to the most frequent outcomes of the data source and necessarily longer descriptions to the less frequent outcomes. For example, in Morse code, the most frequent symbol is represented by a single dot. In this chapter, we find the shortest average description length of a random variable.
We first define the notion of an instantaneous code and then prove the important Kraft inequality, which asserts that the exponentiated codeword length assignments must look like a probability mass function. Elementary calculus then shows that the expected description length must be greater than or equal to the entropy, the first main result. Then Shannon’s simple construction shows that the expected description length can achieve this bound asymptotically for repeated descriptions. This establishes the entropy as a natural measure of efficient description length.
The famous Huffman coding procedure for finding minimum expected description length assignments is provided. Finally, we show that Huffman codes are competitively optimal and that it requires roughly H fair coin flips to generate a sample of a random variable having entropy H. Thus, the entropy is the data compression limit as well as the number of bits needed in random number generation, and codes achieving H turn out to be optimal from many points of view.
5.1
EXAMPLES OF CODES
Definition
A source code C for a random variable X is a mapping from X, the range of X, to D^*, the set of finite-length strings of symbols from a D-ary alphabet. Let C(x) denote the codeword corresponding to x and let l(x) denote the length of C(x).
For example, C(red) = 00, C(blue) = 11 is a source code for X = {red, blue} with alphabet D = {0, 1}.
Definition
The expected length L(C) of a source code C(x) for a random variable X with probability mass function p(x) is given by
L(C) = ∑_{x ∈ X} p(x) l(x),
(5.1)
where l(x) is the length of the codeword associated with x. Without loss of generality, we can assume that the D-ary alphabet is D = {0, 1, ..., D - 1}.
Some examples of codes follow.
Example 5.1.1
Let X be a random variable with the following distribution and codeword assignment:
Pr(X = 1) = 1/2, codeword C(1) = 0
Pr(X = 2) = 1/4, codeword C(2) = 10
Pr(X = 3) = 1/8, codeword C(3) = 110
Pr(X = 4) = 1/8, codeword C(4) = 111.
(5.2)
The entropy H(X) of X is 1.75 bits, and the expected length L(C) of this code is also 1.75 bits. Here we have a code that has the same average length as the entropy. We note that any sequence of bits can be uniquely decoded into a sequence of symbols of X. For example, the bit string 0110111100110 is decoded as 134213.
Example 5.1.2
Consider another simple example of a code for a random variable:
Pr(X = 1) = 1/3, codeword C(1) = 0
Pr(X = 2) = 1/3, codeword C(2) = 10
Pr(X = 3) = 1/3, codeword C(3) = 11.
(5.3)
Just as in Example 5.1.1, the code is uniquely decodable. However, in this case, the entropy is log 3 = 1.58 bits and the average length of the encoding is 1.66 bits. Here El(X) > H(X).
Example 5.1.3
(Morse code) The Morse code is a reasonably efficient code for the English alphabet using an alphabet of four symbols: a dot, a dash, a letter space, and a word space. Short sequences represent frequent letters (e.g., a single dot represents E) and long sequences represent infrequent letters (e.g., Q is represented by "dash,dash,dot,dash"). This is not the optimal representation for the alphabet in four symbols—in fact, many possible codewords are not utilized because the codewords for letters do not contain spaces except for a letter space at the end of every codeword, and no space can follow another space. It is an interesting problem to calculate the number of sequences that can be constructed under these constraints. The problem was solved by Shannon in his original 1948 paper. The problem is also related to coding for magnetic recording, where long strings of 0’s are prohibited [5], [370].
We now define increasingly more stringent conditions on codes. Let x^n denote (x_1, x_2, ..., x_n).
Definition
A code is said to be nonsingular if every element of the range of X maps into a different string in D^*; that is, x ≠ x' ⇒ C(x) ≠ C(x').
(5.4)
Nonsingularity suffices for an unambiguous description of a single value of X. But we usually wish to send a sequence of values of X. In such cases, we can ensure decodability by adding a special symbol (a "comma") between any two codewords. But this is an inefficient use of the special symbol; we can do better by developing the idea of self-punctuating or instantaneous codes. Motivated by the necessity to send sequences of symbols X, we define the extension of a code as follows:
Definition
The extension C^* of a code C is the mapping from finite-length strings of X to finite-length strings of D, defined by
C^*(x_1 x_2 · · · x_n) = C(x_1)C(x_2) · · · C(x_n),
(5.5)
where C(x_1)C(x_2) · · · C(x_n) indicates concatenation of the corresponding codewords.
Example 5.1.4
If C(x_1) = 00 and C(x_2) = 11, then C^*(x_1 x_2) = 0011.
Definition
A code is called uniquely decodable if its extension is nonsingular.
In other words, any encoded string in a uniquely decodable code has only one possible source string producing it. However, one may have to look at the entire string to determine even the first symbol in the corresponding source string.



DATA COMPRESSION

---

### Page 32 ---  
106  
DATA COMPRESSION  

**Definition**  
A code is called a prefix code or an instantaneous code if no codeword is a prefix of any other codeword. An instantaneous code can be decoded without reference to future codewords since the end of a codeword is immediately recognizable. Hence, for an instantaneous code, the symbol \( x_i \) can be decoded as soon as we come to the end of the codeword corresponding to it. We need not wait to see the codewords that come later. An instantaneous code is a self-punctuating code; we can look down the sequence of code symbols and add the commas to separate the codewords without looking at later symbols. For example, the binary string 01011111010 produced by the code of Example 5.1.1 is parsed as 0,10,111,110,10.  

The nesting of these definitions is shown in Figure 5.1. To illustrate the differences between the various kinds of codes, consider the examples of codeword assignments \( C(x) \) to \( x \in X \) in Table 5.1. For the nonsingular code, the code string 010 has three possible source sequences: 2 or 14 or 31, and hence the code is not uniquely decodable. The uniquely decodable code is not prefix-free and hence is not instantaneous. To see that it is uniquely decodable, take any code string and start from the beginning. If the first two bits are 00 or 10, they can be decoded immediately. If the first two bits are 11, we must look at the following bits. If the next bit is a 1, the first source symbol is a 3. If the length of the string of 0’s immediately following the 11 is odd, the first codeword must be 110 and the first source symbol must be 4; if the length of the string of 0’s is even, the first source symbol is a 3. By repeating this argument, we can see that this code is uniquely decodable. Sardinas and Patterson [455] have devised a finite test for unique decodability, which involves forming sets of possible suffixes to the codewords and eliminating them systematically. The test is described more fully in Problem 5.5.27. The fact that the last code in Table 5.1 is instantaneous is obvious since no codeword is a prefix of any other.  

---

### Page 33 ---  
5.2  
KRAFT INEQUALITY  
107  

**Theorem 5.2.1 (Kraft Inequality)**  
For any instantaneous code (prefix code) over an alphabet of size \( D \), the codeword lengths \( l_1, l_2, \ldots, l_m \) must satisfy the inequality:  
\[
\sum_{i} D^{-l_i} \leq 1.
\]  
Conversely, given a set of codeword lengths that satisfy this inequality, there exists an instantaneous code with these word lengths.  

**Proof:**  
Consider a \( D \)-ary tree in which each node has \( D \) children. Let the branches of the tree represent the symbols of the codeword. For example, the \( D \) branches arising from the root node represent the \( D \) possible values of the first symbol of the codeword. Then each codeword is represented by a leaf on the tree. The path from the root traces out the symbols of the codeword. A binary example of such a tree is shown in Figure 5.2. The prefix condition on the codewords implies that no codeword is an ancestor of any other codeword on the tree. Hence, each codeword eliminates its descendants as possible codewords.  

Let \( l_{\text{max}} \) be the length of the longest codeword of the set of codewords. Consider all nodes of the tree at level \( l_{\text{max}} \). Some of them are codewords, some are descendants of codewords, and some are neither. A codeword at level \( l_i \) has \( D^{l_{\text{max}} - l_i} \) descendants at level \( l_{\text{max}} \). Each of these descendant sets must be disjoint. Also, the total number of nodes in these sets must be less than or equal to \( D^{l_{\text{max}}} \). Hence, summing over all the codewords, we have:  
\[
\sum_{i} D^{l_{\text{max}} - l_i} \leq D^{l_{\text{max}}}.
\]  
Or:  
\[
\sum_{i} D^{-l_i} \leq 1,
\]  
which is the Kraft inequality.  

Conversely, given any set of codeword lengths \( l_1, l_2, \ldots, l_m \) that satisfy the Kraft inequality, we can always construct a tree like the one in Figure 5.2. Label the first node (lexicographically) of depth \( l_1 \) as codeword 1, and remove its descendants from the tree. Then label the first remaining node of depth \( l_2 \) as codeword 2, and so on. Proceeding this way, we construct a prefix code with the specified \( l_1, l_2, \ldots, l_m \).  

---

### Page 34 ---  
108  
DATA COMPRESSION  

We now show that an infinite prefix code also satisfies the Kraft inequality.  

**Theorem 5.2.2 (Extended Kraft Inequality)**  
For any countably infinite set of codewords that form a prefix code, the codeword lengths satisfy the extended Kraft inequality:  
\[
\sum_{i=1}^{\infty} D^{-l_i} \leq 1.
\]  
Conversely, given any \( l_1, l_2, \ldots \) satisfying the extended Kraft inequality, we can construct a prefix code with these codeword lengths.  

**Proof:**  
Let the \( D \)-ary alphabet be \( \{0, 1, \ldots, D-1\} \). Consider the \( i \)-th codeword \( y_1 y_2 \cdots y_{l_i} \). Let \( 0.y_1 y_2 \cdots y_{l_i} \) be the real number given by the \( D \)-ary expansion:  
\[
0.y_1 y_2 \cdots y_{l_i} = \sum_{j=1}^{l_i} y_j D^{-j}.
\]  
This codeword corresponds to the interval:  
\[
\left[ 0.y_1 y_2 \cdots y_{l_i}, 0.y_1 y_2 \cdots y_{l_i} + D^{-l_i} \right).
\]  
the set of all real numbers whose \( D \)-ary expansion begins with \( 0.y_1 y_2 \cdots y_{l_i} \). This is a subinterval of the unit interval \([0, 1]\). By the prefix condition, these intervals are disjoint. Hence, the sum of their lengths has to be less than or equal to 1. This proves that:  
\[
\sum_{i=1}^{\infty} D^{-l_i} \leq 1.
\]  

Just as in the finite case, we can reverse the proof to construct the code for a given \( l_1, l_2, \ldots \) that satisfies the Kraft inequality. First, reorder the indexing so that \( l_1 \leq l_2 \leq \cdots \). Then simply assign the intervals in order from the low end of the unit interval. For example, if we wish to construct a binary code with \( l_1 = 1, l_2 = 2, \ldots \), we assign the intervals \([0, 1/2), [1/2, 3/4), \ldots\) to the symbols, with corresponding codewords 0, 10, \(\ldots\).  

---

### Page 35 ---  
5.2  
KRAFT INEQUALITY  
109  

In Section 5.5 we show that the lengths of codewords for a uniquely decodable code also satisfy the Kraft inequality. Before we do that, we consider the problem of finding the shortest instantaneous code.  

---

### Page 36 ---  
110  
DATA COMPRESSION  

### 5.3 OPTIMAL CODES  

In Section 5.2 we proved that any codeword set that satisfies the prefix condition has to satisfy the Kraft inequality and that the Kraft inequality is a sufficient condition for the existence of a codeword set with the specified set of codeword lengths. We now consider the problem of finding the prefix code with the minimum expected length. From the results of Section 5.2, this is equivalent to finding the set of lengths \( l_1, l_2, \ldots, l_m \) satisfying the Kraft inequality and whose expected length \( L = \sum p_i l_i \) is less than the expected length of any other prefix code. This is a standard optimization problem: Minimize  
\[
L = \sum p_i l_i
\]  
over all integers \( l_1, l_2, \ldots, l_m \) satisfying  
\[
\sum D^{-l_i} \leq 1.
\]  

A simple analysis by calculus suggests the form of the minimizing \( l_i^* \). We neglect the integer constraint on \( l_i \) and assume equality in the constraint. Hence, we can write the constrained minimization using Lagrange multipliers as the minimization of:  
\[
J = \sum p_i l_i + \lambda \left( \sum D^{-l_i} \right).
\]  
Differentiating with respect to \( l_i \), we obtain:  
\[
\frac{\partial J}{\partial l_i} = p_i - \lambda D^{-l_i} \log_e D.
\]  
Setting the derivative to 0, we obtain:  
\[
D^{-l_i} = \frac{p_i}{\lambda \log_e D}.
\]  
Substituting this in the constraint to find \( \lambda \), we find \( \lambda = 1 / \log_e D \), and hence:  
\[
p_i = D^{-l_i},
\]  
yielding optimal code lengths:  
\[
l_i^* = -\log_D p_i.
\]  

This non-integer choice of codeword lengths yields expected codeword length:  
\[
L^* = \sum p_i l_i^* = -\sum p_i \log_D p_i = H_D(X).
\]  
But since the \( l_i \) must be integers, we will not always be able to set the codeword lengths as in (5.19). Instead, we should choose a set of codeword lengths \( l_i \) “close” to the optimal set. Rather than demonstrate by calculus that \( l_i^* = -\log_D p_i \) is a global minimum, we verify optimality directly in the proof of the following theorem.  

**Theorem 5.3.1**  
The expected length \( L \) of any instantaneous \( D \)-ary code for a random variable \( X \) is greater than or equal to the entropy \( H_D(X) \); that is,  
\[
L \geq H_D(X),
\]  
with equality if and only if \( D^{-l_i} = p_i \).  

**Proof:**  
We can write the difference between the expected length and the entropy as:  
\[
L - H_D(X) = \sum p_i l_i - \sum p_i \log_D p_i.
\]  
Letting \( r_i = D^{-l_i} / \sum_j D^{-l_j} \) and \( c = \sum D^{-l_i} \), we obtain:  
\[
L - H = \sum p_i \log_D \frac{p_i}{r_i} - \log_D c.
\]  
\[
= D(p \| r) + \log_D \frac{1}{c}.
\]  
\[
\geq 0
\]  
by the non-negativity of relative entropy and the fact (Kraft inequality) that \( c \leq 1 \). Hence, \( L \geq H \) with equality if and only if \( p_i = D^{-l_i} \) (i.e., if and only if \( -\log_D p_i \) is an integer for all \( i \)).  

**Definition**  
A probability distribution is called \( D \)-adic if each of the probabilities is equal to \( D^{-n} \) for some \( n \). Thus, we have equality in the theorem if and only if the distribution of \( X \) is \( D \)-adic.  

The preceding proof also indicates a procedure for finding an optimal code: Find the \( D \)-adic distribution that is closest (in the relative entropy sense) to the distribution of \( X \). This distribution provides the set of codeword lengths. Construct the code by choosing the first available node as in the proof of the Kraft inequality. We then have an optimal code for \( X \). However, this procedure is not easy, since the search for the closest \( D \)-adic distribution is not obvious. In the next section, we give a good suboptimal procedure (Shannon–Fano coding). In Section 5.6, we describe a simple procedure (Huffman coding) for actually finding the optimal code.  

---

### Page 37 ---  
5.3  
OPTIMAL CODES  
111  

### 5.4 BOUNDS ON THE OPTIMAL CODE LENGTH  

We now demonstrate a code that achieves an expected description length \( L \) within 1 bit of the lower bound; that is,  
\[
H(X) \leq L < H(X) + 1.
\]  
Recall the setup of Section 5.3: We wish to minimize \( L = \sum p_i l_i \) subject to the constraint that \( l_1, l_2, \ldots, l_m \) are integers and \( \sum D^{-l_i} \leq 1 \). We proved that the optimal codeword lengths can be found by finding the \( D \)-adic probability distribution closest to the distribution of \( X \) in relative entropy, that is, by finding the \( D \)-adic \( r \) ( \( r_i = D^{-l_i} / \sum_j D^{-l_j} \) ) minimizing:  
\[
L - H_D = D(p \| r) - \log_D \left( \sum D^{-l_i} \right) \geq 0.
\]  

The choice of word lengths \( l_i = \lceil \log_D p_i \rceil \) yields \( L = H \). Since \( \log_D p_i \) may not equal an integer, we round it up to give integer word-length assignments,  
\[
l_i = \lceil \log_D p_i \rceil,
\]  
where \( \lceil x \rceil \) is the smallest integer \( \geq x \). These lengths satisfy the Kraft inequality since:  
\[
\sum D^{-\lceil \log_D p_i \rceil} \leq \sum D^{-\log_D p_i} = \sum p_i = 1.
\]  

This choice of codeword lengths satisfies:  
\[
\log_D p_i \leq l_i < \log_D p_i + 1.
\]  
Multiplying by \( p_i \) and summing over \( i \), we obtain:  
\[
H_D(X) \leq L < H_D(X) + 1.
\]  

Since an optimal code can only be better than this code, we have the following theorem.  

**Theorem 5.4.1**  
Let \( l_1^*, l_2^*, \ldots, l_m^* \) be optimal codeword lengths for a source distribution \( p \) and a \( D \)-ary alphabet, and let \( L^* \) be the associated expected length of an optimal code ( \( L^* = \sum p_i l_i^* \) ). Then:  
\[
H_D(X) \leq L^* < H_D(X) + 1.
\]  

**Proof:**  
Let \( l_i = \lceil \log_D p_i \rceil \). Then \( l_i \) satisfies the Kraft inequality and from:  
\[
H_D(X) \leq L = \sum p_i l_i < H_D(X) + 1.
\]  
But since \( L^* \), the expected length of the optimal code, is less than \( L = \sum p_i l_i \), and since \( L^* \geq H_D(X) \) from Theorem 5.3.1, we have the theorem.  

---

### Page 39 ---  
5.4  
BOUNDS ON THE OPTIMAL CODE LENGTH  
113  

In Theorem 5.4.1, there is an overhead that is at most 1 bit, due to the fact that \( \log_D p_i \) is not always an integer. We can reduce the overhead per symbol by spreading it out over many symbols. With this in mind, let us consider a system in which we send a sequence of \( n \) symbols from \( X \). The symbols are assumed to be drawn i.i.d. according to \( p(x) \). We can consider these \( n \) symbols to be a supersymbol from the alphabet \( X^n \). Define \( L_n \) to be the expected codeword length per input symbol, that is, if \( l(x_1, x_2, \ldots, x_n) \) is the length of the binary codeword associated with the supersymbol \( x_1, x_2, \ldots, x_n \).



DATA COMPRESSION

---

### Page 40

**114**  
**DATA COMPRESSION**  

With \((x_1, x_2, \ldots, x_n)\) (for the rest of this section, we assume that \(D = 2\), for simplicity), then  
\[ L_n = \frac{1}{n} \sum p(x_1, x_2, \ldots, x_n) l(x_1, x_2, \ldots, x_n) = \frac{1}{n} H(X_1, X_2, \ldots, X_n). \]  
(5.35)  

We can now apply the bounds derived above to the code:  
\[ H(X_1, X_2, \ldots, X_n) \leq E l(X_1, X_2, \ldots, X_n) < H(X_1, X_2, \ldots, X_n) + 1. \]  
(5.36)  

Since \(X_1, X_2, \ldots, X_n\) are i.i.d.,  
\[ H(X_1, X_2, \ldots, X_n) = n H(X). \]  
Dividing (5.36) by \(n\), we obtain  
\[ H(X) \leq L_n < H(X) + \frac{1}{n}. \]  
(5.37)  

Hence, by using large block lengths, we can achieve an expected code-length per symbol arbitrarily close to the entropy.  

We can use the same argument for a sequence of symbols from a stochastic process that is not necessarily i.i.d. In this case, we still have the bound  
\[ H(X_1, X_2, \ldots, X_n) \leq E l(X_1, X_2, \ldots, X_n) < H(X_1, X_2, \ldots, X_n) + 1. \]  
(5.38)  

Dividing by \(n\) again and defining \(L_n\) to be the expected description length per symbol, we obtain  
\[ \frac{H(X_1, X_2, \ldots, X_n)}{n} \leq L_n < \frac{H(X_1, X_2, \ldots, X_n)}{n} + \frac{1}{n}. \]  
(5.39)  

If the stochastic process is stationary, then \(\frac{H(X_1, X_2, \ldots, X_n)}{n} \to H(X)\), and the expected description length tends to the entropy rate as \(n \to \infty\). Thus, we have the following theorem:  

**Theorem 5.4.2**  
The minimum expected codeword length per symbol satisfies  
\[ \frac{H(X_1, X_2, \ldots, X_n)}{n} \leq L_n^* < \frac{H(X_1, X_2, \ldots, X_n)}{n} + \frac{1}{n}. \]  
(5.40)  

Moreover, if \(X_1, X_2, \ldots, X_n\) is a stationary stochastic process,  
\[ L_n^* \to H(X), \]  
(5.41)  
where \(H(X)\) is the entropy rate of the process.  

---

### Page 41  

**5.5**  
**KRAFT INEQUALITY FOR UNIQUELY DECODABLE CODES**  

This theorem provides another justification for the definition of entropy rate—it is the expected number of bits per symbol required to describe the process.  

Finally, we ask what happens to the expected description length if the code is designed for the wrong distribution. For example, the wrong distribution may be the best estimate that we can make of the unknown true distribution. We consider the Shannon code assignment \(l(x) = \log \frac{1}{q(x)}\) designed for the probability mass function \(q(x)\). Suppose that the true probability mass function is \(p(x)\). Thus, we will not achieve expected length \(L \approx H(p) = -\sum p(x) \log p(x)\). We now show that the increase in expected description length due to the incorrect distribution is the relative entropy \(D(p||q)\). Thus, \(D(p||q)\) has a concrete interpretation as the increase in descriptive complexity due to incorrect information.  

**Theorem 5.4.3 (Wrong code)**  
The expected length under \(p(x)\) of the code assignment \(l(x) = \log \frac{1}{q(x)}\) satisfies  
\[ H(p) + D(p||q) \leq E_p l(X) < H(p) + D(p||q) + 1. \]  
(5.42)  

*Proof:*  
The expected codelength is  
\[ E_p l(X) = \sum_x p(x) \log \frac{1}{q(x)}. \]  
(5.43)  
\[ < \sum_x p(x) \left( \log \frac{1}{q(x)} + 1 \right) \]  
(5.44)  
\[ = \sum_x p(x) \log p(x) q(x) + \sum_x p(x) \log \frac{1}{p(x)} + 1 \]  
(5.45)  
\[ = D(p||q) + H(p) + 1. \]  
(5.46)  

The lower bound can be derived similarly.  

Thus, believing that the distribution is \(q(x)\) when the true distribution is \(p(x)\) incurs a penalty of \(D(p||q)\) in the average description length.  

---

### Page 42  

**5.5**  
**KRAFT INEQUALITY FOR UNIQUELY DECODABLE CODES**  

We have proved that any instantaneous code must satisfy the Kraft inequality. The class of uniquely decodable codes is larger than the class of instantaneous codes, so one expects to achieve a lower expected codeword length if \(L\) is minimized over all uniquely decodable codes. In this section, we prove that the class of uniquely decodable codes does not offer any further possibilities for the set of codeword lengths than do instantaneous codes. We now give Karush’s elegant proof of the following theorem.  

**Theorem 5.5.1 (McMillan)**  
The codeword lengths of any uniquely decodable \(D\)-ary code must satisfy the Kraft inequality  
\[ \sum D^{-l_i} \leq 1. \]  
(5.48)  

Conversely, given a set of codeword lengths that satisfy this inequality, it is possible to construct a uniquely decodable code with these codeword lengths.  

*Proof:*  
Consider \(C^k\), the \(k\)th extension of the code (i.e., the code formed by the concatenation of \(k\) repetitions of the given uniquely decodable code \(C\)). By the definition of unique decodability, the \(k\)th extension of the code is nonsingular. Since there are only \(D^n\) different \(D\)-ary strings of length \(n\), unique decodability implies that the number of code sequences of length \(n\) in the \(k\)th extension of the code must be no greater than \(D^n\). We now use this observation to prove the Kraft inequality.  

Let the codeword lengths of the symbols \(x \in X\) be denoted by \(l(x)\). For the extension code, the length of the code sequence is  
\[ l(x_1, x_2, \ldots, x_k) = \sum_{i=1}^k l(x_i). \]  
(5.49)  

The inequality that we wish to prove is  
\[ \sum_{x \in X} D^{-l(x)} \leq 1. \]  
(5.50)  

The trick is to consider the \(k\)th power of this quantity. Thus,  
\[ \left( \sum_{x \in X} D^{-l(x)} \right)^k = \sum_{x_1 \in X} \sum_{x_2 \in X} \cdots \sum_{x_k \in X} D^{-l(x_1)} D^{-l(x_2)} \cdots D^{-l(x_k)}. \]  
(5.51)  
\[ = \sum_{x_1, x_2, \ldots, x_k \in X} D^{-l(x_1) - l(x_2) - \cdots - l(x_k)}. \]  
(5.52)  
\[ = \sum_{x_k \in X} D^{-l(x_k)}, \]  
(5.53)  
by (5.49). We now gather the terms by word lengths to obtain  
\[ \sum_{x_k \in X} D^{-l(x_k)} = \sum_{m=1}^{kl_{\text{max}}} a(m) D^{-m}, \]  
(5.54)  
where \(l_{\text{max}}\) is the maximum codeword length and \(a(m)\) is the number of source sequences \(x_k\) mapping into codewords of length \(m\). But the code is uniquely decodable, so there is at most one sequence mapping into each code \(m\)-sequence and there are at most \(D^m\) code \(m\)-sequences. Thus, \(a(m) \leq D^m\), and we have  
\[ \left( \sum_{x \in X} D^{-l(x)} \right)^k \leq \sum_{m=1}^{kl_{\text{max}}} D^m D^{-m} = kl_{\text{max}}. \]  
(5.55)  
\[ = kl_{\text{max}}. \]  
(5.56)  

Hence,  
\[ \left( \sum_{x \in X} D^{-l(x)} \right)^k \leq kl_{\text{max}}. \]  
(5.57)  
Since this inequality is true for all \(k\), it is true in the limit as \(k \to \infty\). Since \((kl_{\text{max}})^{1/k} \to 1\), we have  
\[ \sum_{x \in X} D^{-l(x)} \leq 1, \]  
(5.58)  
which is the Kraft inequality.  

Conversely, given any set of \(l_1, l_2, \ldots, l_m\) satisfying the Kraft inequality, we can construct an instantaneous code as proved in Section 5.2. Since every instantaneous code is uniquely decodable, we have also constructed a uniquely decodable code.  

---

### Page 43  

**5.5**  
**KRAFT INEQUALITY FOR UNIQUELY DECODABLE CODES**  

*Corollary*  
A uniquely decodable code for an infinite source alphabet \(X\) also satisfies the Kraft inequality.  

*Proof:*  
The point at which the preceding proof breaks down for infinite \(|X|\) is at (5.58), since for an infinite code \(l_{\text{max}}\) is infinite. But there is a simple fix to the proof. Any subset of a uniquely decodable code is also uniquely decodable; thus, any finite subset of the infinite set of codewords satisfies the Kraft inequality. Hence,  
\[ \sum_{i=1}^\infty D^{-l_i} = \lim_{N \to \infty} \sum_{i=1}^N D^{-l_i} \leq 1. \]  
(5.60)  

Given a set of word lengths \(l_1, l_2, \ldots\) that satisfy the Kraft inequality, we can construct an instantaneous code as in Section 5.4. Since instantaneous codes are uniquely decodable, we have constructed a uniquely decodable code with an infinite number of codewords. So the McMillan theorem also applies to infinite alphabets.  

---

The theorem implies a rather surprising result—that the class of uniquely decodable codes does not offer any further choices for the set of codeword lengths than the class of prefix codes. The set of achievable codeword lengths is the same for uniquely decodable and instantaneous codes. Hence, the bounds derived on the optimal codeword lengths continue to hold even when we expand the class of allowed codes to the class of all uniquely decodable codes.  

---

### Page 44  

**5.6**  
**HUFFMAN CODES**  

An optimal (shortest expected length) prefix code for a given distribution can be constructed by a simple algorithm discovered by Huffman [283]. We will prove that any other code for the same alphabet cannot have a lower expected length than the code constructed by the algorithm. Before we give any formal proofs, let us introduce Huffman codes with some examples.  

**Example 5.6.1**  
Consider a random variable \(X\) taking values in the set \(X = \{1, 2, 3, 4, 5\}\) with probabilities 0.25, 0.25, 0.2, 0.15, 0.15, respectively. We expect the optimal binary code for \(X\) to have the longest codewords assigned to the symbols 4 and 5. These two lengths must be equal, since otherwise we can delete a bit from the longer codeword and still have a prefix code, but with a shorter expected length. In general, we can construct a code in which the two longest codewords differ only in the last bit. For this code, we can combine the symbols 4 and 5 into a single source symbol, with a probability assignment 0.30. Proceeding this way, combining the two least likely symbols into one symbol until we are finally left with only one symbol, and then assigning codewords to the symbols, we obtain the following table:  

| Codeword | Length | Symbol | Probability |  
|----------|-------|-------|------------|  
| 0        | 1     | 1     | 0.25       |  
| 10      | 2     | 2     | 0.25       |  
| 11      | 2     | 3     | 0.2        |  
| 000     | 3     | 4     | 0.15       |  
| 001     | 3     | 5     | 0.15       |  

This code has an average length of 2.3 bits.  

**Example 5.6.2**  
Consider a ternary code for the same random variable. Now we combine the three least likely symbols into one supersymbol and obtain the following table:  

| Codeword | Symbol | Probability |  
|----------|-------|------------|  
| 1        | 1     | 0.25       |  
| 2        | 2     | 0.25       |  
| 00      | 3     | 0.2        |  
| 01      | 4     | 0.15       |  
| 02      | 5     | 0.15       |  

This code has an average length of 1.5 ternary digits.  

**Example 5.6.3**  
If \(D \geq 3\), we may not have a sufficient number of symbols so that we can combine them \(D\) at a time. In such a case, we add dummy symbols to the end of the set of symbols. The dummy symbols have probability 0 and are inserted to fill the tree. Since at each stage of the reduction, the number of symbols is reduced by \(D - 1\), we want the total number of symbols to be \(1 + k(D - 1)\), where \(k\) is the number of merges. Hence, we add enough dummy symbols so that the total number of symbols is of this form.  

This code has an average length of 1.7 ternary digits.  

---

### Page 45  

**5.6**  
**HUFFMAN CODES**  

A proof of the optimality of Huffman coding is given in Section 5.8.  

---

### Page 46  

**5.7**  
**SOME COMMENTS ON HUFFMAN CODES**  

1. **Equivalence of source coding and 20 questions.** We now digress to show the equivalence of coding and the game “20 questions”. Suppose that we wish to find the most efficient series of yes–no questions to determine an object from a class of objects. Assuming that we know the probability distribution on the objects, can we find the most efficient sequence of questions? (To determine an object, we need to ensure that the responses to the sequence of questions uniquely identify the object from the set of possible objects; it is not necessary that the last question have a “yes” answer.)  

We first show that a sequence of questions is equivalent to a code for the object. Any question depends only on the answers to the questions before it. Since the sequence of answers uniquely determines the object, each object has a different sequence of answers, and if we represent the yes–no answers by 0’s and 1’s, we have a binary code for the set of objects. The average length of this code is the average number of questions for the questioning scheme. Also, from a binary code for the set of objects, we can find a sequence of questions that correspond to the code, with the average number of questions equal to the expected codeword length of the code. The first question in this scheme becomes: Is the first bit equal to 1 in the object’s codeword?  

Since the Huffman code is the best source code for a random variable, the optimal series of questions is that determined by the Huffman code. In Example 5.6.1, the optimal first question is: Is \(X\) equal to 2 or 3? The answer to this determines the first bit of the Huffman code. Assuming that the answer to the first question is “yes,” the next question should be “Is \(X\) equal to 3?”, which determines the second bit. However, we need not wait for the answer to the first question to ask the second. We can ask as our second question “Is \(X\) equal to 1 or 3?”, determining the second bit of the Huffman code independent of the first.  

The expected number of questions \(E_Q\) in this optimal scheme satisfies  
\[ H(X) \leq E_Q < H(X) + 1. \]  
(5.61)



### SOME COMMENTS ON HUFFMAN CODES

#### 2. Huffman coding for weighted codewords
Huffman’s algorithm for minimizing \( \sum p_i l_i \) can be applied to any set of numbers \( p_i \geq 0 \), regardless of \( \sum p_i \). In this case, the Huffman code minimizes the sum of weighted code lengths \( \sum w_i l_i \) rather than the average code length.

**Example 5.7.1**  
We perform the weighted minimization using the same algorithm. In this case, the code minimizes the weighted sum of the codeword lengths, and the minimum weighted sum is 36.

#### 3. Huffman coding and “slice” questions (Alphabetic codes)
We have described the equivalence of source coding with the game of 20 questions. The optimal sequence of questions corresponds to an optimal source code for the random variable. However, Huffman codes ask arbitrary questions of the form “Is \( X \in A \)?” for any set \( A \subseteq \{1, 2, \ldots, m\} \).

Now we consider the game “20 questions” with a restricted set of questions. Specifically, we assume that the elements of \( X = \{1, 2, \ldots, m\} \) are ordered so that \( p_1 \geq p_2 \geq \cdots \geq p_m \) and that the only questions allowed are of the form “Is \( X > a \)?” for some \( a \). The Huffman code constructed by the Huffman algorithm may not correspond to slices (sets of the form \( \{x : x < a\} \)). If we take the codeword lengths \( (l_1 \leq l_2 \leq \cdots \leq l_m \), by Lemma 5.8.1) derived from the Huffman code and use them to assign the symbols to the code tree by taking the first available node at the corresponding level, we will construct another optimal code. However, unlike the Huffman code itself, this code is a slice code, since each question (each bit of the code) splits the tree into sets of the form \( \{x : x > a\} \) and \( \{x : x < a\} \).

**Example 5.7.2**  
Consider the first example of Section 5.6. The code that was constructed by the Huffman coding procedure is not a slice code. But using the codeword lengths from the Huffman procedure, namely \( \{2, 2, 2, 3, 3\} \), and assigning the symbols to the first available node on the tree, we obtain the following code for this random variable:  
- \( 1 \to 00 \)  
- \( 2 \to 01 \)  
- \( 3 \to 10 \)  
- \( 4 \to 110 \)  
- \( 5 \to 111 \)  

It can be verified that this code is a slice code, codes known as alphabetic codes because the codewords are ordered alphabetically.

#### 4. Huffman codes and Shannon codes
Using codeword lengths of \( \lceil \log \frac{1}{p_i} \rceil \) (which is called Shannon coding) may be much worse than the optimal code for some particular symbol. For example, consider two symbols, one of which occurs with probability 0.9999 and the other with probability 0.0001. Then using codeword lengths of \( \lceil \log \frac{1}{p_i} \rceil \) gives codeword lengths of 1 bit and 14 bits, respectively. The optimal codeword length is obviously 1 bit for both symbols. Hence, the codeword for the infrequent symbol is much longer in the Shannon code than in the optimal code.

Is it true that the codeword lengths for an optimal code are always less than \( \lceil \log \frac{1}{p_i} \rceil \)? The following example illustrates that this is not always true.

**Example 5.7.3**  
Consider a random variable \( X \) with a distribution \( \frac{1}{3}, \frac{1}{3}, \frac{1}{4}, \frac{1}{12} \). The Huffman coding procedure results in codeword lengths of \( (2, 2, 2, 2) \) or \( (1, 2, 3, 3) \) (depending on where one puts the merged probabilities, as the reader can verify). Both these codes achieve the same expected codeword length. In the second code, the third symbol has length 3, which is greater than \( \lceil \log \frac{1}{p_3} \rceil \). Thus, the codeword length for a Shannon code could be less than the codeword length of the corresponding symbol of an optimal (Huffman) code. This example also illustrates the fact that the set of codeword lengths for an optimal code is not unique (there may be more than one set of lengths with the same expected value). Although either the Shannon code or the Huffman code can be shorter for individual symbols, the Huffman code is shorter on average. Also, the Shannon code and the Huffman code differ by less than 1 bit in expected codelength (since both lie between \( H \) and \( H + 1 \)).

---

### OPTIMALITY OF HUFFMAN CODES

We prove by induction that the binary Huffman code is optimal. It is important to remember that there are many optimal codes: inverting all the bits or exchanging two codewords of the same length will give another optimal code. The Huffman procedure constructs one such optimal code.

To prove the optimality of Huffman codes, we first prove some properties of a particular optimal code.

Without loss of generality, we will assume that the probability masses are ordered, so that \( p_1 \geq p_2 \geq \cdots \geq p_m \). Recall that a code is optimal if \( \sum p_i l_i \) is minimal.

**Lemma 5.8.1**  
For any distribution, there exists an optimal instantaneous code (with minimum expected length) that satisfies the following properties:  
1. The lengths are ordered inversely with the probabilities (i.e., if \( p_j > p_k \), then \( l_j \leq l_k \)).  
2. The two longest codewords have the same length.  
3. The two longest codewords differ only in the last bit and correspond to the two least likely symbols.

*Proof:*  
The proof amounts to swapping, trimming, and rearranging, as shown in Figure 5.3. Consider an optimal code \( C_m \):  
- If \( p_j > p_k \), then \( l_j \leq l_k \). Here we swap codewords. Consider \( C'_m \), with the codewords \( j \) and \( k \) of \( C_m \) interchanged. Then  
  \[
  L(C'_m) - L(C_m) = p_j l_k + p_k l_j - p_j l_j - p_k l_k = (p_j - p_k)(l_k - l_j).
  \]  
  Since \( p_j - p_k > 0 \), and since \( C_m \) is optimal, \( L(C'_m) - L(C_m) \geq 0 \). Hence, we must have \( l_k \geq l_j \). Thus, \( C_m \) itself satisfies property 1.  

- The two longest codewords are of the same length. Here we trim the codewords. If the two longest codewords are not of the same length, one can delete the last bit of the longer one, preserving the prefix property and achieving lower expected codeword length. Hence, the two longest codewords must have the same length. By property 1, the longest codewords must belong to the least probable source symbols.  

- The two longest codewords differ only in the last bit and correspond to the two least likely symbols. Not all optimal codes satisfy this property, but by rearranging, we can find an optimal code that does. If there is a maximal-length codeword without a sibling, we can delete the last bit of the codeword and still satisfy the prefix property. This reduces the average codeword length and contradicts the optimality of the code. Hence, every maximal-length codeword in any optimal code has a sibling. Now we can exchange the longest codewords so that the two lowest-probability source symbols are associated with two siblings on the tree. This does not change the expected length \( \sum p_i l_i \). Thus, the codewords for the two lowest-probability source symbols have maximal length and agree in all but the last bit.  

Summarizing, we have shown that if \( p_1 \geq p_2 \geq \cdots \geq p_m \), there exists an optimal code with \( l_1 \leq l_2 \leq \cdots \leq l_{m-1} = l_m \), and codewords \( C(x_{m-1}) \) and \( C(x_m) \) that differ only in the last bit.  

Thus, we have shown that there exists an optimal code satisfying the properties of the lemma. We call such codes canonical codes. For any probability mass function for an alphabet of size \( m \), \( p = (p_1, p_2, \ldots, p_m) \) with \( p_1 \geq p_2 \geq \cdots \geq p_m \), we define the Huffman reduction \( p' = (p_1, p_2, \ldots, p_{m-2}, p_{m-1} + p_m) \) over an alphabet of size \( m - 1 \) (Figure 5.4). Let \( C^*_{m-1}(p') \) be an optimal code for \( p' \), and let \( C^*_{m}(p) \) be the canonical optimal code for \( p \).  

The proof of optimality will follow from two constructions: First, we expand an optimal code for \( p' \) to construct a code for \( p \), and then we condense an optimal canonical code for \( p \) to construct a code for the Huffman reduction \( p' \). Comparing the average codeword lengths for the two codes establishes that the optimal code for \( p \) can be obtained by extending the optimal code for \( p' \).  

From the optimal code for \( p' \), we construct an extension code for \( m \) elements as follows: Take the codeword in \( C^*_{m-1} \) corresponding to weight \( p_{m-1} + p_m \) and extend it by adding a 0 to form a codeword for symbol \( m - 1 \) and by adding 1 to form a codeword for symbol \( m \). The code construction is illustrated as follows:  

\[
\begin{aligned}
&C^*_{m-1}(p') \quad \to \quad C^*_m(p) \\
&p_1 \quad w'_1 \quad l'_1 \quad w_1 = w'_1 \quad l_1 = l'_1 \\
&p_2 \quad w'_2 \quad l'_2 \quad w_2 = w'_2 \quad l_2 = l'_2 \\
&\vdots \quad \vdots \quad \vdots \quad \vdots \quad \vdots \\
&p_{m-2} \quad w'_{m-2} \quad l'_{m-2} \quad w_{m-2} = w'_{m-2} \quad l_{m-2} = l'_{m-2} \\
&p_{m-1} + p_m \quad w'_{m-1} \quad l'_{m-1} \quad w_{m-1} = w'_{m-1}0 \quad l_{m-1} = l'_{m-1} + 1 \\
&\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad w_m = w'_{m-1}1 \quad l_m = l'_{m-1} + 1 \\
\end{aligned}
\]  

Calculation of the average length \( \sum p'_i l'_i \) shows that  
\[
L(p) = L^*(p') + p_{m-1} + p_m.
\]  

Similarly, from the canonical code for \( p \), we construct a code for \( p' \) by merging the codewords for the two lowest-probability symbols \( m - 1 \) and \( m \) with probabilities \( p_{m-1} \) and \( p_m \), which are siblings by the properties of the canonical code. The new code for \( p' \) has average length  
\[
L(p') = \sum_{i=1}^{m-2} p_i l_i + p_{m-1}(l_{m-1} - 1) + p_m(l_m - 1).
\]  

Adding these together, we obtain  
\[
L(p') + L(p) = L^*(p') + L^*(p),
\]  
or  
\[
(L(p') - L^*(p')) + (L(p) - L^*(p)) = 0.
\]  

Now examine the two terms in the equation above. By assumption, since \( L^*(p') \) is the optimal length for \( p' \), we have \( L(p') - L^*(p') \geq 0 \). Similarly, the length of the extension of the optimal code for \( p' \) has to have an average length at least as large as the optimal code for \( p \) [i.e., \( L(p) - L^*(p) \geq 0 \)]. But the sum of two nonnegative terms can only be 0 if both of them are 0, which implies that \( L(p) = L^*(p) \) (i.e., the extension of the optimal code for \( p' \) is optimal for \( p \)).  

Consequently, if we start with an optimal code for \( p' \) with \( m - 1 \) symbols and construct a code for \( m \) symbols by extending the codeword corresponding to \( p_{m-1} + p_m \), the new code is also optimal. Starting with a code for two elements, in which case the optimal code is obvious, we can by induction extend this result to prove the following theorem.

**Theorem 5.8.1**  
Huffman coding is optimal; that is, if \( C^* \) is a Huffman code and \( C' \) is any other uniquely decodable code, \( L(C^*) \leq L(C') \).  

Although we have proved the theorem for a binary alphabet, the proof can be extended to establishing optimality of the Huffman coding algorithm for a \( D \)-ary alphabet as well. Incidentally, we should remark that Huffman coding is a “greedy” algorithm in that it coalesces the two least likely symbols at each stage. The proof above shows that this local optimality ensures global optimality of the final code.

---

### SHANNON–FANO–ELIAS CODING

In Section 5.4, we showed that the codeword lengths \( l(x) = \lceil \log \frac{1}{p(x)} \rceil \)



To process the text according to the instructions, I will clean it up by removing unnecessary formatting, fixing broken sentences, ensuring consistent punctuation and capitalization, removing non-standard characters, preserving the original meaning and structure, and making the text more readable. Here is the cleaned text:

---

### 5.9 Shannon–Fano–Elias Coding

To satisfy the Kraft inequality and construct a uniquely decodable code for the source, we describe a simple constructive procedure that uses the cumulative distribution function to allot codewords. Without loss of generality, we can take \( X = \{1, 2, \dots, m\} \). Assume that \( p(x) > 0 \) for all \( x \). The cumulative distribution function \( F(x) \) is defined as:

\[
F(x) = \sum_{a \leq x} p(a). \tag{5.72}
\]

This function is illustrated in Figure 5.5. Consider the modified cumulative distribution function:

\[
F(x) = \sum_{a < x} p(a) + \frac{1}{2}p(x). \tag{5.73}
\]

Here, \( F(x) \) denotes the sum of the probabilities of all symbols less than \( x \) plus half the probability of the symbol \( x \). Since the random variable is discrete, the cumulative distribution function consists of steps of size \( p(x) \). The value of the function \( F(x) \) is the midpoint of the step corresponding to \( x \). Since all probabilities are positive, \( F(a) \neq F(b) \) if \( a \neq b \), and hence we can determine \( x \) if we know \( F(x) \). The value of \( F(x) \) can be used as a code for \( x \).

However, \( F(x) \) is a real number expressible only by an infinite number of bits. Using the exact value of \( F(x) \) as a code for \( x \) is inefficient. If we truncate \( F(x) \) to \( l(x) \) bits (denoted by \( \lfloor F(x) \rfloor_l(x) \)), we use the first \( l(x) \) bits of \( F(x) \) as a code for \( x \). By definition of rounding off:

\[
F(x) - \lfloor F(x) \rfloor_l(x) < \frac{1}{2^{l(x)}}. \tag{5.74}
\]

If \( l(x) = \lceil \log \frac{1}{p(x)} \rceil + 1 \), then:

\[
\frac{1}{2^{l(x)}} < p(x)^2 = F(x) - F(x-1), \tag{5.75}
\]

and therefore \( \lfloor F(x) \rfloor_l(x) \) lies within the step corresponding to \( x \). Thus, \( l(x) \) bits suffice to describe \( x \).

In addition to requiring that the codeword identify the corresponding symbol, we also require the set of codewords to be prefix-free. To check whether the code is prefix-free, we consider each codeword \( z_1z_2 \dots z_l \) to represent not a point but the interval:

\[
\left[0.z_1z_2 \dots z_l, 0.z_1z_2 \dots z_l + \frac{1}{2^l}\right).
\]

The code is prefix-free if and only if the intervals corresponding to codewords are disjoint. The interval corresponding to any codeword has length \( 2^{-l(x)} \), which is less than half the height of the step corresponding to \( x \) by (5.75). The lower end of the interval is in the lower half of the step. Thus, the upper end of the interval lies below the top of the step, and the interval corresponding to any codeword lies entirely within the step corresponding to that symbol in the cumulative distribution function. Therefore, the intervals corresponding to different codewords are disjoint, and the code is prefix-free. Note that this procedure does not require the symbols to be ordered in terms of probability. Another procedure that uses the ordered probabilities is described in Problem 5.5.28.

Since we use \( l(x) = \lceil \log \frac{1}{p(x)} \rceil + 1 \) bits to represent \( x \), the expected length of this code is:

\[
L = \sum_x p(x)l(x) = \sum_x p(x) \left(\lceil \log \frac{1}{p(x)} \rceil + 1\right) < H(X) + 2. \tag{5.76}
\]

Thus, this coding scheme achieves an average codeword length that is within 2 bits of the entropy.

#### Example 5.9.1

We first consider an example where all the probabilities are dyadic. We construct the code in the following table:

| \( x \) | \( p(x) \) | \( F(x) \) | \( F(x) \) in Binary | \( l(x) = \lceil \log \frac{1}{p(x)} \rceil + 1 \) | Codeword |
|--------|------------|------------|-------------------------|--------------------------------------------|----------|
| 1      | 0.25      | 0.25      | 0.125                  | 3                                          | 001      |
| 2      | 0.5       | 0.75      | 0.5                    | 2                                          | 10       |
| 3      | 0.125     | 0.875     | 0.8125                 | 4                                          | 1101     |
| 4      | 0.125     | 1.0       | 0.9375                 | 4                                          | 1111     |

In this case, the average codeword length is 2.75 bits, and the entropy is 1.75 bits. The Huffman code for this case achieves the entropy bound. Looking at the codewords, it is obvious that there is some inefficiency—for example, the last bit of the last two codewords can be omitted. But if we remove the last bit from all the codewords, the code is no longer prefix-free.

#### Example 5.9.2

We now give another example for the construction of the Shannon–Fano–Elias code. In this case, since the distribution is not dyadic, the representation of \( F(x) \) in binary may have an infinite number of bits. We denote \( 0.01010101\ldots \) by \( 0.01 \). We construct the code in the following table:

| \( x \) | \( p(x) \) | \( F(x) \) | \( F(x) \) in Binary | \( l(x) = \lceil \log \frac{1}{p(x)} \rceil + 1 \) | Codeword |
|--------|------------|------------|-------------------------|--------------------------------------------|----------|
| 1      | 0.25      | 0.25      | 0.125                  | 3                                          | 001      |
| 2      | 0.25      | 0.5       | 0.375                  | 3                                          | 011      |
| 3      | 0.2       | 0.7       | 0.6                    | 4                                          | 1001     |
| 4      | 0.15      | 0.85      | 0.775                  | 4                                          | 1100     |
| 5      | 0.15      | 1.0       | 0.925                  | 4                                          | 1110     |

The above code is 1.2 bits longer on average than the Huffman code for this source (Example 5.6.1).

The Shannon–Fano–Elias coding procedure can also be applied to sequences of random variables. The key idea is to use the cumulative distribution function of the sequence, expressed to the appropriate accuracy, as a code for the sequence. Direct application of the method to blocks of length \( n \) would require calculation of the probabilities and cumulative distribution function for all sequences of length \( n \), a calculation that would grow exponentially with the block length. But a simple trick ensures that we can calculate both the probability and the cumulative density function sequentially as we see each symbol in the block, ensuring that the calculation grows only linearly with the block length. Direct application of Shannon–Fano–Elias coding would also need arithmetic whose precision grows with the block size, which is not practical when dealing with long blocks. In Chapter 13, we describe arithmetic coding, which is an extension of the Shannon–Fano–Elias method to sequences of random variables that encodes using fixed-precision arithmetic with a complexity that is linear in the length of the sequence. This method is the basis of many practical compression schemes such as those used in the JPEG and FAX compression standards.

---

### 5.10 Competitive Optimality of the Shannon Code

We have shown that Huffman coding is optimal in that it has minimum expected length. But what does that say about its performance on any particular sequence? For example, is it always better than any other code for all sequences? Obviously not, since there are codes that assign short codewords to infrequent source symbols. Such codes will be better than the Huffman code on those source symbols.

To formalize the question of competitive optimality, consider the following two-person zero-sum game: Two people are given a probability distribution and are asked to design an instantaneous code for the distribution. Then a source symbol is drawn from this distribution, and the payoff to player A is 1 or -1, depending on whether the codeword of player A is shorter or longer than the codeword of player B. The payoff is 0 for ties.

Dealing with Huffman code lengths is difficult, since there is no explicit expression for the codeword lengths. Instead, we consider the Shannon code with codeword lengths \( l(x) = \lceil \log \frac{1}{p(x)} \rceil \). In this case, we have the following theorem.

#### Theorem 5.10.1

Let \( l(x) \) be the codeword lengths associated with the Shannon code, and let \( l'(x) \) be the codeword lengths associated with any other uniquely decodable code. Then:

\[
\Pr\left(l(X) \geq l'(X) + c\right) \leq \frac{1}{2^{c-1}}. \tag{5.77}
\]

For example, the probability that \( l'(X) \) is 5 or more bits shorter than \( l(X) \) is less than \( \frac{1}{16} \).

**Proof:**

\[
\Pr\left(l(X) \geq l'(X) + c\right) = \Pr\left(\lceil \log \frac{1}{p(X)} \rceil \geq l'(X) + c\right) \leq \Pr\left(\log \frac{1}{p(X)} \geq l'(X) + c - 1\right) = \Pr\left(p(X) \leq 2^{-l'(X)-c+1}\right).
\]

\[
= \sum_{x: p(x) \leq 2^{-l'(x)-c+1}} p(x) \leq \sum_{x: p(x) \leq 2^{-l'(x)-c+1}} 2^{-l'(x)-c+1} = \sum_{x} 2^{-l'(x)-c+1}.
\]

\[
\leq 2^{-c+1} \sum_{x} 2^{-l'(x)} \leq 2^{-c+1}.
\]

Hence, no other code can do much better than the Shannon code most of the time. We now strengthen this result. In a game-theoretic setting, one would like to ensure that \( l(x) < l'(x) \) more often than \( l(x) > l'(x) \). The fact that \( l(x) \leq l'(x) + 1 \) with probability \( \geq \frac{1}{2} \) does not ensure this.

We now show that even under this stricter criterion, Shannon coding is optimal. Recall that the probability mass function \( p(x) \) is dyadic if \( \log \frac{1}{p(x)} \) is an integer for all \( x \).

#### Theorem 5.10.2

For a dyadic probability mass function \( p(x) \), let \( l(x) = \log \frac{1}{p(x)} \) be the word lengths of the binary Shannon code for the source, and let \( l'(x) \) be the lengths of any other uniquely decodable binary code for the source. Then:

\[
\Pr(l(X) < l'(X)) \geq \Pr(l(X) > l'(X)), \tag{5.85}
\]

with equality if and only if \( l'(x) = l(x) \) for all \( x \). Thus, the code length assignment \( l(x) = \log \frac{1}{p(x)} \) is uniquely competitively optimal.

**Proof:**

Define the function \( \text{sgn}(t) \) as follows:

\[
\text{sgn}(t) = \begin{cases}
1 & \text{if } t > 0, \\
0 & \text{if } t = 0, \\
-1 & \text{if } t < 0.
\end{cases}
\]

Then it is easy to see from Figure 5.6 that:

\[
\text{sgn}(t) \leq 2t - 1 \tag{5.87}
\]

for \( t = 0, \pm 1, \pm 2, \dots \). Note that though this inequality is not satisfied for all \( t \), it is satisfied at all integer values of \( t \). We can now write:

\[
\Pr\left(l'(X) < l(X)\right) - \Pr\left(l'(X) > l(X)\right) = \sum_{x: l'(x) < l(x)} p(x) - \sum_{x: l'(x) > l(x)} p(x).
\]

\[
= \sum_{x} p(x) \text{sgn}\left(l(X) - l'(X)\right) = E \left[\text{sgn}\left(l(X) - l'(X)\right)\right].
\]

(a) \[ \leq \sum_{x} p(x) \left(2l(x) - l'(x) - 1\right). \]

\[
= \sum_{x} 2^{-l(x)} \left(2l(x) - l'(x) - 1\right) = \sum_{x} 2^{-l'(x)} - \sum_{x} 2^{-l(x)}.
\]

\[
= 1 - 1 = 0,
\]

where (a) follows from the bound on \( \text{sgn}(x) \) and (b) follows from the fact that \( l'(x) \) satisfies the Kraft inequality.

We have equality in the above chain only if we have equality in (a) and (b). We have equality in the bound for \( \text{sgn}(t) \) only if \( t \) is 0 or 1 [i.e., \( l(x) = l'(x) \) or \( l(x) = l'(x) + 1 \)]. Equality in (b) implies that \( l'(x) \) satisfies the Kraft inequality with equality. Combining these two facts implies that \( l'(x) = l(x) \) for all \( x \).

#### Corollary

For nondyadic probability mass functions,

\[
E \left[\text{sgn}\left(l(X) - l'(X) - 1\right)\right] \leq 0, \tag{5.97}
\]

where \( l(x) = \lceil \log \frac{1}{p(x)} \rceil \) and \( l'(x) \) is any other code for the source.

**Proof:**

Along the same lines as the preceding proof.

---

Hence, we have shown that Shannon coding \( l(x) = \lceil \log \frac{1}{p(x)} \rceil \) is uniquely competitively optimal.



is optimal under a variety of criteria; it is robust with respect to the payoff function. In particular, for dyadic p, E(l − l′) ≤ 0, E sgn(l − l′) ≤ 0, and by use of inequality (5.87), E f(l − l′) ≤ 0 for any function f satisfying f(t) ≤ 2t − 1, t = 0, ±1, ±2, . . ..  

5.11  
GENERATION OF DISCRETE DISTRIBUTIONS FROM FAIR COINS  

In the early sections of this chapter, we considered the problem of representing a random variable by a sequence of bits such that the expected length of the representation was minimized. It can be argued (Problem 5.5.29) that the encoded sequence is essentially incompressible and therefore has an entropy rate close to 1 bit per symbol. Therefore, the bits of the encoded sequence are essentially fair coin flips.  

In this section, we take a slight detour from our discussion of source coding and consider the dual question. How many fair coin flips does it take to generate a random variable X drawn according to a specified probability mass function p? We first consider a simple example.  

**Example 5.11.1**  
Given a sequence of fair coin tosses (fair bits), suppose that we wish to generate a random variable X with distribution  

X =  
  a with probability 1/2,  
  b with probability 1/4,  
  c with probability 1/4.  

It is easy to guess the answer. If the first bit is 0, we let X = a. If the first two bits are 10, we let X = b. If we see 11, we let X = c. It is clear that X has the desired distribution.  

We calculate the average number of fair bits required for generating the random variable, in this case as (1/2)(1) + (1/4)(2) + (1/4)(2) = 1.5 bits. This is also the entropy of the distribution. Is this unusual? No, as the results of this section indicate.  

The general problem can now be formulated as follows. We are given a sequence of fair coin tosses Z₁, Z₂, . . . , and we wish to generate a discrete random variable X ∈ X = {1, 2, . . . , m} with probability mass function p = (p₁, p₂, . . . , pₘ). Let the random variable T denote the number of coin flips used in the algorithm.  

We can describe the algorithm mapping strings of bits Z₁, Z₂, . . . , to possible outcomes X by a binary tree. The leaves of the tree are marked by output symbols X, and the path to the leaves is given by the sequence of bits produced by the fair coin. For example, the tree for the distribution (1/2, 1/4, 1/4) is shown in Figure 5.7.  

The tree representing the algorithm must satisfy certain properties:  
1. The tree should be complete (i.e., every node is either a leaf or has two descendants in the tree). The tree may be infinite, as we will see in some examples.  
2. The probability of a leaf at depth k is 2⁻ᵏ. Many leaves may be labeled with the same output symbol—the total probability of all these leaves should equal the desired probability of the output symbol.  
3. The expected number of fair bits E[T] required to generate X is equal to the expected depth of this tree.  

There are many possible algorithms that generate the same output distribution. For example, the mapping 00 → a, 01 → b, 10 → c, 11 → a also yields the distribution (1/2, 1/4, 1/4). However, this algorithm uses two fair bits to generate each sample and is therefore not as efficient as the mapping given earlier, which used only 1.5 bits per sample. This brings up the question: What is the most efficient algorithm to generate a given distribution, and how is this related to the entropy of the distribution?  

We expect that we need at least as much randomness in the fair bits as we produce in the output samples. Since entropy is a measure of randomness, and each fair bit has an entropy of 1 bit, we expect that the number of fair bits used will be at least equal to the entropy of the output. This is proved in the following theorem. We will need a simple lemma about trees in the proof of the theorem. Let Y denote the set of leaves of a complete tree. Consider a distribution on the leaves such that the probability of a leaf at depth k on the tree is 2⁻ᵏ. Let Y be a random variable with this distribution. Then we have the following lemma.  

**Lemma 5.11.1**  
For any complete tree, consider a probability distribution on the leaves such that the probability of a leaf at depth k is 2⁻ᵏ. Then the expected depth of the tree is equal to the entropy of this distribution.  

*Proof:*  
The expected depth of the tree  
E[T] = ∑_{y ∈ Y} k(y)2⁻ᵏ(y)  
and the entropy of the distribution of Y is  
H(Y) = -∑_{y ∈ Y} 1/2^{k(y)} log(1/2^{k(y)})  
= ∑_{y ∈ Y} k(y)2⁻ᵏ(y).  
Thus,  
H(Y) = E[T].  
□  

**Theorem 5.11.1**  
For any algorithm generating X, the expected number of fair bits used is greater than the entropy H(X), that is,  
E[T] ≥ H(X).  

*Proof:*  
Any algorithm generating X from fair bits can be represented by a complete binary tree. Label all the leaves of this tree by distinct symbols y ∈ Y = {1, 2, . . .}. If the tree is infinite, the alphabet Y is also infinite. Now consider the random variable Y defined on the leaves of the tree, such that for any leaf y at depth k, the probability that Y = y is 2⁻ᵏ. By Lemma 5.11.1, the expected depth of this tree is equal to the entropy of Y:  
E[T] = H(Y).  

Now the random variable X is a function of Y (one or more leaves map onto an output symbol), and hence by the result of Problem 2.4, we have  
H(X) ≤ H(Y).  

Thus, for any algorithm generating the random variable X, we have  
H(X) ≤ E[T].  
□  

The same argument answers the question of optimality for a dyadic distribution.  

**Theorem 5.11.2**  
Let the random variable X have a dyadic distribution. The optimal algorithm to generate X from fair coin flips requires an expected number of coin tosses precisely equal to the entropy:  
E[T] = H(X).  

*Proof:*  
Theorem 5.11.1 shows that we need at least H(X) bits to generate X. For the constructive part, we use the Huffman code tree for X as the tree to generate the random variable. For a dyadic distribution, the Huffman code is the same as the Shannon code and achieves the entropy bound. For any x ∈ X, the depth of the leaf in the code tree corresponding to x is the length of the corresponding codeword, which is log(1/p(x)). Hence, when this code tree is used to generate X, the leaf will have a probability 2^{-log(1/p(x))} = p(x). The expected number of coin flips is the expected depth of the tree, which is equal to the entropy (because the distribution is dyadic). Hence, for a dyadic distribution, the optimal generating algorithm achieves  
E[T] = H(X).  
□  

What if the distribution is not dyadic? In this case, we cannot use the same idea, since the code tree for the Huffman code will generate a dyadic distribution on the leaves, not the distribution with which we started. Since all the leaves of the tree have probabilities of the form 2⁻ᵏ, it follows that we should split any probability p_i that is not of this form into atoms of this form. We can then allot these atoms to leaves on the tree. For example, if one of the outcomes x has probability p(x) = 1/4, we need only one atom (leaf of the tree at level 2), but if p(x) = 7/8 = 1/2 + 1/4 + 1/8, we need three atoms, one each at levels 1, 2, and 3 of the tree.  

To minimize the expected depth of the tree, we should use atoms with as large a probability as possible. So given a probability p_i, we find the largest atom of the form 2⁻ᵏ that is less than p_i, and allot this atom to the tree. Then we calculate the remainder and find that largest atom that will fit in the remainder. Continuing this process, we can split all the probabilities into dyadic atoms. This process is equivalent to finding the binary expansions of the probabilities. Let the binary expansion of the probability p_i be  

p_i = ∑_{j ≥ 1} p_j^{(i)},  
where p_j^{(i)} = 2⁻ʲ or 0. Then the atoms of the expansion are the {p_j^{(i)} : i = 1, 2, . . . , m, j ≥ 1}.  

Since ∑_{i} p_i = 1, the sum of the probabilities of these atoms is 1. We will allot an atom of probability 2⁻ʲ to a leaf at depth j on the tree. The depths of the atoms satisfy the Kraft inequality, and hence by Theorem 5.2.1, we can always construct such a tree with all the atoms at the right depths. We illustrate this procedure with an example.  

**Example 5.11.2**  
Let X have the distribution  

X =  
  a with probability 2/3,  
  b with probability 1/3.  

We find the binary expansions of these probabilities:  
2/3 = 0.10101010 . . .₂  
1/3 = 0.01010101 . . .₂.  

Hence, the atoms for the expansion are  
2/3 → {1/2, 1/8, 1/32, . . .},  
1/3 → {1/4, 1/16, 1/64, . . .}.  

These can be allotted to a tree as shown in Figure 5.8.  

This procedure yields a tree that generates the random variable X. We have argued that this procedure is optimal (gives a tree of minimum expected depth), but we will not give a formal proof. Instead, we bound the expected depth of the tree generated by this procedure.  

**Theorem 5.11.3**  
The expected number of fair bits required by the optimal algorithm to generate a random variable X lies between H(X) and H(X) + 2:  
H(X) ≤ E[T] < H(X) + 2.  

*Proof:*  
The lower bound on the expected number of coin tosses is proved in Theorem 5.11.1. For the upper bound, we write down an explicit expression for the expected number of coin tosses required for the procedure described above. We split all the probabilities (p₁, p₂, . . . , pₘ) into dyadic atoms, for example,  

p₁ → {p₁^{(1)}, p₂^{(1)}, . . .},  
and so on. Using these atoms (which form a dyadic distribution), we construct a tree with leaves corresponding to each of these atoms. The number of coin tosses required to generate each atom is its depth in the tree, and therefore the expected number of coin tosses is the expected depth of the tree, which is equal to the entropy of the dyadic distribution of the atoms. Hence,  

E[T] = H(Y),  

where Y has the distribution (p₁^{(1)}, p₂^{(1)}, . . . , p₁^{(2)}, p₂^{(2)}, . . . , p₁^{(m)}, p₂^{(m)}, . . .).  

Now since X is a function of Y, we have  
H(X) ≤ H(Y).  

Thus, for any algorithm generating the random variable X, we have  
H(X) ≤ E[T].  

To show the upper bound, we consider the entropy of Y. Since X is a function of Y, we have  

H(Y) = H(Y, X) = H(X) + H(Y|X).  

Our objective is to show that H(Y|X) < 2. We now give an algebraic proof of this result. Expanding the entropy of Y, we have  

H(Y) = -∑_{i=1}^{m} ∑_{j ≥ 1} p_j^{(i)} log p_j^{(i)}  
= ∑_{i=1}^{m} ∑_{j: p_j^{(i)} > 0} j2⁻ʲ.  

Now consider the term in the expansion corresponding to each i, which we shall call T_i:  

T_i = ∑_{j: p_j^{(i)} > 0} j2⁻ʲ.  

We can find an n such that 2⁻(n-1) > p_i ≥ 2⁻ⁿ, or  
n - 1 < -log p_i ≤ n.  

Then it follows that p_j^{(i)} > 0 only if j ≥ n, so that we can rewrite the above as  

T_i = ∑_{j: j ≥ n, p_j^{(i)} > 0} j2⁻ʲ.  

We use the definition of the atom to write p_i as  

p_i = ∑_{j: j ≥ n, p_j^{(i)} > 0} 2⁻ʲ.  

To prove the upper bound, we first show that T_i < -p_i log p_i + 2p_i. Consider the difference  

T_i + p_i log p_i - 2p_i  
(a) < T_i - p_i(n - 1) - 2p_i  
= T_i - (n - 1 + 2)p_i  
= ∑_{j: j ≥ n, p_j^{(i)} > 0} j2⁻ʲ - (n + 1) ∑_{j: j ≥ n, p_j^{(i)} > 0} 2⁻ʲ  
= ∑_{j: j ≥ n, p_j^{(i)} > 0} (j - n - 1)2⁻ʲ  
= -2⁻ⁿ + 0 + ∑_{j: j ≥ n+2, p_j^{(i)} > 0} (j - n - 1)2⁻ʲ  
(b) = -2⁻ⁿ + ∑_{k: k ≥ 1, p_{k+n+1}^{(i)} > 0} k2⁻(k+n+1)  
(c) ≤ -2⁻ⁿ + ∑_{k: k ≥ 1} k2⁻(k+n+1)  
= -2⁻ⁿ + 2⁻(n+1)2  
= 0,  

where (a) follows from (5.122), (b) follows from a change of variables for the summation, and (c) follows from increasing the range of the summation. Hence, we have shown that  

T_i < -p_i log p_i + 2p_i.  

Since E[T] = ∑_{i} T_i, it follows immediately that  

E[T] < -∑_{i} p_i log p_i + 2 ∑_{i} p_i = H(X) + 2,  

completing the proof of the theorem.  
□  

Thus, an average of H(X) + 2 coin flips suffice to simulate a random variable X.  

**SUMMARY**  
- **Kraft inequality**: Instantaneous codes ⇔ ∑ D^{-l_i} ≤ 1.  
- **McMillan inequality**: Uniquely decodable codes ⇔ ∑ D^{-l_i} ≤ 1.  
- **Entropy bound on data compression**:  
  L = ∑ p_i l_i ≥ H_D(X).  
- **Shannon code**:  
  l(x) = ⌈log_D(1/p(x))⌉,  
  H_D(X) ≤ L < H_D(X) + 1.  
- **Huffman code**:  
  L* = min ∑ D^{-l_i} ≤ 1 ∑ p_i l_i,  
  H_D(X) ≤ L* < H_D(X) + 1.  
- **Wrong code**: X ∼ p(x), l(x) = ⌈log(1/q(x))⌉.



H(X₁, X₂, ..., Xₙ) ≤ Lₙ < H(X₁, X₂, ..., Xₙ) + 1.  
(5.141)  

Stochastic processes  
H(X₁, X₂, ..., Xₙ)  
‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾



--- 

**Page 75**

**PROBLEMS**

**149.** Symbols are encoded into longer codewords. Suppose that the message probabilities are given in decreasing order, \( p_1 > p_2 \geq \cdots \geq p_m \).

**(a)** Prove that for any binary Huffman code, if the most probable message symbol has probability \( p_1 > \frac{1}{2} \), that symbol must be assigned a codeword of length 1.

**(b)** Prove that for any binary Huffman code, if the most probable message symbol has probability \( p_1 < \frac{1}{3} \), that symbol must be assigned a codeword of length \( \geq 2 \).

**5.26. Merges.** Companies with values \( W_1, W_2, \ldots, W_m \) are merged as follows. The two least valuable companies are merged, thus forming a list of \( m-1 \) companies. The value of the merge is the sum of the values of the two merged companies. This continues until one supercompany remains. Let \( V \) equal the sum of the values of the merges. Thus, \( V \) represents the total reported dollar volume of the merges. For example, if \( W = (3, 3, 2, 2) \), the merges yield \( (3, 3, 2, 2) \to (4, 3, 3) \to (6, 4) \to (10) \) and \( V = 4 + 6 + 10 = 20 \).

**(a)** Argue that \( V \) is the minimum volume achievable by sequences of pairwise merges terminating in one supercompany. (Hint: Compare to Huffman coding.)

**(b)** Let \( W = \{ W_i \} \), \( \tilde{W}_i = W_i / W \), and show that the minimum merge volume \( V \) satisfies  
\[
W H(\tilde{W}) \leq V \leq W H(\tilde{W}) + W.
\]  
(5.147)

**5.27. Sardinas–Patterson test for unique decodability.** A code is not uniquely decodable if and only if there exists a finite sequence of code symbols which can be resolved into sequences of codewords in two different ways. That is, a situation such as  
\[
B_1 A_1 A_2 A_3 \cdots A_m B_2 B_3 \cdots B_n \cdots
\]  
must occur where each \( A_i \) and each \( B_i \) is a codeword. Note that \( B_1 \) must be a prefix of \( A_1 \) with some resulting "dangling suffix." Each dangling suffix must in turn be either a prefix of a codeword or have another codeword as its prefix, resulting in another dangling suffix. Finally, the last dangling suffix in the sequence must also be a codeword. Thus, one can set up a test for unique decodability (which is essentially the Sardinas–Patterson test [456]) in the following way: Construct a set \( S \) of all possible dangling suffixes. The code is uniquely decodable if and only if \( S \) contains no codeword.

**(a)** State the precise rules for building the set \( S \).

**(b)** Suppose that the codeword lengths are \( l_i \), \( i = 1, 2, \ldots, m \). Find a good upper bound on the number of elements in the set \( S \).

**(c)** Determine which of the following codes is uniquely decodable:  
**(i)** \( \{0, 10, 11\} \)  
**(ii)** \( \{0, 01, 11\} \)  
**(iii)** \( \{0, 01, 10\} \)  
**(iv)** \( \{0, 01\} \)  
**(v)** \( \{00, 01, 10, 11\} \)  
**(vi)** \( \{110, 11, 10\} \)  
**(vii)** \( \{110, 11, 100, 00, 10\} \).

**(d)** For each uniquely decodable code in part (c), construct, if possible, an infinite encoded sequence with a known starting point such that it can be resolved into codewords in two different ways. (This illustrates that unique decodability does not imply finite decodability.) Prove that such a sequence cannot arise in a prefix code.

**5.28. Shannon code.** Consider the following method for generating a code for a random variable \( X \) that takes on \( m \) values \( \{1, 2, \ldots, m\} \) with probabilities \( p_1, p_2, \ldots, p_m \). Assume that the probabilities are ordered so that \( p_1 \geq p_2 \geq \cdots \geq p_m \). Define  
\[
F_i = \sum_{k=1}^{i-1} p_k,
\]  
(5.148)  
the sum of the probabilities of all symbols less than \( i \). Then the codeword for \( i \) is the number \( F_i \in [0, 1] \) rounded off to \( l_i \) bits, where \( l_i = \lceil \log_2 \frac{1}{p_i} \rceil \).

**(a)** Show that the code constructed by this process is prefix-free and that the average length satisfies  
\[
H(X) \leq L < H(X) + 1.
\]  
(5.149)

**(b)** Construct the code for the probability distribution \( (0.5, 0.25, 0.125, 0.125) \).

---

**Page 76**

**151. DATA COMPRESSION**

**5.29. Optimal codes for dyadic distributions.** For a Huffman code tree, define the probability of a node as the sum of the probabilities of all the leaves under that node. Let the random variable \( X \) be drawn from a dyadic distribution [i.e., \( p(x) = 2^{-i} \), for some \( i \), for all \( x \in X \)]. Now consider a binary Huffman code for this distribution.

**(a)** Argue that for any node in the tree, the probability of the left child is equal to the probability of the right child.

**(b)** Let \( X_1, X_2, \ldots, X_n \) be drawn i.i.d. \( \sim p(x) \). Using the Huffman code for \( p(x) \), we map \( X_1, X_2, \ldots, X_n \) to a sequence of bits \( Y_1, Y_2, \ldots, Y_k(X_1, X_2, \ldots, X_n) \). (The length of this sequence will depend on the outcome \( X_1, X_2, \ldots, X_n \).) Use part (a) to argue that the sequence \( Y_1, Y_2, \ldots \) forms a sequence of fair coin flips [i.e., that \( \Pr\{Y_i = 0\} = \Pr\{Y_i = 1\} = \frac{1}{2} \), independent of \( Y_1, Y_2, \ldots, Y_{i-1} \)]. Thus, the entropy rate of the coded sequence is 1 bit per symbol.

**(c)** Give a heuristic argument why the encoded sequence of bits for any code that achieves the entropy bound cannot be compressible and therefore should have an entropy rate of 1 bit per symbol.

**5.30. Relative entropy is cost of miscoding.** Let the random variable \( X \) have five possible outcomes \( \{1, 2, 3, 4, 5\} \). Consider two distributions \( p(x) \) and \( q(x) \) on this random variable.

| Symbol | \( p(x) \) | \( q(x) \) | \( C_1(x) \) | \( C_2(x) \) |
|--------|-----------|-----------|-------------|-------------|
| 1      | 1/2       | 1/8       | 0           | 00          |
| 2      | 1/4       | 1/8       | 10          | 01          |
| 3      | 1/8       | 1/8       | 110         | 10          |
| 4      | 1/16      | 1/8       | 1110        | 11          |
| 5      | 1/16      | 1/8       | 1111        | 111         |

**(a)** Calculate \( H(p) \), \( H(q) \), \( D(p \| q) \), and \( D(q \| p) \).

**(b)** The last two columns represent codes for the random variable. Verify that the average length of \( C_1 \) under \( p \) is equal to the entropy \( H(p) \). Thus, \( C_1 \) is optimal for \( p \). Verify that \( C_2 \) is optimal for \( q \).

**(c)** Now assume that we use code \( C_2 \) when the distribution is \( p \). What is the average length of the codewords? By how much does it exceed the entropy \( p \)?

**(d)** What is the loss if we use code \( C_1 \) when the distribution is \( q \)?

---

**Page 77**

**152. DATA COMPRESSION**

**5.31. Nonsingular codes.** The discussion in the text focused on instantaneous codes, with extensions to uniquely decodable codes. Both these are required in cases when the code is to be used repeatedly to encode a sequence of outcomes of a random variable. But if we need to encode only one outcome and we know when we have reached the end of a codeword, we do not need unique decodability—the fact that the code is nonsingular would suffice. For example, if a random variable \( X \) takes on three values, \( a \), \( b \), and \( c \), we could encode them by \( 0 \), \( 1 \), and \( 00 \). Such a code is nonsingular but not uniquely decodable.

In the following, assume that we have a random variable \( X \) which takes on \( m \) values with probabilities \( p_1, p_2, \ldots, p_m \) and that the probabilities are ordered so that \( p_1 \geq p_2 \geq \cdots \geq p_m \).

**(a)** By viewing the nonsingular binary code as a ternary code with three symbols, \( 0 \), \( 1 \), and "STOP," show that the expected length of a nonsingular code \( L_{1:1} \) for a random variable \( X \) satisfies the following inequality:  
\[
L_{1:1} \geq \frac{H_2(X)}{\log_2 3 - 1},
\]  
(5.150)  
where \( H_2(X) \) is the entropy of \( X \) in bits. Thus, the average length of a nonsingular code is at least a constant fraction of the average length of an instantaneous code.

**(b)** Let \( L_{\text{INST}} \) be the expected length of the best instantaneous code and \( L_{1:1}^* \) be the expected length of the best nonsingular code for \( X \). Argue that  
\[
L_{1:1}^* \leq L_{\text{INST}}^* \leq H(X) + 1.
\]

**(c)** Give a simple example where the average length of the nonsingular code is less than the entropy.

**(d)** The set of codewords available for a nonsingular code is \( \{0, 1, 00, 01, 10, 11, 000, \ldots\} \). Since \( L_{1:1} = \sum_{i=1}^m p_i l_i \), show that this is minimized if we allot the shortest codewords to the most probable symbols. Thus, \( l_1 = l_2 = 1 \), \( l_3 = l_4 = l_5 = l_6 = 2 \), etc. Show that in general \( l_i = \lceil \log_2 (i + 1) \rceil \), and therefore  
\[
L_{1:1}^* = \sum_{i=1}^m p_i \lceil \log_2 (i + 1) \rceil.
\]

**(e)** Part (d) shows that it is easy to find the optimal nonsingular code for a distribution. However, it is a little more tricky to deal with the average length of this code. We now bound this average length. It follows from part (d) that  
\[
L_{1:1}^* \geq \tilde{L} \triangleq \sum_{i=1}^m p_i \log_2 (i + 1).
\]  
Consider the difference  
\[
F(p) = H(X) - \tilde{L} = -\sum_{i=1}^m p_i \log p_i - \sum_{i=1}^m p_i \log \frac{i + 1}{2}.
\]  
(5.151)  
Prove by the method of Lagrange multipliers that the maximum of \( F(p) \) occurs when \( p_i = \frac{c}{i + 2} \), where \( c = \frac{1}{H_{m+2} - H_2} \) and \( H_k \) is the sum of the harmonic series:  
\[
H_k \triangleq \sum_{i=1}^k \frac{1}{i}.
\]  
(5.152)  
(This can also be done using the nonnegativity of relative entropy.)

**(f)** Complete the arguments for  
\[
H(X) - L_{1:1}^* \leq H(X) - \tilde{L} \leq \log_2 (2(H_{m+2} - H_2)).
\]  
(5.153)  
Now it is well known (see, e.g., Knuth [315]) that \( H_k \approx \ln k \) (more precisely, \( H_k = \ln k + \gamma + \frac{1}{2k} - \frac{1}{12k^2} + \cdots \), and \( \gamma = \) Euler's constant \( = 0.577 \ldots \)). Using either this or a simple approximation that \( H_k \leq \ln k + 1 \), which can be proved by integration of \( \frac{1}{x} \), it can be shown that  
\[
H(X) - L_{1:1}^* < \log \log m + 2.
\]  
Thus, we have  
\[
H(X) - \log \log |X| - 2 \leq L_{1:1}^* \leq H(X) + 1.
\]  
(5.155)  
A nonsingular code cannot do much better than an instantaneous code!

---

**Page 78**

**153. DATA COMPRESSION**

**5.32. Bad wine.** One is given six bottles of wine. It is known that precisely one bottle has gone bad (tastes terrible). From inspection of the bottles, it is determined that the probability \( p_i \) that the \( i \)-th bottle is bad is given by \( (p_1, p_2, \ldots, p_6) = \left( \frac{8}{23}, \frac{6}{23}, \frac{4}{23}, \frac{2}{23}, \frac{2}{23}, \frac{1}{23} \right) \).

Tasting will determine the bad wine. Suppose that you taste the wines one at a time. Choose the order of tasting to minimize the expected number of tastings required to determine the bad bottle. Remember, if the first five wines pass the test, you don’t have to taste the last.

**(a)** What is the expected number of tastings required?

**(b)** Which bottle should be tasted first?

Now you get smart. For the first sample, you mix some of the wines in a fresh glass and sample the mixture. You proceed, mixing and tasting, stopping when the bad bottle has been determined.

**(a)** What is the minimum expected number of tastings required to determine the bad wine?

**(b)** What mixture should be tasted first?

**5.33. Huffman vs. Shannon.** A random variable \( X \) takes on three values with probabilities 0.6, 0.3, and 0.1.

**(a)** What are the lengths of the binary Huffman codewords for \( X \)? What are the lengths of the binary Shannon codewords \( l(x) = \lceil \log_2 \frac{1}{p(x)} \rceil \) for \( X \)?

**(b)** What is the smallest integer \( D \) such that the expected Shannon codeword length with a \( D \)-ary alphabet equals the expected Huffman codeword length with a \( D \)-ary alphabet?

**5.34. Huffman algorithm for tree construction.** Consider the following problem: \( m \) binary signals \( S_1, S_2, \ldots, S_m \) are available at times \( T_1 \leq T_2 \leq \cdots \leq T_m \), and we would like to find their sum \( S_1 \oplus S_2 \oplus \cdots \oplus S_m \) using two-input gates, each gate with one time unit delay, so that the final result is available as quickly as possible. A simple greedy algorithm is to combine the earliest two results, forming the partial result at time \( \max(T_1, T_2) + 1 \). We now have a new problem with \( S_1 \oplus S_2, S_3, \ldots, S_m \), available at times \( \max(T_1, T_2) + 1, T_3, \ldots, T_m \). We can now sort this list of \( T \)'s and apply the same merging step again, repeating this until we have the final result.

**(a)** Argue that the foregoing procedure is optimal, in that it constructs a circuit for which the final result is available as quickly as possible.

**(b)** Show that this procedure finds the tree that minimizes  
\[
C(T) = \max_i (T_i + l_i),
\]  
(5.156)  
where \( T_i \) is the time at which the result allotted to the \( i \)-th leaf is available and \( l_i \) is the length of the path from the \( i \)-th leaf to the root.

**(c)** Show that  
\[
C(T) \geq \log_2 \sum_{i=1}^m 2^{T_i}
\]  
(5.157)  
for any tree \( T \).

**(d)** Show that there exists a tree such that  
\[
C(T) \leq \log_2 \sum_{i=1}^m 2^{T_i}.
\]

--- 

**Page 79**

**154. DATA COMPRESSION**

**5.31. (continued)**  

**(e)** Part (d) shows that it is easy to find the optimal nonsingular code for a distribution. However, it is a little more tricky to deal with the average length of this code. We now bound this average length. It follows from part (d) that  
\[
L_{1:1}^* \geq \tilde{L} \triangleq \sum_{i=1}^m p_i \log_2 (i + 1).
\]  
Consider the difference  
\[
F(p) = H(X) - \tilde{L} = -\sum_{i=1}^m p_i \log p_i - \sum_{i=1}^m p_i \log \frac{i + 1}{2}.
\]  
(5.151)  
Prove by the method of Lagrange multipliers that the maximum of \( F(p) \) occurs when \( p_i = \frac{c}{i + 2} \), where \( c = \frac{1}{H_{m+2} - H_2} \) and \( H_k \) is the sum of the harmonic series:  
\[
H_k \triangleq \sum_{i=1}^k \frac{1}{i}.
\]  
(5.152)  
(This can also be done using the nonnegativity of relative entropy.)

**(f)** Complete the arguments for  
\[
H(X) - L_{1:1}^* \leq H(X) - \tilde{L} \leq \log_2 (2(H_{m+2} - H_2)).
\]  
(5.153)  
Now it is well known (see, e.g., Knuth [315]) that \( H_k \approx \ln k \) (more precisely, \( H_k = \ln k + \gamma + \frac{1}{2k} - \frac{1}{12k^2} + \cdots \), and \( \gamma = \) Euler's constant \( = 0.577 \ldots \)). Using either this or a simple approximation that \( H_k \leq \ln k + 1 \), which can be proved by integration of \( \frac{1}{x} \), it can be shown that  
\[
H(X) - L_{1:1}^* < \log \log m + 2.
\]  
Thus, we have  
\[
H(X) - \log \log |X| - 2 \leq L_{1:1}^* \leq H(X) + 1.
\]  
(5.155)  
A nonsingular code cannot do much better than an instantaneous code!

---

**Page 80**

**155. DATA COMPRESSION**

**5.32. (continued)**  

**(a)** What is the expected number of tastings required?

**(b)** Which bottle should be tasted first?

Now you get smart. For the first sample, you mix some of the wines in a fresh glass and sample the mixture. You proceed, mixing and tasting, stopping when the bad bottle has been determined.

**(a)** What is the minimum expected number of tastings required to determine the bad wine?

**(b)** What mixture should be tasted first?

**5.33. Huffman vs. Shannon.** A random variable \( X \) takes on three values with probabilities 0.6, 0.3, and 0.1.

**(a)** What are the lengths of the binary Huffman codewords for \( X \)? What are the lengths of the binary Shannon codewords \( l(x) = \lceil \log_2 \frac{1}{p(x)} \rceil \) for \( X \)?

**(b)** What is the smallest integer \( D \) such that the expected Shannon codeword length with a \( D \)-ary alphabet equals the expected Huffman codeword length with a \( D \)-ary alphabet?

**5.34. Huffman algorithm for tree construction.** Consider the following problem: \( m \) binary signals \( S_1, S_2, \ldots, S_m \) are available at times \( T_1 \leq T_2 \leq \cdots \leq T_m \), and we would like to find their sum \( S_1 \oplus S_2 \oplus \cdots \oplus S_m \) using two-input gates, each gate with one time unit delay, so that the final result is available as quickly as possible. A simple greedy algorithm is to combine the earliest two results, forming the partial result at time \( \max(T_1, T_2) + 1 \). We now have a new problem with \( S_1 \oplus S_2, S_3, \ldots, S_m \), available at times \( \max(T_1, T_2) + 1, T_3, \ldots, T_m \). We can now sort this list of \( T \)'s and apply the same merging step again, repeating this until we have the final result.

**(a)** Argue that the foregoing procedure is optimal, in that it constructs a circuit for which the final result is available as quickly as possible.

**(b)** Show that this procedure finds the tree that minimizes  
\[
C(T) = \max_i (T_i + l_i),
\]  
(5.156)  
where \( T_i \) is the time at which the result allotted to the \( i \)-th leaf is available and \( l_i \) is the length of the path from the \( i \)-th leaf to the root.

**(c)** Show that  
\[
C(T) \geq \log_2 \sum_{i=1}^m 2^{T_i}
\]  
(5.157)  
for any tree \( T \).

**(d)** Show that there exists a tree such that  
\[
C(T) \leq \log_2 \sum_{i=1}^m 2^{T_i}.
\]

--- 

**Page 81**

**156. DATA COMPRESSION**

**5.34. (continued)**  

**(a)** Argue that the foregoing procedure is optimal, in that it constructs a circuit for which the final result is available as quickly as possible.

**(b)** Show that this procedure finds the tree that minimizes  
\[
C(T) = \max_i (T_i + l_i),
\]  
(5.156)  
where \( T_i \) is the time at which the result allotted to the \( i \)-th leaf is available and \( l_i \) is the length of the path from the \( i \)-th leaf to the root.

**(c)** Show that  
\[
C(T) \geq \log_2 \sum_{i=1}^m 2^{T_i}
\]  
(5.157)  
for any tree \( T \).

**(d)** Show that there exists a tree such that  
\[
C(T) \leq \log_2 \sum_{i=1}^m 2^{T_i}.
\]



### Chapter 5: Data Compression

#### 5.35 Generating Random Variables
One wishes to generate a random variable \( X \) where:
\[
X = 
\begin{cases}
1 & \text{with probability } p \\
0 & \text{with probability } 1 - p
\end{cases}
\]
Given fair coin flips \( Z_1, Z_2, \ldots \), let \( N \) be the (random) number of flips needed to generate \( X \). Find a good way to use \( Z_1, Z_2, \ldots \) to generate \( X \). Show that \( EN \leq 2 \).

#### 5.36 Optimal Word Lengths
(a) Can \( l = (1, 2, 2) \) be the word lengths of a binary Huffman code? What about \( (2, 2, 3, 3) \)?  
(b) What word lengths \( l = (l_1, l_2, \ldots) \) can arise from binary Huffman codes?

#### 5.37 Codes
Which of the following codes are:  
(a) Uniquely decodable?  
(b) Instantaneous?  
- \( C_1 = \{00, 01, 0\} \)  
- \( C_2 = \{00, 01, 100, 101, 11\} \)  
- \( C_3 = \{0, 10, 110, 1110, \ldots\} \)  
- \( C_4 = \{0, 00, 000, 0000\} \)

#### 5.38 Huffman
Find the Huffman \( D \)-ary code for \( (p_1, p_2, p_3, p_4, p_5, p_6) = \left(\frac{6}{25}, \frac{6}{25}, \frac{4}{25}, \frac{4}{25}, \frac{3}{25}, \frac{2}{25}\right) \) and the expected word length:  
(a) For \( D = 2 \).  
(b) For \( D = 4 \).

---

### Page 82

#### 5.39 Entropy of Encoded Bits
Let \( C: X \to \{0, 1\}^* \) be a nonsingular but nonuniquely decodable code. Let \( X \) have entropy \( H(X) \).  
(a) Compare \( H(C(X)) \) to \( H(X) \).  
(b) Compare \( H(C(X^n)) \) to \( H(X^n) \).

#### 5.40 Code Rate
Let \( X \) be a random variable with alphabet \( \{1, 2, 3\} \) and distribution:  
\[
X = 
\begin{cases}
1 & \text{with probability } \frac{1}{2} \\
2 & \text{with probability } \frac{1}{4} \\
3 & \text{with probability } \frac{1}{4}
\end{cases}
\]
The data compression code for \( X \) assigns codewords:  
\[
C(x) = 
\begin{cases}
0 & \text{if } x = 1 \\
10 & \text{if } x = 2 \\
11 & \text{if } x = 3
\end{cases}
\]
Let \( X_1, X_2, \ldots \) be independent, identically distributed according to this distribution, and let \( Z_1Z_2Z_3\ldots = C(X_1)C(X_2)\ldots \) be the string of binary symbols resulting from concatenating the corresponding codewords. For example, \( 122 \) becomes \( 01010 \).  
(a) Find the entropy rate \( H(X) \) and the entropy rate \( H(Z) \) in bits per symbol. Note that \( Z \) is not compressible further.  
(b) Now let the code be:  
\[
C(x) = 
\begin{cases}
00 & \text{if } x = 1 \\
10 & \text{if } x = 2 \\
01 & \text{if } x = 3
\end{cases}
\]
and find the entropy rate \( H(Z) \).  
(c) Finally, let the code be:  
\[
C(x) = 
\begin{cases}
00 & \text{if } x = 1 \\
1 & \text{if } x = 2 \\
01 & \text{if } x = 3
\end{cases}
\]
and find the entropy rate \( H(Z) \).

#### 5.41 Optimal Codes
Let \( l_1, l_2, \ldots, l_{10} \) be the binary Huffman codeword lengths for the probabilities \( p_1 \geq p_2 \geq \cdots \geq p_{10} \). Suppose we get a new distribution by splitting the last probability mass. What can you say about the optimal binary codeword lengths \( \tilde{l}_1, \tilde{l}_2, \ldots, \tilde{l}_{11} \) for the probabilities \( p_1, p_2, \ldots, p_9, \alpha p_{10}, (1 - \alpha)p_{10} \), where \( 0 \leq \alpha \leq 1 \).

#### 5.42 Ternary Codes
Which of the following codeword lengths can be the word lengths of a 3-ary Huffman code, and which cannot?  
(a) \( (1, 2, 2, 2, 2) \)  
(b) \( (2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3) \)

#### 5.43 Piecewise Huffman
Suppose the codeword that we use to describe a random variable \( X \sim p(x) \) always starts with a symbol chosen from the set \( \{A, B, C\} \), followed by binary digits \( \{0, 1\} \). Thus, we have a ternary code for the first symbol and binary thereafter. Give the optimal uniquely decodable code (minimum expected number of symbols) for the probability distribution:  
\[
p = \left(\frac{16}{69}, \frac{15}{69}, \frac{12}{69}, \frac{10}{69}, \frac{8}{69}, \frac{8}{69}\right)
\]

#### 5.44 Huffman
Find the word lengths of the optimal binary encoding of:  
\[
p = \left(\frac{1}{100}, \frac{1}{100}, \ldots, \frac{1}{100}\right)
\]

#### 5.45 Random 20 Questions
Let \( X \) be uniformly distributed over \( \{1, 2, \ldots, m\} \). Assume \( m = 2n \). We ask random questions: "Is \( X \in S_1 \)?", "Is \( X \in S_2 \)?", ... until only one integer remains. All \( 2^m \) subsets \( S \) of \( \{1, 2, \ldots, m\} \) are equally likely to be asked.  
(a) Without loss of generality, suppose that \( X = 1 \) is the random object. What is the probability that object 2 yields the same answers for \( k \) questions as does object 1?  
(b) What is the expected number of objects in \( \{2, 3, \ldots, m\} \) that have the same answers to the questions as does the correct object 1?  
(c) Suppose we ask \( n + \sqrt{n} \) random questions. What is the expected number of wrong objects agreeing with the answers?  
(d) Use Markov’s inequality \( \Pr\{X \geq t\mu\} \leq \frac{1}{t} \) to show that the probability of error (one or more wrong object remaining) goes to zero as \( n \to \infty \).

---

### Historical Notes
The foundations for the material in this chapter can be found in Shannon’s original paper [469], in which Shannon stated the source coding theorem and gave simple examples of codes. He described a simple code construction procedure (described in Problem 5.5.28), which he attributed to Fano. This method is now called the Shannon–Fano code construction procedure.  
The Kraft inequality for uniquely decodable codes was first proved by McMillan [385]; the proof given here is due to Karush [306]. The Huffman coding procedure was first exhibited and proved to be optimal by Huffman [283].  
In recent years, there has been considerable interest in designing source codes that are matched to particular applications, such as magnetic recording. Some of the results for this problem are described by Franaszek [219], Adler et al. [5], and Marcus [370].  
The arithmetic coding procedure has its roots in the Shannon–Fano code developed by Elias (unpublished), which was analyzed by Jelinek [297]. The procedure for the construction of a prefix-free code described in the text is due to Gilbert and Moore [249]. The extension of the Shannon–Fano–Elias method to sequences is based on the enumerative methods in Cover [120] and was described with finite-precision arithmetic by Pasco [414] and Rissanen [441]. The competitive optimality of Shannon codes was proved in Cover [125] and extended to Huffman codes by Feder [203]. Section 5.11 on the generation of discrete distributions from fair coin flips follows the work of Knuth and Yao [317].

---

### Chapter 6: Gambling and Data Compression

At first sight, information theory and gambling seem to be unrelated. But as we shall see, there is a strong duality between the growth rate of investment in a horse race and the entropy rate of the horse race. Indeed, the sum of the growth rate and the entropy rate is a constant. In the process of proving this, we shall argue that the financial value of side information is equal to the mutual information between the horse race and the side information. The horse race is a special case of investment in the stock market, studied in Chapter 16.  
We also show how to use a pair of identical gamblers to compress a sequence of random variables by an amount equal to the growth rate of wealth on that sequence. Finally, we use these gambling techniques to estimate the entropy rate of English.

#### 6.1 The Horse Race
Assume that \( m \) horses run in a race. Let the \( i \)-th horse win with probability \( p_i \). If horse \( i \) wins, the payoff is \( o_i \) for 1 (i.e., an investment of 1 dollar on horse \( i \) results in \( o_i \) dollars if horse \( i \) wins and 0 dollars if horse \( i \) loses).  
There are two ways of describing odds: \( a \)-for-1 and \( b \)-to-1. The first refers to an exchange that takes place before the race—the gambler puts down 1 dollar before the race and, at \( a \)-for-1 odds, will receive \( a \) dollars after the race if his horse wins and will receive nothing otherwise. The second refers to an exchange after the race—at \( b \)-to-1 odds, the gambler will pay 1 dollar after the race if his horse loses and will pick up \( b \) dollars after the race if his horse wins. Thus, a bet at \( b \)-to-1 odds is equivalent to a bet at \( a \)-for-1 odds if \( b = a - 1 \). For example, fair odds on a coin flip would be 2-for-1 or 1-to-1, otherwise known as even odds.  
We assume that the gambler distributes all of his wealth across the horses. Let \( b_i \) be the fraction of the gambler’s wealth invested in horse \( i \), where \( b_i \geq 0 \) and \( \sum_i b_i = 1 \). Then, if horse \( i \) wins the race, the gambler will receive \( o_i \) times the amount of wealth bet on horse \( i \). All the other bets are lost. Thus, at the end of the race, the gambler will have multiplied his wealth by a factor \( b_i o_i \) if horse \( i \) wins, and this will happen with probability \( p_i \). For notational convenience, we use \( b(i) \) and \( b_i \) interchangeably throughout this chapter.  
The wealth at the end of the race is a random variable, and the gambler wishes to "maximize" the value of this random variable. It is tempting to bet everything on the horse that has the maximum expected return (i.e., the one with the maximum \( p_i o_i \)). But this is clearly risky, since all the money could be lost.  
Some clarity results from considering repeated gambles on this race. Now, since the gambler can reinvest his money, his wealth is the product of the gains for each race. Let \( S_n \) be the gambler’s wealth after \( n \) races. Then:  
\[
S_n = \prod_{i=1}^n S(X_i),
\]
where \( S(X) = b(X) o(X) \) is the factor by which the gambler’s wealth is multiplied when horse \( X \) wins.  

**Definition**  
The wealth relative \( S(X) = b(X) o(X) \) is the factor by which the gambler’s wealth grows if horse \( X \) wins the race.  

**Definition**  
The doubling rate of a horse race is:  
\[
W(b, p) = E[\log S(X)] = \sum_{k=1}^m p_k \log b_k o_k.
\]
The definition of doubling rate is justified by the following theorem.  

**Theorem 6.1.1**  
Let the race outcomes \( X_1, X_2, \ldots \) be i.i.d. \( \sim p(x) \). Then the wealth of the gambler using betting strategy \( b \) grows exponentially at rate \( W(b, p) \); that is:  
\[
S_n \approx 2^{n W(b,p)}.
\]

**Proof:**  
Functions of independent random variables are also independent, and hence \( \log S(X_1), \log S(X_2), \ldots \) are i.i.d. Then, by the weak law of large numbers:  
\[
\frac{1}{n} \log S_n = \frac{1}{n} \sum_{i=1}^n \log S(X_i) \to E[\log S(X)]
\]
in probability. Thus:  
\[
S_n \approx 2^{n W(b,p)}.
\]

Now, since the gambler’s wealth grows as \( 2^{n W(b,p)} \), we seek to maximize the exponent \( W(b, p) \) over all choices of the portfolio \( b \).  

**Definition**  
The optimum doubling rate \( W^*(p) \) is the maximum doubling rate over all choices of the portfolio \( b \):  
\[
W^*(p) = \max_b W(b, p) = \max_{b: b_i \geq 0, \sum_i b_i = 1} \sum_{i=1}^m p_i \log b_i o_i.
\]
We maximize \( W(b, p) \) as a function of \( b \) subject to the constraint \( \sum_i b_i = 1 \). Writing the functional with a Lagrange multiplier and changing the base of the logarithm (which does not affect the maximizing \( b \)), we have:  
\[
J(b) = \sum_{i=1}^m p_i \ln b_i o_i + \lambda \sum_{i=1}^m b_i.
\]
Differentiating this with respect to \( b_i \) yields:  
\[
\frac{\partial J}{\partial b_i} = \frac{p_i}{b_i} + \lambda.
\]
Setting the partial derivative equal to 0 for a maximum, we have:  
\[
b_i = -\frac{p_i}{\lambda}.
\]
Substituting this in the constraint \( \sum_i b_i = 1 \) yields \( \lambda = -1 \) and \( b_i = p_i \). Hence, we can conclude that \( b = p \) is a stationary point of the function \( J(b) \). To prove that this is actually a maximum is tedious if we take second derivatives. Instead, we use a method that works for many such problems: Guess and verify. We verify that proportional gambling \( b = p \) is optimal in the following theorem. Proportional gambling is known as Kelly gambling [308].  

**Theorem 6.1.2**  
(Proportional gambling is log-optimal)  
The optimum doubling rate is given by:  
\[
W^*(p) = \sum_{i=1}^m p_i \log o_i - H(p),
\]
and is achieved by the proportional gambling scheme \( b^* = p \).  

**Proof:**  
We rewrite the function \( W(b, p) \) in a form in which the maximum is obvious:  
\[
W(b, p) = \sum_{i=1}^m p_i \log b_i o_i = \sum_{i=1}^m p_i \log \left(\frac{b_i}{p_i} o_i\right) = \sum_{i=1}^m p_i \log o_i - H(p) - D(p \| b),
\]
where \( D(p \| b) \) is the Kullback–Leibler divergence. Thus:  
\[
W(b, p) \leq \sum_{i=1}^m p_i \log o_i - H(p),
\]
with equality iff \( p = b \) (i.e., the gambler bets on each horse in proportion to its probability of winning).  

**Example 6.1.1**  
Consider a case with two horses, where horse 1 wins with probability \( p_1 \) and horse 2 wins with probability \( p_2 \). Assume even odds (2-for-1 on both horses). Then the optimal bet is proportional betting (i.e., \( b_1 = p_1, b_2 = p_2 \)). The optimal doubling rate is:  
\[
W^*(p) = \sum_{i=1}^2 p_i \log o_i - H(p) = 1 - H(p),
\]
and the resulting wealth grows to infinity at this rate:  
\[
S_n \approx 2^{n(1 - H(p))}.
\]
Thus, we have shown that proportional betting is growth rate optimal for a sequence of i.i.d. horse races if the gambler can reinvest his wealth and if there is no alternative of keeping some of the wealth in cash.  

We now consider a special case when the odds are fair with respect to some distribution (i.e., there is no track take and \( \sum_i \frac{1}{o_i} = 1 \)). In this case, we write \( r_i = \frac{1}{o_i} \), where \( r_i \) can be interpreted as a probability mass function.



---

### 6.1 THE HORSE RACE

The doubling rate can be expressed as:

\[
W(b, p) = \sum_{i=1}^{m} p_i \log \frac{b_i}{p_i} = D(p \| r) - D(p \| b).
\]

This equation provides another interpretation for the relative entropy distance: the doubling rate is the difference between the distance of the bookie’s estimate from the true distribution and the distance of the gambler’s estimate from the true distribution. Hence, the gambler can make money only if his estimate (as expressed by \(b\)) is better than the bookie’s.

An even more special case is when the odds are \(m\)-for-1 on each horse. In this case, the odds are fair with respect to the uniform distribution, and the optimum doubling rate is:

\[
W^*(p) = D\left(p \bigg\| \frac{1}{m}\right) = \log m - H(p).
\]

In this case, we can clearly see the duality between data compression and the doubling rate.

#### Theorem 6.1.3 (Conservation Theorem)

For uniform fair odds,

\[
W^*(p) + H(p) = \log m.
\]

Thus, the sum of the doubling rate and the entropy rate is a constant. Every bit of entropy decrease doubles the gambler’s wealth. Low entropy races are the most profitable.

In the analysis above, we assumed that the gambler was fully invested. In general, we should allow the gambler the option of retaining some of his wealth as cash. Let \(b(0)\) be the proportion of wealth held out as cash, and \(b(1), b(2), \ldots, b(m)\) be the proportions bet on the various horses. Then, at the end of a race, the ratio of final wealth to initial wealth (the wealth relative) is:

\[
S(X) = b(0) + b(X)o(X).
\]

Now, the optimum strategy may depend on the odds and will not necessarily have the simple form of proportional gambling. We distinguish three subcases:

1. **Fair odds with respect to some distribution**: For fair odds, the option of withholding cash does not change the analysis. This is because we can get the effect of withholding cash by betting \(b_i = \frac{1}{o_i}\) on the \(i\)-th horse, \(i = 1, 2, \ldots, m\). Then \(S(X) = 1\) irrespective of which horse wins. Thus, whatever money the gambler keeps aside as cash can equally well be distributed over the horses, and the assumption that the gambler must invest all his money does not change the analysis. Proportional betting is optimal.

2. **Superfair odds**: In this case, the odds are even better than fair odds, so one would always want to put all one’s wealth into the race rather than leave it as cash. In this race, too, the optimum strategy is proportional betting. However, it is possible to choose \(b\) so as to form a Dutch book by choosing \(b_i = c \frac{1}{o_i}\), where \(c = \frac{1}{\sum_{i=1}^m \frac{1}{o_i}}\), to get \(o_i b_i = c\), irrespective of which horse wins. With this allotment, one has wealth \(S(X) = \frac{1}{\sum_{i=1}^m \frac{1}{o_i}} > 1\) with probability 1 (i.e., no risk). Needless to say, one seldom finds such odds in real life. Incidentally, a Dutch book, although risk-free, does not optimize the doubling rate.

3. **Subfair odds**: This is more representative of real life. The organizers of the race track take a cut of all the bets. In this case, it is optimal to bet only some of the money and leave the rest aside as cash. Proportional gambling is no longer log-optimal. A parametric form for the optimal strategy can be found using Kuhn–Tucker conditions (Problem 6.6.2); it has a simple “water-filling” interpretation.

---

### 6.2 GAMBLING AND SIDE INFORMATION

Suppose the gambler has some information that is relevant to the outcome of the gamble. For example, the gambler may have some information about the performance of the horses in previous races. What is the value of this side information?

One definition of the financial value of such information is the increase in wealth that results from that information. In the setting described in Section 6.1, the measure of the value of information is the increase in the doubling rate due to that information. We will now derive a connection between mutual information and the increase in the doubling rate.

To formalize the notion, let horse \(X \in \{1, 2, \ldots, m\}\) win the race with probability \(p(x)\) and pay odds of \(o(x)\) for 1. Let \((X, Y)\) have joint probability mass function \(p(x, y)\). Let \(b(x|y) \geq 0\), \(\sum_{x} b(x|y) = 1\) be an arbitrary conditional betting strategy depending on the side information \(Y\), where \(b(x|y)\) is the proportion of wealth bet on horse \(x\) when \(y\) is observed. As before, let \(b(x) \geq 0\), \(\sum_{x} b(x) = 1\) denote the unconditional betting scheme.

Let the unconditional and the conditional doubling rates be:

\[
W^*(X) = \max_{b(x)} \sum_{x} p(x) \log b(x) o(x),
\]

\[
W^*(X|Y) = \max_{b(x|y)} \sum_{x,y} p(x, y) \log b(x|y) o(x),
\]

and let:

\[
\Delta W = W^*(X|Y) - W^*(X).
\]

We observe that for \((X_i, Y_i)\) i.i.d. horse races, wealth grows like \(2^{nW^*(X|Y)}\) with side information and like \(2^{nW^*(X)}\) without side information.

#### Theorem 6.2.1

The increase \(\Delta W\) in doubling rate due to side information \(Y\) for a horse race \(X\) is:

\[
\Delta W = I(X; Y).
\]

**Proof:**

With side information, the maximum value of \(W^*(X|Y)\) with side information \(Y\) is achieved by conditionally proportional gambling [i.e., \(b^*(x|y) = p(x|y)\)]. Thus,

\[
W^*(X|Y) = \max_{b(x|y)} \mathbb{E} \left[ \log S \right] = \max_{b(x|y)} \sum_{x,y} p(x, y) \log o(x) b(x|y).
\]

\[
= \sum_{x,y} p(x, y) \log o(x) p(x|y).
\]

\[
= \sum_{x} p(x) \log o(x) - H(X|Y).
\]

Without side information, the optimal doubling rate is:

\[
W^*(X) = \sum_{x} p(x) \log o(x) - H(X).
\]

Thus, the increase in doubling rate due to the presence of side information \(Y\) is:

\[
\Delta W = W^*(X|Y) - W^*(X) = H(X) - H(X|Y) = I(X; Y).
\]

Hence, the increase in doubling rate is equal to the mutual information between the side information and the horse race. Not surprisingly, independent side information does not increase the doubling rate.

This relationship can also be extended to the general stock market (Chapter 16). In this case, however, one can only show the inequality \(\Delta W \leq I\), with equality if and only if the market is a horse race.

---

### 6.3 DEPENDENT HORSE RACES AND ENTROPY RATE

The most common example of side information for a horse race is the past performance of the horses. If the horse races are independent, this information will be useless. If we assume that there is dependence among the races, we can calculate the effective doubling rate if we are allowed to use the results of previous races to determine the strategy for the next race.

Suppose that the sequence \(\{X_k\}\) of horse race outcomes forms a stochastic process. Let the strategy for each race depend on the results of previous races. In this case, the optimal doubling rate for uniform fair odds is:

\[
W^*(X_k | X_{k-1}, X_{k-2}, \ldots, X_1) = \mathbb{E} \left[ \max_{b(\cdot|X_{k-1}, \ldots, X_1)} \mathbb{E}[\log S(X_k) | X_{k-1}, X_{k-2}, \ldots, X_1] \right].
\]

\[
= \log m - H(X_k | X_{k-1}, X_{k-2}, \ldots, X_1),
\]

which is achieved by \(b^*(x_k | x_{k-1}, \ldots, x_1) = p(x_k | x_{k-1}, \ldots, x_1)\).

At the end of \(n\) races, the gambler’s wealth is:

\[
S_n = \prod_{i=1}^n S(X_i),
\]

and the exponent in the growth rate (assuming \(m\)-for-1 odds) is:

\[
\frac{1}{n} \mathbb{E} \log S_n = \frac{1}{n} \sum_{i=1}^n \mathbb{E} \log S(X_i).
\]

\[
= \frac{1}{n} \sum_{i=1}^n (\log m - H(X_i | X_{i-1}, X_{i-2}, \ldots, X_1)).
\]

\[
= \log m - \frac{1}{n} H(X_1, X_2, \ldots, X_n).
\]

The quantity \(\frac{1}{n} H(X_1, X_2, \ldots, X_n)\) is the average entropy per race. For a stationary process with entropy rate \(H(X)\), the limit in (6.35) yields:

\[
\lim_{n \to \infty} \left( \frac{1}{n} \mathbb{E} \log S_n + H(X) \right) = \log m.
\]

Again, we have the result that the entropy rate plus the doubling rate is a constant.

The expectation in (6.36) can be removed if the process is ergodic. It will be shown in Chapter 16 that for an ergodic sequence of horse races,

\[
S_n \approx 2^{nW}
\]

with probability 1,

where \(W = \log m - H(X)\) and

\[
H(X) = \lim_{n \to \infty} \frac{1}{n} H(X_1, X_2, \ldots, X_n).
\]

#### Example 6.3.1 (Red and Black)

In this example, cards replace horses, and the outcomes become more predictable as time goes on. Consider the case of betting on the color of the next card in a deck of 26 red and 26 black cards. Bets are placed on whether the next card will be red or black, as we go through the deck. We also assume that the game pays 2-for-1; that is, the gambler gets back twice what he bets on the right color. These are fair odds if red and black are equally probable.

We consider two alternative betting schemes:

1. If we bet sequentially, we can calculate the conditional probability of the next card and bet proportionally. Thus, we should bet \((\frac{1}{2}, \frac{1}{2})\) on (red, black) for the first card, \((\frac{26}{51}, \frac{25}{51})\) for the second card if the first card is black, and so on.

2. Alternatively, we can bet on the entire sequence of 52 cards at once. There are \(\binom{52}{26}\) possible sequences of 26 red and 26 black cards, all of them equally likely. Thus, proportional betting implies that we put \(\frac{1}{\binom{52}{26}}\) of our money on each of these sequences and let each bet “ride.”

We will argue that these procedures are equivalent. For example, half the sequences of 52 cards start with red, and so the proportion of money bet on sequences that start with red in scheme 2 is also one-half, agreeing with the proportion used in the first scheme. In general, we can verify that betting \(\frac{1}{\binom{52}{26}}\) of the money on each of the possible outcomes will at each stage give bets that are proportional to the probability of red and black at that stage. Since we bet \(\frac{1}{\binom{52}{26}}\) of the wealth on each possible output sequence, and a bet on a sequence increases wealth by a factor of \(\frac{2}{52}\) on the sequence observed and 0 on all the others, the resulting wealth is:

\[
S^*_{52} = \frac{2}{52} \binom{52}{26} = 9.08.
\]

Rather interestingly, the return does not depend on the actual sequence. This is like the AEP in that the return is the same for all sequences. All sequences are typical in this sense.

---

### 6.4 THE ENTROPY OF ENGLISH

An important example of an information source is English text. It is not immediately obvious whether English is a stationary ergodic process. Probably not! Nonetheless, we will be interested in the entropy rate of English. We discuss various stochastic approximations to English. As we increase the complexity of the model, we can generate text that looks like English. The stochastic models can be used to compress English text. The better the stochastic approximation, the better the compression.

For the purposes of discussion, we assume that the alphabet of English consists of 26 letters and the space symbol. We therefore ignore punctuation and the difference between upper- and lowercase letters. We construct models for English using empirical distributions collected from samples of text. The frequency of letters in English is far from uniform. The most common letter, E, has a frequency of about 13%, and the least common letters, Q and Z, occur with a frequency of about 0.1%. The letter E is so common that it is rare to find a sentence of any length that does not contain the letter. [A surprising exception to this is the 267-page novel, *Gadsby*, by Ernest Vincent Wright (Lightyear Press, Boston, 1997; original publication in 1939), in which the author deliberately makes no use of the letter E.]

The frequency of pairs of letters is also far from uniform. For example, the letter Q is always followed by a U. The most frequent pair is TH, which occurs normally with a frequency of about 3.7%. We can use the frequency of the pairs to estimate the probability that a letter follows any other letter. Proceeding this way, we can also estimate higher-order conditional probabilities and build more complex models for the language. However, we soon run out of data. For example, to build a third-order Markov approximation, we must estimate the values of \(p(x_i | x_{i-1}, x_{i-2}, x_{i-3})\). There are \(27^3 = 19,683\) entries in this table, and we would need to process millions of letters to make accurate estimates of these probabilities.

The conditional probability estimates can be used to generate random samples of letters drawn according to these distributions (using a random number generator). But there is a simpler method to simulate randomness using a sample of text (a book, say). For example, to construct the second-order model, open the book at random and choose a letter at random on the page. This will be the first letter. For the next letter, again open the book at random and starting at a random point, read until the first letter is encountered again. Then take the letter after that as the second letter. We repeat this process by opening to another page, searching for the second letter, and taking the letter after that as the third letter. Proceeding this way, we can generate text that simulates the second-order statistics of the English text.

Here are some examples of Markov approximations to English from Shannon’s original paper:

1. **Zero-order approximation.** (The symbols are independent and equi-probable.)

   XFOML RXKHRJFFJUJ ZLPWCFWKCYJ FFJEYVKCQSGXYD QPAAMKBZAACIBZLHJQD

2. **First-order approximation.** (The symbols are independent. The frequency of letters matches English text.)

   OCRO HLI RGWR NMIELWIS EU LL NBNESEBYA TH EEI ALHENHTTPA OOBTTVA NAH BRL

3. **Second-order approximation.** (The frequency of pairs of letters matches English text.)

   ON IE ANTSOUTINYS ARE T INCTORE ST BE S DEAMY ACHIN D ILONASIVE TUCOOWE AT TEASONARE FUSO TIZIN ANDY TOBE SEACE CTISBE

4. **Third-order approximation.** (The frequency of triplets of letters matches English text.)

   IN NO IST LAT WHEY CRATICT FROURE BERS GROCID PONDENOME OF DEMONSTURES OF THE REPTAGIN IS REGOACTIONA OF CRE



GAMBLING AND DATA COMPRESSION

5. Fourth-order approximation. (The frequency of quadruplets of letters matches English text. Each letter depends on the previous three letters. This sentence is from Lucky’s book, Silicon Dreams [366].)

THE GENERATED JOB PROVIDUAL BETTER TRAND THE DISPLAYED CODE, ABOVERY UPONDULTS WELL THE CODERST IN THESTICAL IT DO HOCK BOTHE MERG. (INSTATES CONS ERATION. NEVER ANY OF PUBLE AND TO THEORY. EVENTIAL CALLEGAND TO ELAST BENERATED IN WITH PIES AS IS WITH THE )

Instead of continuing with the letter models, we jump to word models.

6. First-order word model. (The words are chosen independently but with frequencies as in English.)

REPRESENTING AND SPEEDILY IS AN GOOD APT OR COME CAN DIFFERENT NATURAL HERE HE THE A IN CAME THE TO OF TO EXPERT GRAY COME TO FURNISHES THE LINE MESSAGE HAD BE THESE.

7. Second-order word model. (The word transition probabilities match English text.)

THE HEAD AND IN FRONTAL ATTACK ON AN ENGLISH WRITER THAT THE CHARACTER OF THIS POINT IS THEREFORE ANOTHER METHOD FOR THE LETTERS THAT THE TIME OF WHO EVER TOLD THE PROBLEM FOR AN UNEXPECTED

The approximations get closer and closer to resembling English. For example, long phrases of the last approximation could easily have occurred in a real English sentence. It appears that we could get a very good approximation by using a more complex model. These approximations could be used to estimate the entropy of English. For example, the entropy of the zeroth-order model is log 27 = 4.76 bits per letter. As we increase the complexity of the model, we capture more of the structure of English, and the conditional uncertainty of the next letter is reduced. The first-order model gives an estimate of the entropy of 4.03 bits per letter, while the fourth-order model gives an estimate of 2.8 bits per letter. But even the fourth-order model does not capture all the structure of English. In Section 6.6, we describe alternative methods for estimating the entropy of English.

The distribution of English is useful in decoding encrypted English text. For example, a simple substitution cipher (where each letter is replaced by some other letter) can be solved by looking for the most frequent letter and guessing that it is the substitute for E, and so on. The redundancy in English can be used to fill in some of the missing letters after the other letters are decrypted: for example,

TH R  
S NLY N W Y T F LL N TH V W LS N TH S S NT NC .

Some of the inspiration for Shannon’s original work on information theory came out of his work in cryptography during World War II. The mathematical theory of cryptography and its relationship to the entropy of language is developed in Shannon [481].

Stochastic models of language also play a key role in some speech recognition systems. A commonly used model is the trigram (second-order Markov) word model, which estimates the probability of the next word given the preceding two words. The information from the speech signal is combined with the model to produce an estimate of the most likely word that could have produced the observed speech. Random models do surprisingly well in speech recognition, even when they do not explicitly incorporate the complex rules of grammar that govern natural languages such as English.

We can apply the techniques of this section to estimate the entropy rate of other information sources, such as speech and images. A fascinating nontechnical introduction to these issues may be found in the book by Lucky [366].

6.5 DATA COMPRESSION AND GAMBLING

We now show a direct connection between gambling and data compression, by showing that a good gambler is also a good data compressor. Any sequence on which a gambler makes a large amount of money is also a sequence that can be compressed by a large factor. The idea of using the gambler as a data compressor is based on the fact that the gambler’s bets can be considered to be his estimate of the probability distribution of the data. A good gambler will make a good estimate of the probability distribution. We can use this estimate of the distribution to do arithmetic coding (Section 13.3). This is the essential idea of the scheme described below.

We assume that the gambler has a mechanically identical twin, who will be used for the data decompression. The identical twin will place the same bets on possible sequences of outcomes as the original gambler (and will therefore make the same amount of money). The cumulative amount of money that the gambler would have made on all sequences that are lexicographically less than the given sequence will be used as a code for the sequence. The decoder will use the identical twin to gamble on all sequences and look for the sequence for which the same cumulative amount of money is made. This sequence will be chosen as the decoded sequence.

Let \( X_1, X_2, \ldots, X_n \) be a sequence of random variables that we wish to compress. Without loss of generality, we will assume that the random variables are binary. Gambling on this sequence will be defined by a sequence of bets

\[
b(x_{k+1} | x_1, x_2, \ldots, x_k) \geq 0,
\]

where

\[
\sum_{x_{k+1}} b(x_{k+1} | x_1, x_2, \ldots, x_k) = 1,
\]

and \( b(x_{k+1} | x_1, x_2, \ldots, x_k) \) is the proportion of money bet at time \( k \) on the event that \( X_{k+1} = x_{k+1} \) given the observed past \( x_1, x_2, \ldots, x_k \). Bets are paid at uniform odds (2-for-1). Thus, the wealth \( S_n \) at the end of the sequence is given by

\[
S_n = 2^n \prod_{k=1}^n b(x_k | x_1, \ldots, x_{k-1}),
\]

where

\[
b(x_1, x_2, \ldots, x_n) = \prod_{k=1}^n b(x_k | x_{k-1}, \ldots, x_1).
\]

So sequential gambling can also be considered as an assignment of probabilities (or bets) \( b(x_1, x_2, \ldots, x_n) \geq 0 \), with

\[
\sum_{x_1, \ldots, x_n} b(x_1, \ldots, x_n) = 1,
\]

on the \( 2^n \) possible sequences.

This gambling elicits both an estimate of the true probability of the text sequence \( \hat{p}(x_1, \ldots, x_n) = S_n / 2^n \) as well as an estimate of the entropy

\[
\hat{H} = -\frac{1}{n} \log \hat{p}
\]

of the text from which the sequence was drawn. We now wish to show that high values of wealth \( S_n \) lead to high data compression. Specifically, we argue that if the text in question results in wealth \( S_n \), then \( \log S_n \) bits can be saved in a naturally associated deterministic data compression scheme. We further assert that if the gambling is log optimal, the data compression achieves the Shannon limit \( H \).

Consider the following data compression algorithm that maps the text \( x = x_1 x_2 \cdots x_n \in \{0, 1\}^n \) into a code sequence \( c_1 c_2 \cdots c_k \), \( c_i \in \{0, 1\} \). Both the compressor and the decompressor know \( n \). Let the \( 2^n \) text sequences be arranged in lexicographical order: for example, \( 0100101 < 0101101 \). The encoder observes the sequence \( x(n) = (x_1, x_2, \ldots, x_n) \). He then calculates what his wealth \( S_n(x'(n)) \) would have been on all sequences \( x'(n) \leq x(n) \) and calculates

\[
F(x(n)) = \sum_{x'(n) \leq x(n)} 2^{-n} S_n(x'(n)).
\]

Clearly, \( F(x(n)) \in [0, 1] \). Let \( k = \lceil n - \log S_n(x(n)) \rceil \). Now express \( F(x(n)) \) as a binary decimal to \( k \)-place accuracy: \( \lfloor F(x(n)) \rfloor = .c_1 c_2 \cdots c_k \). The sequence \( c(k) = (c_1, c_2, \ldots, c_k) \) is transmitted to the decoder.

The decoder twin can calculate the precise value \( S(x'(n)) \) associated with each of the \( 2^n \) sequences \( x'(n) \). He thus knows the cumulative sum of \( 2^{-n} S(x'(n)) \) up through any sequence \( x(n) \). He tediously calculates this sum until it first exceeds \( .c_1 \cdots c_k \). The first sequence \( x(n) \) such that the cumulative sum falls in the interval \( [.c_1 \cdots c_k, .c_1 \cdots c_k + (1/2)^k] \) is defined uniquely, and the size of \( S(x(n))/2^n \) guarantees that this sequence will be precisely the encoded \( x(n) \).

Thus, the twin uniquely recovers \( x(n) \). The number of bits required is

\[
k = \lceil n - \log S(x(n)) \rceil.
\]

The number of bits saved is

\[
n - k = \lfloor \log S(x(n)) \rfloor.
\]

For proportional gambling, \( S(x(n)) = 2^n p(x(n)) \). Thus, the expected number of bits is

\[
E[k] = \sum p(x(n)) \lceil -\log p(x(n)) \rceil \leq H(X_1, \ldots, X_n) + 1.
\]

We see that if the betting operation is deterministic and is known to both the encoder and the decoder, the number of bits necessary to encode \( x_1, \ldots, x_n \) is less than \( n - \log S_n + 1 \). Moreover, if \( p(x) \) is known, and if proportional gambling is used, the description length expected is

\[
E[n - \log S_n] \leq H(X_1, \ldots, X_n) + 1.
\]

Thus, the gambling results correspond precisely to the data compression that would have been achieved by the given human encoder–decoder identical twin pair.

The data compression scheme using a gambler is similar to the idea of arithmetic coding (Section 13.3) using a distribution \( b(x_1, x_2, \ldots, x_n) \) rather than the true distribution. The procedure above brings out the duality between gambling and data compression. Both involve estimation of the true distribution. The better the estimate, the greater the growth rate of the gambler’s wealth and the better the data compression.

6.6 GAMBLING ESTIMATE OF THE ENTROPY OF ENGLISH

We now estimate the entropy rate for English using a human gambler to estimate probabilities. We assume that English consists of 27 characters (26 letters and a space symbol). We therefore ignore punctuation and case of letters. Two different approaches have been proposed to estimate the entropy of English.

1. Shannon guessing game.

In this approach, the human subject is given a sample of English text and asked to guess the next letter. An optimal subject will estimate the probabilities of the next letter and guess the most probable letter first, then the second most probable letter next, and so on. The experimenter records the number of guesses required to guess the next letter. The subject proceeds this way through a fairly large sample of text. We can then calculate the empirical frequency distribution of the number of guesses required to guess the next letter. Many of the letters will require only one guess; but a large number of guesses will usually be needed at the beginning of words or sentences.

Now let us assume that the subject can be modeled as a computer making a deterministic choice of guesses given the past text. Then, if we have the same machine and the sequence of guess numbers, we can reconstruct the English text. Just let the machine run, and if the number of guesses at any position is \( k \), choose the \( k \)-th guess of the machine as the next letter. Hence, the amount of information in the sequence of guess numbers is the same as in the English text. The entropy of the guess sequence is the entropy of English text. We can bound the entropy of the guess sequence by assuming that the samples are independent. Hence, the entropy of the guess sequence is bounded above by the entropy of the histogram in the experiment.

The experiment was conducted in 1950 by Shannon [482], who obtained a value of 1.3 bits per symbol for the entropy of English.

2. Gambling estimate. In this approach, we let a human subject gamble on the next letter in a sample of English text. This allows finer gradations of judgment than does guessing. As in the case of a horse race, the optimal bet is proportional to the conditional probability of the next letter. The payoff is 27-for-1 on the correct letter. Since sequential betting is equivalent to betting on the entire sequence, we can write the payoff after \( n \) letters as

\[
S_n = (27)^n b(X_1, X_2, \ldots, X_n).
\]

Thus, after \( n \) rounds of betting, the expected log wealth satisfies

\[
\frac{1}{n} E[\log S_n] = \log 27 + \frac{1}{n} E[\log b(X_1, X_2, \ldots, X_n)].
\]



SUMMARY

175  
= log 27 + 1  
n  
xn  
p(xn) log b(xn)  
(6.46)  
= log 27 − 1  
n  
xn  
p(xn) log b(xn)  
+ 1  
n  
xn  
p(xn) log p(xn)  
(6.47)  
= log 27 − 1  
nD(p(xn)||b(xn)) − 1  
nH(X1, X2, ..., Xn)  
(6.48)  
≤ log 27 − 1  
nH(X1, X2, ..., Xn)  
(6.49)  
≤ log 27 − H(X),  
(6.50)  
where H(X) is the entropy rate of English. Thus, log 27 − 1  
n log Sn  
is an upper bound on the entropy rate of English. The upper bound estimate, ˆH(X) = log 27 − 1  
n log Sn, converges to H(X) with probability 1 if English is ergodic and the gambler uses b(xn) = p(xn).  

An experiment [131] with 12 subjects and a sample of 75 letters from the book *Jefferson the Virginian* by Dumas Malone (Little, Brown, Boston, 1948; the source used by Shannon) resulted in an estimate of 1.34 bits per letter for the entropy of English.  

SUMMARY  
Doubling rate. W(b, p) = E(log S(X)) = m  
k=1 pk log bkok.  
Optimal doubling rate. W∗(p) = maxb W(b, p).  
Proportional gambling is log-optimal.  
W∗(p) = max  
b  
W(b, p) =   
pi log oi − H(p)  
(6.51)  
is achieved by b∗= p.  
Growth rate. Wealth grows as Sn = 2nW∗(p).  

GAMBLING AND DATA COMPRESSION  

Conservation law. For uniform fair odds,  
H(p) + W∗(p) = log m.  
(6.52)  
Side information. In a horse race X, the increase W in doubling rate due to side information Y is  
W = I(X; Y).  
(6.53)  

PROBLEMS  

6.1 Horse race.  
Three horses run a race. A gambler offers 3-for-1 odds on each horse. These are fair odds under the assumption that all horses are equally likely to win the race. The true win probabilities are known to be  
p = (p1, p2, p3) = 1  
2, 1  
4, 1  
4  
(6.54)  
Let b = (b1, b2, b3), bi ≥ 0,  bi = 1, be the amount invested on each of the horses. The expected log wealth is thus  
W(b) = 3  
i=1  
pi log 3bi.  
(6.55)  
(a) Maximize this over b to find b∗ and W∗. Thus, the wealth achieved in repeated horse races should grow to infinity like 2nW∗ with probability 1.  
(b) Show that if instead we put all of our money on horse 1, the most likely winner, we will eventually go broke with probability 1.  

6.2 Horse race with subfair odds.  
If the odds are bad (due to a track take), the gambler may wish to keep money in his pocket. Let b(0) be the amount in his pocket and let b(1), b(2), ..., b(m) be the amount bet on horses 1, 2, ..., m, with odds o(1), o(2), ..., o(m), and win probabilities p(1), p(2), ..., p(m). Thus, the resulting wealth is S(x) = b(0) + b(x)o(x), with probability p(x), x = 1, 2, ..., m.  
(a) Find b∗ maximizing E log S if  1/o(i) < 1.  
(b) Discuss b∗ if  1/o(i) > 1. (There isn’t an easy closed-form solution in this case, but a “water-filling” solution results from the application of the Kuhn–Tucker conditions.)  

6.3 Cards.  
An ordinary deck of cards containing 26 red cards and 26 black cards is shuffled and dealt out one card at a time without replacement. Let Xi be the color of the ith card.  
(a) Determine H(X1).  
(b) Determine H(X2).  
(c) Does H(Xk | X1, X2, ..., Xk−1) increase or decrease?  
(d) Determine H(X1, X2, ..., X52).  

6.4 Gambling.  
Suppose that one gambles sequentially on the card outcomes in Problem 6.6.3. Even odds of 2-for-1 are paid. Thus, the wealth Sn at time n is Sn = 2nb(x1, x2, ..., xn), where b(x1, x2, ..., xn) is the proportion of wealth bet on x1, x2, ..., xn. Find maxb(·) E log S52.  

6.5 Beating the public odds.  
Consider a three-horse race with win probabilities  
(p1, p2, p3) = 1  
2, 1  
4, 1  
4  
and fair odds with respect to the (false) distribution  
(r1, r2, r3) = 1  
4, 1  
4, 1  
2  
Thus, the odds are  
(o1, o2, o3) = (4, 4, 2).  
(a) What is the entropy of the race?  
(b) Find the set of bets (b1, b2, b3) such that the compounded wealth in repeated plays will grow to infinity.  

6.6 Horse race.  
A three-horse race has win probabilities p = (p1, p2, p3), and odds o = (1, 1, 1). The gambler places bets b = (b1, b2, b3), bi ≥ 0,  bi = 1, where bi denotes the proportion of wealth bet on horse i. These odds are very bad. The gambler gets his money back on the winning horse and loses the other bets. Thus, the wealth Sn at time n resulting from independent gambles goes exponentially to zero.  
(a) Find the exponent.  
(b) Find the optimal gambling scheme b (i.e., the bet b∗ that maximizes the exponent).  
(c) Assuming that b is chosen as in part (b), what distribution p causes Sn to go to zero at the fastest rate?  

6.7 Horse race.  
Consider a horse race with four horses. Assume that each horse pays 4-for-1 if it wins. Let the probabilities of winning of the horses be {1  
2, 1  
4, 1  
8, 1  
8}. If you started with $100 and bet optimally to maximize your long-term growth rate, what are your optimal bets on each horse? Approximately how much money would you have after 20 races with this strategy?  

6.8 Lotto.  
The following analysis is a crude approximation to the games of Lotto conducted by various states. Assume that the player of the game is required to pay $1 to play and is asked to choose one number from a range 1 to 8. At the end of every day, the state lottery commission picks a number uniformly over the same range. The jackpot (i.e., all the money collected that day) is split among all the people who chose the same number as the one chosen by the state. For example, if 100 people played today, 10 of them chose the number 2, and the drawing at the end of the day picked 2, the $100 collected is split among the 10 people (i.e., each person who picked 2 will receive $10, and the others will receive nothing). The general population does not choose numbers uniformly—numbers such as 3 and 7 are supposedly lucky and are more popular than 4 or 8. Assume that the fraction of people choosing the various numbers 1, 2, ..., 8 is (f1, f2, ..., f8), and assume that n people play every day. Also assume that n is very large, so that any single person’s choice does not change the proportion of people betting on any number.  
(a) What is the optimal strategy to divide your money among the various possible tickets so as to maximize your long-term growth rate? (Ignore the fact that you cannot buy fractional tickets.)  
(b) What is the optimal growth rate that you can achieve in this game?  
(c) If (f1, f2, ..., f8) = (1  
8, 1  
8, 1  
4, 1  
16, 1  
16, 1  
16, 1  
4, 1  
16), and you start with $1, how long will it be before you become a millionaire?  

6.9 Horse race.  
Suppose that one is interested in maximizing the doubling rate for a horse race. Let p1, p2, ..., pm denote the win probabilities of the m horses. When do the odds (o1, o2, ..., om) yield a higher doubling rate than the odds (o′  
1, o′  
2, ..., o′  
m)?  

6.10 Horse race with probability estimates.  
(a) Three horses race. Their probabilities of winning are (1  
2, 1  
4, 1  
4). The odds are 4-for-1, 3-for-1, and 3-for-1. Let W∗ be the optimal doubling rate. Suppose you believe that the probabilities are (1  
4, 1  
2, 1  
4). If you try to maximize the doubling rate, what doubling rate W will you achieve? By how much has your doubling rate decreased due to your poor estimate of the probabilities (i.e., what is W = W∗−W)?  
(b) Now let the horse race be among m horses, with probabilities p = (p1, p2, ..., pm) and odds o = (o1, o2, ..., om). If you believe the true probabilities to be q = (q1, q2, ..., qm), and try to maximize the doubling rate W, what is W∗−W?  

6.11 Two-envelope problem.  
One envelope contains b dollars, the other 2b dollars. The amount b is unknown. An envelope is selected at random. Let X be the amount observed in this envelope, and let Y be the amount in the other envelope. Adopt the strategy of switching to the other envelope with probability p(x), where p(x) = e−x  
(e−x + ex). Let Z be the amount that the player receives. Thus,  
(X, Y) =  
(  
b, 2b  
) with probability 1  
2  
(2b, b  
) with probability 1  
2  
(6.56)  
Z =  
X with probability 1 − p(x)  
Y with probability p(x).  
(6.57)  
(a) Show that E(X) = E(Y) = 3b  
2.  
(b) Show that E(Y/X) = 5  
4. Since the expected ratio of the amount in the other envelope is 5  
4, it seems that one should always switch. (This is the origin of the switching paradox.) However, observe that E(Y) ̸= E(X)E(Y/X). Thus, although E(Y/X) > 1, it does not follow that E(Y) > E(X).  
(c) Let J be the index of the envelope containing the maximum amount of money, and let J′ be the index of the envelope chosen by the algorithm. Show that for any b, I(J; J′) > 0. Thus, the amount in the first envelope always contains some information about which envelope to choose.  
(d) Show that E(Z) > E(X). Thus, you can do better than always staying or always switching. In fact, this is true for any monotonic decreasing switching function p(x). By randomly switching according to p(x), you are more likely to trade up than to trade down.  

6.12 Gambling.  
Find the horse win probabilities p1, p2, ..., pm:  
(a) Maximizing the doubling rate W∗ for given fixed known odds o1, o2, ..., om.  
(b) Minimizing the doubling rate for given fixed odds o1, o2, ..., om.  

6.13 Dutch book.  
Consider a horse race with m = 2 horses,  
X = 1, 2  
p = 1  
2, 1  
2  
odds (for one) = 10, 30  
bets = b, 1 − b.  
The odds are superfair.  
(a) There is a bet b that guarantees the same payoff regardless of which horse wins. Such a bet is called a Dutch book. Find this b and the associated wealth factor S(X).  
(b) What is the maximum growth rate of the wealth for the optimal choice of b? Compare it to the growth rate for the Dutch book.  

6.14 Horse race.  
Suppose that one is interested in maximizing the doubling rate for a horse race. Let p1, p2, ..., pm denote the win probabilities of the m horses. When do the odds (o1, o2, ..., om) yield a higher doubling rate than the odds (o′  
1, o′  
2, ..., o′  
m)?  

6.15 Entropy of a fair horse race.  
Let X ∼ p(x), x = 1, 2, ..., m, denote the winner of a horse race. Suppose that the odds o(x) are fair with respect to p(x) [i.e., o(x) = 1  
p(x)]. Let b(x) be the amount bet on horse x, b(x) ≥ 0, m  
1 b(x) = 1. Then the resulting wealth factor is S(x) = b(x)o(x), with probability p(x).  
(a) Find the expected wealth ES(X).  
(b) Find W∗, the optimal growth rate of wealth.  
(c) Suppose that  
Y =  
1, if X = 1 or 2  
0, otherwise.  
If this side information is available before the bet, how much does it increase the growth rate W∗?  
(d) Find I(X; Y).



PROBLEMS

181

6.16 Negative horse race.  
Consider a horse race with \( m \) horses with win probabilities \( p_1, p_2, \ldots, p_m \). Here the gambler hopes that a given horse will lose. He places bets \( (b_1, b_2, \ldots, b_m) \), \( \sum_{i=1}^m b_i = 1 \), on the horses, loses his bet \( b_i \) if horse \( i \) wins, and retains the rest of his bets. (No odds.) Thus, \( S = \sum_{j \neq i} b_j \), with probability \( p_i \), and one wishes to maximize \( \sum p_i \ln(1 - b_i) \) subject to the constraint \( \sum b_i = 1 \).  

(a) Find the growth rate optimal investment strategy \( b^* \). Do not constrain the bets to be positive, but do constrain the bets to sum to 1. (This effectively allows short selling and margin.)  

(b) What is the optimal growth rate?  

6.17 St. Petersburg paradox.  
Many years ago in ancient St. Petersburg, the following gambling proposition caused great consternation. For an entry fee of \( c \) units, a gambler receives a payoff of \( 2^k \) units with probability \( 2^{-k} \), \( k = 1, 2, \ldots \).  

(a) Show that the expected payoff for this game is infinite. For this reason, it was argued that \( c = \infty \) was a “fair” price to pay to play this game. Most people find this answer absurd.  

(b) Suppose that the gambler can buy a share of the game. For example, if he invests \( c/2 \) units in the game, he receives \( 1/2 \) a share and a return \( X/2 \), where \( \Pr(X = 2^k) = 2^{-k} \), \( k = 1, 2, \ldots \). Suppose that \( X_1, X_2, \ldots \) are i.i.d. according to this distribution and that the gambler reinvests all his wealth each time. Thus, his wealth \( S_n \) at time \( n \) is given by  

\[
S_n = \prod_{i=1}^n \frac{X_i}{c}.
\]

Show that this limit is \( \infty \) or \( 0 \), with probability 1, accordingly as \( c < c^* \) or \( c > c^* \). Identify the “fair” entry fee \( c^* \).  

More realistically, the gambler should be allowed to keep a proportion \( b = 1 - b \) of his money in his pocket and invest the rest in the St. Petersburg game. His wealth at time \( n \) is then  

\[
S_n = \prod_{i=1}^n \left( b + \frac{b X_i}{c} \right).
\]

Let  

\[
W(b, c) = \sum_{k=1}^\infty 2^{-k} \log \left( 1 - b + \frac{b 2^k}{c} \right).
\]

We have  

\[
S_n \approx 2^{n W(b, c)}.
\]

Let  

\[
W^*(c) = \max_{0 \leq b \leq 1} W(b, c).
\]

Here are some questions about \( W^*(c) \):  

(a) For what value of the entry fee \( c \) does the optimizing value \( b^* \) drop below 1?  

(b) How does \( b^* \) vary with \( c \)?  

(c) How does \( W^*(c) \) fall off with \( c \)?  

Note that since \( W^*(c) > 0 \), for all \( c \), we can conclude that any entry fee \( c \) is fair.  

6.18 Super St. Petersburg.  
Finally, we have the super St. Petersburg paradox, where \( \Pr(X = 2^{2k}) = 2^{-k} \), \( k = 1, 2, \ldots \). Here the expected log wealth is infinite for all \( b > 0 \), for all \( c \), and the gambler’s wealth grows to infinity faster than exponentially for any \( b > 0 \). But that doesn’t mean that all investment ratios \( b \) are equally good. To see this, we wish to maximize the relative growth rate with respect to some other portfolio, say, \( b = \left( \frac{1}{2}, \frac{1}{2} \right) \). Show that there exists a unique \( b \) maximizing  

\[
E \ln \left( b + \frac{b X / c}{\frac{1}{2} + \frac{1}{2} X / c} \right)
\]

and interpret the answer.  

HISTORICAL NOTES  
The original treatment of gambling on a horse race is due to Kelly [308], who found that \( \mathcal{W} = I \). Log-optimal portfolios go back to the work of Bernoulli, Kelly [308], Latané [346], and Latané and Tuttle [347]. Proportional gambling is sometimes referred to as the Kelly gambling scheme. The improvement in the probability of winning by switching envelopes in Problem 6.11 is based on Cover [130].  

Shannon studied stochastic models for English in his original paper [472]. His guessing game for estimating the entropy rate of English is described in [482]. Cover and King [131] provide a gambling estimate for the entropy of English. The analysis of the St. Petersburg paradox is from Bell and Cover [39]. An alternative analysis can be found in Feller [208].  

CHAPTER 7  
CHANNEL CAPACITY  

What do we mean when we say that A communicates with B? We mean that the physical acts of A have induced a desired physical state in B. This transfer of information is a physical process and therefore is subject to the uncontrollable ambient noise and imperfections of the physical signaling process itself. The communication is successful if the receiver B and the transmitter A agree on what was sent.  

In this chapter, we find the maximum number of distinguishable signals for \( n \) uses of a communication channel. This number grows exponentially with \( n \), and the exponent is known as the channel capacity. The characterization of the channel capacity (the logarithm of the number of distinguishable signals) as the maximum mutual information is the central and most famous success of information theory.  

The mathematical analog of a physical signaling system is shown in Figure 7.1. Source symbols from some finite alphabet are mapped into some sequence of channel symbols, which then produces the output sequence of the channel. The output sequence is random but has a distribution that depends on the input sequence. From the output sequence, we attempt to recover the transmitted message.  

Each of the possible input sequences induces a probability distribution on the output sequences. Since two different input sequences may give rise to the same output sequence, the inputs are confusable. In the next few sections, we show that we can choose a “nonconfusable” subset of input sequences so that with high probability there is only one highly likely input that could have caused the particular output. We can then reconstruct the input sequences at the output with a negligible probability of error. By mapping the source into the appropriate “widely spaced” input sequences to the channel, we can transmit a message with very low probability of error and reconstruct the source message at the output. The maximum rate at which this can be done is called the capacity of the channel.  

DEFINITION  
We define a discrete channel to be a system consisting of an input alphabet \( \mathcal{X} \) and output alphabet \( \mathcal{Y} \) and a probability transition matrix \( p(y|x) \) that expresses the probability of observing the output symbol \( y \) given that we send the symbol \( x \). The channel is said to be memoryless if the probability distribution of the output depends only on the input at that time and is conditionally independent of previous channel inputs or outputs.  

DEFINITION  
We define the “information” channel capacity of a discrete memoryless channel as  

\[
C = \max_{p(x)} I(X; Y),
\]

where the maximum is taken over all possible input distributions \( p(x) \).  

We shall soon give an operational definition of channel capacity as the highest rate in bits per channel use at which information can be sent with arbitrarily low probability of error. Shannon’s second theorem establishes that the information channel capacity is equal to the operational channel capacity. Thus, we drop the word “information” in most discussions of channel capacity.  

There is a duality between the problems of data compression and data transmission. During compression, we remove all the redundancy in the data to form the most compressed version possible, whereas during data transmission, we add redundancy in a controlled fashion to combat errors in the channel. In Section 7.13, we show that a general communication system can be broken into two parts and that the problems of data compression and data transmission can be considered separately.  

7.1 EXAMPLES OF CHANNEL CAPACITY  

7.1.1 Noiseless Binary Channel  
Suppose that we have a channel whose binary input is reproduced exactly at the output (Figure 7.2). In this case, any transmitted bit is received without error. Hence, one error-free bit can be transmitted per use of the channel, and the capacity is 1 bit. We can also calculate the information capacity \( C = \max I(X; Y) = 1 \) bit, which is achieved by using \( p(x) = \left( \frac{1}{2}, \frac{1}{2} \right) \).  

7.1.2 Noisy Channel with Nonoverlapping Outputs  
This channel has two possible outputs corresponding to each of the two inputs (Figure 7.3). The channel appears to be noisy, but really is not. Even though the output of the channel is a random consequence of the input, the input can be determined from the output, and hence every transmitted bit can be recovered without error. The capacity of this channel is also 1 bit per transmission. We can also calculate the information capacity \( C = \max I(X; Y) = 1 \) bit, which is achieved by using \( p(x) = \left( \frac{1}{2}, \frac{1}{2} \right) \).  

7.1.3 Noisy Typewriter  
In this case, the channel input is either received unchanged at the output with probability \( \frac{1}{2} \) or is transformed into the next letter with probability \( \frac{1}{2} \) (Figure 7.4). If the input has 26 symbols and we use every alternate input symbol, we can transmit one of 13 symbols without error with each transmission. Hence, the capacity of this channel is \( \log 13 \) bits per transmission. We can also calculate the information capacity \( C = \max I(X; Y) = \max (H(Y) - H(Y|X)) = \max H(Y) - 1 = \log 26 - 1 = \log 13 \), achieved by using \( p(x) \) distributed uniformly over all the inputs.  

7.1.4 Binary Symmetric Channel  
Consider the binary symmetric channel (BSC), which is shown in Figure 7.5. This is a binary channel in which the input symbols are complemented with probability \( p \). This is the simplest model of a channel with errors, yet it captures most of the complexity of the general problem.  

When an error occurs, a 0 is received as a 1, and vice versa. The bits received do not reveal where the errors have occurred. In a sense, all the bits received are unreliable. Later, we show that we can still use such a communication channel to send information at a nonzero rate with an arbitrarily small probability of error.  

We bound the mutual information by  

\[
I(X; Y) = H(Y) - H(Y|X) = \sum p(x) H(Y|X = x) = H(Y) - H(p) \leq 1 - H(p),
\]

where the last inequality follows because \( Y \) is a binary random variable. Equality is achieved when the input distribution is uniform. Hence, the information capacity of a binary symmetric channel with parameter \( p \) is  

\[
C = 1 - H(p) \text{ bits}.
\]

7.1.5 Binary Erasure Channel  
The analog of the binary symmetric channel in which some bits are lost (rather than corrupted) is the binary erasure channel. In this channel, a fraction \( \alpha \) of the bits are erased. The receiver knows which bits have been erased. The binary erasure channel has two inputs and three outputs (Figure 7.6).  

We calculate the capacity of the binary erasure channel as follows:  

\[
C = \max_{p(x)} I(X; Y) = \max_{p(x)} \left( H(Y) - H(Y|X) \right).
\]

The first guess for the maximum of \( H(Y) \) would be \( \log 3 \), but we cannot achieve this by any choice of input distribution \( p(x) \). Letting \( E \) be the event \( \{ Y = e \} \), using the expansion  

\[
H(Y) = H(Y, E) = H(E) + H(Y|E),
\]

and letting \( \Pr(X = 1) = \pi \), we have  

\[
H(Y) = H \left( (1 - \pi)(1 - \alpha), \alpha, \pi(1 - \alpha) \right) = H(\alpha) + (1 - \alpha) H(\pi).
\]

Hence,  

\[
C = \max_{p(x)} H(Y) - H(\alpha) = \max_{\pi} \left( (1 - \alpha) H(\pi) + H(\alpha) - H(\alpha) \right) = \max_{\pi} (1 - \alpha) H(\pi) = 1 - \alpha,
\]

where capacity is achieved by \( \pi = \frac{1}{2} \).  

The expression for the capacity has some intuitive meaning: Since a proportion \( \alpha \) of the bits are lost in the channel, we can recover (at most) a proportion \( 1 - \alpha \) of the bits. Hence, the capacity is at most \( 1 - \alpha \). It is not immediately obvious that it is possible to achieve this rate. This will follow from Shannon’s second theorem.  

In many practical channels, the sender receives some feedback from the receiver. If feedback is available for the binary erasure channel, it is very clear what to do: If a bit is lost, retransmit it until it gets through. Since the bits get through with probability \( 1 - \alpha \), the effective rate of transmission is \( 1 - \alpha \). In this way, we are easily able to achieve a capacity of \( 1 - \alpha \) with feedback.  

Later in the chapter, we prove that the rate \( 1 - \alpha \) is the best that can be achieved both with and without feedback. This is one of the consequences of the surprising fact that feedback does not increase the capacity of discrete memoryless channels.  

7.2 SYMMETRIC CHANNELS  
The capacity of the binary symmetric channel is \( C = 1 - H(p) \) bits per transmission, and the capacity of the binary erasure channel is \( C = 1 - \alpha \) bits per transmission. Now consider the channel with transition matrix:  

\[
p(y|x) = \begin{bmatrix}
0.3 & 0.2 & 0.5 \\
0.5 & 0.3 & 0.2 \\
0.2 & 0.5 & 0.3 \\
\end{bmatrix}.
\]

Here, the entry in the \( x \)-th row and the \( y \)-th column denotes the conditional probability \( p(y|x) \) that \( y \) is received when \( x \) is sent. In this channel, all the rows of the probability transition matrix are permutations of each other, and so are the columns. Such a channel is said to be symmetric. Another example of a symmetric channel is one of the form  

\[
Y = X + Z \pmod{c},
\]

where \( Z \) is a uniform random variable over \( \mathcal{Z} \).



CHANNEL CAPACITY

where \( Z \) has some distribution on the integers \(\{0, 1, 2, \ldots, c-1\}\), \( X \) has the same alphabet as \( Z \), and \( Z \) is independent of \( X \).

In both these cases, we can easily find an explicit expression for the capacity of the channel. Letting \( r \) be a row of the transition matrix, we have

\[
I(X; Y) = H(Y) - H(Y|X)
\]
\[
= H(Y) - H(r)
\]
\[
\leq \log |Y| - H(r)
\]
with equality if the output distribution is uniform. But \( p(x) = 1/|X| \) achieves a uniform distribution on \( Y \), as seen from

\[
p(y) = \sum_{x \in X} p(y|x)p(x) = \frac{1}{|X|} \sum_{x \in X} p(y|x) = \frac{c}{|X|} = \frac{1}{|Y|},
\]

where \( c \) is the sum of the entries in one column of the probability transition matrix.

Thus, the channel in (7.17) has the capacity

\[
C = \max_{p(x)} I(X; Y) = \log 3 - H(0.5, 0.3, 0.2),
\]

and \( C \) is achieved by a uniform distribution on the input.

The transition matrix of the symmetric channel defined above is doubly stochastic. In the computation of the capacity, we used the facts that the rows were permutations of one another and that all the column sums were equal.

Considering these properties, we can define a generalization of the concept of a symmetric channel as follows:

**Definition**  
A channel is said to be symmetric if the rows of the channel transition matrix \( p(y|x) \) are permutations of each other and the columns are permutations of each other. A channel is said to be weakly symmetric if every row of the transition matrix \( p(\cdot|X) \) is a permutation of every other row and all the column sums \( \sum_{x} p(y|x) \) are equal.

For example, the channel with transition matrix

\[
p(y|x) = \begin{pmatrix}
\frac{1}{3} & \frac{1}{6} & \frac{1}{2} \\
\frac{1}{3} & \frac{1}{2} & \frac{1}{6} \\
\frac{1}{3} & \frac{1}{6} & \frac{1}{2}
\end{pmatrix}
\]

is weakly symmetric but not symmetric.

---

### Preview of the Channel Coding Theorem

The above derivation for symmetric channels carries over to weakly symmetric channels as well. We have the following theorem for weakly symmetric channels:

**Theorem 7.2.1**  
For a weakly symmetric channel,

\[
C = \log |Y| - H(\text{row of transition matrix}),
\]

and this is achieved by a uniform distribution on the input alphabet.

---

### Properties of Channel Capacity

1. \( C \geq 0 \) since \( I(X; Y) \geq 0 \).
2. \( C \leq \log |X| \) since \( C = \max I(X; Y) \leq \max H(X) = \log |X| \).
3. \( C \leq \log |Y| \) for the same reason.
4. \( I(X; Y) \) is a continuous function of \( p(x) \).
5. \( I(X; Y) \) is a concave function of \( p(x) \) (Theorem 2.7.4). Since \( I(X; Y) \) is a concave function over a closed convex set, a local maximum is a global maximum. From properties 2 and 3, the maximum is finite, and we are justified in using the term "maximum" rather than "supremum" in the definition of capacity. The maximum can then be found by standard nonlinear optimization techniques such as gradient search. Some of the methods that can be used include the following:

- Constrained maximization using calculus and the Kuhn–Tucker conditions.
- The Frank–Wolfe gradient search algorithm.
- An iterative algorithm developed by Arimoto [25] and Blahut [65]. We describe the algorithm in Section 10.8.

In general, there is no closed-form solution for the capacity. But for many simple channels, it is possible to calculate the capacity using properties such as symmetry. Some of the examples considered earlier are of this form.

---

### Preview of the Channel Coding Theorem

So far, we have defined the information capacity of a discrete memoryless channel. In the next section, we prove Shannon’s second theorem, which gives an operational meaning to the definition of capacity as the number of bits we can transmit reliably over the channel. But first, we will try to give an intuitive idea of why we can transmit \( C \) bits of information over a channel. The basic idea is that for large block lengths, every channel looks like the noisy typewriter channel (Figure 7.4), and the channel has a subset of inputs that produce essentially disjoint sequences at the output.

For each (typical) input \( n \)-sequence, there are approximately \( 2^{nH(Y|X)} \) possible \( Y \) sequences, all of them equally likely (Figure 7.7). We wish to ensure that no two \( X \) sequences produce the same \( Y \) output sequence. Otherwise, we will not be able to decide which \( X \) sequence was sent.

The total number of possible (typical) \( Y \) sequences is approximately \( 2^{nH(Y)} \). This set has to be divided into sets of size \( 2^{nH(Y|X)} \) corresponding to the different input \( X \) sequences. The total number of disjoint sets is less than or equal to \( 2^{n(H(Y) - H(Y|X))} = 2^{nI(X; Y)} \). Hence, we can send at most approximately \( 2^{nI(X; Y)} \) distinguishable sequences of length \( n \).

Although the above derivation outlines an upper bound on the capacity, a stronger version of the above argument will be used in the next section to prove that this rate \( I \) is achievable with an arbitrarily low probability of error.

Before we proceed to the proof of Shannon’s second theorem, we need a few definitions.

---

### Definitions

We analyze a communication system as shown in Figure 7.8.

A message \( W \), drawn from the index set \(\{1, 2, \ldots, M\}\), results in the signal \( X^n(W) \), which is received by the receiver as a random sequence \( Y^n \sim p(y^n|x^n) \). The receiver then guesses the index \( W \) by an appropriate decoding rule \( \hat{W} = g(Y^n) \). The receiver makes an error if \( \hat{W} \) is not the same as the index \( W \) that was transmitted. We now define these ideas formally.

**Definition**  
A discrete channel, denoted by \((X, p(y|x), Y)\), consists of two finite sets \( X \) and \( Y \) and a collection of probability mass functions \( p(y|x) \), one for each \( x \in X \), such that for every \( x \) and \( y \), \( p(y|x) \geq 0 \), and for every \( x \), \( \sum_{y} p(y|x) = 1 \), with the interpretation that \( X \) is the input and \( Y \) is the output of the channel.

**Definition**  
The \( n \)-th extension of the discrete memoryless channel (DMC) is the channel \((X^n, p(y^n|x^n), Y^n)\), where

\[
p(y_k|x_k, y_{k-1}) = p(y_k|x_k), \quad k = 1, 2, \ldots, n.
\]

**Remark**  
If the channel is used without feedback [i.e., if the input symbols do not depend on the past output symbols, namely \( p(x_k|x_{k-1}, y_{k-1}) = p(x_k|x_{k-1}) \)], the channel transition function for the \( n \)-th extension of the discrete memoryless channel reduces to

\[
p(y^n|x^n) = \prod_{i=1}^n p(y_i|x_i).
\]

When we refer to the discrete memoryless channel, we mean the discrete memoryless channel without feedback unless we state explicitly otherwise.

**Definition**  
An \((M, n)\) code for the channel \((X, p(y|x), Y)\) consists of the following:

1. An index set \(\{1, 2, \ldots, M\}\).
2. An encoding function \( X^n: \{1, 2, \ldots, M\} \to X^n \), yielding codewords \( x^n(1), x^n(2), \ldots, x^n(M) \). The set of codewords is called the codebook.

3. A decoding function

\[
g: Y^n \to \{1, 2, \ldots, M\},
\]

which is a deterministic rule that assigns a guess to each possible received vector.

**Definition**  
(Conditional probability of error) Let

\[
\lambda_i = \Pr(g(Y^n) \neq i \mid X^n = x^n(i)) = \sum_{y^n} p(y^n|x^n(i))I(g(y^n) \neq i)
\]

be the conditional probability of error given that index \( i \) was sent, where \( I(\cdot) \) is the indicator function.

**Definition**  
The maximal probability of error \( \lambda(n) \) for an \((M, n)\) code is defined as

\[
\lambda(n) = \max_{i \in \{1, 2, \ldots, M\}} \lambda_i.
\]

**Definition**  
The (arithmetic) average probability of error \( P_e^{(n)} \) for an \((M, n)\) code is defined as

\[
P_e^{(n)} = \frac{1}{M} \sum_{i=1}^M \lambda_i.
\]

Note that if the index \( W \) is chosen according to a uniform distribution over the set \(\{1, 2, \ldots, M\}\), and \( X^n = x^n(W) \), then

\[
P_e^{(n)} \triangleq \Pr(W \neq g(Y^n)),
\]

(i.e., \( P_e^{(n)} \) is the probability of error). Also, obviously,

\[
P_e^{(n)} \leq \lambda(n).
\]

One would expect the maximal probability of error to behave quite differently from the average probability of error. But in the next section, we prove that a small average probability of error implies a small maximal probability of error at essentially the same rate.

---

### Jointly Typical Sequences

It is worth noting that \( P_e^{(n)} \) defined in (7.32) is only a mathematical construct of the conditional probabilities of error \( \lambda_i \) and is itself a probability of error only if the message is chosen uniformly over the message set \(\{1, 2, \ldots, 2^M\}\). However, both in the proof of achievability and the converse, we choose a uniform distribution on \( W \) to bound the probability of error. This allows us to establish the behavior of \( P_e^{(n)} \) and the maximal probability of error \( \lambda(n) \) and thus characterize the behavior of the channel regardless of how it is used (i.e., no matter what the distribution of \( W \) is).

**Definition**  
The rate \( R \) of an \((M, n)\) code is

\[
R = \frac{\log M}{n} \text{ bits per transmission}.
\]

**Definition**  
A rate \( R \) is said to be achievable if there exists a sequence of \((2^{nR}, n)\) codes such that the maximal probability of error \( \lambda(n) \) tends to 0 as \( n \to \infty \).

Later, we write \((2^{nR}, n)\) codes to mean \((2^{nR}, n)\) codes. This will simplify the notation.

**Definition**  
The capacity of a channel is the supremum of all achievable rates.

Thus, rates less than capacity yield arbitrarily small probability of error for sufficiently large block lengths.

---

### Jointly Typical Sequences

Roughly speaking, we decode a channel output \( Y^n \) as the \( i \)-th index if the codeword \( X^n(i) \) is "jointly typical" with the received signal \( Y^n \). We now define the important idea of joint typicality and find the probability of joint typicality when \( X^n(i) \) is the true cause of \( Y^n \) and when it is not.

**Definition**  
The set \( A^{(n)}_{\epsilon} \) of jointly typical sequences \(\{(x^n, y^n)\}\) with respect to the distribution \( p(x, y) \) is the set of \( n \)-sequences with empirical entropies \( \epsilon \)-close to the true entropies:

\[
A^{(n)}_{\epsilon} = \left\{ (x^n, y^n) \in X^n \times Y^n : \right.
\]

\[
-\frac{1}{n} \log p(x^n) - H(X) < \epsilon,
\]

\[
-\frac{1}{n} \log p(y^n) - H(Y) < \epsilon,
\]

\[
-\frac{1}{n} \log p(x^n, y^n) - H(X, Y) < \epsilon \left. \right\},
\]

where

\[
p(x^n, y^n) = \prod_{i=1}^n p(x_i, y_i).
\]

**Theorem 7.6.1** (Joint AEP)  
Let \((X^n, Y^n)\) be sequences of length \( n \) drawn i.i.d. according to \( p(x^n, y^n) = \prod_{i=1}^n p(x_i, y_i) \). Then:

1. \( \Pr((X^n, Y^n) \in A^{(n)}_{\epsilon}) \to 1 \) as \( n \to \infty \).
2. \( |A^{(n)}_{\epsilon}| \leq 2^{n(H(X,Y) + \epsilon)} \).
3. If \( (\tilde{X}^n, \tilde{Y}^n) \sim p(x^n)p(y^n) \) [i.e., \( \tilde{X}^n \) and \( \tilde{Y}^n \) are independent with the same marginals as \( p(x^n, y^n) \)], then

\[
\Pr\left( (\tilde{X}^n, \tilde{Y}^n) \in A^{(n)}_{\epsilon} \right) \leq 2^{-n(I(X; Y) - 3\epsilon)}.
\]

Also, for sufficiently large \( n \),

\[
\Pr\left( (\tilde{X}^n, \tilde{Y}^n) \in A^{(n)}_{\epsilon} \right) \geq (1 - \epsilon)2^{-n(I(X; Y) + 3\epsilon)}.
\]

**Proof**  
1. We begin by showing that with high probability, the sequence is in the typical set. By the weak law of large numbers,

\[
-\frac{1}{n} \log p(X^n) \to -E[\log p(X)] = H(X)
\]

in probability.

Given \( \epsilon > 0 \), there exists \( n_1 \), such that for all \( n > n_1 \),

\[
\Pr\left( \left| -\frac{1}{n} \log p(X^n) - H(X) \right| \geq \epsilon \right) < \frac{\epsilon}{3}.
\]

Similarly, by the weak law,

\[
-\frac{1}{n} \log p(Y^n) \to -E[\log p(Y)] = H(Y)
\]

in probability,

and

\[
-\frac{1}{n} \log p(X^n, Y^n) \to -E[\log p(X, Y)] = H(X, Y)
\]

in probability,

and there exist \( n_2 \) and \( n_3 \), such that for all \( n \geq n_2 \),

\[
\Pr\left( \left| -\frac{1}{n} \log p(Y^n) - H(Y) \right| \geq \epsilon \right) < \frac{\epsilon}{3},
\]

and for all \( n \geq n_3 \),

\[
\Pr\left( \left| -\frac{1}{n} \log p(X^n, Y^n) - H(X, Y) \right| \geq \epsilon \right) < \frac{\epsilon}{3}.
\]

Choosing \( n > \max\{n_1, n_2, n_3\} \), the probability of the union of the sets in (7.42), (7.45), and (7.46) must be less than \( \epsilon \). Hence, for \( n \) sufficiently large, the probability of the set \( A^{(n)}_{\epsilon} \) is greater than \( 1 - \epsilon \), establishing the first part of the theorem.

2. To prove the second part of the theorem, we have

\[
1 = \sum_{(x^n, y^n)} p(x^n, y^n)
\]

\[
\geq \sum_{(x^n, y^n) \in A^{(n)}_{\epsilon}} p(x^n, y^n)
\]

\[
\geq |A^{(n)}_{\epsilon}| 2^{-n(H(X,Y) + \epsilon)}.
\]

Hence,

\[
|A^{(n)}_{\epsilon}| \leq 2^{n(H(X,Y) + \epsilon)}.
\]

3. Now, if \( \tilde{X}^n \) and \( \tilde{Y}^n \) are independent but have the same marginals as \( X^n \) and \( Y^n \), then

\[
\Pr\left( (\tilde{X}^n, \tilde{Y}^n) \in A^{(n)}_{\epsilon} \right) = \sum_{(x^n, y^n) \in A^{(n)}_{\epsilon}} p(x^n)p(y^n)
\]

\[
\leq |A^{(n)}_{\epsilon}| 2^{-n(H(X) - \epsilon)} 2^{-n(H(Y) - \epsilon)}
\]

\[
= 2^{-n(I(X; Y) - 3\epsilon)}.
\]



CHANNEL CAPACITY

For sufficiently large \( n \), \( \Pr(A(n)_\varepsilon) \geq 1 - \varepsilon \), and therefore,

\[
1 - \varepsilon \leq \sum_{(x^n, y^n) \in A(n)_\varepsilon} p(x^n, y^n) \leq |A(n)_\varepsilon| 2^{-n(H(X,Y) - \varepsilon)}.
\]

This gives:

\[
|A(n)_\varepsilon| \geq (1 - \varepsilon) 2^{n(H(X,Y) - \varepsilon)}.
\]

By similar arguments, we can show that for \( n \) sufficiently large,

\[
\Pr((\tilde{X}^n, \tilde{Y}^n) \in A(n)_\varepsilon) = \sum_{A(n)_\varepsilon} p(x^n) p(y^n) \geq (1 - \varepsilon) 2^{n(H(X,Y) - \varepsilon)} 2^{-n(H(X) + \varepsilon)} 2^{-n(H(Y) + \varepsilon)} = (1 - \varepsilon) 2^{-n(I(X;Y) + 3\varepsilon)}.
\]

The jointly typical set is illustrated in Figure 7.9. There are about \( 2^{nH(X)} \) typical \( X \) sequences and about \( 2^{nH(Y)} \) typical \( Y \) sequences. However, since there are only \( 2^{nH(X,Y)} \) jointly typical sequences, not all pairs of typical \( X^n \) and typical \( Y^n \) are also jointly typical. The probability that any randomly chosen pair is jointly typical is about \( 2^{-nI(X;Y)} \). Hence, we can consider about \( 2^{nI(X;Y)} \) such pairs before we are likely to come across a jointly typical pair. This suggests that there are about \( 2^{nI(X;Y)} \) distinguishable signals \( X^n \).

Another way to look at this is in terms of the set of jointly typical sequences for a fixed output sequence \( Y^n \), presumably the output sequence resulting from the true input signal \( X^n \). For this sequence \( Y^n \), there are about \( 2^{nH(X|Y)} \) conditionally typical input signals. The probability that some randomly chosen (other) input signal \( X^n \) is jointly typical with \( Y^n \) is about \( 2^{nH(X|Y)} / 2^{nH(X)} = 2^{-nI(X;Y)} \). This again suggests that we can choose about \( 2^{nI(X;Y)} \) codewords \( X^n(W) \) before one of these codewords will get confused with the codeword that caused the output \( Y^n \).

CHANNEL CODING THEOREM

We now prove what is perhaps the basic theorem of information theory, the achievability of channel capacity, first stated and essentially proved by Shannon in his original 1948 paper. The result is rather counterintuitive; if the channel introduces errors, how can one correct them all? Any correction process is also subject to error, ad infinitum.

Shannon used a number of new ideas to prove that information can be sent reliably over a channel at all rates up to the channel capacity. These ideas include:

- Allowing an arbitrarily small but nonzero probability of error
- Using the channel many times in succession, so that the law of large numbers comes into effect
- Calculating the average of the probability of error over a random choice of codebooks, which symmetrizes the probability, and which can then be used to show the existence of at least one good code

Shannon’s outline of the proof was based on the idea of typical sequences, but the proof was not made rigorous until much later. The proof given below makes use of the properties of typical sequences and is probably the simplest of the proofs developed so far. As in all the proofs, we use the same essential ideas—random code selection, calculation of the average probability of error for a random choice of codewords, and so on. The main difference is in the decoding rule. In the proof, we decode by joint typicality; we look for a codeword that is jointly typical with the received sequence. If we find a unique codeword satisfying this property, we declare that word to be the transmitted codeword. By the properties of joint typicality stated previously, with high probability the transmitted codeword and the received sequence are jointly typical, since they are probabilistically related. Also, the probability that any other codeword looks jointly typical with the received sequence is \( 2^{-nI} \). Hence, if we have fewer than \( 2^{nI} \) codewords, then with high probability there will be no other codewords that can be confused with the transmitted codeword, and the probability of error is small.

Although jointly typical decoding is suboptimal, it is simple to analyze and still achieves all rates below capacity.

We now give the complete statement and proof of Shannon’s second theorem:

**Theorem 7.7.1 (Channel Coding Theorem)**  
For a discrete memoryless channel, all rates below capacity \( C \) are achievable. Specifically, for every rate \( R < C \), there exists a sequence of \( (2^{nR}, n) \) codes with maximum probability of error \( \lambda(n) \to 0 \).

Conversely, any sequence of \( (2^{nR}, n) \) codes with \( \lambda(n) \to 0 \) must have \( R \leq C \).

**Proof:**  
We prove that rates \( R < C \) are achievable and postpone proof of the converse to Section 7.9.

**Achievability:** Fix \( p(x) \). Generate a \( (2^{nR}, n) \) code at random according to the distribution \( p(x) \). Specifically, we generate \( 2^{nR} \) codewords independently according to the distribution

\[
p(x^n) = \prod_{i=1}^n p(x_i).
\]

We exhibit the \( 2^{nR} \) codewords as the rows of a matrix:

\[
C = \begin{bmatrix}
x_1(1) & x_2(1) & \cdots & x_n(1) \\
\vdots & \vdots & \ddots & \vdots \\
x_1(2^{nR}) & x_2(2^{nR}) & \cdots & x_n(2^{nR})
\end{bmatrix}.
\]

Each entry in this matrix is generated i.i.d. according to \( p(x) \). Thus, the probability that we generate a particular code \( C \) is

\[
\Pr(C) = \prod_{w=1}^{2^{nR}} \prod_{i=1}^n p(x_i(w)).
\]

Consider the following sequence of events:

1. A random code \( C \) is generated as described according to \( p(x) \).
2. The code \( C \) is then revealed to both sender and receiver. Both sender and receiver are also assumed to know the channel transition matrix \( p(y|x) \) for the channel.
3. A message \( W \) is chosen according to a uniform distribution

\[
\Pr(W = w) = 2^{-nR}, \quad w = 1, 2, \ldots, 2^{nR}.
\]

4. The \( w \)-th codeword \( X^n(w) \), corresponding to the \( w \)-th row of \( C \), is sent over the channel.
5. The receiver receives a sequence \( Y^n \) according to the distribution

\[
P(y^n|X^n = x^n(w)) = \prod_{i=1}^n p(y_i|x_i(w)).
\]

6. The receiver guesses which message was sent. (The optimum procedure to minimize probability of error is maximum likelihood decoding. But this procedure is difficult to analyze. Instead, we will use jointly typical decoding, which is described below. Jointly typical decoding is easier to analyze and is asymptotically optimal.) In jointly typical decoding, the receiver declares that the index \( \hat{W} \) was sent if the following conditions are satisfied:

- \( (X^n(\hat{W}), Y^n) \) is jointly typical.
- There is no other index \( W' \neq \hat{W} \) such that \( (X^n(W'), Y^n) \in A(n)_\varepsilon \).

If no such \( \hat{W} \) exists or if there is more than one such, an error is declared. (We may assume that the receiver outputs a dummy index such as 0 in this case.)

7. There is a decoding error if \( \hat{W} \neq W \). Let \( E \) be the event \( \{ \hat{W} \neq W \} \).

**Analysis of the Probability of Error**  
Outline: We first outline the analysis. Instead of calculating the probability of error for a single code, we calculate the average over all codes generated at random according to the distribution \( p(x) \). By the symmetry of the code construction, the average probability of error does not depend on the particular index that was sent. For a typical codeword, there are two different sources of error when we use jointly typical decoding: Either the output \( Y^n \) is not jointly typical with the transmitted codeword or there is some other codeword that is jointly typical with the received sequence. The probability that the transmitted codeword and the received sequence are jointly typical goes to 1, as shown by the joint AEP. For any rival codeword, the probability that it is jointly typical with the received sequence is approximately \( 2^{-nI} \), and hence we can use about \( 2^{nI} \) codewords and still have a low probability of error. We will later extend the argument to find a code with a low maximal probability of error.

**Detailed Calculation of the Probability of Error:**  
We let \( W \) be drawn according to a uniform distribution over \( \{1, 2, \ldots, 2^{nR}\} \) and use jointly typical decoding \( \hat{W}(y^n) \) as described in step 6. Let \( E = \{ \hat{W}(Y^n) \neq W \} \) denote the error event. We will calculate the average probability of error, averaged over all codewords in the codebook, and averaged over all codebooks; that is, we calculate

\[
\Pr(E) = \sum_C \Pr(C) P_e(C),
\]

where \( P_e(C) \) is defined for jointly typical decoding. By the symmetry of the code construction, the average probability of error averaged over all codes does not depend on the particular index that was sent. Thus, we can assume without loss of generality that the message \( W = 1 \) was sent, since

\[
\Pr(E) = \frac{1}{2^{nR}} \sum_{w=1}^{2^{nR}} \sum_C \Pr(C) \lambda_w(C).
\]

Define the following events:

\[
E_i = \{ (X^n(i), Y^n) \in A(n)_\varepsilon \}, \quad i \in \{1, 2, \ldots, 2^{nR}\}.
\]

Then an error occurs in the decoding scheme if either \( E_1^c \) occurs (when the transmitted codeword and the received sequence are not jointly typical) or \( E_2 \cup E_3 \cup \cdots \cup E_{2^{nR}} \) occurs (when a wrong codeword is jointly typical with the received sequence). Hence, letting \( P(E) \) denote \( \Pr(E|W = 1) \), we have

\[
\Pr(E|W = 1) = \Pr(E_1^c \cup E_2 \cup E_3 \cup \cdots \cup E_{2^{nR}}|W = 1).
\]

By the union of events bound for probabilities,

\[
\Pr(E|W = 1) \leq \Pr(E_1^c|W = 1) + \sum_{i=2}^{2^{nR}} \Pr(E_i|W = 1).
\]

By the joint AEP, \( \Pr(E_1^c|W = 1) \to 0 \), and hence \( \Pr(E_1^c|W = 1) \leq \varepsilon \) for \( n \) sufficiently large. Since by the code generation process, \( X^n(1) \) and \( X^n(i) \) are independent for \( i \neq 1 \), so are \( Y^n \) and \( X^n(i) \). Hence, the probability that \( X^n(i) \) and \( Y^n \) are jointly typical is \( \leq 2^{-n(I(X;Y) - 3\varepsilon)} \) by the joint AEP. Consequently,

\[
\Pr(E|W = 1) \leq \varepsilon + \sum_{i=2}^{2^{nR}} 2^{-n(I(X;Y) - 3\varepsilon)}.
\]

This simplifies to

\[
\Pr(E|W = 1) \leq \varepsilon + (2^{nR} - 1) 2^{-n(I(X;Y) - 3\varepsilon)}.
\]

For \( n \) sufficiently large and \( R < I(X;Y) - 3\varepsilon \), this becomes

\[
\Pr(E|W = 1) \leq 2\varepsilon.
\]

To finish the proof, we strengthen this conclusion by a series of code selections:

1. Choose \( p(x) \) in the proof to be \( p^*(x) \), the distribution on \( X \) that achieves capacity. Then the condition \( R < I(X;Y) \) can be replaced by the achievability condition \( R < C \).

2. Get rid of the average over codebooks. Since the average probability of error over codebooks is small (\( \leq 2\varepsilon \)), there exists at least one codebook \( C^* \) with a small average probability of error. Thus, \( \Pr(E|C^*) \leq 2\varepsilon \). Determination of \( C^* \) can be achieved by an exhaustive search over all \( (2^{nR}, n) \) codes.

3. Throw away the worst half of the codewords in the best codebook \( C^* \). Since the arithmetic average probability of error \( P_e(C^*) \) for this code is less than \( 2\varepsilon \), we have

\[
\Pr(E|C^*) \leq \frac{1}{2^{nR}} \sum_{i=1}^{2^{nR}} \lambda_i(C^*) \leq 2\varepsilon,
\]

which implies that at least half the indices \( i \) and their associated codewords \( X^n(i) \) must have conditional probability of error \( \lambda_i \) less than \( 4\varepsilon \). Hence, the best half of the codewords have a maximal probability of error less than \( 4\varepsilon \). If we reindex these codewords, we have \( 2^{nR} - 1 \) codewords. Throwing out half the codewords has changed the rate from \( R \) to \( R - \frac{1}{n} \), which is negligible for large \( n \).

Combining all these improvements, we have constructed a code of rate \( R' = R - \frac{1}{n} \), with maximal probability of error \( \lambda(n) \leq 4\varepsilon \). This proves the achievability of any rate below capacity.

Random coding is the method of proof for Theorem 7.7.1, not the method of signaling. Codes are selected at random in the proof merely to symmetrize the mathematics and to show the existence of a good deterministic code. We proved that the average over all codes of block length \( n \) has a small probability of error. We can find the best code within this set by an exhaustive search. Incidentally, this shows that the Kolmogorov complexity of the best code is a small constant. This means that the revelation to the sender and receiver of the best code \( C^* \) requires no channel. The sender and receiver merely agree to use the best \( (2^{nR}, n) \) code for the channel.

Although the theorem shows that there exist good codes with arbitrarily small probability of error for long block lengths, it does not provide a way of constructing the best codes. If we used the scheme suggested



ZERO-ERROR CODES

by the proof and generate a code at random with the appropriate distribution, the code constructed is likely to be good for long block lengths. However, without some structure in the code, it is very difficult to decode (the simple scheme of table lookup requires an exponentially large table). Hence, the theorem does not provide a practical coding scheme. Ever since Shannon’s original paper on information theory, researchers have tried to develop structured codes that are easy to encode and decode. In Section 7.11, we discuss Hamming codes, the simplest of a class of algebraic error-correcting codes that can correct one error in a block of bits. Since Shannon’s paper, a variety of techniques have been used to construct error-correcting codes, and with turbo codes, we have come close to achieving capacity for Gaussian channels.

ZERO-ERROR CODES

The outline of the proof of the converse is most clearly motivated by going through the argument when absolutely no errors are allowed. We will now prove that \( P^{(n)}_e = 0 \) implies that \( R \leq C \). Assume that we have a \((2^{nR}, n)\) code with zero probability of error [i.e., the decoder output \( g(Y^n) \) is equal to the input index \( W \) with probability 1]. Then the input index \( W \) is determined by the output sequence [i.e., \( H(W|Y^n) = 0 \)]. Now, to obtain a strong bound, we arbitrarily assume that \( W \) is uniformly distributed over \(\{1, 2, \ldots, 2^{nR}\}\). Thus, \( H(W) = nR \). We can now write the string of inequalities:

\[
nR = H(W) = H(W|Y^n)
\]
\[
= 0 + I(W; Y^n)
\]
\[
= I(W; Y^n)
\]
\[
\leq I(X^n; Y^n)
\]
\[
\leq \sum_{i=1}^n I(X_i; Y_i)
\]
\[
\leq nC,
\]

where (a) follows from the data-processing inequality (since \( W \to X^n(W) \to Y^n \) forms a Markov chain), (b) will be proved in Lemma 7.9.2 using the discrete memoryless assumption, and (c) follows from the definition of (information) capacity. Hence, for any zero-error \((2^{nR}, n)\) code, for all \( n \),

\[
R \leq C.
\]

CHANNEL CAPACITY

FANO’S INEQUALITY AND THE CONVERSE TO THE CODING THEOREM

We now extend the proof that was derived for zero-error codes to the case of codes with very small probabilities of error. The new ingredient will be Fano’s inequality, which gives a lower bound on the probability of error in terms of the conditional entropy. Recall the proof of Fano’s inequality, which is repeated here in a new context for reference.

Let us define the setup under consideration. The index \( W \) is uniformly distributed on the set \( \mathcal{W} = \{1, 2, \ldots, 2^{nR}\} \), and the sequence \( Y^n \) is related probabilistically to \( W \). From \( Y^n \), we estimate the index \( W \) that was sent. Let the estimate be \( \hat{W} = g(Y^n) \). Thus, \( W \to X^n(W) \to Y^n \to \hat{W} \) forms a Markov chain. Note that the probability of error is

\[
\Pr(\hat{W} \neq W) = \frac{1}{2^{nR}} \sum_{i} \lambda_i = P^{(n)}_e.
\]

We begin with the following lemma, which has been proved in Section 2.10:

**Lemma 7.9.1 (Fano’s inequality)**  
For a discrete memoryless channel with a codebook \( \mathcal{C} \) and the input message \( W \) uniformly distributed over \( 2^{nR} \), we have

\[
H(W|\hat{W}) \leq 1 + P^{(n)}_e nR.
\]

*Proof:*  
Since \( W \) is uniformly distributed, we have \( P^{(n)}_e = \Pr(W \neq \hat{W}) \). We apply Fano’s inequality (Theorem 2.10.1) for \( W \) in an alphabet of size \( 2^{nR} \).  
□

We will now prove a lemma which shows that the capacity per transmission is not increased if we use a discrete memoryless channel many times.

**Lemma 7.9.2**  
Let \( Y^n \) be the result of passing \( X^n \) through a discrete memoryless channel of capacity \( C \). Then

\[
I(X^n; Y^n) \leq nC
\]

for all \( p(x^n) \).

*Proof:*  
\[
I(X^n; Y^n) = H(Y^n) - H(Y^n|X^n)
\]
\[
= H(Y^n) - \sum_{i=1}^n H(Y_i|Y_1, \ldots, Y_{i-1}, X^n)
\]
\[
= H(Y^n) - \sum_{i=1}^n H(Y_i|X_i),
\]

since by the definition of a discrete memoryless channel, \( Y_i \) depends only on \( X_i \) and is conditionally independent of everything else. Continuing the series of inequalities, we have

\[
I(X^n; Y^n) = H(Y^n) - \sum_{i=1}^n H(Y_i|X_i)
\]
\[
\leq \sum_{i=1}^n H(Y_i) - \sum_{i=1}^n H(Y_i|X_i)
\]
\[
= \sum_{i=1}^n I(X_i; Y_i)
\]
\[
\leq nC,
\]

where (7.95) follows from the fact that the entropy of a collection of random variables is less than the sum of their individual entropies, and (7.97) follows from the definition of capacity. Thus, we have proved that using the channel many times does not increase the information capacity in bits per transmission.  
□

We are now in a position to prove the converse to the channel coding theorem.

*Proof:*  
Converse to Theorem 7.7.1 (Channel coding theorem). We have to show that any sequence of \((2^{nR}, n)\) codes with \( \lambda(n) \to 0 \) must have \( R \leq C \). If the maximal probability of error tends to zero, the average probability of error for the sequence of codes also goes to zero [i.e., \( \lambda(n) \to 0 \) implies \( P^{(n)}_e \to 0 \), where \( P^{(n)}_e \) is defined in (7.32)]. For a fixed encoding rule \( X^n(\cdot) \) and a fixed decoding rule \( \hat{W} = g(Y^n) \), we have \( W \to X^n(W) \to Y^n \to \hat{W} \). For each \( n \), let \( W \) be drawn according to a uniform distribution over \( \{1, 2, \ldots, 2^{nR}\} \). Since \( W \) has a uniform distribution, \( \Pr(\hat{W} \neq W) = P^{(n)}_e = \frac{1}{2^{nR}} \sum_{i} \lambda_i \). Hence,

\[
nR
\]
\[
= H(W)
\]
\[
= H(W|\hat{W}) + I(W; \hat{W})
\]
\[
\leq 1 + P^{(n)}_e nR + I(W; \hat{W})
\]
\[
\leq 1 + P^{(n)}_e nR + I(X^n; Y^n)
\]
\[
\leq 1 + P^{(n)}_e nR + nC,
\]

where (a) follows from the assumption that \( W \) is uniform over \( \{1, 2, \ldots, 2^{nR}\} \), (b) is an identity, (c) is Fano’s inequality for \( W \) taking on at most \( 2^{nR} \) values, (d) is the data-processing inequality, and (e) is from Lemma 7.9.2. Dividing by \( n \), we obtain

\[
R \leq P^{(n)}_e R + \frac{1}{n} + C.
\]

Now letting \( n \to \infty \), we see that the first two terms on the right-hand side tend to 0, and hence

\[
R \leq C.
\]

We can rewrite (7.103) as

\[
P^{(n)}_e \geq 1 - \frac{C}{R} - \frac{1}{nR}.
\]

This equation shows that if \( R > C \), the probability of error is bounded away from 0 for sufficiently large \( n \) (and hence for all \( n \), since if \( P^{(n)}_e = 0 \) for small \( n \), we can construct codes for large \( n \) with \( P^{(n)}_e = 0 \) by concatenating these codes). Hence, we cannot achieve an arbitrarily low probability of error at rates above capacity.  
□

This converse is sometimes called the weak converse to the channel coding theorem. It is also possible to prove a strong converse, which states that for rates above capacity, the probability of error goes exponentially to 1. Hence, the capacity is a very clear dividing point—at rates below capacity, \( P^{(n)}_e \to 0 \) exponentially, and at rates above capacity, \( P^{(n)}_e \to 1 \) exponentially.

EQUALITY IN THE CONVERSE TO THE CHANNEL CODING THEOREM

We have proved the channel coding theorem and its converse. In essence, these theorems state that when \( R < C \), it is possible to send information with an arbitrarily low probability of error, and when \( R > C \), the probability of error is bounded away from zero.

It is interesting and rewarding to examine the consequences of equality in the converse; hopefully, it will give some ideas as to the kinds of codes that achieve capacity. Repeating the steps of the converse in the case when \( P_e = 0 \), we have

\[
nR = H(W)
\]
\[
= H(W|\hat{W}) + I(W; \hat{W})
\]
\[
= I(W; \hat{W}),
\]

\[
\leq I(X^n(W); Y^n)
\]
\[
= H(Y^n) - H(Y^n|X^n)
\]
\[
= H(Y^n) - \sum_{i=1}^n H(Y_i|X_i)
\]
\[
\leq \sum_{i=1}^n H(Y_i) - \sum_{i=1}^n H(Y_i|X_i)
\]
\[
= \sum_{i=1}^n I(X_i; Y_i)
\]
\[
\leq nC.
\]

We have equality in (a), the data-processing inequality, only if \( I(Y^n; X^n(W)|W) = 0 \) and \( I(X^n; Y^n|\hat{W}) = 0 \), which is true if all the codewords are distinct and if \( \hat{W} \) is a sufficient statistic for decoding. We have equality in (b) only if the \( Y_i \)’s are independent, and equality in (c) only if the distribution of \( X_i \) is \( p^*(x) \), the distribution on \( X \) that achieves capacity.

We have equality in the converse only if these conditions are satisfied. This indicates that a capacity-achieving zero-error code has distinct codewords and the distribution of the \( Y_i \)’s must be i.i.d. with

\[
p^*(y) = \sum_{x} p^*(x)p(y|x),
\]

the distribution on \( Y \) induced by the optimum distribution on \( X \). The distribution referred to in the converse is the empirical distribution on \( X \) and \( Y \) induced by a uniform distribution over codewords, that is,

\[
p(x_i, y_i) = \frac{1}{2^{nR}} \sum_{w=1}^{2^{nR}} I(X_i(w) = x_i)p(y_i|X_i(w)).
\]

We can check this result in examples of codes that achieve capacity:

1. Noisy typewriter. In this case, we have an input alphabet of 26 letters, and each letter is either printed out correctly or changed to the next letter with probability \( \frac{1}{2} \). A simple code that achieves capacity (\( \log 13 \)) for this channel is to use every alternate input letter so that no two letters can be confused. In this case, there are 13 codewords of block length 1. If we choose the codewords i.i.d. according to a uniform distribution on \( \{1, 3, 5, 7, \ldots, 25\} \), the output of the channel is also i.i.d. and uniformly distributed on \( \{1, 2, \ldots, 26\} \), as expected.

2. Binary symmetric channel. Since given any input sequence, every possible output sequence has some positive probability, it will not be possible to distinguish even two codewords with zero probability of error. Hence, the zero-error capacity of the BSC is zero. However, even in this case, we can draw some useful conclusions. The efficient codes will still induce a distribution on \( Y \) that looks i.i.d. \( \sim \) Bernoulli(\( \frac{1}{2} \)). Also, from the arguments that lead up to the converse, we can see that at rates close to capacity, we have almost entirely covered the set of possible output sequences with decoding sets corresponding to the codewords. At rates above capacity, the decoding sets begin to overlap, and the probability of error can no longer be made arbitrarily small.

HAMMING CODES

The channel coding theorem promises the existence of block codes that will allow us to transmit information at rates below capacity with an arbitrarily small probability of error if the block length is large enough. Ever since the appearance of Shannon’s original paper [471], people have searched for such codes. In addition to achieving low probabilities of error, useful codes should be “simple,” so that they can be encoded and decoded efficiently.

The search for simple good codes has come a long way since the publication of Shannon’s original paper in 1948. The entire field of coding theory has been developed during this search. We will not be able to describe the many elegant and intricate coding schemes that have been developed since 1948. We will only describe the simplest such scheme developed by Hamming [266]. It illustrates some of the basic ideas underlying most codes.

The object of coding is to introduce redundancy so that even if some of the information is lost or corrupted, it will still be possible to recover the message at the receiver. The most obvious coding scheme is to repeat information. For example, to send a 1, we send 11111, and to send a 0, we send 00000. This scheme uses five symbols to send 1 bit, and therefore has a rate of \( \frac{1}{5} \) bit per symbol. If this code is used on a binary symmetric channel, the optimum decoding scheme is to take the majority vote of each block of five received bits. If three or more bits are 1, we decode



HAMMING CODES

the block as a 1; otherwise, we decode it as 0. An error occurs if and only if more than three of the bits are changed. By using longer repetition codes, we can achieve an arbitrarily low probability of error. But the rate of the code also goes to zero with block length, so even though the code is “simple,” it is really not a very useful code.

Instead of simply repeating the bits, we can combine the bits in some intelligent fashion so that each extra bit checks whether there is an error in some subset of the information bits. A simple example of this is a parity check code. Starting with a block of \( n - 1 \) information bits, we choose the \( n \)-th bit so that the parity of the entire block is 0 (the number of 1’s in the block is even). Then if there is an odd number of errors during the transmission, the receiver will notice that the parity has changed and detect the error. This is the simplest example of an error-detecting code. The code does not detect an even number of errors and does not give any information about how to correct the errors that occur.

We can extend the idea of parity checks to allow for more than one parity check bit and to allow the parity checks to depend on various subsets of the information bits. The Hamming code that we describe below is an example of a parity check code. We describe it using some simple ideas from linear algebra.

To illustrate the principles of Hamming codes, we consider a binary code of block length 7. All operations will be done modulo 2. Consider the set of all nonzero binary vectors of length 3. Arrange them in columns to form a matrix:

\[
H = \begin{bmatrix}
0 & 0 & 0 & 1 & 1 & 1 & 1 \\
1 & 0 & 1 & 1 & 0 & 1 & 0 \\
0 & 1 & 1 & 0 & 1 & 0 & 1 \\
\end{bmatrix}.
\]

Consider the set of vectors of length 7 in the null space of \( H \) (the vectors which when multiplied by \( H \) give 000). From the theory of linear spaces, since \( H \) has rank 3, we expect the null space of \( H \) to have dimension 4. These 16 codewords are:

\[
\begin{aligned}
&0000000, \ 0100101, \ 1000011, \ 1100110, \ 0001111, \ 0101010, \\
&1001100, \ 1101001, \ 0010110, \ 0110011, \ 1010101, \ 1110000, \\
&0011001, \ 0111100, \ 1011010, \ 1111111.
\end{aligned}
\]

Since the set of codewords is the null space of a matrix, it is linear in the sense that the sum of any two codewords is also a codeword. The set of codewords therefore forms a linear subspace of dimension 4 in the vector space of dimension 7.

CHANNEL CAPACITY

Looking at the codewords, we notice that other than the all-0 codeword, the minimum number of 1’s in any codeword is 3. This is called the minimum weight of the code. We can see that the minimum weight of a code has to be at least 3 since all the columns of \( H \) are different, so no two columns can add to 000. The fact that the minimum distance is exactly 3 can be seen from the fact that the sum of any two columns must be one of the columns of the matrix.

Since the code is linear, the difference between any two codewords is also a codeword, and hence any two codewords differ in at least three places. The minimum number of places in which two codewords differ is called the minimum distance of the code. The minimum distance of the code is a measure of how far apart the codewords are and will determine how distinguishable the codewords will be at the output of the channel. The minimum distance is equal to the minimum weight for a linear code. We aim to develop codes that have a large minimum distance.

For the code described above, the minimum distance is 3. Hence if a codeword \( c \) is corrupted in only one place, it will differ from any other codeword in at least two places and therefore be closer to \( c \) than to any other codeword. But can we discover which is the closest codeword without searching over all the codewords?

The answer is yes. We can use the structure of the matrix \( H \) for decoding. The matrix \( H \), called the parity check matrix, has the property that for every codeword \( c \), \( Hc = 0 \). Let \( e_i \) be a vector with a 1 in the \( i \)-th position and 0’s elsewhere. If the codeword is corrupted at position \( i \), the received vector \( r = c + e_i \). If we multiply this vector by the matrix \( H \), we obtain:

\[
Hr = H(c + e_i) = Hc + He_i = He_i,
\]

which is the vector corresponding to the \( i \)-th column of \( H \). Hence looking at \( Hr \), we can find which position of the vector was corrupted. Reversing this bit will give us a codeword. This yields a simple procedure for correcting one error in the received sequence. We have constructed a codebook with 16 codewords of block length 7, which can correct up to one error. This code is called a Hamming code.

We have not yet identified a simple encoding procedure; we could use any mapping from a set of 16 messages into the codewords. But if we examine the first 4 bits of the codewords in the table, we observe that they cycle through all 16 combinations of 4 bits. Thus, we could use these 4 bits to be the 4 bits of the message we want to send; the other 3 bits are then determined by the code. In general, it is possible to modify a linear code so that the mapping is explicit, so that the first \( k \) bits in each codeword represent the message, and the last \( n - k \) bits are parity check bits. Such a code is called a systematic code. The code is often identified by its block length \( n \), the number of information bits \( k \), and the minimum distance \( d \). For example, the above code is called a (7,4,3) Hamming code (i.e., \( n = 7 \), \( k = 4 \), and \( d = 3 \)).

An easy way to see how Hamming codes work is by means of a Venn diagram. Consider the following Venn diagram with three circles and with four intersection regions as shown in Figure 7.10. To send the information sequence 1101, we place the 4 information bits in the four intersection regions as shown in the figure. We then place a parity bit in each of the three remaining regions so that the parity of each circle is even (i.e., there are an even number of 1’s in each circle). Thus, the parity bits are as shown in Figure 7.11.

Now assume that one of the bits is changed; for example, one of the information bits is changed from 1 to 0 as shown in Figure 7.12. Then the parity constraints are violated for two of the circles (highlighted in the figure), and it is not hard to see that given these violations, the only single bit error that could have caused it is at the intersection of the two circles (i.e., the bit that was changed). Similarly, working through the other error cases, it is not hard to see that this code can detect and correct any single bit error in the received codeword.

We can easily generalize this procedure to construct larger matrices \( H \). In general, if we use \( l \) rows in \( H \), the code that we obtain will have block length \( n = 2l - 1 \), \( k = 2l - l - 1 \), and minimum distance 3. All these codes are called Hamming codes and can correct one error.

Hamming codes are the simplest examples of linear parity check codes. They demonstrate the principle that underlies the construction of other linear codes. But with large block lengths, it is likely that there will be more than one error in the block. In the early 1950s, Reed and Solomon found a class of multiple error-correcting codes for nonbinary channels. In the late 1950s, Bose and Ray-Chaudhuri [72] and Hocquenghem [278] generalized the ideas of Hamming codes using Galois field theory to construct \( t \)-error correcting codes (called BCH codes) for any \( t \). Since then, various authors have developed other codes and also developed efficient decoding algorithms for these codes. With the advent of integrated circuits, it has become feasible to implement fairly complex codes in hardware and realize some of the error-correcting performance promised by Shannon’s channel capacity theorem. For example, all compact disc players include error-correction circuitry based on two interleaved (32, 28, 5) and (28, 24, 5) Reed–Solomon codes that allow the decoder to correct bursts of up to 4000 errors.

All the codes described above are block codes—they map a block of information bits onto a channel codeword and there is no dependence on past information bits. It is also possible to design codes where each output block depends not only on the current input block but also on some of the past inputs as well. A highly structured form of such a code is called a convolutional code. The theory of convolutional codes has developed considerably over the last 40 years. We will not go into the details but refer the interested reader to textbooks on coding theory [69, 356].

For many years, none of the known coding algorithms came close to achieving the promise of Shannon’s channel capacity theorem. For a binary symmetric channel with crossover probability \( p \), we would need a code that could correct up to \( np \) errors in a block of length \( n \) and have \( n(1 - H(p)) \) information bits. For example, the repetition code suggested earlier corrects up to \( n/2 \) errors in a block of length \( n \), but its rate goes to 0 with \( n \). Until 1972, all known codes that could correct \( n\alpha \) errors for block length \( n \) had asymptotic rate 0. In 1972, Justesen [301] described a class of codes with positive asymptotic rate and positive asymptotic minimum distance as a fraction of the block length.

In 1993, a paper by Berrou et al. [57] introduced the notion that the combination of two interleaved convolution codes with a parallel cooperative decoder achieved much better performance than any of the earlier codes. Each decoder feeds its “opinion” of the value of each bit to the other decoder and uses the opinion of the other decoder to help it decide the value of the bit. This iterative process is repeated until both decoders agree on the value of the bit. The surprising fact is that this iterative procedure allows for efficient decoding at rates close to capacity for a variety of channels. There has also been a renewed interest in the theory of low-density parity check (LDPC) codes that were introduced by Robert Gallager in his thesis [231, 232]. In 1997, MacKay and Neal [368] showed that an iterative message-passing algorithm similar to the algorithm used for decoding turbo codes could achieve rates close to capacity with high probability for LDPC codes. Both Turbo codes and LDPC codes remain active areas of research and have been applied to wireless and satellite communication channels.

FEEDBACK CAPACITY

A channel with feedback is illustrated in Figure 7.13. We assume that all the received symbols are sent back immediately and noiselessly to the transmitter, which can then use them to decide which symbol to send next. Can we do better with feedback? The surprising answer is no, which we shall now prove. We define a \( (2nR, n) \) feedback code as a sequence of mappings \( x_i(W, Y_{i-1}) \), where each \( x_i \) is a function only of the message \( W \in 2nR \) and the previous received values, \( Y_1, Y_2, \ldots, Y_{i-1} \), and a sequence of decoding functions \( g : Y^n \to \{1, 2, \ldots, 2nR\} \). Thus,

\[
P_e^{(n)} = \Pr(g(Y^n) \neq W),
\]

when \( W \) is uniformly distributed over \( \{1, 2, \ldots, 2nR\} \).

**Definition**  
The capacity with feedback, \( C_{FB} \), of a discrete memoryless channel is the supremum of all rates achievable by feedback codes.

**Theorem 7.12.1**  
(Feedback capacity)  
\[
C_{FB} = C = \max p(x) I(X; Y).
\]

**Proof:**  
Since a nonfeedback code is a special case of a feedback code, any rate that can be achieved without feedback can be achieved with feedback, and hence  
\[
C_{FB} \geq C.
\]

Proving the inequality the other way is slightly more tricky. We cannot use the same proof that we used for the converse to the coding theorem without feedback. Lemma 7.9.2 is no longer true, since \( X_i \) depends on the past received symbols, and it is no longer true that \( Y_i \) depends only on \( X_i \) and is conditionally independent of the future \( X \)’s in (7.93).

There is a simple change that will fix the problem with the proof. Instead of using \( X^n \), we will use the index \( W \) and prove a similar series of inequalities. Let \( W \) be uniformly distributed over \( \{1, 2, \ldots, 2nR\} \). Then  

\[
\Pr(W \neq \hat{W}) = P_e^{(n)},
\]

and  

\[
nR = H(W) = H(W|\hat{W}) + I(W; \hat{W})
\]

\[
\leq 1 + P_e^{(n)} nR + I(W; \hat{W})
\]

\[
\leq 1 + P_e^{(n)} nR + I(W; Y^n),
\]

by Fano’s inequality and the data-processing inequality. Now we can bound \( I(W; Y^n) \) as follows:

\[
I(W; Y^n) = H(Y^n) - H(Y^n|W)
\]

\[
= H(Y^n) - \sum_{i=1}^n H(Y_i|Y_1, Y_2, \ldots, Y_{i-1}, W)
\]

\[
= H(Y^n) - \sum_{i=1}^n H(Y_i|Y_1, Y_2, \ldots, Y_{i-1}, W, X_i)
\]

\[
= H(Y^n) - \sum_{i=1}^n H(Y_i|X_i),
\]

since \( X_i \) is a function of \( Y_1, \ldots, Y_{i-1} \) and \( W \); and conditional on \( X_i \), \( Y_i \) is independent of \( W \) and past samples of \( Y \). Continuing, we have  

\[
I(W; Y^n) = H(Y^n) - \sum_{i=1}^n H(Y_i|X_i)
\]

\[
\leq \sum_{i=1}^n H(Y_i) - \sum_{i=1}^n H(Y_i|X_i)
\]

\[
= \sum_{i=1}^n I(X_i; Y_i)
\]

\[
\leq nC
\]

from the definition of capacity for a discrete memoryless channel. Putting these together, we obtain  

\[
nR \leq P_e^{(n)} nR + 1 + nC,
\]



CHANNEL CAPACITY

and dividing by n and letting n → ∞, we conclude that R ≤ C.  
(7.134)  
Thus, we cannot achieve any higher rates with feedback than we can without feedback, and CFB = C.  
□  
(7.135)  

As we have seen in the example of the binary erasure channel, feedback can help enormously in simplifying encoding and decoding. However, it cannot increase the capacity of the channel.

---

### 7.13 SOURCE–CHANNEL SEPARATION THEOREM

It is now time to combine the two main results that we have proved so far: data compression (R > H: Theorem 5.4.2) and data transmission (R < C: Theorem 7.7.1). Is the condition H < C necessary and sufficient for sending a source over a channel? For example, consider sending digitized speech or music over a discrete memoryless channel. We could design a code to map the sequence of speech samples directly into the input of the channel, or we could compress the speech into its most efficient representation, then use the appropriate channel code to send it over the channel. It is not immediately clear that we are not losing something by using the two-stage method, since data compression does not depend on the channel and the channel coding does not depend on the source distribution.

We will prove in this section that the two-stage method is as good as any other method of transmitting information over a noisy channel. This result has some important practical implications. It implies that we can consider the design of a communication system as a combination of two parts, source coding and channel coding. We can design source codes for the most efficient representation of the data. We can, separately and independently, design channel codes appropriate for the channel. The combination will be as efficient as anything we could design by considering both problems together.

The common representation for all kinds of data uses a binary alphabet. Most modern communication systems are digital, and data are reduced to a binary representation for transmission over the common channel. This offers an enormous reduction in complexity. Networks like ATM networks and the Internet use the common binary representation to allow speech, video, and digital data to use the same communication channel.

---

The result—that a two-stage process is as good as any one-stage process—seems so obvious that it may be appropriate to point out that it is not always true. There are examples of multiuser channels where the decomposition breaks down. We also consider two simple situations where the theorem appears to be misleading. A simple example is that of sending English text over an erasure channel. We can look for the most efficient binary representation of the text and send it over the channel. But the errors will be very difficult to decode. If, however, we send the English text directly over the channel, we can lose up to about half the letters and yet be able to make sense of the message. Similarly, the human ear has some unusual properties that enable it to distinguish speech under very high noise levels if the noise is white. In such cases, it may be appropriate to send the uncompressed speech over the noisy channel rather than the compressed version. Apparently, the redundancy in the source is suited to the channel.

Let us define the setup under consideration. We have a source V that generates symbols from an alphabet V. We will not make any assumptions about the kind of stochastic process produced by V other than that it is from a finite alphabet and satisfies the AEP. Examples of such processes include a sequence of i.i.d. random variables and the sequence of states of a stationary irreducible Markov chain. Any stationary ergodic source satisfies the AEP, as we show in Section 16.8.

We want to send the sequence of symbols V^n = V₁, V₂, ..., Vₙ over the channel so that the receiver can reconstruct the sequence. To do this, we map the sequence onto a codeword Xⁿ(Vⁿ) and send the codeword over the channel. The receiver looks at his received sequence Yⁿ and makes an estimate ˆVⁿ of the sequence Vⁿ that was sent. The receiver makes an error if Vⁿ ≠ ˆVⁿ. We define the probability of error as:  
Pr(Vⁿ ≠ ˆVⁿ) =  
∑_{vⁿ, yⁿ} p(vⁿ)p(yⁿ|Xⁿ(vⁿ))I(g(yⁿ) ≠ vⁿ),  
(7.136)  
where I is the indicator function and g(yⁿ) is the decoding function. The system is illustrated in Figure 7.14.

---

### THEOREM 7.13.1 (SOURCE–CHANNEL CODING THEOREM)

If V₁, V₂, ..., Vₙ is a finite alphabet stochastic process that satisfies the AEP and H(V) < C, there exists a source–channel code with probability of error Pr(ˆVⁿ ≠ Vⁿ) → 0. Conversely, for any stationary stochastic process, if H(V) > C, the probability of error is bounded away from zero, and it is not possible to send the process over the channel with arbitrarily low probability of error.

**Proof:**  
*Achievability.* The essence of the forward part of the proof is the two-stage encoding described earlier. Since we have assumed that the stochastic process satisfies the AEP, it implies that there exists a typical set Aⁿ_ε of size ≤ 2ⁿ(H(V) + ε) which contains most of the probability. We will encode only the source sequences belonging to the typical set; all other sequences will result in an error. This will contribute at most ε to the probability of error.

We index all the sequences belonging to Aⁿ_ε. Since there are at most 2ⁿ(H + ε) such sequences, n(H + ε) bits suffice to index them. We can transmit the desired index to the receiver with probability of error less than ε if  
H(V) + ε = R < C.  
(7.137)  

The receiver can reconstruct Vⁿ by enumerating the typical set Aⁿ_ε and choosing the sequence corresponding to the estimated index. This sequence will agree with the transmitted sequence with high probability. To be precise,  
P(Vⁿ ≠ ˆVⁿ) ≤ P(Vⁿ ∉ Aⁿ_ε) + P(g(Yⁿ) ≠ Vⁿ | Vⁿ ∈ Aⁿ_ε)  
(7.138)  
≤ ε + ε = 2ε  
(7.139)  
for n sufficiently large. Hence, we can reconstruct the sequence with low probability of error for n sufficiently large if  
H(V) < C.  
(7.140)  

*Converse.* We wish to show that Pr(ˆVⁿ ≠ Vⁿ) → 0 implies that H(V) ≤ C for any sequence of source-channel codes  
Xⁿ(·) : Vⁿ → Xⁿ,  
(7.141)  
gₙ(·) : Yⁿ → Vⁿ.  
(7.142)  

Thus, Xⁿ(·) is an arbitrary (perhaps random) assignment of codewords to data sequences Vⁿ, and gₙ(·) is any decoding function (assignment of estimates ˆVⁿ to output sequences Yⁿ. By Fano’s inequality, we must have  
H(Vⁿ | ˆVⁿ) ≤ 1 + Pr(ˆVⁿ ≠ Vⁿ) log |Vⁿ| = 1 + Pr(ˆVⁿ ≠ Vⁿ) n log |V|.  
(7.143)  

Hence, for the code,  
H(V)  
(a)  
≤ H(V₁, V₂, ..., Vₙ) / n  
(7.144)  
= H(Vⁿ) / n  
(7.145)  
= (1 / n) H(Vⁿ | ˆVⁿ) + (1 / n) I(Vⁿ; ˆVⁿ)  
(7.146)  
(b)  
≤ (1 / n)(1 + Pr(ˆVⁿ ≠ Vⁿ) n log |V|) + (1 / n) I(Vⁿ; ˆVⁿ)  
(7.147)  
(c)  
≤ (1 / n)(1 + Pr(ˆVⁿ ≠ Vⁿ) n log |V|) + (1 / n) I(Xⁿ; Yⁿ)  
(7.148)  
(d)  
≤ (1 / n) + Pr(ˆVⁿ ≠ Vⁿ) log |V| + C,  
(7.149)  
where (a) follows from the definition of entropy rate of a stationary process, (b) follows from Fano’s inequality, (c) follows from the data-processing inequality (since Vⁿ → Xⁿ → Yⁿ → ˆVⁿ forms a Markov chain) and (d) follows from the memorylessness of the channel. Now, letting n → ∞, we have Pr(ˆVⁿ ≠ Vⁿ) → 0 and hence  
H(V) ≤ C.  
(7.150)  
□  

Hence, we can transmit a stationary ergodic source over a channel if and only if its entropy rate is less than the capacity of the channel. The joint source–channel separation theorem enables us to consider the problem of source coding separately from the problem of channel coding. The source coder tries to find the most efficient representation of the source, and the channel coder encodes the message to combat the noise and errors introduced by the channel. The separation theorem says that the separate encoders (Figure 7.15) can achieve the same rates as the joint encoder (Figure 7.14).

With this result, we have tied together the two basic theorems of information theory: data compression and data transmission. We will try to summarize the proofs of the two results in a few words. The data compression theorem is a consequence of the AEP, which shows that there exists a “small” subset (of size 2ⁿH) of all possible source sequences that contain most of the probability and that we can therefore represent the source with a small probability of error using H bits per symbol. The data transmission theorem is based on the joint AEP; it uses the fact that for long block lengths, the output sequence of the channel is very likely to be jointly typical with the input codeword, while any other codeword is jointly typical with probability ≈ 2⁻ⁿI. Hence, we can use about 2ⁿI codewords and still have negligible probability of error. The source–channel separation theorem shows that we can design the source code and the channel code separately and combine the results to achieve optimal performance.

---

### SUMMARY

**Channel capacity.** The logarithm of the number of distinguishable inputs is given by  
C = maxₚ(x) I(X; Y).  

**Examples**  
• Binary symmetric channel: C = 1 − H(p).  
• Binary erasure channel: C = 1 − α.  
• Symmetric channel: C = log |Y| − H(row of transition matrix).  

**Properties of C**  
1. 0 ≤ C ≤ min{log |X|, log |Y|}.  
2. I(X; Y) is a continuous concave function of p(x).  

**Joint typicality.** The set Aⁿ_ε of jointly typical sequences {(xⁿ, yⁿ)} with respect to the distribution p(x, y) is given by  
Aⁿ_ε = {  
(xⁿ, yⁿ) ∈ Xⁿ × Yⁿ :  
(1 / n) log p(xⁿ) − H(X) < ε,  
(1 / n) log p(yⁿ) − H(Y) < ε,  
(1 / n) log p(xⁿ, yⁿ) − H(X, Y) < ε  
}.  
(7.151)  

**Joint AEP.** Let (Xⁿ, Yⁿ) be sequences of length n drawn i.i.d. according to p(xⁿ, yⁿ) = ∏_{i=1}^n p(xᵢ, yᵢ). Then:  
1. Pr((Xⁿ, Yⁿ) ∈ Aⁿ_ε) → 1 as n → ∞.  
2. |Aⁿ_ε| ≤ 2ⁿ(H(X, Y) + ε).  
3. If ( ˜Xⁿ, ˜Yⁿ) ∼ p(xⁿ)p(yⁿ), then Pr( ˜Xⁿ, ˜Yⁿ ∈ Aⁿ_ε) ≤ 2⁻ⁿ(I(X; Y) − 3ε).  

**Channel coding theorem.** All rates below capacity C are achievable, and all rates above capacity are not; that is, for all rates R < C, there exists a sequence of (2ⁿʳ, n) codes with probability of error λ(n) → 0. Conversely, for rates R > C, λ(n) is bounded away from 0.  

**Feedback capacity.** Feedback does not increase capacity for discrete memoryless channels (i.e., CFB = C).  

**Source–channel theorem.** A stochastic process with entropy rate H cannot be sent reliably over a discrete memoryless channel if H > C. Conversely, if the process satisfies the AEP, the source can be transmitted reliably if H < C.  

---

### PROBLEMS

**7.1 Preprocessing the output.** One is given a communication channel with transition probabilities p(y|x) and channel capacity C = maxₚ(x) I(X; Y). A helpful statistician preprocesses the output by forming ˜Y = g(Y). He claims that this will strictly improve the capacity.  
(a) Show that he is wrong.  
(b) Under what conditions does he not strictly decrease the capacity?  

**7.2 Additive noise channel.** Find the channel capacity of the following discrete memoryless channel:  
Z → Y  
X → Y  
where Pr{Z = 0} = Pr{Z = a} = 1/2. The alphabet for X is X = {0, 1}. Assume that Z is independent of X. Observe that the channel capacity depends on the value of a.  

**7.3 Channels with memory have higher capacity.** Consider a binary symmetric channel with Yᵢ = Xᵢ ⊕ Zᵢ, where ⊕ is mod 2 addition, and Xᵢ, Yᵢ ∈ {0, 1}. Suppose that {Zᵢ} has constant marginal probabilities Pr{Zᵢ = 1} = p = 1 − Pr{Zᵢ = 0}, but that Z₁, Z₂, ..., Zⁿ are not necessarily independent. Assume that Zⁿ is independent of the input Xⁿ. Let C = 1 − H(p, 1 − p). Show that  
maxₚ(x₁,x₂,...,xⁿ) I(X₁, X₂, ..., Xⁿ; Y₁, Y₂, ..., Yⁿ) ≥ nC.  

**7.4 Channel capacity.** Consider the discrete memoryless channel Y = X + Z (mod 11), where  
Z = {  
1, 2, 3  
1, 3, 1  
3, 1, 3  
}  
and X ∈ {0, 1, ..., 10}. Assume that Z is independent of X.  
(a) Find the capacity.  
(b) What is the maximizing p∗(x)?  

**7.5 Using two channels at once.** Consider two discrete memoryless channels (X₁, p(y₁ | x₁), Y₁) and (X₂, p(y₂ | x₂), Y₂) with capacities C₁ and C₂, respectively. A new channel (X₁ × X₂, p(y₁ | x₁) × p(y₂ | x₂), Y₁ × Y₂) is formed in which x₁ ∈ X₁ and x₂ ∈ X₂ are sent simultaneously, resulting in y₁, y₂. Find the capacity of this channel.  

**7.6 Noisy typewriter.** Consider a 26-key typewriter.  
(a) If pushing a key results in printing the associated letter, what is the capacity C in bits?



--- Page 51 ---
PROBLEMS
225
(b) Now suppose that pushing a key results in printing that letter or the next (with equal probability). Thus, A → A or B, ..., Z → Z or A. What is the capacity?
(c) What is the highest rate code with block length one that you can find that achieves zero probability of error for the channel in part (b)?

7.7 Cascade of binary symmetric channels. Show that a cascade of n identical independent binary symmetric channels, X₀ → BSC → X₁ → · · · → X_{n−1} → BSC → X_n, each with raw error probability p, is equivalent to a single BSC with error probability \frac{1}{2}(1 - (1 - 2p)^n) and hence that \lim_{n→∞} I(X₀; X_n) = 0 if p ≠ 0, 1. No encoding or decoding takes place at the intermediate terminals X₁, ..., X_{n−1}. Thus, the capacity of the cascade tends to zero.

7.8 Z-channel. The Z-channel has binary input and output alphabets and transition probabilities p(y|x) given by the following matrix:
Q = \begin{pmatrix} 1 & 0 \\ \frac{1}{2} & \frac{1}{2} \end{pmatrix} for x, y ∈ {0, 1}. Find the capacity of the Z-channel and the maximizing input probability distribution.

7.9 Suboptimal codes. For the Z-channel of Problem 7.8, assume that we choose a (2^{nR}, n) code at random, where each codeword is a sequence of fair coin tosses. This will not achieve capacity. Find the maximum rate R such that the probability of error P(n), averaged over the randomly generated codes, tends to zero as the block length n tends to infinity.

7.10 Zero-error capacity. A channel with alphabet {0, 1, 2, 3, 4} has transition probabilities of the form p(y|x) = \frac{1}{2} if y = x ± 1 mod 5, and 0 otherwise.
(a) Compute the capacity of this channel in bits.
(b) The zero-error capacity of a channel is the number of bits per channel use that can be transmitted with zero probability of error. Clearly, the zero-error capacity of this pentagonal channel is at least 1 bit (transmit 0 or 1 with probability 1/2). Find a block code that shows that the zero-error capacity is greater than 1 bit. Can you estimate the exact value of the zero-error capacity? (Hint: Consider codes of length 2 for this channel.) The zero-error capacity of this channel was finally found by Lovász [365].

7.11 Time-varying channels. Consider a time-varying discrete memoryless channel. Let Y₁, Y₂, ..., Y_n be conditionally independent given X₁, X₂, ..., X_n, with conditional distribution given by p(y| x) = \prod_{i=1}^n p_i(y_i| x_i). Let X = (X₁, X₂, ..., X_n), Y = (Y₁, Y₂, ..., Y_n). Find \max p(x) I(X; Y).

7.12 Unused symbols. Show that the capacity of the channel with probability transition matrix P_{Y|x} = \begin{pmatrix} \frac{2}{3} & 0 & \frac{1}{3} \\ 0 & 1 & 0 \\ \frac{1}{3} & 0 & \frac{2}{3} \end{pmatrix} is achieved by a distribution that places zero probability on one of the input symbols. What is the capacity of this channel? Give an intuitive reason why that letter is not used.

7.13 Erasures and errors in a binary channel. Consider a channel with binary inputs that has both erasures and errors. Let the probability of error be ε and the probability of erasure be α, so the channel is follows:
\begin{pmatrix} 1 - α - ε & α \\ ε & 1 - α - ε \end{pmatrix}.
(a) Find the capacity of this channel.
(b) Specialize to the case of the binary symmetric channel (α = 0).
(c) Specialize to the case of the binary erasure channel (ε = 0).

7.14 Channels with dependence between the letters. Consider the following channel over a binary alphabet that takes in 2-bit symbols and produces a 2-bit output, as determined by the following mapping: 00 → 01, 01 → 10, 10 → 11, and 11 → 00. Thus, if the 2-bit sequence 01 is the input to the channel, the output is 10 with probability 1. Let X₁, X₂ denote the two input symbols and Y₁, Y₂ denote the corresponding output symbols.
(a) Calculate the mutual information I(X₁, X₂; Y₁, Y₂) as a function of the input distribution on the four possible pairs of inputs.
(b) Show that the capacity of a pair of transmissions on this channel is 2 bits.
(c) Show that under the maximizing input distribution, I(X₁; Y₁) = 0. Thus, the distribution on the input sequences that achieves capacity does not necessarily maximize the mutual information between individual symbols and their corresponding outputs.

7.15 Jointly typical sequences. As we did in Problem 3.13 for the typical set for a single random variable, we will calculate the jointly typical set for a pair of random variables connected by a binary symmetric channel, and the probability of error for jointly typical decoding for such a channel.
We consider a binary symmetric channel with crossover probability 0.1. The input distribution that achieves capacity is the uniform distribution [i.e., p(x) = (\frac{1}{2}, \frac{1}{2})], which yields the joint distribution p(x, y) for this channel is given by:
\begin{pmatrix} 0.45 & 0.05 \\ 0.05 & 0.45 \end{pmatrix}.
The marginal distribution of Y is also (\frac{1}{2}, \frac{1}{2}).
(a) Calculate H(X), H(Y), H(X, Y), and I(X; Y) for the joint distribution above.
(b) Let X₁, X₂, ..., X_n be drawn i.i.d. according to the Bernoulli(\frac{1}{2}) distribution. Of the 2^n possible input sequences of length n, which of them are typical [i.e., members of A(n)_ε(X) for ε = 0.2]? Which are the typical sequences in A(n)_ε(Y)?
(c) The jointly typical set A(n)_ε(X, Y) is defined as the set of sequences that satisfy equations (7.35-7.37). The first two equations correspond to the conditions that x^n and y^n are in A(n)_ε(X) and A(n)_ε(Y), respectively. Consider the last condition, which can be rewritten to state that -\frac{1}{n} log p(x^n, y^n) ∈ (H(X, Y) - ε, H(X, Y) + ε). Let k be the number of places in which the sequence x^n differs from y^n (k is a function of the two sequences). Then we can write:
p(x^n, y^n) = \prod_{i=1}^n p(x_i, y_i) = (0.45)^{n−k}(0.05)^k = (\frac{1}{2})^n(1 − p)^{n−k}p^k.
An alternative way to look at this probability is to look at the binary symmetric channel as an additive channel Y = X ⊕ Z, where Z is a binary random variable that is equal to 1 with probability p, and is independent of X. In this case,
p(x^n, y^n) = p(x^n)p(y^n|x^n) = p(x^n)p(z^n|x^n) = p(x^n)p(z^n) = (\frac{1}{2})^n(1 − p)^{n−k}p^k.
Show that the condition that (x^n, y^n) being jointly typical is equivalent to the condition that x^n is typical and z^n = y^n − x^n is typical.
(d) We now calculate the size of A(n)_ε(Z) for n = 25 and ε = 0.2. As in Problem 3.13, here is a table of the probabilities and numbers of sequences with k ones:
\begin{pmatrix} k & \binom{n}{k} & p(k)(1 − p)^{n−k} & -\frac{1}{n} \log p(x^n) \end{pmatrix}.
What is the size of the set A(n)_ε(Z)?
(e) Now consider random coding for the channel, as in the proof of the channel coding theorem. Assume that 2^{nR} codewords X^n(1), X^n(2), ..., X^n(2^{nR}) are chosen uniformly over the 2^n possible binary sequences of length n. One of these codewords is chosen and sent over the channel. The receiver looks at the received sequence and tries to find a codeword in the code that is jointly typical with the received sequence. As argued above, this corresponds to finding a codeword X^n(i) such that Y^n − X^n(i) ∈ A(n)_ε(Z). For a fixed codeword x^n(i), what is the probability that the received sequence Y^n is such that (x^n(i), Y^n) is jointly typical?
(f) Now consider a particular received sequence y^n = 000000...0, say. Assume that we choose a sequence X^n at random, uniformly distributed among all the 2^n possible binary n-sequences. What is the probability that the chosen sequence is jointly typical with this y^n? [Hint: This is the probability of all sequences x^n such that y^n − x^n ∈ A(n)_ε(Z).]
(g) Now consider a code with 2^9 = 512 codewords of length 12 chosen at random, uniformly distributed among all the 2^n sequences of length n = 25. One of these codewords, say the one corresponding to i = 1, is chosen and sent over the channel. As calculated in part (e), the received sequence, with high probability, is jointly typical with the codeword that was sent. What is the probability that one or more of the other codewords (which were chosen at random, independent of the sent codeword) is jointly typical with the received sequence? [Hint: You could use the union bound, but you could also calculate this probability exactly, using the result of part (f) and the independence of the codewords.]
(h) Given that a particular codeword was sent, the probability of error (averaged over the probability distribution of the channel and over the random choice of other codewords) can be written as:
Pr(Error|X^n(1) sent) = \sum_{y^n: y^n causes error} p(y^n|X^n(1)).
There are two kinds of error: the first occurs if the received sequence y^n is not jointly typical with the transmitted codeword, and the second occurs if there is another codeword jointly typical with the received sequence. Using the result of the preceding parts, calculate this probability of error. By the symmetry of the random coding argument, this does not depend on which codeword was sent.
The calculations above show that the average probability of error for a random code with 512 codewords of length 25 over the binary symmetric channel of crossover probability 0.1 is about 0.34. This seems quite high, but the reason for this is that the value of ε that we have chosen is too large. By choosing a smaller ε and a larger n in the definitions of A(n)_ε, we can get the probability of error to be as small as we want as long as the rate of the code is less than I(X; Y) − 3ε. Also note that the decoding procedure described in the problem is not optimal. The optimal decoding procedure is maximum likelihood (i.e., to choose the codeword that is closest to the received sequence). It is possible to calculate the average probability of error for a random code for which the decoding is based on an approximation to maximum likelihood decoding, where we decode a received sequence to the unique codeword that differs from the received sequence in ≤ 4 bits, and declare an error otherwise. The only difference with the jointly typical decoding described above is that in the case when the codeword is equal to the received sequence! The average probability of error for this decoding scheme can be shown to be about 0.285.

7.16 Encoder and decoder as part of the channel. Consider a binary symmetric channel with crossover probability 0.1. A possible coding scheme for this channel with two codewords of length 3 is to encode message a₁ as 000 and a₂ as 111. With this coding scheme, we can consider the combination of encoder, channel, and decoder as forming a new BSC, with two inputs a₁ and a₂ and two outputs a₁ and a₂.
(a) Calculate the crossover probability of this channel.
(b) What is the capacity of this channel in bits per transmission of the original channel?
(c) What is the capacity of the original BSC with crossover probability 0.1?
(d) Prove a general result that for any channel, considering the encoder, channel, and decoder together as a new channel from messages to estimated messages will not increase the capacity in bits per transmission of the original channel.

7.17 Codes of length 3 for a BSC and BEC. In Problem 7.16, the probability of error was calculated for a code with two codewords of length 3. [Problem continues...]



--- Page 58 ---
232  
CHANNEL CAPACITY  
**Problem 7.17**  
(a) For a binary symmetric channel with crossover probability ε = 0.1, we need to find the best code of length 3 with four codewords. The probability of error for this code should also be calculated, ensuring all possible received sequences are mapped to possible codewords.  

(b) Calculate the probability of error if all eight possible sequences of length 3 are used as codewords.  

(c) For a binary erasure channel with erasure probability 0.1, using the two-codeword code 000 and 111, determine the probability of error. Specific decoding rules are provided for sequences with erasures.  

(d) Determine the probability of error for the codes from parts (a) and (b) when used over the binary erasure channel.  

**Problem 7.18**  
Calculate the capacity of the following channels with given probability transition matrices:  
(a) X = Y = {0, 1, 2} with a specific transition matrix.  
(b) X = Y = {0, 1, 2} with a different transition matrix.  
(c) X = Y = {0, 1, 2, 3} with a given transition matrix.  

--- Page 59 ---  
PROBLEMS  
233  
**Problem 7.19**  
Capacity of the carrier pigeon channel:  
(a) Calculate the capacity in bits/hour assuming all pigeons reach safely.  
(b) Adjust the capacity calculation to account for a fraction α of pigeons being shot down.  
(c) Further adjust the capacity if enemies send dummy pigeons with random letters.  

**Problem 7.20**  
Channel with two independent looks at Y:  
(a) Show that I(X; Y1, Y2) = 2I(X; Y1) - I(Y1, Y2).  
(b) Conclude the capacity of the channel X → (Y1, Y2) is less than twice the capacity of X → Y1.  

**Problem 7.21**  
Tall, fat people:  
(a) Argue that no more than one-third of the population is 15 feet tall.  
(b) Find an upper bound on the fraction of 300-lb 10-footers in the room.  

--- Page 60 ---  
234  
CHANNEL CAPACITY  
**Problem 7.22**  
Show that adding a row to a channel transition matrix does not decrease capacity.  

**Problem 7.23**  
Binary multiplier channel:  
(a) Find the capacity and the maximizing distribution on X for Y = XZ, where Z is Bernoulli(α).  
(b) Determine the capacity if the receiver can observe Z as well as Y.  

**Problem 7.24**  
Noise alphabets:  
(a) Find the maximum capacity over all choices of the Z alphabet and provide distinct integer values for Z and a distribution on X.  
(b) Determine the minimum capacity and provide distinct integer values for Z and a distribution on X.  

**Problem 7.25**  
Bottleneck channel: Show that the capacity C ≤ log k for the given channel model.  

--- Page 61 ---  
PROBLEMS  
235  
**Problem 7.26**  
Noisy typewriter:  
(a) Find the capacity of the channel with the given transition probabilities.  
(b) Define Z = g(Y) and compute I(X; Z) for two PMFs of X.  
(c) Find the capacity of the channel X → Z → Y.  
(d) Check if X → Z → Y forms a Markov chain for the X distribution in part (i) of (b).  

**Problem 7.27**  
Erasure channel: Determine the capacity of the cascaded channel with an erasure probability α.  

**Problem 7.28**  
Choice of channels:  
(a) Show that 2C = 2C1 + 2C2.  
(b) Compare with Problem 2.10 and interpret in terms of effective noise-free symbols.  
(c) Calculate the capacity of the given channel.  

**Problem 7.29**  
Binary multiplier channel:  
(a) Find the capacity and the maximizing distribution on X for Y = XZ.  
(b) Determine the capacity if the receiver can observe Z as well as Y.  

--- Page 62 ---  
236  
CHANNEL CAPACITY  
**Problem 7.30**  
Noise alphabets:  
(a) Find the maximum capacity and provide distinct integer values for Z.  
(b) Determine the minimum capacity and provide distinct integer values for Z.  

**Problem 7.31**  
Source and channel: Find conditions on α and p so that the probability of error can be made to go to zero as n → ∞.  

**Problem 7.32**  
Random 20 questions:  
(a) Determine the number of deterministic questions needed to identify X.  
(b) Calculate the probability that object 2 yields the same answers as object 1 for k questions.  
(c) Find the expected number of wrong objects agreeing with the answers.  
(d) Determine the expected number of wrong objects if n + √n questions are asked.  
(e) Use Markov’s inequality to show the probability of error goes to zero as n → ∞.  

--- Page 63 ---  
PROBLEMS  
237  
**Problem 7.33**  
BSC with feedback:  
(a) Find the limit of 1/n I(Xn; Yn) as n → ∞.  
(b) Show that for some p, this limit can be higher than capacity.  
(c) Determine the asymptotic communication rate achieved.  

**Problem 7.34**  
Capacity:  
(a) Two parallel BSCs.  
(b) BSC and a single symbol.  
(c) BSC and a ternary channel.  
(d) Ternary channel with a specific transition matrix.  

**Problem 7.35**  
Capacity:  
(a) Determine the capacity of the channel matrix P with an added row.  
(b) Determine the capacity of the channel matrix P with an identity matrix.  

**Problem 7.36**  
Channel with memory: Determine the capacity when {Zi} is i.i.d. with Zi = 1 or -1 with equal probability.



CHANNEL CAPACITY

Now consider the channel with memory. Before transmission begins, \( Z \) is randomly chosen and fixed for all time. Thus, \( Y_i = Z X_i \).

(b) What is the capacity if  
\[ Z = \begin{cases} 
1, & p = 0.5 \\
-1, & p = 0.5 
\end{cases} \]  
(7.169)  

---

### 7.37 Joint Typicality

Let \( (X_i, Y_i, Z_i) \) be i.i.d. according to \( p(x, y, z) \). We will say that \( (x^n, y^n, z^n) \) is jointly typical (written \( (x^n, y^n, z^n) \in A_\varepsilon^{(n)} \)) if:  

1. \( p(x^n) \in 2^{-n(H(X) \pm \varepsilon)} \).  
2. \( p(y^n) \in 2^{-n(H(Y) \pm \varepsilon)} \).  
3. \( p(z^n) \in 2^{-n(H(Z) \pm \varepsilon)} \).  
4. \( p(x^n, y^n) \in 2^{-n(H(X, Y) \pm \varepsilon)} \).  
5. \( p(x^n, z^n) \in 2^{-n(H(X, Z) \pm \varepsilon)} \).  
6. \( p(y^n, z^n) \in 2^{-n(H(Y, Z) \pm \varepsilon)} \).  
7. \( p(x^n, y^n, z^n) \in 2^{-n(H(X, Y, Z) \pm \varepsilon)} \).  

Now suppose that \( (\tilde{X}^n, \tilde{Y}^n, \tilde{Z}^n) \) is drawn according to \( p(x^n)p(y^n)p(z^n) \). Thus, \( \tilde{X}^n, \tilde{Y}^n, \tilde{Z}^n \) have the same marginals as \( p(x^n, y^n, z^n) \) but are independent. Find bounds on \( \Pr\{(\tilde{X}^n, \tilde{Y}^n, \tilde{Z}^n) \in A_\varepsilon^{(n)}\} \) in terms of the entropies \( H(X), H(Y), H(Z), H(X, Y), H(X, Z), H(Y, Z), \) and \( H(X, Y, Z) \).  

---

### HISTORICAL NOTES

The idea of mutual information and its relationship to channel capacity was developed by Shannon in his original paper [472]. In this paper, he stated the channel capacity theorem and outlined the proof using typical sequences in an argument similar to the one described here. The first rigorous proof was due to Feinstein [205], who used a painstaking "cookie-cutting" argument to find the number of codewords that can be sent with a low probability of error. A simpler proof using a random coding exponent was developed by Gallager [224]. Our proof is based on Cover [121] and on Forney’s unpublished course notes [216].  

The converse was proved by Fano [201], who used the inequality bearing his name. The strong converse was first proved by Wolfowitz [565], using techniques that are closely related to typical sequences. An iterative algorithm to calculate the channel capacity was developed independently by Arimoto [25] and Blahut [65].  

---

### CHAPTER 8  
**DIFFERENTIAL ENTROPY**  

We now introduce the concept of differential entropy, which is the entropy of a continuous random variable. Differential entropy is also related to the shortest description length and is similar in many ways to the entropy of a discrete random variable. But there are some important differences, and there is a need for some care in using the concept.  

---

### 8.1 DEFINITIONS  

**Definition**  
Let \( X \) be a random variable with cumulative distribution function \( F(x) = \Pr(X \leq x) \). If \( F(x) \) is continuous, the random variable is said to be continuous. Let \( f(x) = F'(x) \) when the derivative is defined. If  
\[ \int_{-\infty}^\infty f(x) = 1, \]  
\( f(x) \) is called the probability density function for \( X \). The set where \( f(x) > 0 \) is called the support set of \( X \).  

**Definition**  
The differential entropy \( h(X) \) of a continuous random variable \( X \) with density \( f(x) \) is defined as  
\[ h(X) = -\int_S f(x) \log f(x) \, dx, \]  
where \( S \) is the support set of the random variable.  

As in the discrete case, the differential entropy depends only on the probability density of the random variable, and therefore the differential entropy is sometimes written as \( h(f) \) rather than \( h(X) \).  

**Remark**  
As in every example involving an integral, or even a density, we should include the statement if it exists. It is easy to construct examples of random variables for which a density function does not exist or for which the above integral does not exist.  

**Example 8.1.1 (Uniform distribution)**  
Consider a random variable distributed uniformly from 0 to \( a \) so that its density is \( 1/a \) from 0 to \( a \) and 0 elsewhere. Then its differential entropy is  
\[ h(X) = -\int_0^a \frac{1}{a} \log \frac{1}{a} \, dx = \log a. \]  
Note: For \( a < 1 \), \( \log a < 0 \), and the differential entropy is negative. Hence, unlike discrete entropy, differential entropy can be negative. However,  
\[ 2h(X) = 2\log a = a \]  
is the volume of the support set, which is always non-negative, as we expect.  

**Example 8.1.2 (Normal distribution)**  
Let \( X \sim \phi(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-x^2 / 2\sigma^2}. \) Then calculating the differential entropy in nats, we obtain  
\[ h(\phi) = -\int \phi \ln \phi \, dx \]  
\[ = -\int \phi(x) \left( -\frac{x^2}{2\sigma^2} - \ln \sqrt{2\pi\sigma^2} \right) dx \]  
\[ = \frac{1}{2\sigma^2} \mathbb{E}[X^2] + \frac{1}{2} \ln 2\pi\sigma^2 \]  
\[ = \frac{1}{2} + \frac{1}{2} \ln 2\pi\sigma^2 \]  
\[ = \frac{1}{2} \ln e + \frac{1}{2} \ln 2\pi\sigma^2 \]  
\[ = \frac{1}{2} \ln 2\pi e \sigma^2 \ \text{nats}. \]  
Changing the base of the logarithm, we have  
\[ h(\phi) = \frac{1}{2} \log 2\pi e \sigma^2 \ \text{bits}. \]  

---

### 8.2 AEP FOR CONTINUOUS RANDOM VARIABLES  

One of the important roles of the entropy for discrete random variables is in the AEP, which states that for a sequence of i.i.d. random variables, \( p(X_1, X_2, \ldots, X_n) \) is close to \( 2^{-nH(X)} \) with high probability. This enables us to define the typical set and characterize the behavior of typical sequences. We can do the same for a continuous random variable.  

**Theorem 8.2.1**  
Let \( X_1, X_2, \ldots, X_n \) be a sequence of random variables drawn i.i.d. according to the density \( f(x) \). Then  
\[ -\frac{1}{n} \log f(X_1, X_2, \ldots, X_n) \to E[-\log f(X)] = h(X) \]  
in probability.  

**Proof:**  
The proof follows directly from the weak law of large numbers.  

This leads to the following definition of the typical set.  

**Definition**  
For \( \varepsilon > 0 \) and any \( n \), we define the typical set \( A_\varepsilon^{(n)} \) with respect to \( f(x) \) as follows:  
\[ A_\varepsilon^{(n)} = \left\{ (x_1, x_2, \ldots, x_n) \in \mathbb{R}^n : \left| -\frac{1}{n} \log f(x_1, x_2, \ldots, x_n) - h(X) \right| \leq \varepsilon \right\}. \]  

The properties of the typical set for continuous random variables parallel those for discrete random variables. The analog of the cardinality of the typical set for the discrete case is the volume of the typical set for continuous random variables.  

**Definition**  
The volume \( \text{Vol}(A) \) of a set \( A \subset \mathbb{R}^n \) is defined as  
\[ \text{Vol}(A) = \int_A dx_1 dx_2 \cdots dx_n. \]  

**Theorem 8.2.2**  
The typical set \( A_\varepsilon^{(n)} \) has the following properties:  
1. \( \Pr\left( A_\varepsilon^{(n)} \right) > 1 - \varepsilon \) for \( n \) sufficiently large.  
2. \( \text{Vol}\left( A_\varepsilon^{(n)} \right) \leq 2^n (h(X) + \varepsilon) \) for all \( n \).  
3. \( \text{Vol}\left( A_\varepsilon^{(n)} \right) \geq (1 - \varepsilon) 2^n (h(X) - \varepsilon) \) for \( n \) sufficiently large.  

**Proof:**  
By Theorem 8.2.1, \( -\frac{1}{n} \log f(X^n) \to h(X) \) in probability, establishing property 1. Also,  
\[ 1 = \int_{\mathbb{R}^n} f(x_1, x_2, \ldots, x_n) dx_1 dx_2 \cdots dx_n \]  
\[ \geq \int_{A_\varepsilon^{(n)}} f(x_1, x_2, \ldots, x_n) dx_1 dx_2 \cdots dx_n \]  
\[ \geq \int_{A_\varepsilon^{(n)}} 2^{-n(h(X) + \varepsilon)} dx_1 dx_2 \cdots dx_n \]  
\[ = 2^{-n(h(X) + \varepsilon)} \text{Vol}\left( A_\varepsilon^{(n)} \right). \]  
Hence, we have property 2. We argue further that the volume of the typical set is at least this large. If \( n \) is sufficiently large so that property 1 is satisfied, then  
\[ 1 - \varepsilon \leq \int_{A_\varepsilon^{(n)}} f(x_1, x_2, \ldots, x_n) dx_1 dx_2 \cdots dx_n \]  
\[ \leq \int_{A_\varepsilon^{(n)}} 2^{-n(h(X) - \varepsilon)} dx_1 dx_2 \cdots dx_n \]  
\[ = 2^{-n(h(X) - \varepsilon)} \text{Vol}\left( A_\varepsilon^{(n)} \right), \]  
establishing property 3. Thus, for \( n \) sufficiently large, we have  
\[ (1 - \varepsilon) 2^n (h(X) - \varepsilon) \leq \text{Vol}\left( A_\varepsilon^{(n)} \right) \leq 2^n (h(X) + \varepsilon). \]  

**Theorem 8.2.3**  
The set \( A_\varepsilon^{(n)} \) is the smallest volume set with probability \( \geq 1 - \varepsilon \), to first order in the exponent.  

**Proof:**  
Same as in the discrete case.  

This theorem indicates that the volume of the smallest set that contains most of the probability is approximately \( 2^n h \). This is an \( n \)-dimensional volume, so the corresponding side length is \( (2^n h)^{1/n} = 2h \). This provides an interpretation of the differential entropy: It is the logarithm of the equivalent side length of the smallest set that contains most of the probability. Hence, low entropy implies that the random variable is confined to a small effective volume, and high entropy indicates that the random variable is widely dispersed.  

**Note:** Just as the entropy is related to the volume of the typical set, there is a quantity called Fisher information, which is related to the surface area of the typical set. We discuss Fisher information in more detail in Sections 11.10 and 17.8.  

---

### 8.3 RELATION OF DIFFERENTIAL ENTROPY TO DISCRETE ENTROPY  

Consider a random variable \( X \) with density \( f(x) \) illustrated in Figure 8.1. Suppose that we divide the range of \( X \) into bins of length \( \Delta \). Let us assume that the density is continuous within the bins. Then, by the mean value theorem, there exists a value \( x_i \) within each bin such that  
\[ f(x_i) \Delta = \int_{i\Delta}^{(i+1)\Delta} f(x) \, dx. \]  

Consider the quantized random variable \( \hat{X}_\Delta \), which is defined by  
\[ \hat{X}_\Delta = x_i \ \text{if} \ i\Delta \leq X < (i+1)\Delta. \]  

Then the probability that \( \hat{X}_\Delta = x_i \) is  
\[ p_i = \int_{i\Delta}^{(i+1)\Delta} f(x) \, dx = f(x_i) \Delta. \]  

The entropy of the quantized version is  
\[ H(\hat{X}_\Delta) = -\sum_{i=-\infty}^\infty p_i \log p_i \]  
\[ = -\sum_{i=-\infty}^\infty f(x_i) \Delta \log(f(x_i) \Delta) \]  
\[ = -\sum_{i=-\infty}^\infty f(x_i) \Delta \log f(x_i) - \sum_{i=-\infty}^\infty f(x_i) \Delta \log \Delta \]  
\[ = -\Delta \sum_{i=-\infty}^\infty f(x_i) \log f(x_i) - \log \Delta. \]  

If \( f(x) \log f(x) \) is Riemann integrable (a condition to ensure that the limit is well-defined [556]), the first term in the above expression approaches the integral of \( -f(x) \log f(x) \) as \( \Delta \to 0 \) by definition of Riemann integrability. This proves the following.  

**Theorem 8.3.1**  
If the density \( f(x) \) of the random variable \( X \) is Riemann integrable, then  
\[ H(\hat{X}_\Delta) + \log \Delta \to h(f) = h(X) \]  
as \( \Delta \to 0 \).  

Thus, the entropy of an \( n \)-bit quantization of a continuous random variable \( X \) is approximately \( h(X) + n \).  

**Example 8.3.1**  
1. If \( X \) has a uniform distribution on \( [0, 1] \) and we let \( \Delta = 2^{-n} \), then \( h = 0 \), \( H(\hat{X}_\Delta) = n \), and \( n \) bits suffice to describe \( X \) to \( n \)-bit accuracy.  
2. If \( X \) is uniformly distributed on \( [0, 1/8] \), the first 3 bits to the right of the decimal point must be 0. To describe \( X \) to \( n \)-bit accuracy requires only \( n - 3 \) bits, which agrees with \( h(X) = -3 \).  
3. If \( X \sim \mathcal{N}(0, \sigma^2) \) with \( \sigma^2 = 100 \), describing \( X \) to \( n \)-bit accuracy would require on average \( n + \frac{1}{2} \log(2\pi e \sigma^2) = n + 5.37 \) bits.



JOINT AND CONDITIONAL DIFFERENTIAL ENTROPY

In general, \( h(X) + n \) is the number of bits on average required to describe \( X \) to \( n \)-bit accuracy. The differential entropy of a discrete random variable can be considered to be \( -\infty \). Note that \( 2^{-\infty} = 0 \), agreeing with the idea that the volume of the support set of a discrete random variable is zero.

As in the discrete case, we can extend the definition of differential entropy of a single random variable to several random variables.

**Definition**  
The differential entropy of a set \( X_1, X_2, \ldots, X_n \) of random variables with density \( f(x_1, x_2, \ldots, x_n) \) is defined as  
\[
h(X_1, X_2, \ldots, X_n) = -\int f(x_n) \log f(x_n) \, dx_n. \quad (8.31)
\]

**Definition**  
If \( X, Y \) have a joint density function \( f(x, y) \), we can define the conditional differential entropy \( h(X|Y) \) as  
\[
h(X|Y) = -\int f(x, y) \log f(x|y) \, dx \, dy. \quad (8.32)
\]  
Since in general \( f(x|y) = \frac{f(x, y)}{f(y)} \), we can also write  
\[
h(X|Y) = h(X, Y) - h(Y). \quad (8.33)
\]  
But we must be careful if any of the differential entropies are infinite.

The next entropy evaluation is used frequently in the text.

**Theorem 8.4.1**  
(Entropy of a multivariate normal distribution)  
Let \( X_1, X_2, \ldots, X_n \) have a multivariate normal distribution with mean \( \mu \) and covariance matrix \( K \). Then  
\[
h(X_1, X_2, \ldots, X_n) = h(\mathcal{N}_n(\mu, K)) = \frac{1}{2} \log(2\pi e)^n |K| \text{ bits}, \quad (8.34)
\]  
where \( |K| \) denotes the determinant of \( K \).

**Proof:**  
The probability density function of \( X_1, X_2, \ldots, X_n \) is  
\[
f(x) = \frac{1}{\sqrt{(2\pi)^n |K|}} \exp\left(-\frac{1}{2}(x - \mu)^T K^{-1}(x - \mu)\right). \quad (8.35)
\]  
Then  
\[
h(f) = -\int f(x) \left(-\frac{1}{2}(x - \mu)^T K^{-1}(x - \mu) - \ln \sqrt{(2\pi)^n |K|}\right) dx \quad (8.36)
\]  
\[
= \frac{1}{2} \mathbb{E}\left[\sum_{i,j} (X_i - \mu_i) K^{-1}_{ij} (X_j - \mu_j)\right] + \frac{1}{2} \ln(2\pi)^n |K| \quad (8.37)
\]  
\[
= \frac{1}{2} \mathbb{E}\left[\sum_{i,j} (X_i - \mu_i)(X_j - \mu_j) K^{-1}_{ij}\right] + \frac{1}{2} \ln(2\pi)^n |K| \quad (8.38)
\]  
\[
= \frac{1}{2} \sum_{i,j} \mathbb{E}[(X_j - \mu_j)(X_i - \mu_i)] K^{-1}_{ij} + \frac{1}{2} \ln(2\pi)^n |K| \quad (8.39)
\]  
\[
= \frac{1}{2} \sum_{j} \sum_{i} K_{ji} K^{-1}_{ij} + \frac{1}{2} \ln(2\pi)^n |K| \quad (8.40)
\]  
\[
= \frac{1}{2} \sum_{j} (KK^{-1})_{jj} + \frac{1}{2} \ln(2\pi)^n |K| \quad (8.41)
\]  
\[
= \frac{1}{2} \sum_{j} I_{jj} + \frac{1}{2} \ln(2\pi)^n |K| \quad (8.42)
\]  
\[
= \frac{n}{2} + \frac{1}{2} \ln(2\pi e)^n |K| \quad (8.43)
\]  
\[
= \frac{1}{2} \ln(2\pi e)^n |K| \text{ nats} \quad (8.44)
\]  
\[
= \frac{1}{2} \log(2\pi e)^n |K| \text{ bits}. \quad (8.45)
\]  
\(\Box\)

RELATIVE ENTROPY AND MUTUAL INFORMATION

We now extend the definition of two familiar quantities, \( D(f || g) \) and \( I(X; Y) \), to probability densities.

**Definition**  
The relative entropy (or Kullback–Leibler distance) \( D(f || g) \) between two densities \( f \) and \( g \) is defined by  
\[
D(f || g) = \int f \log \frac{f}{g}. \quad (8.46)
\]  
Note that \( D(f || g) \) is finite only if the support set of \( f \) is contained in the support set of \( g \). (Motivated by continuity, we set \( 0 \log 0 = 0 \).)

**Definition**  
The mutual information \( I(X; Y) \) between two random variables with joint density \( f(x, y) \) is defined as  
\[
I(X; Y) = \int f(x, y) \log \frac{f(x, y)}{f(x)f(y)} \, dx \, dy. \quad (8.47)
\]  
From the definition, it is clear that  
\[
I(X; Y) = h(X) - h(X|Y) = h(Y) - h(Y|X) = h(X) + h(Y) - h(X, Y) \quad (8.48)
\]  
and  
\[
I(X; Y) = D(f(x, y) || f(x)f(y)). \quad (8.49)
\]  
The properties of \( D(f || g) \) and \( I(X; Y) \) are the same as in the discrete case. In particular, the mutual information between two random variables is the limit of the mutual information between their quantized versions, since  
\[
I(X^b; Y^b) = H(X^b) - H(X^b | Y^b) \quad (8.50)
\]  
\[
\approx h(X) - \log b - (h(X|Y) - \log b) \quad (8.51)
\]  
\[
= I(X; Y). \quad (8.52)
\]  
More generally, we can define mutual information in terms of finite partitions of the range of the random variable. Let \( \mathcal{X} \) be the range of a random variable \( X \). A partition \( P \) of \( \mathcal{X} \) is a finite collection of disjoint sets \( P_i \) such that \( \cup_i P_i = \mathcal{X} \). The quantization of \( X \) by \( P \) (denoted \( [X]_P \)) is the discrete random variable defined by  
\[
\Pr([X]_P = i) = \Pr(X \in P_i) = \int_{P_i} dF(x). \quad (8.53)
\]  
For two random variables \( X \) and \( Y \) with partitions \( P \) and \( Q \), we can calculate the mutual information between the quantized versions of \( X \) and \( Y \) using (2.28). Mutual information can now be defined for arbitrary pairs of random variables as follows:

**Definition**  
The mutual information between two random variables \( X \) and \( Y \) is given by  
\[
I(X; Y) = \sup_{P,Q} I([X]_P; [Y]_Q), \quad (8.54)
\]  
where the supremum is over all finite partitions \( P \) and \( Q \). This is the master definition of mutual information that always applies, even to joint distributions with atoms, densities, and singular parts. Moreover, by continuing to refine the partitions \( P \) and \( Q \), one finds a monotonically increasing sequence \( I([X]_P; [Y]_Q) \uparrow I \). By arguments similar to (8.52), we can show that this definition of mutual information is equivalent to (8.47) for random variables that have a density. For discrete random variables, this definition is equivalent to the definition of mutual information in (2.28).

**Example 8.5.1**  
(Mutual information between correlated Gaussian random variables with correlation \( \rho \))  
Let \( (X, Y) \sim \mathcal{N}(0, K) \), where  
\[
K = \begin{pmatrix} \sigma^2 & \rho \sigma^2 \\ \rho \sigma^2 & \sigma^2 \end{pmatrix}. \quad (8.55)
\]  
Then \( h(X) = h(Y) = \frac{1}{2} \log(2\pi e) \sigma^2 \) and \( h(X, Y) = \frac{1}{2} \log(2\pi e)^2 |K| = \frac{1}{2} \log(2\pi e)^2 \sigma^4 (1 - \rho^2) \), and therefore  
\[
I(X; Y) = h(X) + h(Y) - h(X, Y) = -\frac{1}{2} \log(1 - \rho^2). \quad (8.56)
\]  
If \( \rho = 0 \), \( X \) and \( Y \) are independent and the mutual information is 0. If \( \rho = \pm 1 \), \( X \) and \( Y \) are perfectly correlated and the mutual information is infinite.

PROPERTIES OF DIFFERENTIAL ENTROPY, RELATIVE ENTROPY, AND MUTUAL INFORMATION

**Theorem 8.6.1**  
\( D(f || g) \geq 0 \) \quad (8.57)  
with equality if and only if \( f = g \) almost everywhere (a.e.).

**Proof:**  
Let \( S \) be the support set of \( f \). Then  
\[
-D(f || g) = \int_S f \log \frac{g}{f} \leq \log \int_S g \quad (8.58)
\]  
\[
= \log 1 = 0. \quad (8.61)
\]  
We have equality if and only if we have equality in Jensen’s inequality, which occurs if and only if \( f = g \) a.e.  
\(\Box\)

**Corollary**  
\( I(X; Y) \geq 0 \) with equality if and only if \( X \) and \( Y \) are independent.

**Corollary**  
\( h(X|Y) \leq h(X) \) with equality if and only if \( X \) and \( Y \) are independent.

**Theorem 8.6.2**  
(Chain rule for differential entropy)  
\[
h(X_1, X_2, \ldots, X_n) = \sum_{i=1}^n h(X_i | X_1, X_2, \ldots, X_{i-1}). \quad (8.62)
\]  
**Proof:**  
Follows directly from the definitions.  
\(\Box\)

**Corollary**  
\[
h(X_1, X_2, \ldots, X_n) \leq \sum_{i=1}^n h(X_i), \quad (8.63)
\]  
with equality if and only if \( X_1, X_2, \ldots, X_n \) are independent.  
**Proof:**  
Follows directly from Theorem 8.6.2 and the corollary to Theorem 8.6.1.  
\(\Box\)

**Application**  
(Hadamard’s inequality)  
If we let \( X \sim \mathcal{N}(0, K) \) be a multivariate normal random variable, calculating the entropy in the above inequality gives us  
\[
|K| \leq \prod_{i=1}^n K_{ii}, \quad (8.64)
\]  
which is Hadamard’s inequality. A number of determinant inequalities can be derived in this fashion from information-theoretic inequalities (Chapter 17).

**Theorem 8.6.3**  
\[
h(X + c) = h(X). \quad (8.65)
\]  
Translation does not change the differential entropy.  
**Proof:**  
Follows directly from the definition of differential entropy.  
\(\Box\)

**Theorem 8.6.4**  
\[
h(aX) = h(X) + \log |a|. \quad (8.66)
\]  
**Proof:**  
Let \( Y = aX \). Then \( f_Y(y) = \frac{1}{|a|} f_X\left(\frac{y}{a}\right) \), and  
\[
h(aX) = -\int f_Y(y) \log f_Y(y) \, dy \quad (8.67)
\]  
\[
= -\int \frac{1}{|a|} f_X\left(\frac{y}{a}\right) \log \left(\frac{1}{|a|} f_X\left(\frac{y}{a}\right)\right) dy \quad (8.68)
\]  
\[
= -\int f_X(x) \log f_X(x) \, dx + \log |a| \quad (8.69)
\]  
\[
= h(X) + \log |a|. \quad (8.70)
\]  
after a change of variables in the integral.  
\(\Box\)

Similarly, we can prove the following corollary for vector-valued random variables.

**Corollary**  
\[
h(AX) = h(X) + \log |\det(A)|. \quad (8.71)
\]

We now show that the multivariate normal distribution maximizes the entropy over all distributions with the same covariance.

**Theorem 8.6.5**  
Let the random vector \( X \in \mathbb{R}^n \) have zero mean and covariance \( K = \mathbb{E}XX^T \) (i.e., \( K_{ij} = \mathbb{E}X_iX_j \), \( 1 \leq i, j \leq n \)). Then  
\[
h(X) \leq \frac{1}{2} \log(2\pi e)^n |K|, \quad (8.72)
\]  
with equality if and only if \( X \sim \mathcal{N}(0, K) \).  
**Proof:**  
Let \( g(x) \) be any density satisfying  
\[
\int g(x) x_i x_j \, dx = K_{ij} \quad \forall i, j. \quad (8.73)
\]  
Let \( \phi_K(x) \) be the density of a \( \mathcal{N}(0, K) \) vector as given in (8.35), where we set \( \mu = 0 \). Note that \( \log \phi_K(x) \) is a quadratic form and  
\[
\int x_i x_j \phi_K(x) \, dx = K_{ij}. \quad (8.74)
\]  
Then  
\[
0 \leq D(g || \phi_K) \quad (8.75)
\]  
\[
= \int g \log \frac{g}{\phi_K} \quad (8.76)
\]  
\[
= -h(g) - \int g \log \phi_K \quad (8.77)
\]  
\[
= -h(g) + h(\phi_K), \quad (8.78)
\]  
where the substitution  
\[
\int g \log \phi_K = \int \phi_K \log \phi_K \quad (8.79)
\]  
follows from the fact that \( g \) and \( \phi_K \) yield the same moments of the quadratic form \( \log \phi_K(x) \).  
\(\Box\)

In particular, the Gaussian distribution maximizes the entropy over all distributions with the same variance. This leads to the estimation counterpart to Fano’s inequality. Let \( X \) be a random variable with differential entropy \( h(X) \). Let \( \hat{X} \) be an estimate of \( X \), and let \( \mathbb{E}(X - \hat{X})^2 \) be the expected prediction error. Let \( h(X) \) be in nats.

**Theorem 8.6.6**  
(Estimation error and differential entropy)  
For any random variable \( X \) and estimator \( \hat{X} \),  
\[
\mathbb{E}(X - \hat{X})^2 \geq \frac{1}{2\pi e} e^{2h(X)}, \quad (8.80)
\]  
with equality if and only if \( X \) is Gaussian and \( \hat{X} \) is the mean of \( X \).  
**Proof:**  
Let \( \hat{X} \) be any estimator of \( X \); then  
\[
\mathbb{E}(X - \hat{X})^2 \geq \min_{\hat{X}} \mathbb{E}(X - \hat{X})^2 \quad (8.81)
\]  
\[
= \mathbb{E}(X - \mathbb{E}(X))^2 \quad (8.82)
\]  
\[
= \text{var}(X) \quad (8.83)
\]  
\[
\geq \frac{1}{2\pi e} e^{2h(X)}, \quad (8.84)
\]  
where (8.82) follows from the fact that the mean of \( X \) is the best estimator for \( X \) and the last inequality follows from the fact that the Gaussian distribution has the maximum entropy for a given variance. We have equality only in (8.82) only if \( \hat{X} \) is the best estimator (i.e., \( \hat{X} \) is the mean of \( X \)) and equality in (8.84) only if \( X \) is Gaussian.  
\(\Box\)

**Corollary**  
Given side information \( Y \) and estimator \( \hat{X}(Y) \), it follows that  
\[
\mathbb{E}(X - \hat{X}(Y))^2 \geq \frac{1}{2\pi e} e^{2h(X|Y)}. \quad (8.85)
\]

SUMMARY  
\[
h(X) = h(f) = -\int_S f(x) \log f(x) \, dx \quad (8.81)
\]  
\[
f(X^n) = 2^{-n} h(X) \quad (8.82)
\]  
\[
\text{Vol}(A(n)_\epsilon) = 2^{n h(X)}. \quad (8.83)
\]  
\[
H([X]^{2^{-n}}) \approx h(X) + n. \quad (8.84)
\]  
\[
h(\mathcal{N}(0, \sigma^2)) = \frac{1}{2} \log 2\pi e \sigma^2. \quad (8.85)
\]  
\[
h(\mathcal{N}_n(\mu, K)) = \frac{1}{2} \log(2\pi e)^n |K|. \quad (8.86)
\]  
\[
D(f || g) = \int f \log \frac{f}{g} \geq 0. \quad (8.87)
\]  
\[
h(X_1, X_2, \ldots, X_n) = \sum_{i=1}^n h(X_i | X_1, X_2, \ldots, X_{i-1}). \quad (8.88)
\]  
\[
h(X|Y) \leq h(X). \quad (8.89)
\]  
\[
h(aX) = h(X) + \log |a|. \quad (8.90)
\]  
\[
I(X; Y) = \int f(x, y) \log \frac{f(x, y)}{f(x)f(y)} \geq 0. \quad (8.91)
\]  
\[
\max_{\mathbb{E}XX^T = K} h(X) = \frac{1}{2} \log(2\pi e)^n |K|. \quad (8.92)
\]  
\( 2^n H(X) \) is the effective alphabet size for a discrete random variable.  
\( 2^n h(X) \) is the effective support set size for a continuous random variable.  
\( 2^C \) is the effective alphabet size of a channel of capacity \( C \).

PROBLEMS

**8.1**  
Differential entropy. Evaluate the differential entropy \( h(X) = -\int f \ln f \) for the following:  
(a) The exponential density, \( f(x) = \lambda e^{-\lambda x} \), \( x \geq 0 \).



--- Page 83 ---
PROBLEMS
257
(b) The Laplace density, f(x) = (1/(2λ))e^(-λ|x|).
(c) The sum of X1 and X2, where X1 and X2 are independent normal random variables with means μi and variances σi², i = 1, 2.

8.2 Concavity of determinants. Let K1 and K2 be two symmetric non-negative definite n × n matrices. Prove the result of Ky Fan [199]:
|λK1 + (1 - λ)K2| ≥ |K1|λ|K2|^(1 - λ)
for 0 ≤ λ ≤ 1, where |K| denotes the determinant of K. [Hint: Let Z = Xθ, where X1 ∼ N(0, K1), X2 ∼ N(0, K2), and θ = Bernoulli(λ). Then use h(Z | θ) ≤ h(Z).]

8.3 Uniformly distributed noise. Let the input random variable X to a channel be uniformly distributed over the interval -1/2 ≤ x ≤ +1/2. Let the output of the channel be Y = X + Z, where the noise random variable is uniformly distributed over the interval -a/2 ≤ z ≤ +a/2.
(a) Find I(X; Y) as a function of a.
(b) For a = 1, find the capacity of the channel when the input X is peak-limited; that is, the range of X is limited to -1/2 ≤ x ≤ +1/2. What probability distribution on X maximizes the mutual information I(X; Y)?
(c) (Optional) Find the capacity of the channel for all values of a, again assuming that the range of X is limited to -1/2 ≤ x ≤ +1/2.

8.4 Quantized random variables. Roughly how many bits are required on average to describe to three-digit accuracy the decay time (in years) of a radium atom if the half-life of radium is 80 years? Note that half-life is the median of the distribution.

8.5 Scaling. Let h(X) = -∫ f(x) log f(x) dx. Show h(AX) = log |det(A)| + h(X).

8.6 Variational inequality. Verify for positive random variables X that
log E_P(X) = sup_Q [E_Q(log X) - D(Q||P)],
where E_P(X) = ∑ x P(x) and D(Q||P) = ∑ Q(x) log Q(x)/P(x), and the supremum is over all Q(x) ≥ 0, ∑ Q(x) = 1. It is enough to extremize J(Q) = E_Q(log X) - D(Q||P) + λ(∑ Q(x) - 1).

--- Page 84 ---
DIFFERENTIAL ENTROPY
8.7 Differential entropy bound on discrete entropy. Let X be a discrete random variable on the set X = {a1, a2, ...} with Pr(X = ai) = pi. Show that
H(p1, p2, ...) ≤ (1/2) log(2πe) [∑ pi² - (∑ ipi)² + 1/12].
Moreover, for every permutation σ, H(p1, p2, ...) ≤ (1/2) log(2πe) [∑ pσ(i)i² - (∑ ipσ(i))² + 1/12]. [Hint: Construct a random variable X' such that Pr(X' = i) = pi. Let U be a uniform (0,1] random variable and let Y = X' + U, where X' and U are independent. Use the maximum entropy bound on Y to obtain the bounds in the problem. This bound is due to Massey (unpublished) and Willems (unpublished).]

8.8 Channel with uniformly distributed noise. Consider an additive channel whose input alphabet X = {0, ±1, ±2} and whose output Y = X + Z, where Z is distributed uniformly over the interval [-1, 1]. Thus, the input of the channel is a discrete random variable, whereas the output is continuous. Calculate the capacity C = max p(x) I(X; Y) of this channel.

8.9 Gaussian mutual information. Suppose that (X, Y, Z) are jointly Gaussian and that X → Y → Z forms a Markov chain. Let X and Y have correlation coefficient ρ1 and let Y and Z have correlation coefficient ρ2. Find I(X; Z).

8.10 Shape of the typical set. Let Xi be i.i.d. ∼ f(x), where f(x) = c e^{-x⁴}. Let h = -∫ f(x) log f(x) dx. Describe the shape (or form) of the typical set Aⁿ_ε = {xn ∈ ℝⁿ: f(xn) ∈ 2^{-n(h±ε)}}.

8.11 Nonergodic Gaussian process. Consider a constant signal V in the presence of i.i.d. observational noise {Zi}. Thus, Xi = V + Zi, where V ∼ N(0, S) and Zi are i.i.d. ∼ N(0, N). Assume that V and {Zi} are independent.
(a) Is {Xi} stationary?
(b) Find lim_{n→∞} (1/n) ∑_{i=1}^n Xi. Is the limit random?
(c) What is the entropy rate h of {Xi}?
(d) Find the least-mean-squared error predictor X̂_{n+1}(Xn), and find σ²_∞ = lim_{n→∞} E( X̂_{n} - Xn )².
(e) Does {Xi} have an AEP? That is, does -1/n log f(Xn) → h?

--- Page 85 ---
HISTORICAL NOTES
Differential entropy and discrete entropy were introduced in Shannon’s original paper [472]. The general rigorous definition of relative entropy and mutual information for arbitrary random variables was developed by Kolmogorov [319] and Pinsker [425], who defined mutual information as sup_{P,Q} I([X]_P; [Y]_Q), where the supremum is over all finite partitions P and Q.

--- Page 86 ---

--- Page 87 ---
CHAPTER 9
GAUSSIAN CHANNEL
The most important continuous alphabet channel is the Gaussian channel depicted in Figure 9.1. This is a time-discrete channel with output Yi at time i, where Yi is the sum of the input Xi and the noise Zi. The noise Zi is drawn i.i.d. from a Gaussian distribution with variance N. Thus, Yi = Xi + Zi, Zi ∼ N(0, N). The noise Zi is assumed to be independent of the signal Xi. This channel is a model for some common communication channels, such as wired and wireless telephone channels and satellite links. Without further conditions, the capacity of this channel may be infinite. If the noise variance is zero, the receiver receives the transmitted symbol perfectly. Since X can take on any real value, the channel can transmit an arbitrary real number with no error. If the noise variance is nonzero and there is no constraint on the input, we can choose an infinite subset of inputs arbitrarily far apart, so that they are distinguishable at the output with arbitrarily small probability of error. Such a scheme has an infinite capacity as well. Thus, if the noise variance is zero or the input is unconstrained, the capacity of the channel is infinite. The most common limitation on the input is an energy or power constraint. We assume an average power constraint. For any codeword (x1, x2, ..., xn) transmitted over the channel, we require that (1/n) ∑_{i=1}^n x_i² ≤ P. This communication channel models many practical channels, including radio and satellite links. The additive noise in such channels may be due to a variety of causes. However, by the central limit theorem, the cumulative effect of a large number of small random effects will be approximately normal, so the Gaussian assumption is valid in a large number of situations. We first analyze a simple suboptimal way to use this channel. Assume that we want to send 1 bit over the channel in one use of the channel. Given the power constraint, the best that we can do is to send one of two levels, +√P or -√P. The receiver looks at the corresponding Y received and tries to decide which of the two levels was sent. Assuming that both levels are equally likely (this would be the case if we wish to send exactly 1 bit of information), the optimum decoding rule is to decide that +√P was sent if Y > 0 and decide -√P was sent if Y < 0. The probability of error with such a decoding scheme is Pe = (1/2) Pr(Y < 0|X = +√P) + (1/2) Pr(Y > 0|X = -√P) = Pr(Z > √P) = 1 - Φ(√(P/N)), where Φ(x) is the cumulative normal function Φ(x) = ∫_{-∞}^x (1/√(2π)) e^{-t²/2} dt. Using such a scheme, we have converted the Gaussian channel into a discrete binary symmetric channel with crossover probability Pe. Similarly, by using a four-level input signal, we can convert the Gaussian channel into a discrete four-input channel. In some practical modulation schemes, similar ideas are used to convert the continuous channel into a discrete channel. The main advantage of a discrete channel is ease of processing of the output signal for error correction, but some information is lost in the quantization. 9.1 Gaussian channel: definitions We now define the (information) capacity of the channel as the maximum of the mutual information between the input and output over all distributions on the input that satisfy the power constraint. Definition The information capacity of the Gaussian channel with power constraint P is C = max_{f(x): E[X²] ≤ P} I(X; Y). We can calculate the information capacity as follows: Expanding I(X; Y), we have I(X; Y) = h(Y) - h(Y|X) = h(Y) - h(X + Z|X) = h(Y) - h(Z|X) = h(Y) - h(Z), since Z is independent of X. Now, h(Z) = (1/2) log(2πeN). Also, E[Y²] = E[(X + Z)²] = E[X²] + 2E[X]E[Z] + E[Z²] = P + N, since X and Z are independent and E[Z] = 0. Given E[Y²] = P + N, the entropy of Y is bounded by (1/2) log(2πe(P + N)) by Theorem 8.6.5 (the normal maximizes the entropy for a given variance). Applying this result to bound the mutual information, we obtain I(X; Y) = h(Y) - h(Z) ≤ (1/2) log(2πe(P + N)) - (1/2) log(2πeN) = (1/2) log(1 + P/N). Hence, the information capacity of the Gaussian channel is C = max_{E[X²] ≤ P} I(X; Y) = (1/2) log(1 + P/N), and the maximum is attained when X ∼ N(0, P). We will now show that this capacity is also the supremum of the rates achievable for the channel. The arguments are similar to the arguments for a discrete channel. We will begin with the corresponding definitions. Definition An (M, n) code for the Gaussian channel with power constraint P consists of the following: 1. An index set {1, 2, ..., M}. 2. An encoding function x: {1, 2, ..., M} → X^n, yielding codewords x^n(1), x^n(2), ..., x^n(M), satisfying the power constraint P; that is, for every codeword (1/n) ∑_{i=1}^n x_i²(w) ≤ nP, w = 1, 2, ..., M. 3. A decoding function g: Y^n → {1, 2, ..., M}. The rate and probability of error of the code are defined as in Chapter 7 for the discrete case. The arithmetic average of the probability of error is defined by P_e^{(n)} = (1/M) ∑_{w=1}^M λ_w, where λ_w is the error probability for codeword w. Definition A rate R is said to be achievable for a Gaussian channel with a power constraint P if there exists a sequence of (2^{nR}, n) codes with codewords satisfying the power constraint such that the maximal probability of error λ(n) tends to zero. The capacity of the channel is the supremum of the achievable rates. Theorem 9.1.1 The capacity of a Gaussian channel with power constraint P and noise variance N is C = (1/2) log(1 + P/N) bits per transmission. Remark We first present a plausibility argument as to why we may be able to construct (2^{nC}, n) codes with a low probability of error. Consider any codeword of length n. The received vector is normally distributed with mean equal to the true codeword and variance equal to the noise variance. With high probability, the received vector is contained in a sphere of radius √(n(N + ε)) around the true codeword. If we assign everything within this sphere to the given codeword, when this codeword is sent, there will be an error only if the received vector falls outside the sphere, which has low probability. Similarly, we can choose other codewords and their corresponding decoding spheres. How many such codewords can we choose? The volume of an n-dimensional sphere is of the form C_n r^n, where r is the radius of the sphere. In this case, each decoding sphere has radius √(nN). These spheres are scattered throughout the space of received vectors. The received vectors have energy no greater than n(P + N), so they lie in a sphere of radius √(n(P + N)). The maximum number of nonintersecting decoding spheres in this volume is no more than [C_n (n(P + N))^{n/2} / C_n (nN)^{n/2}] = 2^{nC}, where C = (1/2) log(1 + P/N). This idea is illustrated in Figure 9.2. Sphere packing for the Gaussian channel.



GAUSSIAN CHANNEL

This sphere-packing argument indicates that we cannot hope to send at rates greater than \( C \) with low probability of error. However, we can actually do almost as well as this, as is proved next.

**Proof:**

**(Achievability).** We will use the same ideas as in the proof of the channel coding theorem in the case of discrete channels: namely, random codes and joint typicality decoding. However, we must make some modifications to take into account the power constraint and the fact that the variables are continuous and not discrete.

1. **Generation of the codebook.** We wish to generate a codebook in which all the codewords satisfy the power constraint. To ensure this, we generate the codewords with each element i.i.d. according to a normal distribution with variance \( P - \varepsilon \). Since for large \( n \), \( \frac{1}{n} \sum_{i=1}^{n} X_i^2 \to P - \varepsilon \), the probability that a codeword does not satisfy the power constraint will be small. Let \( X_i(w) \), \( i = 1, 2, \ldots, n \), \( w = 1, 2, \ldots, 2nR \) be i.i.d. \( \sim \mathcal{N}(0, P - \varepsilon) \), forming codewords \( X^n(1), X^n(2), \ldots, X^n(2nR) \in \mathbb{R}^n \).

2. **Encoding.** After the generation of the codebook, the codebook is revealed to both the sender and the receiver. To send the message index \( w \), the transmitter sends the \( w \)-th codeword \( X^n(w) \) in the codebook.

3. **Decoding.** The receiver looks down the list of codewords \( \{X^n(w)\} \) and searches for one that is jointly typical with the received vector. If there is one and only one such codeword \( X^n(w) \), the receiver declares \( \hat{W} = w \) to be the transmitted codeword. Otherwise, the receiver declares an error. The receiver also declares an error if the chosen codeword does not satisfy the power constraint.

4. **Probability of error.** Without loss of generality, assume that codeword 1 was sent. Thus, \( Y^n = X^n(1) + Z^n \). Define the following events:

   \[
   E_0 = \left\{ \frac{1}{n} \sum_{j=1}^{n} X_j^2(1) > P \right\}
   \]

   and

   \[
   E_i = \left\{ (X^n(i), Y^n) \text{ is in } A^{(n)}_{\varepsilon} \right\}
   \]

   Then an error occurs if \( E_0 \) occurs (the power constraint is violated) or \( E_c^1 \) occurs (the transmitted codeword and the received sequence are not jointly typical) or \( E_2 \cup E_3 \cup \cdots \cup E_{2nR} \) occurs (some wrong codeword is jointly typical with the received sequence). Let \( E \) denote the event \( \hat{W} \neq W \) and let \( P \) denote the conditional probability given that \( W = 1 \). Hence,

   \[
   \Pr(E|W = 1) = P(E) = P\left( E_0 \cup E_c^1 \cup E_2 \cup E_3 \cup \cdots \cup E_{2nR} \right)
   \]

   \[
   \leq P(E_0) + P(E_c^1) + \sum_{i=2}^{2nR} P(E_i)
   \]

   by the union of events bound for probabilities. By the law of large numbers, \( P(E_0) \to 0 \) as \( n \to \infty \). Now, by the joint AEP (which can be proved using the same argument as that used in the discrete case), \( P(E_c^1) \to 0 \), and hence

   \[
   P(E_c^1) \leq \varepsilon
   \]

   for \( n \) sufficiently large.

   Since by the code generation process, \( X^n(1) \) and \( X^n(i) \) are independent, so are \( Y^n \) and \( X^n(i) \). Hence, the probability that \( X^n(i) \) and \( Y^n \) will be jointly typical is \( \leq 2^{-n(I(X;Y) - 3\varepsilon)} \) by the joint AEP.

   Now let \( W \) be uniformly distributed over \( \{1, 2, \ldots, 2nR\} \), and consequently,

   \[
   \Pr(E) = \frac{1}{2nR} \sum_{i=1}^{2nR} P^{(n)}_e
   \]

   Then

   \[
   P^{(n)}_e = \Pr(E) = \Pr(E|W = 1)
   \]

   \[
   \leq P(E_0) + P(E_c^1) + \sum_{i=2}^{2nR} P(E_i)
   \]

   \[
   \leq \varepsilon + \varepsilon + \sum_{i=2}^{2nR} 2^{-n(I(X;Y) - 3\varepsilon)}
   \]

   \[
   = 2\varepsilon + (2nR - 1)2^{-n(I(X;Y) - 3\varepsilon)}
   \]

   \[
   \leq 2\varepsilon + 2^{3n\varepsilon}2^{-n(I(X;Y) - R)}
   \]

   \[
   \leq 3\varepsilon
   \]

   for \( n \) sufficiently large and \( R < I(X; Y) - 3\varepsilon \). This proves the existence of a good \( (2nR, n) \) code.

   Now choosing a good codebook and deleting the worst half of the codewords, we obtain a code with low maximal probability of error. In particular, the power constraint is satisfied by each of the remaining codewords (since the codewords that do not satisfy the power constraint have probability of error 1 and must belong to the worst half of the codewords). Hence we have constructed a code that achieves a rate arbitrarily close to capacity. The forward part of the theorem is proved. In the next section, we show that the achievable rate cannot exceed the capacity.

   \[
   \Box
   \]

**9.2 Converse to the Coding Theorem for Gaussian Channels**

In this section, we complete the proof that the capacity of a Gaussian channel is \( C = \frac{1}{2} \log(1 + \frac{P}{N}) \) by proving that rates \( R > C \) are not achievable. The proof parallels the proof for the discrete channel. The main new ingredient is the power constraint.

**Proof:**

**(Converse to Theorem 9.1.1).** We must show that if \( P^{(n)}_e \to 0 \) for a sequence of \( (2nR, n) \) codes for a Gaussian channel with power constraint \( P \), then

\[
R \leq C = \frac{1}{2} \log\left(1 + \frac{P}{N}\right)
\]

Consider any \( (2nR, n) \) code that satisfies the power constraint, that is,

\[
\frac{1}{n} \sum_{i=1}^{n} x_i^2(w) \leq P
\]

for \( w = 1, 2, \ldots, 2nR \). Proceeding as in the converse for the discrete case, let \( W \) be distributed uniformly over \( \{1, 2, \ldots, 2nR\} \). The uniform distribution over the index set \( W \in \{1, 2, \ldots, 2nR\} \) induces a distribution on the input codewords, which in turn induces a distribution over the input alphabet. This specifies a joint distribution on \( W \to X^n(W) \to Y^n \to \hat{W} \).

To relate probability of error and mutual information, we can apply Fano’s inequality to obtain

\[
H(W|\hat{W}) \leq 1 + nR P^{(n)}_e = n\varepsilon_n
\]

where \( \varepsilon_n \to 0 \) as \( P^{(n)}_e \to 0 \). Hence,

\[
nR = H(W) = I(W; \hat{W}) + H(W|\hat{W})
\]

\[
\leq I(W; \hat{W}) + n\varepsilon_n
\]

\[
\leq I(X^n; Y^n) + n\varepsilon_n
\]

\[
= h(Y^n) - h(Y^n|X^n) + n\varepsilon_n
\]

\[
= h(Y^n) - h(Z^n) + n\varepsilon_n
\]

\[
\leq \sum_{i=1}^{n} h(Y_i) - h(Z^n) + n\varepsilon_n
\]

\[
= \sum_{i=1}^{n} h(Y_i) - \sum_{i=1}^{n} h(Z_i) + n\varepsilon_n
\]

\[
= \sum_{i=1}^{n} I(X_i; Y_i) + n\varepsilon_n
\]

Here \( X_i = x_i(W) \), where \( W \) is drawn according to the uniform distribution on \( \{1, 2, \ldots, 2nR\} \). Now let \( P_i \) be the average power of the \( i \)-th column of the codebook, that is,

\[
P_i = \frac{1}{2nR} \sum_{w} x_i^2(w)
\]

Then, since \( Y_i = X_i + Z_i \) and since \( X_i \) and \( Z_i \) are independent, the average power \( E[Y_i^2] \) of \( Y_i \) is \( P_i + N \). Hence, since entropy is maximized by the normal distribution,

\[
h(Y_i) \leq \frac{1}{2} \log 2\pi e(P_i + N)
\]

Continuing with the inequalities of the converse, we obtain

\[
nR \leq \sum_{i=1}^{n} \left( h(Y_i) - h(Z_i) \right) + n\varepsilon_n
\]

\[
\leq \sum_{i=1}^{n} \left( \frac{1}{2} \log(2\pi e(P_i + N)) - \frac{1}{2} \log(2\pi eN) \right) + n\varepsilon_n
\]

\[
= \sum_{i=1}^{n} \frac{1}{2} \log\left(1 + \frac{P_i}{N}\right) + n\varepsilon_n
\]

Since each of the codewords satisfies the power constraint, so does their average, and hence

\[
\frac{1}{n} \sum_{i=1}^{n} P_i \leq P
\]

Since \( f(x) = \frac{1}{2} \log(1 + x) \) is a concave function of \( x \), we can apply Jensen’s inequality to obtain

\[
\frac{1}{n} \sum_{i=1}^{n} \frac{1}{2} \log\left(1 + \frac{P_i}{N}\right) \leq \frac{1}{2} \log\left(1 + \frac{\frac{1}{n} \sum_{i=1}^{n} P_i}{N}\right)
\]

\[
\leq \frac{1}{2} \log\left(1 + \frac{P}{N}\right)
\]

Thus

\[
R \leq \frac{1}{2} \log\left(1 + \frac{P}{N}\right) + \varepsilon_n
\]

where \( \varepsilon_n \to 0 \). This completes the converse.

**9.3 Bandlimited Channels**

A common model for communication over a radio network or a telephone line is a bandlimited channel with white noise. This is a continuous-time channel. The output of such a channel can be described as the convolution

\[
Y(t) = (X(t) + Z(t)) * h(t)
\]

where \( X(t) \) is the signal waveform, \( Z(t) \) is the waveform of the white Gaussian noise, and \( h(t) \) is the impulse response of an ideal bandpass filter, which cuts out all frequencies greater than \( W \). In this section, we give simplified arguments to calculate the capacity of such a channel.

We begin with a representation theorem due to Nyquist and Shannon, which shows that sampling a bandlimited signal at a sampling rate \( \frac{1}{2W} \) is sufficient to reconstruct the signal from the samples. Intuitively, this is due to the fact that if a signal is bandlimited to \( W \), it cannot change by a substantial amount in a time less than half a cycle of the maximum frequency in the signal, i.e., the signal cannot change very much in time intervals less than \( \frac{1}{2W} \) seconds.

**Theorem 9.3.1**

Suppose that a function \( f(t) \) is bandlimited to \( W \), namely, the spectrum of the function is 0 for all frequencies greater than \( W \). Then the function is completely determined by samples of the function spaced \( \frac{1}{2W} \) seconds apart.

**Proof:**

Let \( F(\omega) \) be the Fourier transform of \( f(t) \). Then

\[
f(t) = \frac{1}{2\pi} \int_{-\infty}^{\infty} F(\omega) e^{i\omega t} d\omega
\]

\[
= \frac{1}{2\pi} \int_{-2\pi W}^{2\pi W} F(\omega) e^{i\omega t} d\omega
\]

since \( F(\omega) \) is zero outside the band \( -2\pi W \leq \omega \leq 2\pi W \). If we consider samples spaced \( \frac{1}{2W} \) seconds apart, the value of the signal at the sample points can be written

\[
f\left(\frac{n}{2W}\right) = \frac{1}{2\pi} \int_{-2\pi W}^{2\pi W} F(\omega) e^{i\omega \frac{n}{2W}} d\omega
\]

The right-hand side of this equation is also the definition of the coefficients of the Fourier series expansion of the periodic extension of the function \( F(\omega) \), taking the interval \( -2\pi W \) to \( 2\pi W \) as the fundamental period. Thus, the sample values \( f\left(\frac{n}{2W}\right) \) determine the Fourier coefficients and, by extension, they determine the value of \( F(\omega) \) in the interval \( (-2\pi W, 2\pi W) \). Since a function is uniquely specified by its Fourier transform, and since \( F(\omega) \) is zero outside the band \( W \), we can determine the function uniquely from the samples.

Consider the function

\[
\text{sinc}(t) = \frac{\sin(2\pi Wt)}{2\pi Wt}
\]

This function is 1 at \( t = 0 \) and is 0 for \( t = \frac{n}{2W} \), \( n \neq 0 \). The spectrum of this function is constant in the band \( (-W, W) \) and is zero outside this band. Now define

\[
g(t) = \sum_{n=-\infty}^{\infty} f\left(\frac{n}{2W}\right) \text{sinc}\left(t - \frac{n}{2W}\right)
\]

From the properties of the sinc function, it follows that \( g(t) \) is bandlimited to \( W \) and is equal to \( f\left(\frac{n}{2W}\right) \) at \( t = \frac{n}{2W} \). Since there is only one function satisfying these constraints, we must have \( g(t) = f(t) \). This provides an explicit representation of \( f(t) \) in terms of its samples.

\[
\Box
\]

A general function has an infinite number of degrees of freedom—the value of the function at every point can be chosen independently. The Nyquist-Shannon sampling theorem shows that a bandlimited function has only \( 2W \) degrees of freedom per second. The values of the function at the sample points can be chosen independently, and this specifies the entire function.

If a function is bandlimited, it cannot be limited in time. But we can consider functions that have most of their energy in bandwidth \( W \) and have most of their energy in a finite time interval, say \( (0, T) \). We can describe these functions using a basis of prolate spheroidal functions. We do not go into the details of this theory here; it suffices to say that there are about \( 2TW \) orthonormal basis functions for the set of almost time-limited, almost bandlimited functions, and we can describe any function within the set by its coordinates in this basis. The details can be found in a series of papers by Landau, Pollak, and Slepian. Moreover, the projection of white noise on these basis vectors forms an i.i.d. Gaussian process. The above arguments enable us to view the bandlimited, time-limited functions as vectors in a vector space of \( 2TW \) dimensions.

Now we return to the problem of communication over a bandlimited channel. Assuming that the channel has bandwidth \( W \), we can represent both the input and the output by samples taken \( \frac{1}{2W} \) seconds apart. Each of the input samples is corrupted by noise to produce the corresponding output sample. Since the noise is white and Gaussian, it can be shown that each noise sample is an independent, identically distributed Gaussian random variable.

If the noise has power spectral density \( \frac{N_0}{2} \) watts/hertz and bandwidth \( W \) hertz, the noise has power \( N_0W \) and each of the \( 2TW \) noise samples in time \( T \) has variance \( \frac{N_0WT}{2WT} = \frac{N_0}{2} \). Looking at the input as a vector in the \( 2TW \)-dimensional space, we see that the received signal is spherically normally distributed about this point with covariance \( \frac{N_0}{2}I \).

Now we can use the theory derived earlier for discrete-time Gaussian channels, where it was shown that the capacity of such a channel is

\[
C = \frac{1}{2} \log\left(1 + \frac{P}{N}\right)
\]

bits per transmission.

Let the channel be used over the time interval \( [0, T] \). In this case, the energy per sample is \( \frac{PT}{2WT} = \frac{P}{2W} \), the noise variance per sample is \( \frac{N_0}{2} \).


Okay, so I need to help clean up this text. Let me read through it first to understand what's going on. It looks like it's from a textbook, probably about information theory or communication systems, given the terms like channel capacity, Gaussian channels, and modems.

First, I notice that the text is split into two pages, 99 and 100. There are some equations and examples. My task is to remove unnecessary formatting, fix any broken sentences, ensure consistent punctuation and capitalization, remove non-standard characters, and make the text more readable while preserving the original meaning and structure.

Looking at the text, I see some line breaks that might be causing sentences to be split awkwardly. For example, in the example about the telephone line, the sentence about multiplexing is broken across lines. I should fix that by combining them into a single sentence.

There are also some equations with fractions and logs. I need to make sure that the LaTeX or whatever formatting is consistent. For instance, in equation (9.61), the fraction is written with \[ \frac{P}{2W} \]. I should check if all the equations are properly formatted and correct any issues.

I notice some extra spaces and line breaks that don't seem necessary. For example, after "bits per sample." there's a line break before "(9.61)". I should remove those unnecessary breaks to make the text flow better.

There are also some non-standard characters like the special quotes and dashes. I should replace them with standard ASCII characters to avoid any encoding issues. For example, the hyphen in "inﬁnite" should be a regular hyphen.

Capitalization seems mostly consistent, but I should double-check. For example, "GAUSSIAN CHANNEL" is in all caps, which might be a section header, so that's probably intentional. I'll leave that as is unless it's part of a sentence.

Punctuation-wise, I see some missing periods or commas. For example, in the sentence about the V.90 modems, it ends abruptly without a period. I should add that to make it grammatically correct.

I also need to ensure that the equations are properly numbered and referenced. For example, equation (9.62) is mentioned, so I should make sure it's correctly placed and the text refers to it accurately.

In the section about parallel Gaussian channels, the equations are a bit messy. I should make sure that the summations and expectations are correctly formatted. For example, the power constraint in (9.66) should have the expectation operator properly placed.

Overall, the goal is to make the text clean and readable without losing any of the technical details. I'll go through each paragraph, fix the line breaks, correct the punctuation, standardize the characters, and ensure that the equations are properly presented.

I should also check for any broken sentences or run-on sentences. For example, the part about DSL should be a separate sentence, so I'll make sure it's properly punctuated.

Finally, I'll read through the cleaned text to ensure that it makes sense and that all the technical terms are correctly used. This way, the text will be more accessible and easier to understand for someone reading it.
</think>

BANDLIMITED CHANNELS

The capacity per sample is given by:

\[ C = \frac{1}{2} \log \left( 1 + \frac{P}{2W N_0} \right) \text{ bits per sample.} \]

Since there are \(2W\) samples each second, the capacity of the channel can be rewritten as:

\[ C = W \log \left( 1 + \frac{P}{N_0 W} \right) \text{ bits per second.} \]

This equation is one of the most famous formulas of information theory. It gives the capacity of a bandlimited Gaussian channel with noise spectral density \(N_0/2\) watts/Hz and power \(P\) watts.

A more precise version of the capacity argument involves consideration of signals with a small fraction of their energy outside the bandwidth \(W\) of the channel and a small fraction of their energy outside the time interval \((0, T)\). The capacity above is then obtained as a limit as the fraction of energy outside the band goes to zero.

If we let \(W \to \infty\) in (9.62), we obtain:

\[ C = \frac{P}{N_0} \log_2 e \text{ bits per second} \]

as the capacity of a channel with an infinite bandwidth, power \(P\), and noise spectral density \(N_0/2\). Thus, for infinite bandwidth channels, the capacity grows linearly with the power.

**Example 9.3.1 (Telephone line)**

To allow multiplexing of many channels, telephone signals are bandlimited to 3300 Hz. Using a bandwidth of 3300 Hz and a SNR (signal-to-noise ratio) of 33 dB (i.e., \(P/N_0 W = 2000\)) in (9.62), we find the capacity of the telephone channel to be about 36,000 bits per second. Practical modems achieve transmission rates up to 33,600 bits per second in both directions over a telephone channel.

In real telephone channels, there are other factors, such as crosstalk, interference, echoes, and non-flat channels, which must be compensated for to achieve this capacity.

The V.90 modems that achieve 56 kb/s over the telephone channel achieve this rate in only one direction, taking advantage of a purely digital channel from the server to the final telephone switch in the network. In this case, the only impairments are due to the digital-to-analog conversion at this switch and the noise in the copper link from the switch to the home.

The actual bandwidth available on the copper wire that links a home to a telephone switch is on the order of a few megahertz; it depends on the length of the wire. The frequency response is far from flat over this band. If the entire bandwidth is used, it is possible to send a few megabits per second through this channel; schemes such as DSL (Digital Subscriber Line) achieve this using special equipment at both ends of the telephone line (unlike modems, which do not require modification at the telephone switch).

**9.4 PARALLEL GAUSSIAN CHANNELS**

In this section, we consider \(k\) independent Gaussian channels in parallel with a common power constraint. The objective is to distribute the total power among the channels so as to maximize the capacity. This channel models a nonwhite additive Gaussian noise channel where each parallel component represents a different frequency.

Assume that we have a set of Gaussian channels in parallel as illustrated in Figure 9.3. The output of each channel is the sum of the input and Gaussian noise. For channel \(j\),

\[ Y_j = X_j + Z_j, \quad j = 1, 2, \ldots, k, \]

where

\[ Z_j \sim \mathcal{N}(0, N_j), \]

and the noise is assumed to be independent from channel to channel. We assume that there is a common power constraint on the total power used, that is,

\[ \mathbb{E} \left( \sum_{j=1}^k X_j^2 \right) \leq P. \]

We wish to distribute the power among the various channels so as to maximize the total capacity.

The information capacity of the channel \(C\) is

\[ C = \max_{f(x_1, x_2, \ldots, x_k): \mathbb{E} \left( \sum_{i=1}^k X_i^2 \right) \leq P} I(X_1, X_2, \ldots, X_k; Y_1, Y_2, \ldots, Y_k). \]



### PARALLEL GAUSSIAN CHANNELS

We calculate the distribution that achieves the information capacity for this channel. The fact that the information capacity is the supremum of achievable rates can be proved by methods identical to those in the proof of the capacity theorem for single Gaussian channels and will be omitted. Since \(Z_1, Z_2, \ldots, Z_k\) are independent,

\[
I(X_1, X_2, \ldots, X_k; Y_1, Y_2, \ldots, Y_k) = h(Y_1, Y_2, \ldots, Y_k) - h(Y_1, Y_2, \ldots, Y_k|X_1, X_2, \ldots, X_k)
\]

\[
= h(Y_1, Y_2, \ldots, Y_k) - h(Z_1, Z_2, \ldots, Z_k|X_1, X_2, \ldots, X_k)
\]

\[
= h(Y_1, Y_2, \ldots, Y_k) - h(Z_1, Z_2, \ldots, Z_k)
\]

\[
= h(Y_1, Y_2, \ldots, Y_k) - \sum_i h(Z_i)
\]

\[
\leq \sum_i \left( h(Y_i) - h(Z_i) \right)
\]

\[
\leq \sum_i \frac{1}{2} \log \left( 1 + \frac{P_i}{N_i} \right),
\]

where \(P_i = \mathbb{E}X_i^2\), and \(\sum P_i = P\). Equality is achieved by \((X_1, X_2, \ldots, X_k) \sim \mathcal{N}\left(0, \text{diag}(P_1, P_2, \ldots, P_k)\right)\).

So the problem is reduced to finding the power allotment that maximizes the capacity subject to the constraint that \(\sum P_i = P\). This is a standard optimization problem and can be solved using Lagrange multipliers. Writing the functional as

\[
J(P_1, \ldots, P_k) = \sum_{i=1}^k \frac{1}{2} \log \left( 1 + \frac{P_i}{N_i} \right) + \lambda \left( \sum_{i=1}^k P_i \right),
\]

and differentiating with respect to \(P_i\), we have

\[
\frac{1}{2} \frac{1}{P_i + N_i} + \lambda = 0,
\]

or

\[
P_i = \nu - N_i.
\]

However, since the \(P_i\)'s must be nonnegative, it may not always be possible to find a solution of this form. In this case, we use the Kuhn–Tucker conditions to verify that the solution

\[
P_i = (\nu - N_i)^+
\]

is the assignment that maximizes capacity, where \(\nu\) is chosen so that \(\sum (\nu - N_i)^+ = P\). Here \((x)^+\) denotes the positive part of \(x\):

\[
(x)^+ = \begin{cases} 
x & \text{if } x \geq 0, \\
0 & \text{if } x < 0.
\end{cases}
\]

This solution is illustrated graphically in Figure 9.4. The vertical levels indicate the noise levels in the various channels. As the signal power is increased from zero, we allot the power to the channels with the lowest noise. When the available power is increased still further, some of the power is put into noisier channels. The process by which the power is distributed among the various bins is identical to the way in which water distributes itself in a vessel, hence this process is sometimes referred to as water-filling.

---

### CHANNELS WITH COLORED GAUSSIAN NOISE

In Section 9.4, we considered the case of a set of parallel independent Gaussian channels in which the noise samples from different channels were independent. Now we will consider the case when the noise is dependent. This represents not only the case of parallel channels but also the case when the channel has Gaussian noise with memory. For channels with memory, we can consider a block of \(n\) consecutive uses of the channel as \(n\) channels in parallel with dependent noise. As in Section 9.4, we will calculate only the information capacity for this channel.

Let \(K_Z\) be the covariance matrix of the noise, and let \(K_X\) be the input covariance matrix. The power constraint on the input can then be written as

\[
\frac{1}{n} \sum_{i=1}^n \mathbb{E}X_i^2 \leq P,
\]

or equivalently,

\[
\frac{1}{n} \text{tr}(K_X) \leq P.
\]

Unlike Section 9.4, the power constraint here depends on \(n\); the capacity will have to be calculated for each \(n\).

Just as in the case of independent channels, we can write

\[
I(X_1, X_2, \ldots, X_n; Y_1, Y_2, \ldots, Y_n) = h(Y_1, Y_2, \ldots, Y_n) - h(Z_1, Z_2, \ldots, Z_n).
\]

Here \(h(Z_1, Z_2, \ldots, Z_n)\) is determined only by the distribution of the noise and is not dependent on the choice of input distribution. So finding the capacity amounts to maximizing \(h(Y_1, Y_2, \ldots, Y_n)\). The entropy of the output is maximized when \(Y\) is normal, which is achieved when the input is normal. Since the input and the noise are independent, the covariance of the output \(Y\) is \(K_Y = K_X + K_Z\), and the entropy is

\[
h(Y_1, Y_2, \ldots, Y_n) = \frac{1}{2} \log \left( (2\pi e)^n |K_X + K_Z| \right).
\]

Now the problem is reduced to choosing \(K_X\) so as to maximize \(|K_X + K_Z|\), subject to a trace constraint on \(K_X\). To do this, we decompose \(K_Z\) into its diagonal form,

\[
K_Z = Q \Lambda Q^T,
\]

where \(Q^T Q = I\).

Then

\[
|K_X + K_Z| = |K_X + Q \Lambda Q^T|
\]

\[
= |Q||Q^T K_X Q + \Lambda||Q^T|
\]

\[
= |Q^T K_X Q + \Lambda|,
\]

where \(A = Q^T K_X Q\). Since for any matrices \(B\) and \(C\),

\[
\text{tr}(BC) = \text{tr}(CB),
\]

we have

\[
\text{tr}(A) = \text{tr}(Q^T K_X Q)
\]

\[
= \text{tr}(Q^T Q K_X)
\]

\[
= \text{tr}(K_X).
\]

Now the problem is reduced to maximizing \(|A + \Lambda|\) subject to a trace constraint \(\text{tr}(A) \leq nP\).

We apply Hadamard’s inequality, mentioned in Chapter 8. Hadamard’s inequality states that the determinant of any positive definite matrix \(K\) is less than the product of its diagonal elements, that is,

\[
|K| \leq \prod_{i=1}^n K_{ii},
\]

with equality if and only if the matrix is diagonal. Thus,

\[
|A + \Lambda| \leq \prod_{i=1}^n (A_{ii} + \lambda_i),
\]

with equality if and only if \(A\) is diagonal. Since \(A\) is subject to a trace constraint,

\[
\frac{1}{n} \sum_{i=1}^n A_{ii} \leq P,
\]

and \(A_{ii} \geq 0\), the maximum value of \(\prod_{i=1}^n (A_{ii} + \lambda_i)\) is attained when

\[
A_{ii} + \lambda_i = \nu.
\]

However, given the constraints, it may not always be possible to satisfy this equation with positive \(A_{ii}\). In such cases, we can show by the standard Kuhn–Tucker conditions that the optimum solution corresponds to setting

\[
A_{ii} = (\nu - \lambda_i)^+,
\]

where the water level \(\nu\) is chosen so that \(\sum A_{ii} = nP\). This value of \(A\) maximizes the entropy of \(Y\) and hence the mutual information. We can use Figure 9.4 to see the connection between the methods described above and water-filling.

Consider a channel in which the additive Gaussian noise is a stochastic process with finite-dimensional covariance matrix \(K_Z^{(n)}\). If the process is stationary, the covariance matrix is Toeplitz, and the density of eigenvalues on the real line tends to the power spectrum of the stochastic process. In this case, the above water-filling argument translates to water-filling in the spectral domain.

Hence, for channels in which the noise forms a stationary stochastic process, the input signal should be chosen to be a Gaussian process with a spectrum that is large at frequencies where the noise spectrum is small.

---

### GAUSSIAN CHANNELS WITH FEEDBACK

In Chapter 7, we proved that feedback does not increase the capacity for discrete memoryless channels, although it can help greatly in reducing the complexity of encoding or decoding. The same is true of an additive noise channel with white noise. As in the discrete case, feedback does not increase capacity for memoryless Gaussian channels.

However, for channels with memory, where the noise is correlated from one time instant to another, feedback does increase capacity. The capacity without feedback can be calculated using water-filling, but we do not have a simple explicit characterization of the capacity with feedback.

In this section, we describe an expression for the capacity in terms of the covariance matrix of the noise \(Z\). We prove a converse for this expression for capacity. We then derive a simple bound on the increase in capacity due to feedback.

The Gaussian channel with feedback is illustrated in Figure 9.6. The output of the channel \(Y_i\) is

\[
Y_i = X_i + Z_i, \quad Z_i \sim \mathcal{N}(0, K_Z^{(n)}).
\]

The feedback allows the input of the channel to depend on the past values of the output.

A \((2nR, n)\) code for the Gaussian channel with feedback consists of a sequence of mappings \(x_i(W, Y^{i-1})\), where \(W \in \{1, 2, \ldots, 2nR\}\) is the input message and \(Y^{i-1}\) is the sequence of past values of the output. Thus, \(x(W, \cdot)\) is a code function rather than a codeword. In addition, we require that the code satisfy a power constraint,

\[
\mathbb{E} \left[ \frac{1}{n} \sum_{i=1}^n x_i^2(W, Y^{i-1}) \right] \leq P,
\]

where the expectation is over all possible noise sequences.

We characterize the capacity of the Gaussian channel in terms of the covariance matrices of the input \(X\) and the noise \(Z\). Because of the feedback, \(X_n\) and \(Z_n\) are not independent; \(X_i\) depends causally on the past values of \(Z\). In the next section, we prove a converse for the Gaussian channel with feedback and show that we achieve capacity if we take \(X\) to be Gaussian.

We now state an informal characterization of the capacity of the channel with and without feedback.

1. **With feedback.** The capacity \(C_{n,\text{FB}}\) in bits per transmission of the time-varying Gaussian channel with feedback is

\[
C_{n,\text{FB}} = \max_{\frac{1}{n} \text{tr}(K_X) \leq P} \frac{1}{2n} \log \left| K_X + K_Z \right| \left| K_Z \right|^{-1}.
\]

2. **Without feedback.** The capacity \(C_n\) of the time-varying Gaussian channel without feedback is given by

\[
C_n = \max_{\frac{1}{n} \text{tr}(K_X) \leq P} \frac{1}{2n} \log \left| K_X + K_Z \right| \left| K_Z \right|^{-1}.
\]

This reduces to water-filling on the eigenvalues \(\{\lambda_i^{(n)}\}\) of \(K_Z^{(n)}\). Thus,

\[
C_n = \frac{1}{2n} \sum_{i=1}^n \log \left( 1 + \frac{(\lambda - \lambda_i^{(n)})^+}{\lambda_i^{(n)}} \right),
\]

where \((y)^+ = \max\{y, 0\}\) and where \(\lambda\) is chosen so that

\[
\sum_{i=1}^n (\lambda - \lambda_i^{(n)})^+ = nP.
\]



GAUSSIAN CHANNELS WITH FEEDBACK

We now prove an upper bound for the capacity of the Gaussian channel with feedback. This bound is actually achievable [136], and is therefore the capacity, but we do not prove this here.

**Theorem 9.6.1**  
For a Gaussian channel with feedback, the rate \( R_n \) for any sequence of \((2nR_n, n)\) codes with \( P_e^{(n)} \to 0 \) satisfies  
\[ R_n \leq C_n^{FB} + \varepsilon_n, \]  
where \( \varepsilon_n \to 0 \) as \( n \to \infty \), and \( C_n^{FB} \) is defined in (9.100).

**Proof:**  
Let \( W \) be uniform over \( 2nR_n \), and therefore the probability of error \( P_e^{(n)} \) is bounded by Fano’s inequality,  
\[ H(W|\hat{W}) \leq 1 + nR_n P_e^{(n)} = n\varepsilon_n, \]  
where \( \varepsilon_n \to 0 \) as \( P_e^{(n)} \to 0 \). We can then bound the rate as follows:  
\[ nR_n = H(W) \]  
\[ = I(W; \hat{W}) + H(W|\hat{W}) \]  
\[ \leq I(W; \hat{W}) + n\varepsilon_n \]  
\[ \leq I(W; Y^n) + n\varepsilon_n \]  
\[ = \sum_{i=1}^n I(W; Y_i|Y^{i-1}) + n\varepsilon_n \]  
(a)  
\[ = \sum_{i=1}^n \left[ h(Y_i|Y^{i-1}) - h(Y_i|W, Y^{i-1}, X_i, X^{i-1}, Z^{i-1}) \right] + n\varepsilon_n \]  
(b)  
\[ = \sum_{i=1}^n \left[ h(Y_i|Y^{i-1}) - h(Z_i|W, Y^{i-1}, X_i, X^{i-1}, Z^{i-1}) \right] + n\varepsilon_n \]  
(c)  
\[ = \sum_{i=1}^n \left[ h(Y_i|Y^{i-1}) - h(Z_i|Z^{i-1}) \right] + n\varepsilon_n \]  
\[ = h(Y^n) - h(Z^n) + n\varepsilon_n, \]  
where (a) follows from the fact that \( X_i \) is a function of \( W \) and the past \( Y \)'s, and \( Z^{i-1} \) is \( Y^{i-1} - X^{i-1} \); (b) follows from \( Y_i = X_i + Z_i \) and the fact that \( h(X + Z|X) = h(Z|X) \); and (c) follows from the fact \( Z_i \) and \( (W, Y^{i-1}, X_i) \) are conditionally independent given \( Z^{i-1} \). Continuing the chain of inequalities after dividing by \( n \), we have  
\[ R_n \leq \frac{1}{n} \left[ h(Y^n) - h(Z^n) \right] + \varepsilon_n \]  
\[ \leq \frac{1}{2n} \log |K_Y^{(n)}| / |K_Z^{(n)}| + \varepsilon_n \]  
\[ \leq C_n^{FB} + \varepsilon_n, \]  
by the entropy-maximizing property of the normal.  

We have proved an upper bound on the capacity of the Gaussian channel with feedback in terms of the covariance matrix \( K_{X+Z}^{(n)} \). We now derive bounds on the capacity with feedback in terms of \( K_X^{(n)} \) and \( K_Z^{(n)} \), which will then be used to derive bounds in terms of the capacity without feedback. For simplicity of notation, we will drop the superscript \( n \) in the symbols for covariance matrices.

We first prove a series of lemmas about matrices and determinants.

**Lemma 9.6.1**  
Let \( X \) and \( Z \) be \( n \)-dimensional random vectors. Then  
\[ K_{X+Z} + K_{X-Z} = 2K_X + 2K_Z. \]  

**Proof:**  
\[ K_{X+Z} = E(X+Z)(X+Z)^T \]  
\[ = E(XX^T) + E(XZ^T) + E(ZX^T) + E(ZZ^T) \]  
\[ = K_X + K_{XZ} + K_{ZX} + K_Z. \]  
Similarly,  
\[ K_{X-Z} = K_X - K_{XZ} - K_{ZX} + K_Z. \]  
Adding these two equations completes the proof.  

**Lemma 9.6.2**  
For two \( n \times n \) nonnegative definite matrices \( A \) and \( B \), if \( A - B \) is nonnegative definite, then \( |A| \geq |B| \).  

**Proof:**  
Let \( C = A - B \). Since \( B \) and \( C \) are nonnegative definite, we can consider them as covariance matrices. Consider two independent normal random vectors \( X_1 \sim N(0, B) \) and \( X_2 \sim N(0, C) \). Let \( Y = X_1 + X_2 \). Then  
\[ h(Y) \geq h(Y|X_2) \]  
\[ = h(X_1|X_2) \]  
\[ = h(X_1), \]  
where the inequality follows from the fact that conditioning reduces differential entropy, and the final equality from the fact that \( X_1 \) and \( X_2 \) are independent. Substituting the expressions for the differential entropies of a normal random variable, we obtain  
\[ \frac{1}{2} \log(2\pi e)^n |A| > \frac{1}{2} \log(2\pi e)^n |B|, \]  
which is equivalent to the desired lemma.  

**Lemma 9.6.3**  
For two \( n \)-dimensional random vectors \( X \) and \( Z \),  
\[ |K_{X+Z}| \leq 2^n |K_X + K_Z|. \]  

**Proof:**  
From Lemma 9.6.1,  
\[ 2(K_X + K_Z) - K_{X+Z} \succeq 0, \]  
where \( A \succeq 0 \) means that \( A \) is nonnegative definite. Hence, applying Lemma 9.6.2, we have  
\[ |K_{X+Z}| \leq |2(K_X + K_Z)| = 2^n |K_X + K_Z|, \]  
which is the desired result.  

**Lemma 9.6.4**  
For \( A, B \) nonnegative definite matrices and \( 0 \leq \lambda \leq 1 \),  
\[ |\lambda A + (1 - \lambda)B| \geq |A|^{\lambda} |B|^{1-\lambda}. \]  

**Proof:**  
Let \( X \sim N_n(0, A) \) and \( Y \sim N_n(0, B) \). Let \( Z \) be the mixture random vector  
\[ Z = \begin{cases} X & \text{if } \theta = 1, \\ Y & \text{if } \theta = 2, \end{cases} \]  
where \( \theta \) is independent of \( X \) and \( Y \) and  
\[ \theta = \begin{cases} 1 & \text{with probability } \lambda, \\ 2 & \text{with probability } 1 - \lambda. \end{cases} \]  
Then  
\[ K_Z = \lambda A + (1 - \lambda)B. \]  
We observe that  
\[ \frac{1}{2} \log(2\pi e)^n |K_Z| \geq h(Z) \]  
\[ \geq h(Z|\theta) \]  
\[ = \lambda h(X) + (1 - \lambda)h(Y) \]  
\[ = \frac{1}{2} \log(2\pi e)^n |A|^{\lambda} |B|^{1-\lambda}, \]  
which proves the result. The first inequality follows from the entropy-maximizing property of the Gaussian under the covariance constraint.  

**Definition**  
We say that a random vector \( X^n \) is causally related to \( Z^n \) if  
\[ f(x^n, z^n) = \prod_{i=1}^n f(x_i | x^{i-1}, z^{i-1}). \]  
Note that the feedback codes necessarily yield causally related \( (X^n, Z^n) \).  

**Lemma 9.6.5**  
If \( X^n \) and \( Z^n \) are causally related, then  
\[ h(X^n - Z^n) \geq h(Z^n) \]  
and  
\[ |K_{X-Z}| \geq |K_Z|, \]  
where \( K_{X-Z} \) and \( K_Z \) are the covariance matrices of \( X^n - Z^n \) and \( Z^n \), respectively.  

**Proof:**  
We have  
\[ h(X^n - Z^n) \]  
\[ = \sum_{i=1}^n h(X_i - Z_i | X^{i-1} - Z^{i-1}) \]  
\[ \geq \sum_{i=1}^n h(X_i - Z_i | X^{i-1}, Z^{i-1}, X_i) \]  
\[ = \sum_{i=1}^n h(Z_i | X^{i-1}, Z^{i-1}, X_i) \]  
\[ = \sum_{i=1}^n h(Z_i | Z^{i-1}) \]  
\[ = h(Z^n). \]  
Here, (a) follows from the chain rule, (b) follows from conditioning \( h(A|B) \geq h(A|B, C) \), (c) follows from the conditional determinism of \( X_i \) and the invariance of differential entropy under translation, (d) follows from the causal relationship of \( X^n \) and \( Z^n \), and (e) follows from the chain rule.

Finally, suppose that \( X^n \) and \( Z^n \) are causally related and the associated covariance matrices for \( Z^n \) and \( X^n - Z^n \) are \( K_Z \) and \( K_{X-Z} \). There obviously exists a multivariate normal (causally related) pair of random vectors \( \tilde{X}^n, \tilde{Z}^n \) with the same covariance structure. Thus, from (9.148), we have  
\[ \frac{1}{2} \log(2\pi e)^n |K_{X-Z}| = h(\tilde{X}^n - \tilde{Z}^n) \]  
\[ \geq h(\tilde{Z}^n) \]  
\[ = \frac{1}{2} \log(2\pi e)^n |K_Z|, \]  
thus proving (9.143).  

We are now in a position to prove that feedback increases the capacity of a nonwhite Gaussian additive noise channel by at most half a bit.  

**Theorem 9.6.2**  
\[ C_n^{FB} \leq C_n + \frac{1}{2} \text{ bits per transmission.} \]  

**Proof:**  
Combining all the lemmas, we obtain  
\[ C_n^{FB} \leq \max_{\text{tr}(K_X) \leq nP} \frac{1}{2n} \log \frac{|K_Y|}{|K_Z|} \]  
\[ \leq \max_{\text{tr}(K_X) \leq nP} \frac{1}{2n} \log 2^n |K_X + K_Z| / |K_Z| \]  
\[ = \max_{\text{tr}(K_X) \leq nP} \frac{1}{2n} \log |K_X + K_Z| / |K_Z| + \frac{1}{2} \]  
\[ \leq C_n + \frac{1}{2} \text{ bits per transmission,} \]  
where the inequalities follow from Theorem 9.6.1, Lemma 9.6.3, and the definition of capacity without feedback, respectively.  

We now prove Pinsker’s statement that feedback can at most double the capacity of colored noise channels.  

**Theorem 9.6.3**  
\[ C_n^{FB} \leq 2C_n. \]  

**Proof:**  
It is enough to show that  
\[ \frac{1}{2n} \log \frac{|K_{X+Z}|}{|K_Z|} \leq \frac{1}{2n} \log |K_X + K_Z| / |K_Z|, \]  
for it will then follow that by maximizing the right side and then the left side that  
\[ \frac{1}{2} C_n^{FB} \leq C_n. \]  
We have  
\[ \frac{1}{2n} \log \frac{|K_{X+Z}|}{|K_Z|} \]  
(a)  
\[ = \frac{1}{2n} \log \left| \frac{1}{2} K_{X+Z} + \frac{1}{2} K_{X-Z} \right| / |K_Z| \]  
(b)  
\[ \geq \frac{1}{2n} \log \left| K_{X+Z} \right|^{1/2} \left| K_{X-Z} \right|^{1/2} / |K_Z| \]  
(c)  
\[ \geq \frac{1}{2n} \log \left| K_{X+Z} \right|^{1/2} \left| K_Z \right|^{1/2} / |K_Z| \]  
(d)  
\[ = \frac{1}{2n} \log \left| K_{X+Z} \right| / |K_Z| \]  
and the result is proved. Here (a) follows from Lemma 9.6.1, (b) is the inequality in Lemma 9.6.4, and (c) is Lemma 9.6.5 in which causality is used.  

**SUMMARY**  
Thus, we have shown that Gaussian channel capacity is not increased by more than half a bit or by more than a factor of 2 when we have feedback; feedback helps, but not by much.  

**SUMMARY**  
Maximum entropy. For \( X \) with \( EX^2 = \alpha \), \( h(X) = \frac{1}{2} \log 2\pi e \alpha \).  

Gaussian channel. \( Y_i = X_i + Z_i \); \( Z_i \sim N(0, N) \); power constraint \( \frac{1}{n} \sum_{i=1}^n X_i^2 \leq P \); and  
\[ C = \frac{1}{2} \log \left( 1 + \frac{P}{N} \right) \text{ bits per transmission.} \]  

Bandlimited additive white Gaussian noise channel. Bandwidth \( W \); two-sided power spectral density \( N_0/2 \); signal power \( P \); and  
\[ C = W \log \left( 1 + \frac{P}{N_0 W} \right) \text{ bits per second.} \]  

Water-filling (k parallel Gaussian channels). \( Y_j = X_j + Z_j \), \( j = 1, 2, \dots, k \); \( Z_j \sim N(0, N_j) \); \( \sum_{j=1}^k X_j^2 \leq P \); and  
\[ C = \sum_{i=1}^k \frac{1}{2} \log \left( 1 + \frac{(\nu - N_i)}{N_i} \right), \]  
where \( \nu \) is chosen so that \( \sum (\nu - N_i)^+ = nP \).  

Additive nonwhite Gaussian noise channel. \( Y_i = X_i + Z_i \); \( Z^n \sim N(0, K_Z) \); and  
\[ C = \frac{1}{n} \sum_{i=1}^n \frac{1}{2} \log \left( 1 + \frac{(\nu - \lambda_i)}{\lambda_i} \right), \]  
where \( \lambda_1, \lambda_2, \dots, \lambda_n \) are the eigenvalues of \( K_Z \) and \( \nu \) is chosen so that \( \sum (\nu - \lambda_i)^+ = P \).  

Capacity without feedback  
\[ C_n = \max_{\text{tr}(K_X) \leq nP} \frac{1}{2n} \log \frac{|K_X + K_Z|}{|K_Z|}. \]  

Capacity with feedback  
\[ C_n^{FB} = \max_{\text{tr}(K_X) \leq nP} \frac{1}{2n} \log \frac{|K_{X+Z}|}{|K_Z|}. \]  

Feedback bounds  
\[ C_n^{FB} \leq C_n + \frac{1}{2}. \]  
\[ C_n^{FB} \leq 2C_n. \]  

**PROBLEMS**  

**9.1 Channel with two independent looks at \( Y \).**  
Let \( Y_1 \) and \( Y_2 \) be conditionally independent and conditionally identically distributed given \( X \).  
(a) Show that \( I(X; Y_1, Y_2) = 2I(X; Y_1) - I(Y_1; Y_2) \).  
(b) Conclude that the capacity of the channel \( X \to (Y_1, Y_2) \) is less than twice the capacity of the channel \( X \to Y_1 \).  

**9.2 Two-look Gaussian channel \( X \to (Y_1, Y_2) \).**  
Consider the ordinary Gaussian channel with two correlated looks at \( X \), that is, \( Y = (Y_1, Y_2) \), where  
\[ Y_1 = X + Z_1 \]  
\[ Y_2 = X + Z_2 \]  
with a power constraint \( P \) on \( X \), and \( (Z_1, Z_2) \sim N_2(0, K) \), where  
\[ K = \begin{pmatrix} N & N\rho \\ N\rho & N \end{pmatrix}. \]  
Find the capacity \( C \) for  
(a) \( \rho = 1 \)  
(b) \( \rho = 0 \)  
(c) \( \rho = -1 \)  

**9.3 Output power constraint.**  
Consider an additive white Gaussian noise channel with an expected output power constraint \( P \). Thus, \( Y = X + Z \), \( Z \sim N(0, \sigma^2) \), \( Z \) is independent of \( X \), and \( E[Y^2] \leq P \). Find the channel capacity.  

**9.4 Exponential noise channels.**  
\( Y_i = X_i + Z_i \), where \( Z_i \) is i.i.d. exponentially distributed noise with mean \( \mu \). Assume that we have a mean constraint on the signal (i.e., \( E[X_i] \leq \lambda \)). Show that the capacity of such a channel is \( C = \log(1 + \lambda / \mu) \).  

**9.5 Fading channel.**  
Consider an additive noise fading channel \( V \to X \to Z \to Y \), where \( Y = XV + Z \), \( Z \) is additive noise, \( V \) is a random variable representing fading, and \( Z \) and \( V \) are independent of each other and of \( X \). Argue that knowledge of the fading factor \( V \) improves capacity by showing that \( I(X; Y|V) \geq I(X; Y) \).  

**9.6 Parallel channels and water-filling.**  
Consider a pair of parallel Gaussian channels:  
\[ \begin{pmatrix} Y_1 \\ Y_2 \end{pmatrix} = \begin{pmatrix} X_1 \\ X_2 \end{pmatrix} + \begin{pmatrix} Z_1 \\ Z_2 \end{pmatrix}. \]



--- Page 18 ---  
292  
GAUSSIAN CHANNEL  
where  
Z₁, Z₂ ~ ∼ N(0, σ²₁, 0, 0, σ²₂) with σ²₁ > σ²₂.  
And there is a power constraint E(X²₁ + X²₂) ≤ 2P. Assume that σ²₁ > σ²₂. At what power does the channel stop behaving like a single channel with noise variance σ²₂ and begin behaving like a pair of channels?  

9.7 Multipath Gaussian channel.  
Consider a Gaussian noise channel with power constraint P, where the signal takes two different paths, and the received noisy signals are added together at the antenna.  

∑ X Y₁ Y₂ Z₁ Z₂  

(a) Find the capacity of this channel if Z₁ and Z₂ are jointly normal with covariance matrix K_Z = [σ² ρσ²; ρσ² σ²].  
(b) What is the capacity for ρ = 0, ρ = 1, ρ = -1?  

9.8 Parallel Gaussian channels.  
Consider the following parallel Gaussian channel:  

Z₁ ~ N(0, N₁) X₁ Y₁ Z₂ ~ N(0, N₂) X₂ Y₂  

--- Page 19 ---  
293  
where Z₁ ~ N(0, N₁) and Z₂ ~ N(0, N₂) are independent Gaussian random variables, and Y_i = X_i + Z_i. We wish to allocate power to the two parallel channels. Let β₁ and β₂ be fixed. Consider a total cost constraint β₁P₁ + β₂P₂ ≤ β, where P_i is the power allocated to the i-th channel and β_i is the cost per unit power in that channel. Thus, P₁ ≥ 0 and P₂ ≥ 0 can be chosen subject to the cost constraint β.  

(a) For what value of β does the channel stop acting like a single channel and start acting like a pair of channels?  
(b) Evaluate the capacity and find P₁ and P₂ that achieve capacity for β₁ = 1, β₂ = 2, N₁ = 3, N₂ = 2, and β = 10.  

9.9 Vector Gaussian channel.  
Consider the vector Gaussian noise channel Y = X + Z, where X = (X₁, X₂, X₃), Z = (Z₁, Z₂, Z₃), Y = (Y₁, Y₂, Y₃), E‖X‖² ≤ P, and Z ~ N(0, K) with K = [1 0 1; 0 1 1; 1 1 2]. Find the capacity. The answer may be surprising.  

9.10 Capacity of photographic film.  
Here is a problem with a nice answer that takes a little time. We’re interested in the capacity of photographic film. The film consists of silver iodide crystals, Poisson distributed, with a density of λ particles per square inch. The film is illuminated without knowledge of the position of the silver iodide particles. It is then developed, and the receiver sees only the silver iodide particles that have been illuminated. It is assumed that light incident on a cell exposes the grain if it is there and otherwise results in a blank response. Silver iodide particles that are not illuminated and vacant portions of the film remain blank. The question is: What is the capacity of this film?  

We make the following assumptions. We grid the film very finely into cells of area dA. It is assumed that there is at most one silver iodide particle per cell and that no silver iodide particle is intersected by the cell boundaries. Thus, the film can be considered to be a large number of parallel binary asymmetric channels with crossover probability 1 - λdA. By calculating the capacity of this binary asymmetric channel to first order in dA (making the necessary approximations), one can calculate the capacity of the film in bits per square inch. It is, of course, proportional to λ. The question is: What is the multiplicative constant?  

The answer would be λ bits per unit area if both the illuminator and receiver knew the positions of the crystals.  

9.11 Gaussian mutual information.  
Suppose that (X, Y, Z) are jointly Gaussian and that X → Y → Z forms a Markov chain. Let X and Y have correlation coefficient ρ₁ and let Y and Z have correlation coefficient ρ₂. Find I(X; Z).  

9.12 Time-varying channel.  
A train pulls out of the station at constant velocity. The received signal energy thus falls off with time as 1/i². The total received signal at time i is Y_i = (1/i) X_i + Z_i, where Z₁, Z₂, ... are i.i.d. ~ N(0, N). The transmitter constraint for block length n is (1/n) ∑_{i=1}^n E[X_i²] ≤ P, w ∈ {1, 2, ..., 2nR}. Using Fano’s inequality, show that the capacity C is equal to zero for this channel.  

9.13 Feedback capacity.  
Let (Z₁, Z₂) ~ N(0, K), K = [1 ρ; ρ 1]. Find the maximum of (1/2) log |K_X + Z| / |K_Z| with and without feedback given a trace (power) constraint tr(K_X) ≤ 2P.  

9.14 Additive noise channel.  
Consider the channel Y = X + Z, where X is the transmitted signal with power constraint P, Z is independent additive noise, and Y is the received signal. Let Z = [0 with probability 1/10; Z* with probability 9/10], where Z* ~ N(0, N). Thus, Z has a mixture distribution that is a mixture of a Gaussian distribution and a degenerate distribution with mass 1 at 0.  

(a) What is the capacity of this channel? This should be a pleasant surprise.  
(b) How would you signal to achieve capacity?  

9.15 Discrete input, continuous output channel.  
Let Pr{X = 1} = p, Pr{X = 0} = 1 - p, and let Y = X + Z, where Z is uniform over the interval [0, a], a > 1, and Z is independent of X.  

(a) Calculate I(X; Y) = H(X) - H(X|Y).  
(b) Now calculate I(X; Y) the other way by I(X; Y) = h(Y) - h(Y|X).  
(c) Calculate the capacity of this channel by maximizing over p.  

9.16 Gaussian mutual information.  
Suppose that (X, Y, Z) are jointly Gaussian and that X → Y → Z forms a Markov chain. Let X and Y have correlation coefficient ρ₁ and let Y and Z have correlation coefficient ρ₂. Find I(X; Z).  

9.17 Impulse power.  
Consider the additive white Gaussian channel Y_i = X_i + Z_i, where Z_i ~ N(0, N), and the input signal has average power constraint P.  

(a) Suppose that we use all our power at time 1 (i.e., E[X²₁] = nP and E[X²_i] = 0 for i = 2, 3, ..., n). Find max_{f(x^n)} (1/n) I(X^n; Y^n), where the maximization is over all distributions f(x^n) subject to the constraint E[X²₁] = nP and E[X²_i] = 0 for i = 2, 3, ..., n.  

(b) Find max_{f(x^n): (1/n) ∑_{i=1}^n E[X²_i] ≤ P} (1/n) I(X^n; Y^n) and compare to part (a).  

--- Page 22 ---  
296  
GAUSSIAN CHANNEL  
9.18 Gaussian channel with time-varying mean.  
Find the capacity of the following Gaussian channel:  

Z_i X_i Y_i  

Let Z₁, Z₂, ... be independent and let there be a power constraint P on x^n(W). Find the capacity when:  
(a) μ_i = 0, for all i.  
(b) μ_i = e_i, i = 1, 2, .... Assume that μ_i is known to the transmitter and receiver.  
(c) μ_i unknown, but μ_i i.i.d. ~ N(0, N) for all i.  

9.19 Parametric form for channel capacity.  
Consider m parallel Gaussian channels, Y_i = X_i + Z_i, where Z_i ~ N(0, λ_i) and the noises X_i are independent random variables. Thus, C = ∑_{i=1}^m (1/2) log(1 + (λ - λ_i)^+ / λ_i), where λ is chosen to satisfy ∑_{i=1}^m (λ - λ_i)^+ = P. Show that this can be rewritten in the form:  

P(λ) = ∑_{i: λ_i ≤ λ} (λ - λ_i)  
C(λ) = ∑_{i: λ_i ≤ λ} (1/2) log(λ / λ_i).  

Here P(λ) is piecewise linear and C(λ) is piecewise logarithmic in λ.  

9.20 Robust decoding.  
Consider an additive noise channel whose output Y is given by Y = X + Z, where the channel input X is average power limited, E[X²] ≤ P, and the noise process {Z_k}_{k=-∞}^∞ is i.i.d. with marginal distribution p_Z(z) (not necessarily Gaussian) of power N, E[Z²] = N.  

(a) Show that the channel capacity, C = max_{E[X²] ≤ P} I(X; Y), is lower bounded by C_G, where C_G = (1/2) log(1 + P / N) (i.e., the capacity C_G corresponding to white Gaussian noise).  

(b) Decoding the received vector to the codeword that is closest to it in Euclidean distance is in general suboptimal if the noise is non-Gaussian. Show, however, that the rate C_G is achievable even if one insists on performing nearest-neighbor decoding (minimum Euclidean distance decoding) rather than the optimal maximum-likelihood or joint typicality decoding (with respect to the true noise distribution).  

(c) Extend the result to the case where the noise is not i.i.d. but is stationary and ergodic with power N.  

(Hint for b and c: Consider a random codebook of size 2nR whose codewords are drawn independently of each other according to a uniform distribution over the n-dimensional sphere of radius √(nP).)  

(a) Using a symmetry argument, show that conditioned on the noise vector, the ensemble average probability of error depends on the noise vector only via its Euclidean norm ‖z‖.  
(b) Use a geometric argument to show that this dependence is monotonic.  
(c) Given a rate R < C_G, choose some N' > N such that R < (1/2) log(1 + P / N'). Compare the case where the noise is i.i.d. N(0, N') to the case at hand.  
(d) Conclude the proof using the fact that the above ensemble of codebooks can achieve the capacity of the Gaussian channel (no need to prove that).  

--- Page 24 ---  
298  
GAUSSIAN CHANNEL  
9.21 Mutual information game.  
Consider the following channel:  

Z X Y  

Throughout this problem, we shall constrain the signal power E[X] = 0, E[X²] = P, and the noise power E[Z] = 0, E[Z²] = N, and assume that X and Z are independent. The channel capacity is given by I(X; X + Z).  

Now for the game. The noise player chooses a distribution on Z to minimize I(X; X + Z), while the signal player chooses a distribution on X to maximize I(X; X + Z). Letting X* ~ N(0, P), Z* ~ N(0, N), show that Gaussian X* and Z* satisfy the saddle-point conditions:  

I(X; X + Z*) ≤ I(X*; X* + Z*) ≤ I(X*; X* + Z).  

Thus,  
min_Z max_X I(X; X + Z) = max_X min_Z I(X; X + Z)  
= (1/2) log(1 + P / N),  

and the game has a value. In particular, a deviation from normal for either player worsens the mutual information from that player’s standpoint. Can you discuss the implications of this?  

Note: Part of the proof hinges on the entropy power inequality from Section 17.8, which states that if X and Y are independent random n-vectors with densities, then  
2^{nh(X+Y)} ≥ 2^{nh(X)} + 2^{nh(Y)}.  

--- Page 25 ---  
HISTORICAL NOTES  
299  
The Gaussian channel was first analyzed by Shannon in his original paper [472]. The water-filling solution to the capacity of the colored noise Gaussian channel was developed by Shannon [480] and treated in detail by Pinsker [425]. The time-continuous Gaussian channel is treated in Wyner [576], Gallager [233], and Landau, Pollak, and Slepian [340, 341, 500].  

Pinsker [421] and Ebert [178] argued that feedback at most doubles the capacity of a nonwhite Gaussian channel; the proof in the text is from Cover and Pombra [136], who also show that feedback increases the capacity of the nonwhite Gaussian channel by at most half a bit.  

The most recent feedback capacity results for nonwhite Gaussian noise channels are due to Kim [314].  

--- Page 26 ---  

--- Page 27 ---  
CHAPTER 10  
RATE DISTORTION THEORY  
The description of an arbitrary real number requires an infinite number of bits, so a finite representation of a continuous random variable can never be perfect. How well can we do? To frame the question appropriately, it is necessary to define the "goodness" of a representation of a source. This is accomplished by defining a distortion measure, which is a measure of distance between the random variable and its representation.  

The basic problem in rate distortion theory can then be stated as follows: Given a source distribution and a distortion measure, what is the minimum expected distortion achievable at a particular rate? Or, equivalently, what is the minimum rate description required to achieve a particular distortion?  

One of the most intriguing aspects of this theory is that joint descriptions are more efficient than individual descriptions. It is simpler to describe an elephant and a chicken with one description than to describe each alone. This is true even for independent random variables. It is simpler to describe X₁ and X₂ together (at a given distortion for each) than to describe each by itself. Why don’t independent problems have independent solutions? The answer is found in the geometry. Apparently, rectangular grid points (arising from independent descriptions) do not fill up the space efficiently.  

Rate distortion theory can be applied to both discrete and continuous random variables. The zero-error data compression theory of Chapter 5 is an important special case of rate distortion theory applied to a discrete source with zero distortion. We begin by considering the simple problem of representing a single continuous random variable by a finite number of bits.  

10.1 QUANTIZATION  
In this section, we motivate the elegant theory of rate distortion by showing how complicated it is to solve the quantization problem exactly for a single continuous random variable.  

Elements of Information Theory, Second Edition,  
By Thomas M. Cover and Joy A. Thomas  
Copyright © 2006 John Wiley & Sons, Inc.  
301  

(Continued...)



RATE DISTORTION THEORY

A continuous random source requires infinite precision to represent exactly, so we cannot reproduce it exactly using a finite-rate code. The goal is to find the best possible representation for any given data rate. We start by considering the problem of representing a single sample from the source. Let the random variable be X, and let the representation of X be denoted as ˆX(X). If we are given R bits to represent X, the function ˆX can take on 2^R values. The problem is to find the optimum set of values for ˆX (called the reproduction points or code points) and the regions associated with each value ˆX.

For example, let X ∼ N(0, σ²), and assume a squared-error distortion measure. We wish to find the function ˆX(X) such that ˆX takes on at most 2^R values and minimizes E[(X - ˆX(X))²]. If we are given one bit to represent X, it is clear that the bit should distinguish whether or not X > 0. To minimize squared error, each reproduced symbol should be the conditional mean of its region. This is illustrated in Figure 10.1.

If we are given 2 bits to represent the sample, the situation is more complex. We want to divide the real line into four regions and use a point within each region to represent the sample. However, it is no longer immediately obvious what the representation regions and reconstruction points should be. We can state two properties of optimal regions and reconstruction points for quantization:

1. Given a set of reconstruction points {ˆX(w)}, the distortion is minimized by mapping a source random variable X to the representation ˆX(w) that is closest to it. The set of regions defined by this mapping is called a Voronoi or Dirichlet partition defined by the reconstruction points.
2. The reconstruction points should minimize the conditional expected distortion over their respective assignment regions.

These properties enable us to construct the Lloyd algorithm to find a "good" quantizer. We start with a set of reconstruction points, find the optimal regions (nearest-neighbor regions), then find the optimal reconstruction points (centroids if the distortion is squared error), and repeat. The algorithm converges to a local minimum of distortion.

Instead of quantizing a single random variable, consider a set of n i.i.d. random variables drawn from a Gaussian distribution. These are represented using nR bits. Since the source is i.i.d., the symbols are independent, but rate distortion theory shows that representing the entire sequence achieves lower distortion than independent quantization.

The rate distortion function R(D) is the infimum of rates R such that (R, D) is in the rate distortion region of the source for a given distortion D. The distortion rate function D(R) is the infimum of distortions D such that (R, D) is in the rate distortion region for a given rate R. The information rate distortion function R(I)(D) is defined as:

R(I)(D) = min_{p(ˆx|x): E[d(x, ˆx)] ≤ D} I(X; ˆX),

where the minimization is over conditional distributions p(ˆx|x) satisfying the expected distortion constraint.

The main theorem states that the rate distortion function for an i.i.d. source with distribution p(x) and bounded distortion function d(x, ˆx) is equal to the information rate distortion function:

R(D) = R(I)(D) = min_{p(ˆx|x): E[d(x, ˆx)] ≤ D} I(X; ˆX).

This theorem shows that the operational definition of the rate distortion function equals the information definition. We use R(D) to denote both definitions.

For a binary source with Hamming distortion, the rate distortion function is:

R(D) = H(p) - H(D), for 0 ≤ D ≤ min{p, 1 - p}, and 0 otherwise.

This is achieved by a joint distribution that meets the distortion constraint and has I(X; ˆX) = R(D). The proof involves showing that the lower bound is achievable using a binary symmetric channel.



### CALCULATION OF THE RATE DISTORTION FUNCTION

We choose the distribution of ˆX at the input of the channel so that the output distribution of X is the specified distribution. Let \( r = \Pr(\hat{X} = 1) \). Then choose \( r \) so that  
\[
r(1 - D) + (1 - r)D = p,
\]  
or  
\[
r = \frac{p - D}{1 - 2D}.
\]  
If \( D \leq p \leq \frac{1}{2} \), then \( \Pr(\hat{X} = 1) \geq 0 \) and \( \Pr(\hat{X} = 0) \geq 0 \). We then have  
\[
I(X; \hat{X}) = H(X) - H(X|\hat{X}) = H(p) - H(D),
\]  
and the expected distortion is \( \Pr(X \neq \hat{X}) = D \).  

If \( D \geq p \), we can achieve \( R(D) = 0 \) by letting \( \hat{X} = 0 \) with probability 1. In this case, \( I(X; \hat{X}) = 0 \) and \( D = p \). Similarly, if \( D \geq 1 - p \), we can achieve \( R(D) = 0 \) by setting \( \hat{X} = 1 \) with probability 1. Hence, the rate distortion function for a binary source is  
\[
R(D) = 
\begin{cases}
H(p) - H(D), & 0 \leq D \leq \min\{p, 1 - p\}, \\
0, & D > \min\{p, 1 - p\}.
\end{cases}
\]  
This function is illustrated in Figure 10.4.  

---

### RATE DISTORTION THEORY

The above calculations may seem entirely unmotivated. Why should minimizing mutual information have anything to do with quantization? The answer to this question must wait until we prove Theorem 10.2.1.  

#### Gaussian Source

Although Theorem 10.2.1 is proved only for discrete sources with a bounded distortion measure, it can also be proved for well-behaved continuous sources and unbounded distortion measures. Assuming this general theorem, we calculate the rate distortion function for a Gaussian source with squared-error distortion.  

**Theorem 10.3.2**  
The rate distortion function for a \( \mathcal{N}(0, \sigma^2) \) source with squared-error distortion is  
\[
R(D) = 
\begin{cases}
\frac{1}{2} \log \frac{\sigma^2}{D}, & 0 \leq D \leq \sigma^2, \\
0, & D > \sigma^2.
\end{cases}
\]  

**Proof:**  
Let \( X \) be \( \mathcal{N}(0, \sigma^2) \). By the rate distortion theorem extended to continuous alphabets, we have  
\[
R(D) = \min_{f(\hat{x}|x): \mathbb{E}(\hat{X} - X)^2 \leq D} I(X; \hat{X}).
\]  
As in the preceding example, we first find a lower bound for the rate distortion function and then prove that this is achievable. Since \( \mathbb{E}(X - \hat{X})^2 \leq D \), we observe that  
\[
I(X; \hat{X}) = h(X) - h(X|\hat{X}),
\]  
\[
= \frac{1}{2} \log(2\pi e \sigma^2) - h(X - \hat{X}|\hat{X}),
\]  
\[
\geq \frac{1}{2} \log(2\pi e \sigma^2) - h(X - \hat{X}),
\]  
\[
\geq \frac{1}{2} \log(2\pi e \sigma^2) - h(\mathcal{N}(0, \mathbb{E}(X - \hat{X})^2)),
\]  
\[
= \frac{1}{2} \log(2\pi e \sigma^2) - \frac{1}{2} \log(2\pi e D),
\]  
\[
= \frac{1}{2} \log \frac{\sigma^2}{D}.
\]  
Hence,  
\[
R(D) \geq \frac{1}{2} \log \frac{\sigma^2}{D}.
\]  

To find the conditional density \( f(\hat{x}|x) \) that achieves this lower bound, it is usually more convenient to look at the conditional density \( f(x|\hat{x}) \), which is sometimes called the test channel. As in the binary case, we construct \( f(x|\hat{x}) \) to achieve equality in the bound. We choose the joint distribution as shown in Figure 10.5. If \( D \leq \sigma^2 \), we choose  
\[
X = \hat{X} + Z, \quad \hat{X} \sim \mathcal{N}(0, \sigma^2 - D), \quad Z \sim \mathcal{N}(0, D),
\]  
where \( \hat{X} \) and \( Z \) are independent. For this joint distribution, we calculate  
\[
I(X; \hat{X}) = \frac{1}{2} \log \frac{\sigma^2}{D},
\]  
and \( \mathbb{E}(X - \hat{X})^2 = D \), thus achieving the bound in (10.33). If \( D > \sigma^2 \), we choose \( \hat{X} = 0 \) with probability 1, achieving \( R(D) = 0 \). Hence, the rate distortion function for the Gaussian source with squared-error distortion is  
\[
R(D) = 
\begin{cases}
\frac{1}{2} \log \frac{\sigma^2}{D}, & 0 \leq D \leq \sigma^2, \\
0, & D > \sigma^2,
\end{cases}
\]  
as illustrated in Figure 10.6.  

We can rewrite (10.36) to express the distortion in terms of the rate,  
\[
D(R) = \sigma^2 2^{-2R}.
\]  

---

### SIMULTANEOUS DESCRIPTION OF INDEPENDENT GAUSSIAN RANDOM VARIABLES

Consider the case of representing \( m \) independent (but not identically distributed) normal random sources \( X_1, \dots, X_m \), where \( X_i \sim \mathcal{N}(0, \sigma_i^2) \), with squared-error distortion. Assume that we are given \( R \) bits with which to represent this random vector. The question naturally arises as to how we should allot these bits to the various components to minimize the total distortion. Extending the definition of the information rate distortion function to the vector case, we have  
\[
R(D) = \min_{f(\hat{x}_m|x_m): \mathbb{E}d(x_m, \hat{x}_m) \leq D} I(X_m; \hat{X}_m),
\]  
where \( d(x_m, \hat{x}_m) = \sum_{i=1}^m (x_i - \hat{x}_i)^2 \). Now using the arguments in the preceding example, we have  
\[
I(X_m; \hat{X}_m) = h(X_m) - h(X_m|\hat{X}_m),
\]  
\[
= \sum_{i=1}^m h(X_i) - \sum_{i=1}^m h(X_i|\hat{X}_m),
\]  
\[
\geq \sum_{i=1}^m h(X_i) - \sum_{i=1}^m h(X_i|\hat{X}_i),
\]  
\[
= \sum_{i=1}^m I(X_i; \hat{X}_i),
\]  
\[
\geq \sum_{i=1}^m \frac{1}{2} \log \frac{\sigma_i^2}{D_i},
\]  
where \( D_i = \mathbb{E}(X_i - \hat{X}_i)^2 \) and (10.41) follows from the fact that conditioning reduces entropy. We can achieve equality in (10.41) by choosing \( f(x_m|\hat{x}_m) = \prod_{i=1}^m f(x_i|\hat{x}_i) \) and in (10.43) by choosing the distribution of each \( \hat{X}_i \sim \mathcal{N}(0, \sigma_i^2 - D_i) \), as in the preceding example. Hence, the problem of finding the rate distortion function can be reduced to the following optimization (using nats for convenience):  
\[
R(D) = \min_{\sum_{i=1}^m D_i = D} \sum_{i=1}^m \max\left(\frac{1}{2} \log \frac{\sigma_i^2}{D_i}, 0\right).
\]  

Using Lagrange multipliers, we construct the functional  
\[
J(D) = \sum_{i=1}^m \frac{1}{2} \log \frac{\sigma_i^2}{D_i} + \lambda \sum_{i=1}^m D_i,
\]  
and differentiating with respect to \( D_i \) and setting equal to 0, we have  
\[
\frac{\partial J}{\partial D_i} = -\frac{1}{2D_i} + \lambda = 0,
\]  
or  
\[
D_i = \lambda'.
\]  

Hence, the optimum allotment of the bits to the various descriptions results in an equal distortion for each random variable. This is possible if the constant \( \lambda' \) in (10.48) is less than \( \sigma_i^2 \) for all \( i \). As the total allowable distortion \( D \) is increased, the constant \( \lambda' \) increases until it exceeds \( \sigma_i^2 \) for some \( i \). At this point the solution (10.48) is on the boundary of the allowable region of distortions. If we increase the total distortion, we must use the Kuhn–Tucker conditions to find the minimum in (10.46). In this case the Kuhn–Tucker conditions yield  
\[
\frac{\partial J}{\partial D_i} = -\frac{1}{2D_i} + \lambda,
\]  
where \( \lambda \) is chosen so that  
\[
\frac{\partial J}{\partial D_i} \leq 0 \quad \text{if} \quad D_i < \sigma_i^2 \leq 0 \quad \text{if} \quad D_i \geq \sigma_i^2.
\]  

It is easy to check that the solution to the Kuhn–Tucker equations is given by the following theorem:  

**Theorem 10.3.3 (Rate distortion for a parallel Gaussian source)**  
Let \( X_i \sim \mathcal{N}(0, \sigma_i^2) \), \( i = 1, 2, \dots, m \), be independent Gaussian random variables, and let the distortion measure be \( d(x_m, \hat{x}_m) = \sum_{i=1}^m (x_i - \hat{x}_i)^2 \). Then the rate distortion function is given by  
\[
R(D) = \sum_{i=1}^m \frac{1}{2} \log \frac{\sigma_i^2}{D_i},
\]  
where  
\[
D_i = \begin{cases}
\lambda, & \lambda < \sigma_i^2, \\
\sigma_i^2, & \lambda \geq \sigma_i^2,
\end{cases}
\]  
where \( \lambda \) is chosen so that \( \sum_{i=1}^m D_i = D \).  

---

### CONVERSE TO THE RATE DISTORTION THEOREM

In this section we prove the converse to Theorem 10.2.1 by showing that we cannot achieve a distortion of less than \( D \) if we describe \( X \) at a rate less than \( R(D) \), where  
\[
R(D) = \min_{p(\hat{x}|x): \mathbb{E}d(x, \hat{x}) \leq D} I(X; \hat{X}).
\]



RATE DISTORTION THEORY

The minimization is over all conditional distributions \( p(\hat{x}|x) \) for which the joint distribution \( p(x, \hat{x}) = p(x)p(\hat{x}|x) \) satisfies the expected distortion constraint. Before proving the converse, we establish some simple properties of the information rate distortion function.

**Lemma 10.4.1 (Convexity of \( R(D) \))**  
The rate distortion function \( R(D) \) given in (10.53) is a nonincreasing convex function of \( D \).

**Proof:**  
\( R(D) \) is the minimum of the mutual information over increasingly larger sets as \( D \) increases. Thus, \( R(D) \) is nonincreasing in \( D \). To prove that \( R(D) \) is convex, consider two rate distortion pairs, \( (R_1, D_1) \) and \( (R_2, D_2) \), which lie on the rate distortion curve. Let the joint distributions that achieve these pairs be \( p_1(x, \hat{x}) = p(x)p_1(\hat{x}|x) \) and \( p_2(x, \hat{x}) = p(x)p_2(\hat{x}|x) \). Consider the distribution \( p_\lambda = \lambda p_1 + (1 - \lambda) p_2 \). Since the distortion is a linear function of the distribution, we have \( D(p_\lambda) = \lambda D_1 + (1 - \lambda) D_2 \). Mutual information, on the other hand, is a convex function of the conditional distribution (Theorem 2.7.4), and hence  
\[
I_{p_\lambda}(X; \hat{X}) \leq \lambda I_{p_1}(X; \hat{X}) + (1 - \lambda) I_{p_2}(X; \hat{X}).
\]  
Hence, by the definition of the rate distortion function,  
\[
R(D_\lambda) \leq I_{p_\lambda}(X; \hat{X}) \leq \lambda R(D_1) + (1 - \lambda) R(D_2),
\]  
which proves that \( R(D) \) is a convex function of \( D \).  

The converse can now be proved.

**Proof (Converse in Theorem 10.2.1):**  
We must show that for any source \( X \) drawn i.i.d. \( \sim p(x) \) with distortion measure \( d(x, \hat{x}) \) and any \( (2nR, n) \) rate distortion code with distortion \( \leq D \), the rate \( R \) of the code satisfies \( R \geq R(D) \). In fact, we prove that \( R \geq R(D) \) even for randomized mappings \( f_n \) and \( g_n \), as long as \( f_n \) takes on at most \( 2nR \) values.

Consider any \( (2nR, n) \) rate distortion code defined by functions \( f_n \) and \( g_n \) as given in (10.7) and (10.8). Let \( \hat{X}_n = \hat{X}_n(X_n) = g_n(f_n(X_n)) \) be the reproduced sequence corresponding to \( X_n \). Assume that \( E[d(X_n, \hat{X}_n)] \geq D \) for this code. Then we have the following chain of inequalities:

\[
nR \geq H(f_n(X_n)) \geq H(f_n(X_n)) - H(f_n(X_n)|X_n) = I(X_n; f_n(X_n)) \geq I(X_n; \hat{X}_n) = H(X_n) - H(X_n|\hat{X}_n) = \sum_{i=1}^n H(X_i) - \sum_{i=1}^n H(X_i|\hat{X}_n, X_{i-1}, \ldots, X_1) \geq \sum_{i=1}^n H(X_i) - \sum_{i=1}^n H(X_i|\hat{X}_i) = \sum_{i=1}^n I(X_i; \hat{X}_i) \geq \sum_{i=1}^n R(E[d(X_i, \hat{X}_i)]) = n \cdot \frac{1}{n} \sum_{i=1}^n R(E[d(X_i, \hat{X}_i)]) \geq nR\left(\frac{1}{n} \sum_{i=1}^n E[d(X_i, \hat{X}_i)]\right) = nR(D),
\]  
where  
(a) follows from the fact that the range of \( f_n \) is at most \( 2nR \),  
(b) follows from the fact that \( H(f_n(X_n)|X_n) \geq 0 \),  
(c) follows from the data-processing inequality,  
(d) follows from the fact that the \( X_i \) are independent,  
(e) follows from the chain rule for entropy,  
(f) follows from the fact that conditioning reduces entropy,  
(g) follows from the definition of the rate distortion function,  
(h) follows from the convexity of the rate distortion function (Lemma 10.4.1) and Jensen’s inequality,  
(i) follows from the definition of distortion for blocks of length \( n \),  
(j) follows from the fact that \( R(D) \) is a nonincreasing function of \( D \) and \( E[d(X_n, \hat{X}_n)] \leq D \).  

This shows that the rate \( R \) of any rate distortion code exceeds the rate distortion function \( R(D) \) evaluated at the distortion level \( D = E[d(X_n, \hat{X}_n)] \) achieved by that code.

A similar argument can be applied when the encoded source is passed through a noisy channel, and hence we have the equivalent of the source-channel separation theorem with distortion:

**Theorem 10.4.1 (Source–channel separation theorem with distortion):**  
Let \( V_1, V_2, \ldots, V_n \) be a finite alphabet i.i.d. source which is encoded as a sequence of \( n \) input symbols \( X_n \) of a discrete memoryless channel with capacity \( C \). The output of the channel \( Y_n \) is mapped onto the reconstruction alphabet \( \hat{V}_n = g(Y_n) \). Let \( D = E[d(V_n, \hat{V}_n)] = \frac{1}{n} \sum_{i=1}^n E[d(V_i, \hat{V}_i)] \) be the average distortion achieved by this combined source and channel coding scheme. Then distortion \( D \) is achievable if and only if \( C > R(D) \).

**Proof:**  
See Problem 10.17.

### 10.5 Achievability of the Rate Distortion Function

We now prove the achievability of the rate distortion function. We begin with a modified version of the joint AEP in which we add the condition that the pair of sequences be typical with respect to the distortion measure.

**Definition:**  
Let \( p(x, \hat{x}) \) be a joint probability distribution on \( X \times \hat{X} \) and let \( d(x, \hat{x}) \) be a distortion measure on \( X \times \hat{X} \). For any \( \epsilon > 0 \), a pair of sequences \( (x^n, \hat{x}^n) \) is said to be distortion \( \epsilon \)-typical or simply distortion typical if  
\[
\left| \frac{1}{n} \log p(x^n) - H(X) \right| < \epsilon,
\]  
\[
\left| \frac{1}{n} \log p(\hat{x}^n) - H(\hat{X}) \right| < \epsilon,
\]  
\[
\left| \frac{1}{n} \log p(x^n, \hat{x}^n) - H(X, \hat{X}) \right| < \epsilon,
\]  
\[
\left| d(x^n, \hat{x}^n) - E[d(X, \hat{X})] \right| < \epsilon.
\]  
The set of distortion typical sequences is called the distortion typical set and is denoted \( A^{(n)}_{d,\epsilon} \). Note that this is the definition of the jointly typical set (Section 7.6) with the additional constraint that the distortion be close to the expected value. Hence, the distortion typical set is a subset of the jointly typical set (i.e., \( A^{(n)}_{d,\epsilon} \subset A^{(n)}_{\epsilon} \)). If \( (X_i, \hat{X}_i) \) are drawn i.i.d. \( \sim p(x, \hat{x}) \), the distortion between two random sequences  
\[
d(X^n, \hat{X}^n) = \frac{1}{n} \sum_{i=1}^n d(X_i, \hat{X}_i)
\]  
is an average of i.i.d. random variables, and the law of large numbers implies that it is close to its expected value with high probability. Hence, we have the following lemma.

**Lemma 10.5.1:**  
Let \( (X_i, \hat{X}_i) \) be drawn i.i.d. \( \sim p(x, \hat{x}) \). Then \( \Pr(A^{(n)}_{d,\epsilon}) \to 1 \) as \( n \to \infty \).

**Proof:**  
The sums in the four conditions in the definition of \( A^{(n)}_{d,\epsilon} \) are all normalized sums of i.i.d. random variables and hence, by the law of large numbers, tend to their respective expected values with probability 1. Hence, the set of sequences satisfying all four conditions has probability tending to 1 as \( n \to \infty \).

The following lemma is a direct consequence of the definition of the distortion typical set.

**Lemma 10.5.2:**  
For all \( (x^n, \hat{x}^n) \in A^{(n)}_{d,\epsilon} \),  
\[
p(\hat{x}^n) \geq p(\hat{x}^n|x^n) 2^{-n(I(X; \hat{X}) + 3\epsilon)}.
\]

**Proof:**  
Using the definition of \( A^{(n)}_{d,\epsilon} \), we can bound the probabilities \( p(x^n) \), \( p(\hat{x}^n) \), and \( p(x^n, \hat{x}^n) \) for all \( (x^n, \hat{x}^n) \in A^{(n)}_{d,\epsilon} \), and hence  
\[
p(\hat{x}^n|x^n) = \frac{p(x^n, \hat{x}^n)}{p(x^n)} \geq \frac{2^{-n(H(X, \hat{X}) - \epsilon)}}{2^{-n(H(X) + \epsilon)}} = 2^{-n(I(X; \hat{X}) + 2\epsilon)}.
\]  
Thus,  
\[
p(\hat{x}^n) \geq p(\hat{x}^n|x^n) 2^{-n(I(X; \hat{X}) + 3\epsilon)}.
\]

We also need the following interesting inequality.

**Lemma 10.5.3:**  
For \( 0 \leq x, y \leq 1 \), \( n > 0 \),  
\[
(1 - xy)^n \leq 1 - x + e^{-yn}.
\]

**Proof:**  
Let \( f(y) = e^{-y} - 1 + y \). Then \( f(0) = 0 \) and \( f'(y) = -e^{-y} + 1 > 0 \) for \( y > 0 \), and hence \( f(y) > 0 \) for \( y > 0 \). Hence, for \( 0 \leq y \leq 1 \), we have \( 1 - y \leq e^{-y} \), and raising this to the \( n \)-th power, we obtain  
\[
(1 - y)^n \leq e^{-yn}.
\]  
Thus, the lemma is satisfied for \( x = 1 \). By examination, it is clear that the inequality is also satisfied for \( x = 0 \). By differentiation, it is easy to see that \( g_y(x) = (1 - xy)^n \) is a convex function of \( x \), and hence for \( 0 \leq x \leq 1 \), we have  
\[
(1 - xy)^n = g_y(x) \leq (1 - x)g_y(0) + x g_y(1) = (1 - x)1 + x(1 - y)^n \leq 1 - x + x e^{-yn}.
\]

We use the preceding proof to prove the achievability of Theorem 10.2.1.

**Proof (Achievability in Theorem 10.2.1):**  
Let \( X_1, X_2, \ldots, X_n \) be drawn i.i.d. \( \sim p(x) \) and let \( d(x, \hat{x}) \) be a bounded distortion measure for this source. Let the rate distortion function for this source be \( R(D) \). Then for any \( D \), and any \( R > R(D) \), we will show that the rate distortion pair \( (R, D) \) is achievable by proving the existence of a sequence of rate distortion codes with rate \( R \) and asymptotic distortion \( D \). Fix \( p(\hat{x}|x) \), where \( p(\hat{x}|x) \) achieves equality in (10.53). Thus, \( I(X; \hat{X}) = R(D) \). Calculate \( p(\hat{x}) = \sum_x p(x)p(\hat{x}|x) \). Choose \( \delta > 0 \). We will prove the existence of a rate distortion code with rate \( R \) and distortion less than or equal to \( D + \delta \).

**Generation of codebook:**  
Randomly generate a rate distortion codebook \( C \) consisting of \( 2nR \) sequences \( \hat{X}_n \) drawn i.i.d. \( \sim p(\hat{x}) \). Index these codewords by \( w \in \{1, 2, \ldots, 2nR\} \). Reveal this codebook to the encoder and decoder.

**Encoding:**  
Encode \( X_n \) by \( w \) if there exists a \( w \) such that \( (X_n, \hat{X}_n(w)) \in A^{(n)}_{d,\epsilon} \). If there is more than one such \( w \), send the least. If there is no such \( w \), let \( w = 1 \). Thus, \( nR \) bits suffice to describe the index \( w \) of the jointly typical codeword.

**Decoding:**  
The reproduced sequence is \( \hat{X}_n(w) \).

**Calculation of distortion:**  
As in the case of the channel coding theorem, we calculate the expected distortion over the random choice of codebooks \( C \) as  
\[
D = E_{X_n, C} [d(X_n, \hat{X}_n)],
\]  
where the expectation is over the random choice of codebooks and over \( X_n \).

For a fixed codebook \( C \) and choice of \( \epsilon > 0 \), we divide the sequences \( x^n \in X^n \) into two categories:  
1. Sequences \( x^n \) such that there exists a codeword \( \hat{X}_n(w) \) that is distortion typical with \( x^n \) [i.e., \( d(x^n, \hat{x}^n(w)) < D + \epsilon \)]. Since the total probability of these sequences is at most 1, these sequences contribute at most \( D + \epsilon \) to the expected distortion.  
2. Sequences \( x^n \) such that there does not exist a codeword \( \hat{X}_n(w) \) that is distortion typical with \( x^n \). Let \( P_e \) be the total probability of these sequences. Since the distortion for any individual sequence is bounded by \( d_{\text{max}} \), these sequences contribute at most \( P_e d_{\text{max}} \) to the expected distortion.

Hence, we can bound the total distortion by  
\[
E[d(X_n, \hat{X}_n(X_n))] \leq D + \epsilon + P_e d_{\text{max}}.
\]



RATE DISTORTION THEORY

which can be made less than \( D + \delta \) for an appropriate choice of \( \varepsilon \) if \( P_e \) is small enough. Hence, if we show that \( P_e \) is small, the expected distortion is close to \( D \) and the theorem is proved.

Calculation of \( P_e \): We must bound the probability that for a random choice of codebook \( C \) and a randomly chosen source sequence, there is no codeword that is distortion typical with the source sequence. Let \( J(C) \) denote the set of source sequences \( x^n \) such that at least one codeword in \( C \) is distortion typical with \( x^n \). Then

\[
P_e = \sum_{C} P(C) \sum_{x^n: x^n \notin J(C)} p(x^n).
\]

This is the probability of all sequences not well represented by a code, averaged over the randomly chosen code. By changing the order of summation, we can also interpret this as the probability of choosing a codebook that does not well represent sequence \( x^n \), averaged with respect to \( p(x^n) \). Thus,

\[
P_e = \sum_{x^n} p(x^n) \sum_{C: x^n \notin J(C)} p(C).
\]

Let us define

\[
K(x^n, \hat{x}^n) = 
\begin{cases}
1 & \text{if } (x^n, \hat{x}^n) \in A(n)^{d,\varepsilon}, \\
0 & \text{otherwise}.
\end{cases}
\]

The probability that a single randomly chosen codeword \( \hat{X}^n \) does not well represent a fixed \( x^n \) is

\[
\Pr((x^n, \hat{X}^n) \notin A(n)^{d,\varepsilon}) = \Pr(K(x^n, \hat{X}^n) = 0) = 1 - \sum_{\hat{x}^n} p(\hat{x}^n) K(x^n, \hat{x}^n),
\]

and therefore the probability that \( 2nR \) independently chosen codewords do not represent \( x^n \), averaged over \( p(x^n) \), is

\[
P_e = \sum_{x^n} p(x^n) \left(1 - \sum_{\hat{x}^n} p(\hat{x}^n) K(x^n, \hat{x}^n)\right)^{2nR}.
\]

We now use Lemma 10.5.2 to bound the sum within the brackets. From Lemma 10.5.2, it follows that

\[
\sum_{\hat{x}^n} p(\hat{x}^n) K(x^n, \hat{x}^n) \geq \sum_{\hat{x}^n} p(\hat{x}^n|x^n) 2^{-n(I(X; \hat{X}) + 3\varepsilon)} K(x^n, \hat{x}^n),
\]

and hence

\[
P_e \leq \sum_{x^n} p(x^n) \left(1 - 2^{-n(I(X; \hat{X}) + 3\varepsilon)} \sum_{\hat{x}^n} p(\hat{x}^n|x^n) K(x^n, \hat{x}^n)\right)^{2nR}.
\]

We now use Lemma 10.5.3 to bound the term on the right-hand side of the above inequality and obtain

\[
\left(1 - 2^{-n(I(X; \hat{X}) + 3\varepsilon)} \sum_{\hat{x}^n} p(\hat{x}^n|x^n) K(x^n, \hat{x}^n)\right)^{2nR} \leq 1 - \sum_{\hat{x}^n} p(\hat{x}^n|x^n) K(x^n, \hat{x}^n) + e^{-(2^{-n(I(X; \hat{X}) + 3\varepsilon)) 2nR}.
\]

Substituting this inequality, we obtain

\[
P_e \leq 1 - \sum_{x^n} \sum_{\hat{x}^n} p(x^n) p(\hat{x}^n|x^n) K(x^n, \hat{x}^n) + e^{-2n(R - I(X; \hat{X}) - 3\varepsilon)}.
\]

The last term in the bound is equal to

\[
e^{-2n(R - I(X; \hat{X}) - 3\varepsilon)},
\]

which goes to zero exponentially fast with \( n \) if \( R > I(X; \hat{X}) + 3\varepsilon \). Hence, if we choose \( p(\hat{x}|x) \) to be the conditional distribution that achieves the minimum in the rate distortion function, then \( R > R(D) \) implies that \( R > I(X; \hat{X}) \) and we can choose \( \varepsilon \) small enough so that the last term in the bound goes to 0.

The first two terms in the bound give the probability under the joint distribution \( p(x^n, \hat{x}^n) \) that the pair of sequences is not distortion typical. Hence, using Lemma 10.5.1, we obtain

\[
1 - \sum_{x^n} \sum_{\hat{x}^n} p(x^n, \hat{x}^n) K(x^n, \hat{x}^n) = \Pr((X^n, \hat{X}^n) \notin A(n)^{d,\varepsilon}) < \varepsilon
\]

for \( n \) sufficiently large. Therefore, by an appropriate choice of \( \varepsilon \) and \( n \), we can make \( P_e \) as small as we like.

So, for any choice of \( \delta > 0 \), there exists an \( \varepsilon \) and \( n \) such that over all randomly chosen rate \( R \) codes of block length \( n \), the expected distortion is less than \( D + \delta \). Hence, there must exist at least one code \( C^* \) with this rate and block length with average distortion less than \( D + \delta \). Since \( \delta \) was arbitrary, we have shown that \( (R, D) \) is achievable if \( R > R(D) \).

We have proved the existence of a rate distortion code with an expected distortion close to \( D \) and a rate close to \( R(D) \). The similarities between the random coding proof of the rate distortion theorem and the random coding proof of the channel coding theorem are now evident. We will explore the parallels further by considering the Gaussian example, which provides some geometric insight into the problem. It turns out that channel coding is sphere packing and rate distortion coding is sphere covering.

Channel coding for the Gaussian channel. Consider a Gaussian channel, \( Y_i = X_i + Z_i \), where the \( Z_i \) are i.i.d. \( \sim N(0, N) \) and there is a power constraint \( P \) on the power per symbol of the transmitted codeword. Consider a sequence of \( n \) transmissions. The power constraint implies that the transmitted sequence lies within a sphere of radius \( \sqrt{nP} \) in \( \mathbb{R}^n \). The coding problem is equivalent to finding a set of \( 2nR \) sequences within this sphere such that the probability of any of them being mistaken for any other is small—the spheres of radius \( \sqrt{nN} \) around each of them are almost disjoint. This corresponds to filling a sphere of radius \( \sqrt{n(P + N)} \) with spheres of radius \( \sqrt{nN} \). One would expect that the largest number of spheres that could be fit would be the ratio of their volumes, or, equivalently, the \( n \)-th power of the ratio of their radii. Thus, if \( M \) is the number of codewords that can be transmitted efficiently, we have

\[
M \leq \left(\frac{\sqrt{n(P + N)}}{\sqrt{nN}}\right)^n = \left(\frac{P + N}{N}\right)^n.
\]

The results of the channel coding theorem show that it is possible to do this efficiently for large \( n \); it is possible to find approximately

\[
2nC = \left(\frac{P + N}{N}\right)^n
\]

codewords such that the noise spheres around them are almost disjoint (the total volume of their intersection is arbitrarily small).

Rate distortion for the Gaussian source. Consider a Gaussian source of variance \( \sigma^2 \). A \( (2nR, n) \) rate distortion code for this source with distortion \( D \) is a set of \( 2nR \) sequences in \( \mathbb{R}^n \) such that most source sequences of length \( n \) (all those that lie within a sphere of radius \( \sqrt{n\sigma^2} \)) are within a distance \( \sqrt{nD} \) of some codeword. Again, by the sphere-packing argument, it is clear that the minimum number of codewords required is

\[
2nR(D) = \left(\frac{\sigma^2}{D}\right)^n.
\]

The rate distortion theorem shows that this minimum rate is asymptotically achievable (i.e., that there exists a collection of spheres of radius \( \sqrt{nD} \) that cover the space except for a set of arbitrarily small probability).

The above geometric arguments also enable us to transform a good code for channel transmission into a good code for rate distortion. In both cases, the essential idea is to fill the space of source sequences: In channel transmission, we want to find the largest set of codewords that have a large minimum distance between codewords, whereas in rate distortion, we wish to find the smallest set of codewords that covers the entire space. If we have any set that meets the sphere packing bound for one, it will meet the sphere packing bound for the other. In the Gaussian case, choosing the codewords to be Gaussian with the appropriate variance is asymptotically optimal for both rate distortion and channel coding.

STRONGLY TYPICAL SEQUENCES AND RATE DISTORTION

In Section 10.5, we proved the existence of a rate distortion code of rate \( R(D) \) with average distortion close to \( D \). In fact, not only is the average distortion close to \( D \), but the total probability that the distortion is greater than \( D + \delta \) is close to 0. The proof of this is similar to the proof in Section 10.5; the main difference is that we will use strongly typical sequences rather than weakly typical sequences. This will enable us to give an upper bound to the probability that a typical source sequence is not well represented by a randomly chosen codeword in (10.94). We now outline an alternative proof based on strong typicality that will provide a stronger and more intuitive approach to the rate distortion theorem.

We begin by defining strong typicality and quoting a basic theorem bounding the probability that two sequences are jointly typical. The properties of strong typicality were introduced by Berger and explored in detail in the book by Csiszár and Körner. We will define strong typicality (as in Chapter 11) and state a fundamental lemma (Lemma 10.6.2).

Definition: A sequence \( x^n \in X^n \) is said to be \( \varepsilon \)-strongly typical with respect to a distribution \( p(x) \) on \( X \) if:

1. For all \( a \in X \) with \( p(a) > 0 \), we have

\[
\left| \frac{N(a|x^n)}{n} - p(a) \right| < \frac{\varepsilon}{|X|}.
\]

2. For all \( a \in X \) with \( p(a) = 0 \), \( N(a|x^n) = 0 \).

\( N(a|x^n) \) is the number of occurrences of the symbol \( a \) in the sequence \( x^n \).

The set of sequences \( x^n \in X^n \) such that \( x^n \) is strongly typical is called the strongly typical set and is denoted \( A^*(n)^{\varepsilon}(X) \) or \( A^*(n)^{\varepsilon} \) when the random variable is understood from the context.

Definition: A pair of sequences \( (x^n, y^n) \in X^n \times Y^n \) is said to be \( \varepsilon \)-strongly typical with respect to a distribution \( p(x, y) \) on \( X \times Y \) if:

1. For all \( (a, b) \in X \times Y \) with \( p(a, b) > 0 \), we have

\[
\left| \frac{N(a, b|x^n, y^n)}{n} - p(a, b) \right| < \frac{\varepsilon}{|X||Y|}.
\]

2. For all \( (a, b) \in X \times Y \) with \( p(a, b) = 0 \), \( N(a, b|x^n, y^n) = 0 \).

\( N(a, b|x^n, y^n) \) is the number of occurrences of the pair \( (a, b) \) in the pair of sequences \( (x^n, y^n) \).

The set of sequences \( (x^n, y^n) \in X^n \times Y^n \) such that \( (x^n, y^n) \) is strongly typical is called the strongly typical set and is denoted \( A^*(n)^{\varepsilon}(X, Y) \) or \( A^*(n)^{\varepsilon} \). From the definition, it follows that if \( (x^n, y^n) \in A^*(n)^{\varepsilon}(X, Y) \), then \( x^n \in A^*(n)^{\varepsilon}(X) \). From the strong law of large numbers, the following lemma is immediate.

Lemma 10.6.1: Let \( (X_i, Y_i) \) be drawn i.i.d. \( \sim p(x, y) \). Then \( \Pr(A^*(n)^{\varepsilon}) \to 1 \) as \( n \to \infty \).

We will use one basic result, which bounds the probability that an independently drawn sequence will be seen as jointly strongly typical with a given sequence. Theorem 7.6.1 shows that if we choose \( X^n \) and \( Y^n \) independently, the probability that they will be weakly jointly typical is \( \approx 2^{-nI(X; Y)} \). The following lemma extends the result to strongly typical sequences. This is stronger than the earlier result in that it gives a lower bound on the probability that a randomly chosen sequence is jointly typical with a fixed typical \( x^n \).

Lemma 10.6.2: Let \( Y_1, Y_2, \ldots, Y_n \) be drawn i.i.d. \( \sim p(y) \). For \( x^n \in A^*(n)^{\varepsilon}(X) \), the probability that \( (x^n, Y^n) \in A^*(n)^{\varepsilon} \) is bounded by

\[
2^{-n(I(X; Y) + \varepsilon_1)} \leq \Pr((x^n, Y^n) \in A^*(n)^{\varepsilon}) \leq 2^{-n(I(X; Y) - \varepsilon_1)},
\]

where \( \varepsilon_1 \to 0 \) as \( \varepsilon \to 0 \) and \( n \to \infty \).

We will not prove this lemma but instead outline the proof in Problem 10.16 at the end of the chapter. In essence, the proof involves finding a lower bound on the size of the conditionally typical set.

We will proceed directly to the achievability of the rate distortion function. We will only give an outline to illustrate the main ideas. The construction of the codebook and the encoding and decoding are similar to the proof in Section 10.5.

Proof: Fix \( p(\hat{x}|x) \). Calculate \( p(\hat{x}) = \sum_{x} p(x) p(\hat{x}|x) \). Fix \( \varepsilon > 0 \). Later we will choose \( \varepsilon \) appropriately to achieve an expected distortion less than \( D + \delta \).

Generation of codebook: Generate a rate distortion codebook \( C \) consisting of \( 2nR \) sequences \( \hat{X}^n \) drawn i.i.d. \( \sim p(\hat{x}^n) \). Denote the sequences \( \hat{X}^n(1), \ldots, \hat{X}^n(2nR) \).

Encoding: Given a sequence \( X^n \), index it by \( w \) if there exists a \( w \) such that \( (X^n, \hat{X}^n(w)) \in A^*(n)^{\varepsilon} \), the strongly jointly typical set. If there is more than one such \( w \), send the first in lexicographic order. If there is no such \( w \), let \( w = 1 \).

Decoding: Let the reproduced sequence be \( \hat{X}^n(w) \).

Calculation of distortion: As in the case of the proof in Section 10.5, we calculate the expected distortion over the random choice of codebook as

\[
D = \mathbb{E}_{X^n, C} d(X^n, \hat{X}^n)
\]

\[
= \mathbb{E}_C \sum_{x^n} p(x^n) d(x^n, \hat{X}^n(x^n))
\]

\[
= \sum_{x^n} p(x^n) \mathbb{E}_C d(x^n, \hat{X}^n).
\]

Perform the following actions:

1. Remove unnecessary formatting, extra spaces, or line breaks.
2. Fix broken sentences or paragraphs.
3. Ensure consistent punctuation and capitalization.
4. Remove any non-standard characters or encoding artifacts.
5. Preserve the original meaning and structure of the text.
6. Make the text more readable and well-formatted.

Return only the cleaned text without additional explanations.



RATE DISTORTION THEORY

Nontypical sequences  
Typical sequences with jointly typical codeword  
Typical sequences without jointly typical codeword  

FIGURE 10.8. Classes of source sequences in rate distortion theorem.  

Where the expectation is over the random choice of codebook. For a fixed codebook \( C \), we divide the sequences \( x^n \in \mathcal{X}^n \) into three categories, as shown in Figure 10.8.  

- **Nontypical sequences** \( x^n \notin A^*(n, \epsilon) \). The total probability of these sequences can be made less than \( \epsilon \) by choosing \( n \) large enough. Since the individual distortion between any two sequences is bounded by \( d_{\text{max}} \), the nontypical sequences can contribute at most \( \epsilon d_{\text{max}} \) to the expected distortion.  

- **Typical sequences** \( x^n \in A^*(n, \epsilon) \) such that there exists a codeword \( \hat{x}^n(w) \) that is jointly typical with \( x^n \). In this case, since the source sequence and the codeword are strongly jointly typical, the continuity of the distortion as a function of the joint distribution ensures that they are also distortion typical. Hence, the distortion between these \( x^n \) and their codewords is bounded by \( D + \epsilon d_{\text{max}} \), and since the total probability of these sequences is at most 1, these sequences contribute at most \( D + \epsilon d_{\text{max}} \) to the expected distortion.  

- **Typical sequences** \( x^n \in A^*(n, \epsilon) \) such that there does not exist a codeword \( \hat{x}^n \) that is jointly typical with \( x^n \). Let \( P_e \) be the total probability of these sequences. Since the distortion for any individual sequence is bounded by \( d_{\text{max}} \), these sequences contribute at most \( P_e d_{\text{max}} \) to the expected distortion.  

---

### CHARACTERIZATION OF THE RATE DISTORTION FUNCTION  

The sequences in the first and third categories are the sequences that may not be well represented by this rate distortion code. The probability of the first category of sequences is less than \( \epsilon \) for sufficiently large \( n \). The probability of the last category is \( P_e \), which we will show can be made small. This will prove the theorem that the total probability of sequences that are not well represented is small. In turn, we use this to show that the average distortion is close to \( D \).  

**Calculation of \( P_e \):** We must bound the probability that there is no codeword that is jointly typical with the given sequence \( x^n \). From the joint AEP, we know that the probability that \( x^n \) and any \( \hat{x}^n \) are jointly typical is  

\[
\Pr((x^n, \hat{x}^n) \in A^*(n, \epsilon)) = 2^{-nI(X; \hat{X})}.
\]

Hence, the expected number of jointly typical \( \hat{x}^n(w) \) is \( 2^{nR} 2^{-nI(X; \hat{X})} \), which is exponentially large if \( R > I(X; \hat{X}) \).  

But this is not sufficient to show that \( P_e \to 0 \). We must show that the probability that there is no codeword that is jointly typical with \( x^n \) goes to zero. The fact that the expected number of jointly typical codewords is exponentially large does not ensure that there will be at least one with high probability. Just as in (10.94), we can expand the probability of error as  

\[
P_e = \sum_{x^n \in A^*(n, \epsilon)} p(x^n) \left(1 - \Pr((x^n, \hat{x}^n) \in A^*(n, \epsilon))\right)^{2^{nR}}.
\]

From Lemma 10.6.2, we have  

\[
\Pr((x^n, \hat{x}^n) \in A^*(n, \epsilon)) \geq 2^{-n(I(X; \hat{X}) + \epsilon_1)}.
\]

Substituting this in (10.112) and using the inequality \( (1 - x)^n \leq e^{-nx} \), we have  

\[
P_e \leq e^{-n(2^{nR} 2^{-n(I(X; \hat{X}) + \epsilon_1)})},
\]

which goes to 0 as \( n \to \infty \) if \( R > I(X; \hat{X}) + \epsilon_1 \). Hence, for an appropriate choice of \( \epsilon \) and \( n \), we can get the total probability of all badly represented sequences to be as small as we want. Not only is the expected distortion close to \( D \), but with probability going to 1, we will find a codeword whose distortion with respect to the given sequence is less than \( D + \delta \).  

---

### CHARACTERIZATION OF THE RATE DISTORTION FUNCTION  

We have defined the information rate distortion function as  

\[
R(D) = \min_{q(\hat{x}|x): \mathbb{E}[d(x, \hat{x})] \leq D} I(X; \hat{X}),
\]

where the minimization is over all conditional distributions \( q(\hat{x}|x) \) for which the joint distribution \( p(x)q(\hat{x}|x) \) satisfies the expected distortion constraint. This is a standard minimization problem of a convex function over the convex set of all \( q(\hat{x}|x) \geq 0 \) satisfying  

\[
\sum_{\hat{x}} q(\hat{x}|x) = 1 \quad \text{for all } x,
\]

and  

\[
\sum_{x, \hat{x}} p(x)q(\hat{x}|x)d(x, \hat{x}) \leq D.
\]

We can use the method of Lagrange multipliers to find the solution. We set up the functional  

\[
J(q) = \sum_{x, \hat{x}} p(x)q(\hat{x}|x) \log q(\hat{x}|x) - \lambda \sum_{x, \hat{x}} p(x)q(\hat{x}|x)d(x, \hat{x}) + \sum_{x} \nu(x) \sum_{\hat{x}} q(\hat{x}|x),
\]

where the last term corresponds to the constraint that \( q(\hat{x}|x) \) is a conditional probability mass function. If we let \( q(\hat{x}) = \sum_{x} p(x)q(\hat{x}|x) \) be the distribution on \( \hat{X} \) induced by \( q(\hat{x}|x) \), we can rewrite \( J(q) \) as  

\[
J(q) = \sum_{x, \hat{x}} p(x)q(\hat{x}|x) \log \frac{q(\hat{x}|x)}{q(\hat{x})} + \lambda \sum_{x, \hat{x}} p(x)q(\hat{x}|x)d(x, \hat{x}) + \sum_{x} \nu(x) \sum_{\hat{x}} q(\hat{x}|x).
\]

Differentiating with respect to \( q(\hat{x}|x) \), we have  

\[
\frac{\partial J}{\partial q(\hat{x}|x)} = p(x) \log q(\hat{x}|x) - p(x) \sum_{x'} p(x')q(\hat{x}|x') \frac{1}{q(\hat{x})} + \lambda p(x)d(x, \hat{x}) + \nu(x) = 0.
\]

Setting \( \log \mu(x) = \nu(x)/p(x) \), we obtain  

\[
p(x) \left( \log q(\hat{x}|x) - \log q(\hat{x}) + \lambda d(x, \hat{x}) + \log \mu(x) \right) = 0,
\]

or  

\[
q(\hat{x}|x) = \frac{q(\hat{x})e^{-\lambda d(x, \hat{x})}}{\mu(x)}.
\]

Since \( \sum_{\hat{x}} q(\hat{x}|x) = 1 \), we must have  

\[
\mu(x) = \sum_{\hat{x}} q(\hat{x})e^{-\lambda d(x, \hat{x})},
\]

or  

\[
q(\hat{x}|x) = \frac{q(\hat{x})e^{-\lambda d(x, \hat{x})}}{\sum_{\hat{x}} q(\hat{x})e^{-\lambda d(x, \hat{x})}}.
\]

Multiplying this by \( p(x) \) and summing over all \( x \), we obtain  

\[
q(\hat{x}) = q(\hat{x}) \sum_{x} p(x)e^{-\lambda d(x, \hat{x})} \sum_{\hat{x}'} q(\hat{x}')e^{-\lambda d(x, \hat{x}')}.
\]

If \( q(\hat{x}) > 0 \), we can divide both sides by \( q(\hat{x}) \) and obtain  

\[
\sum_{x} p(x)e^{-\lambda d(x, \hat{x})} \sum_{\hat{x}'} q(\hat{x}')e^{-\lambda d(x, \hat{x}')} = 1
\]

for all \( \hat{x} \in \hat{\mathcal{X}} \). We can combine these \( |\hat{\mathcal{X}}| \) equations with the equation defining the distortion and calculate \( \lambda \) and the \( |\hat{\mathcal{X}}| \) unknowns \( q(\hat{x}) \). We can use this and (10.124) to find the optimum conditional distribution.  

The above analysis is valid if \( q(\hat{x}) \) is unconstrained (i.e., \( q(\hat{x}) > 0 \) for all \( \hat{x} \)). The inequality condition \( q(\hat{x}) > 0 \) is covered by the Kuhn–Tucker conditions, which reduce to  

\[
\frac{\partial J}{\partial q(\hat{x}|x)} = 0 \quad \text{if } q(\hat{x}|x) > 0,
\]

\[
\frac{\partial J}{\partial q(\hat{x}|x)} \geq 0 \quad \text{if } q(\hat{x}|x) = 0.
\]

Substituting the value of the derivative, we obtain the conditions for the minimum as  

\[
\sum_{x} p(x)e^{-\lambda d(x, \hat{x})} \sum_{\hat{x}'} q(\hat{x}')e^{-\lambda d(x, \hat{x}')} = 1 \quad \text{if } q(\hat{x}) > 0,
\]

\[
\sum_{x} p(x)e^{-\lambda d(x, \hat{x})} \sum_{\hat{x}'} q(\hat{x}')e^{-\lambda d(x, \hat{x}')} \leq 1 \quad \text{if } q(\hat{x}) = 0.
\]

This characterization will enable us to check if a given \( q(\hat{x}) \) is a solution to the minimization problem. However, it is not easy to solve for the optimum output distribution from these equations. In the next section, we provide an iterative algorithm for computing the rate distortion function. This algorithm is a special case of a general algorithm for finding the minimum relative entropy distance between two convex sets of probability densities.  

---

### COMPUTATION OF CHANNEL CAPACITY AND THE RATE DISTORTION FUNCTION  

Consider the following problem: Given two convex sets \( A \) and \( B \) in \( \mathbb{R}^n \) as shown in Figure 10.9, we would like to find the minimum distance between them:  

\[
d_{\text{min}} = \min_{a \in A, b \in B} d(a, b),
\]

where \( d(a, b) \) is the Euclidean distance between \( a \) and \( b \). An intuitively obvious algorithm to do this would be to take any point \( x \in A \), and find the \( y \in B \) that is closest to it. Then fix this \( y \) and find the closest point in \( A \). Repeating this process, it is clear that the distance decreases at each stage. Does it converge to the minimum distance between the two sets?  

Csiszár and Tusnády [155] have shown that if the sets are convex and if the distance satisfies certain conditions, this alternating minimization algorithm will indeed converge to the minimum. In particular, if the sets are sets of probability distributions and the distance measure is the relative entropy, the algorithm does converge to the minimum relative entropy between the two sets of distributions.  

FIGURE 10.9. Distance between convex sets.  

---

To apply this algorithm to rate distortion, we have to rewrite the rate distortion function as a minimum of the relative entropy between two sets. We begin with a simple lemma. A form of this lemma comes up again in theorem 13.1.1, establishing the duality of channel capacity universal data compression.  

**Lemma 10.8.1**  
Let \( p(x)p(y|x) \) be a given joint distribution. Then the distribution \( r(y) \) that minimizes the relative entropy \( D(p(x)p(y|x) \parallel p(x)r(y)) \) is the marginal distribution \( r^*(y) \) corresponding to \( p(y|x) \):  

\[
D(p(x)p(y|x) \parallel p(x)r^*(y)) = \min_{r(y)} D(p(x)p(y|x) \parallel p(x)r(y)),
\]

where \( r^*(y) = \sum_{x} p(x)p(y|x) \). Also,  

\[
\max_{r(x|y)} \sum_{x,y} p(x)p(y|x) \log \frac{r(x|y)}{p(x)} = \sum_{x,y} p(x)p(y|x) \log \frac{r^*(x|y)}{p(x)},
\]

where  

\[
r^*(x|y) = \frac{p(x)p(y|x)}{\sum_{x} p(x)p(y|x)}.
\]

*Proof:*  
The proof of the second part of the lemma is left as an exercise.  

We can use this lemma to rewrite the minimization in the definition of the rate distortion function as a double minimization,  

\[
R(D) = \min_{r(\hat{x})} \min_{q(\hat{x}|x): \mathbb{E}[d(x, \hat{x})] \leq D} \sum_{x, \hat{x}} p(x)q(\hat{x}|x) \log \frac{q(\hat{x}|x)}{r(\hat{x})}.
\]

If \( A \) is the set of all joint distributions with marginal \( p(x) \) that satisfy the distortion constraints and if \( B \) is the set of product distributions \( p(x)r(\hat{x}) \) with arbitrary \( r(\hat{x}) \), we can write  

\[
R(D) = \min_{q \in B} \min_{p \in A} D(p \parallel q).
\]

We now apply the process of alternating minimization, which is called the Blahut–Arimoto algorithm in this case. We begin with a choice of \( \lambda \) and an initial output distribution \( r(\hat{x}) \) and calculate the \( q(\hat{x}|x) \) that minimizes the mutual information subject to the distortion constraint. We can use the method of Lagrange multipliers for this minimization to obtain  

\[
q(\hat{x}|x) = \frac{r(\hat{x})e^{-\lambda d(x, \hat{x})}}{\sum_{\hat{x}} r(\hat{x})e^{-\lambda d(x, \hat{x})}}.
\]

For this conditional distribution \( q(\hat{x}|x) \), we calculate the output distribution \( r(\hat{x}) \) that minimizes the mutual information, which by Lemma 10.8.1 is  

\[
r(\hat{x}) = \sum_{x} p(x)q(\hat{x}|x).
\]

We use this output distribution as the starting point of the next iteration. Each step in the iteration, minimizing over \( q(\cdot| \cdot) \) and then minimizing over \( r(\cdot) \), reduces the right-hand side of (10.140). Thus, there is a limit, and the limit has been shown to be \( R(D) \) by Csiszár [139], where the value of \( D \) and \( R(D) \) depends on \( \lambda \). Thus, choosing \( \lambda \) appropriately sweeps out the \( R(D) \) curve.  

A similar procedure can be applied to the calculation of channel capacity. Again we rewrite the definition of channel capacity,  

\[
C = \max_{r(x)} I(X; Y) = \max_{r(x)} \sum_{x} \sum_{y} r(x)p(y|x) \log \frac{r(x)p(y|x)}{\sum_{x'} r(x')p(y|x')}.
\]



---  
**Page 61**  
**SUMMARY**  
The rate distortion function for a source \( X \sim p(x) \) and distortion measure \( d(x, \hat{x}) \) is  
\[
R(D) = \min_{p(\hat{x}|x): \mathbb{E}[d(x, \hat{x})] \leq D} I(X; \hat{X}),
\]  
where the minimization is over all conditional distributions \( p(\hat{x}|x) \) for which the joint distribution \( p(x, \hat{x}) = p(x)p(\hat{x}|x) \) satisfies the expected distortion constraint.  

**RATE DISTORTION THEORY**  
- **Rate Distortion Theorem**: If \( R > R(D) \), there exists a sequence of codes \( \hat{X}^n(X^n) \) with the number of codewords \( |\hat{X}^n(\cdot)| \leq 2^{nR} \) such that \( \mathbb{E}[d(X^n, \hat{X}^n(X^n))] \to D \). If \( R < R(D) \), no such codes exist.  
- **Bernoulli Source**: For a Bernoulli source with Hamming distortion,  
  \[
  R(D) = H(p) - H(D).
  \]  
- **Gaussian Source**: For a Gaussian source with squared-error distortion,  
  \[
  R(D) = \frac{1}{2} \log \frac{\sigma^2}{D}.
  \]  
- **Source-Channel Separation**: A source with rate distortion \( R(D) \) can be sent over a channel of capacity \( C \) and recovered with distortion \( D \) if and only if \( R(D) < C \).  
- **Multivariate Gaussian Source**: The rate distortion function for a multivariate normal vector with Euclidean mean-squared-error distortion is given by reverse water-filling on the eigenvalues.  

**PROBLEMS**  

**10.1 One-bit quantization of a single Gaussian random variable**  
Let \( X \sim \mathcal{N}(0, \sigma^2) \) and let the distortion measure be squared error. Show that the optimum reproduction points for 1-bit quantization are \( \pm \sqrt{2\pi}\sigma \) and that the expected distortion for 1-bit quantization is \( \frac{\pi-2}{\pi}\sigma^2 \). Compare this with the distortion-rate bound \( D = \sigma^2 2^{-2R} \) for \( R = 1 \).  

**10.2 Rate distortion function with infinite distortion**  
Find the rate distortion function \( R(D) = \min I(X; \hat{X}) \) for \( X \sim \text{Bernoulli}(\frac{1}{2}) \) and distortion  
\[
d(x, \hat{x}) = \begin{cases} 
0, & x = \hat{x}, \\ 
1, & x = 1, \hat{x} = 0, \\ 
\infty, & x = 0, \hat{x} = 1.
\end{cases}
\]  

**10.3 Rate distortion for binary source with asymmetric distortion**  
Fix \( p(\hat{x}|x) \) and evaluate \( I(X; \hat{X}) \) and \( D \) for \( X \sim \text{Bernoulli}(\frac{1}{2}) \) and distortion  
\[
d(x, \hat{x}) = \begin{pmatrix} 
0 & a \\ 
b & 0 
\end{pmatrix}.
\]  
(The rate distortion function cannot be expressed in closed form.)  

**10.4 Properties of \( R(D) \)**  
Consider a discrete source \( X \in \mathcal{X} = \{1, 2, \dots, m\} \) with distribution \( p_1, p_2, \dots, p_m \) and a distortion measure \( d(i, j) \). Let \( R(D) \) be the rate distortion function for this source and distortion measure. Let \( d'(i, j) = d(i, j) - w_i \) be a new distortion measure, and let \( R'(D) \) be the corresponding rate distortion function. Show that \( R'(D) = R(D + w) \), where \( w = \sum p_i w_i \), and use this to show that there is no essential loss of generality in assuming that \( \min_{\hat{x}} d(i, \hat{x}) = 0 \) (i.e., for each \( x \in \mathcal{X} \), there is one symbol \( \hat{x} \) that reproduces the source with zero distortion). This result is due to Pinkston.  

**10.5 Rate distortion for uniform source with Hamming distortion**  
Consider a source \( X \) uniformly distributed on the set \( \{1, 2, \dots, m\} \). Find the rate distortion function for this source with Hamming distortion; that is,  
\[
d(x, \hat{x}) = \begin{cases} 
0, & x = \hat{x}, \\ 
1, & x \neq \hat{x}.
\end{cases}
\]  

**10.6 Shannon lower bound for the rate distortion function**  
Consider a source \( X \) with a distortion measure \( d(x, \hat{x}) \) that satisfies the following property: All columns of the distortion matrix are permutations of the set \( \{d_1, d_2, \dots, d_m\} \). Define the function  
\[
\phi(D) = \max_{p: \sum_{i=1}^m p_i d_i \leq D} H(p).
\]  
The Shannon lower bound on the rate distortion function is proved by the following steps:  
(a) Show that \( \phi(D) \) is a concave function of \( D \).  
(b) Justify the following series of inequalities for \( I(X; \hat{X}) \) if \( \mathbb{E}[d(X, \hat{X})] \leq D \):  
\[
I(X; \hat{X}) = H(X) - H(X|\hat{X}),
\]  
\[
= H(X) - \sum_{\hat{x}} p(\hat{x}) H(X|\hat{X} = \hat{x}),
\]  
\[
\geq H(X) - \sum_{\hat{x}} p(\hat{x}) \phi(D_{\hat{x}}),
\]  
\[
\geq H(X) - \phi\left(\sum_{\hat{x}} p(\hat{x}) D_{\hat{x}}\right),
\]  
\[
\geq H(X) - \phi(D),
\]  
where \( D_{\hat{x}} = \sum_{x} p(x|\hat{x}) d(x, \hat{x}) \).  
(c) Argue that  
\[
R(D) \geq H(X) - \phi(D),
\]  
which is the Shannon lower bound on the rate distortion function.  
(d) If, in addition, we assume that the source has a uniform distribution and that the rows of the distortion matrix are permutations of each other, then \( R(D) = H(X) - \phi(D) \) (i.e., the lower bound is tight).  

**10.7 Erasure distortion**  
Consider \( X \sim \text{Bernoulli}(\frac{1}{2}) \), and let the distortion measure be given by the matrix  
\[
d(x, \hat{x}) = \begin{pmatrix} 
0 & 1 \\ 
\infty & 0 
\end{pmatrix}.
\]  
Calculate the rate distortion function for this source. Can you suggest a simple scheme to achieve any value of the rate distortion function for this source?  

**10.8 Bounds on the rate distortion function for squared-error distortion**  
For the case of a continuous random variable \( X \) with mean zero and variance \( \sigma^2 \) and squared-error distortion, show that  
\[
h(X) - \frac{1}{2} \log(2\pi e D) \leq R(D) \leq \frac{1}{2} \log \frac{\sigma^2}{D}.
\]  
For the upper bound, consider the following joint distribution:  
\[
Z \sim \mathcal{N}\left(0, \frac{D}{s^2 - D}s^2\right), \quad \hat{X} = X + Z.
\]  
Are Gaussian random variables harder or easier to describe than other random variables with the same variance?  

**10.9 Properties of optimal rate distortion code**  
A good \( (R, D) \) rate distortion code with \( R \approx R(D) \) puts severe constraints on the relationship of the source \( X^n \) and the representations \( \hat{X}^n \). Examine the chain of inequalities (10.58–10.71) considering the conditions for equality and interpret as properties of a good code. For example, equality in (10.59) implies that \( \hat{X}^n \) is a deterministic function of \( X^n \).  

**10.10 Rate distortion**  
Find and verify the rate distortion function \( R(D) \) for \( X \) uniform on \( \mathcal{X} = \{1, 2, \dots, 2m\} \) and  
\[
d(x, \hat{x}) = \begin{cases} 
1, & \text{for } x - \hat{x} \text{ odd}, \\ 
0, & \text{for } x - \hat{x} \text{ even},
\end{cases}
\]  
where \( \hat{X} \) is defined on \( \hat{\mathcal{X}} = \{1, 2, \dots, 2m\} \). (You may wish to use the Shannon lower bound in your argument.)  

**10.11 Lower bound**  
Let \( X \sim \frac{e^{-x^4}}{\int_{-\infty}^\infty e^{-x^4} dx} \) and  
\[
g(a) = \max h(X) \text{ over all densities such that } \mathbb{E}[X^4] \leq a.
\]  
Let \( R(D) \) be the rate distortion function for \( X \) with the density above and with distortion criterion \( d(x, \hat{x}) = (x - \hat{x})^4 \). Show that \( R(D) \geq g(c) - g(D) \).  

**10.12 Adding a column to the distortion matrix**  
Let \( R(D) \) be the rate distortion function for an i.i.d. process with probability mass function \( p(x) \) and distortion function \( d(x, \hat{x}) \), \( x \in \mathcal{X} \), \( \hat{x} \in \hat{\mathcal{X}} \). Now suppose that we add a new reproduction symbol \( \hat{x}_0 \) to \( \hat{\mathcal{X}} \) with associated distortion \( d(x, \hat{x}_0) \), \( x \in \mathcal{X} \). Does this increase or decrease \( R(D) \), and why?  

**10.13 Simplification**  
Suppose that \( \mathcal{X} = \{1, 2, 3, 4\} \), \( \hat{\mathcal{X}} = \{1, 2, 3, 4\} \), \( p(i) = \frac{1}{4} \), \( i = 1, 2, 3, 4 \), and \( X_1, X_2, \dots \) are i.i.d. \( \sim p(x) \). The distortion matrix \( d(x, \hat{x}) \) is given by  
\[
\begin{array}{cccc} 
1 & 2 & 3 & 4 \\ 
0 & 0 & 1 & 1 \\ 
2 & 0 & 0 & 1 \\ 
3 & 1 & 1 & 0 \\ 
4 & 1 & 0 & 0 
\end{array}
\]  
(a) Find \( R(0) \), the rate necessary to describe the process with zero distortion.  
(b) Find the rate distortion function \( R(D) \). There are some irrelevant distinctions in alphabets \( \mathcal{X} \) and \( \hat{\mathcal{X}} \), which allow the problem to be collapsed.  
(c) Suppose that we have a nonuniform distribution \( p(i) = p_i \), \( i = 1, 2, 3, 4 \). What is \( R(D) \)?  

**10.14 Rate distortion for two independent sources**  
Can one compress two independent sources simultaneously better than by compressing the sources individually? The following problem addresses this question. Let \( \{X_i\} \) be i.i.d. \( \sim p(x) \) with distortion \( d(x, \hat{x}) \) and rate distortion function \( R_X(D) \). Similarly, let \( \{Y_i\} \) be i.i.d. \( \sim p(y) \) with distortion \( d(y, \hat{y}) \) and rate distortion function \( R_Y(D) \). Suppose we now wish to describe the process \( \{(X_i, Y_i)\} \) subject to distortions \( \mathbb{E}[d(X, \hat{X})] \leq D_1 \) and \( \mathbb{E}[d(Y, \hat{Y})] \leq D_2 \). Thus, a rate \( R_{X,Y}(D_1, D_2) \) is sufficient, where  
\[
R_{X,Y}(D_1, D_2) = \min_{p(\hat{x}, \hat{y}|x,y): \mathbb{E}[d(X, \hat{X})] \leq D_1, \mathbb{E}[d(Y, \hat{Y})] \leq D_2} I(X, Y; \hat{X}, \hat{Y}).
\]  
Now suppose that the \( \{X_i\} \) process and the \( \{Y_i\} \) process are independent of each other.  
(a) Show that  
\[
R_{X,Y}(D_1, D_2) \geq R_X(D_1) + R_Y(D_2).
\]  
(b) Does equality hold?  
Now answer the question.  

**10.15 Distortion rate function**  
Let  
\[
D(R) = \min_{p(\hat{x}|x): I(X; \hat{X}) \leq R} \mathbb{E}[d(X, \hat{X})]
\]  
be the distortion rate function.  
(a) Is \( D(R) \) increasing or decreasing in \( R \)?  
(b) Is \( D(R) \) convex or concave in \( R \)?  
(c) Converse for distortion rate functions: We now wish to prove the converse by focusing on \( D(R) \). Let \( X_1, X_2, \dots, X_n \) be i.i.d. \( \sim p(x) \). Suppose that one is given a \( (2nR, n) \) rate distortion code \( X^n \to i(X^n) \to \hat{X}^n(i(X^n)) \), with \( i(X^n) \in 2^{nR} \), and suppose that the resulting distortion is \( D = \mathbb{E}[d(X^n, \hat{X}^n(i(X^n)))] \). We must show that \( D \geq D(R) \). Give reasons for the following steps in the proof:  
\[
D = \mathbb{E}[d(X^n, \hat{X}^n(i(X^n)))]
\]  
(a)  
\[
= \mathbb{E}\left[\frac{1}{n} \sum_{i=1}^n d(X_i, \hat{X}_i)\right]
\]  
(b)  
\[
= \frac{1}{n} \sum_{i=1}^n \mathbb{E}[d(X_i, \hat{X}_i)]
\]  
(c)  
\[
\geq \frac{1}{n} \sum_{i=1}^n D(I(X_i; \hat{X}_i))
\]  
(d)  
\[
\geq D\left(\frac{1}{n} \sum_{i=1}^n I(X_i; \hat{X}_i)\right)
\]  
(e)  
\[
\geq D\left(\frac{1}{n} I(X^n; \hat{X}^n)\right)
\]  
(f)  
\[
\geq D(R).
\]  

**10.16 Probability of conditionally typical sequences**  
In Chapter 7, we calculated the probability that two independently drawn sequences \( X^n \) and \( Y^n \) are weakly jointly typical. To prove the rate distortion theorem, however, we need to calculate this probability when...



RATE DISTORTION THEORY

One of the sequences is fixed and the other is random. The techniques of weak typicality allow us only to calculate the average set size of the conditionally typical set. Using the ideas of strong typicality, on the other hand, provides us with stronger bounds that work for all typical \( x^n \) sequences. We outline the proof that 

\[
\Pr\{(x^n, Y^n) \in A^{(n)}_\varepsilon\} \approx 2^{-nI(X;Y)}
\]

for all typical \( x^n \). This approach was introduced by Berger [53] and is fully developed in the book by Csiszár and Körner [149].

Let \( (X_i, Y_i) \) be drawn i.i.d. \( \sim p(x, y) \). Let the marginals of \( X \) and \( Y \) be \( p(x) \) and \( p(y) \), respectively.

(a) Let \( A^{(n)}_\varepsilon \) be the strongly typical set for \( X \). Show that 

\[
|A^{(n)}_\varepsilon| = 2^{nH(X)}.
\]

(Hint: Theorems 11.1.1 and 11.1.3.)

(b) The joint type of a pair of sequences \( (x^n, y^n) \) is the proportion of times \( (x_i, y_i) = (a, b) \) in the pair of sequences:

\[
\hat{p}_{x^n, y^n}(a, b) = \frac{1}{n}N(a, b|x^n, y^n) = \frac{1}{n}\sum_{i=1}^n I(x_i = a, y_i = b).
\]

The conditional type of a sequence \( y^n \) given \( x^n \) is a stochastic matrix that gives the proportion of times a particular element of \( Y \) occurred with each element of \( X \) in the pair of sequences. Specifically, the conditional type \( \hat{V}_{y^n|x^n}(b|a) \) is defined as

\[
\hat{V}_{y^n|x^n}(b|a) = \frac{N(a, b|x^n, y^n)}{N(a|x^n)}.
\]

Show that the number of conditional types is bounded by \( (n + 1)|X||Y| \).

(c) The set of sequences \( y^n \in \mathcal{Y}^n \) with conditional type \( V \) with respect to a sequence \( x^n \) is called the conditional type class \( \mathcal{T}_V(x^n) \). Show that

\[
\frac{1}{(n + 1)|X||Y|}2^{nH(Y|X)} \leq |\mathcal{T}_V(x^n)| \leq 2^{nH(Y|X)}.
\]

(d) The sequence \( y^n \in \mathcal{Y}^n \) is said to be \( \varepsilon \)-strongly conditionally typical with the sequence \( x^n \) with respect to the conditional distribution \( V(\cdot| \cdot) \) if the conditional type satisfies the following two conditions:

(i) For all \( (a, b) \in \mathcal{X} \times \mathcal{Y} \) with \( V(b|a) > 0 \),

\[
\left| \frac{1}{n}N(a, b|x^n, y^n) - V(b|a)\right| \leq \frac{\varepsilon}{|Y| + 1}.
\]

(ii) \( N(a, b|x^n, y^n) = 0 \) for all \( (a, b) \) such that \( V(b|a) = 0 \).

The set of such sequences is called the conditionally typical set and is denoted \( A^{(n)}_\varepsilon(Y|x^n) \). Show that the number of sequences \( y^n \) that are conditionally typical with a given \( x^n \in \mathcal{X}^n \) is bounded by

\[
\frac{1}{(n + 1)|X||Y|}2^{n(H(Y|X) - \varepsilon_1)} \leq |A^{(n)}_\varepsilon(Y|x^n)| \leq (n + 1)|X||Y|2^{n(H(Y|X) + \varepsilon_1)},
\]

where \( \varepsilon_1 \to 0 \) as \( \varepsilon \to 0 \).

(e) For a pair of random variables \( (X, Y) \) with joint distribution \( p(x, y) \), the \( \varepsilon \)-strongly typical set \( A^{(n)}_\varepsilon \) is the set of sequences \( (x^n, y^n) \in \mathcal{X}^n \times \mathcal{Y}^n \) satisfying:

(i) For every pair \( (a, b) \in \mathcal{X} \times \mathcal{Y} \) with \( p(a, b) > 0 \),

\[
\left| \frac{1}{n}N(a, b|x^n, y^n) - p(a, b) \right| < \frac{\varepsilon}{|X||Y|}.
\]

(ii) \( N(a, b|x^n, y^n) = 0 \) for all \( (a, b) \in \mathcal{X} \times \mathcal{Y} \) with \( p(a, b) = 0 \).

The set of \( \varepsilon \)-strongly jointly typical sequences is called the \( \varepsilon \)-strongly jointly typical set and is denoted \( A^{(n)}_\varepsilon(X, Y) \). Let \( (X, Y) \) be drawn i.i.d. \( \sim p(x, y) \). For any \( x^n \) such that there exists at least one pair \( (x^n, y^n) \in A^{(n)}_\varepsilon(X, Y) \), the set of sequences \( y^n \) such that \( (x^n, y^n) \in A^{(n)}_\varepsilon \) satisfies

\[
\frac{1}{(n + 1)|X||Y|}2^{n(H(Y|X) - \delta(\varepsilon))} \leq |\{y^n : (x^n, y^n) \in A^{(n)}_\varepsilon\}| \leq (n + 1)|X||Y|2^{n(H(Y|X) + \delta(\varepsilon))},
\]

where \( \delta(\varepsilon) \to 0 \) as \( \varepsilon \to 0 \). In particular, we can write

\[
2^{n(H(Y|X) - \varepsilon_2)} \leq |\{y^n : (x^n, y^n) \in A^{(n)}_\varepsilon\}| \leq 2^{n(H(Y|X) + \varepsilon_2)},
\]

where \( \varepsilon_2 \) can be made arbitrarily small with an appropriate choice of \( \varepsilon \) and \( n \).

(f) Let \( Y_1, Y_2, \ldots, Y_n \) be drawn i.i.d. \( \sim p(y_i) \). For \( x^n \in A^{(n)}_\varepsilon \), the probability that \( (x^n, Y^n) \in A^{(n)}_\varepsilon \) is bounded by

\[
2^{-n(I(X;Y) + \varepsilon_3)} \leq \Pr((x^n, Y^n) \in A^{(n)}_\varepsilon) \leq 2^{-n(I(X;Y) - \varepsilon_3)},
\]

where \( \varepsilon_3 \to 0 \) as \( \varepsilon \to 0 \) and \( n \to \infty \).

10.17 Source-Channel Separation Theorem with Distortion.

Let \( V_1, V_2, \ldots, V_n \) be a finite alphabet i.i.d. source which is encoded as a sequence of \( n \) input symbols \( X^n \) of a discrete memoryless channel. The output of the channel \( Y^n \) is mapped onto the reconstruction alphabet \( \hat{V}^n = g(Y^n) \). Let \( D = \mathbb{E}d(V^n, \hat{V}^n) = \frac{1}{n}\sum_{i=1}^n \mathbb{E}d(V_i, \hat{V}_i) \) be the average distortion achieved by this combined source and channel coding scheme.

Channel Capacity \( C \)

\[
V^n \xrightarrow{} X^n(V^n) \xrightarrow{} \hat{V}^n = g(Y^n)
\]

(a) Show that if \( C > R(D) \), where \( R(D) \) is the rate distortion function for \( V \), it is possible to find encoders and decoders that achieve an average distortion arbitrarily close to \( D \).

(b) (Converse) Show that if the average distortion is equal to \( D \), the capacity of the channel \( C \) must be greater than \( R(D) \).

10.18 Rate Distortion.

Let \( d(x, \hat{x}) \) be a distortion function. We have a source \( X \sim p(x) \). Let \( R(D) \) be the associated rate distortion function.

(a) Find \( \tilde{R}(D) \) in terms of \( R(D) \), where \( \tilde{R}(D) \) is the rate distortion function associated with the distortion \( \tilde{d}(x, \hat{x}) = d(x, \hat{x}) + a \) for some constant \( a > 0 \). (They are not equal.)

(b) Now suppose that \( d(x, \hat{x}) \geq 0 \) for all \( x, \hat{x} \) and define a new distortion function \( d^*(x, \hat{x}) = b d(x, \hat{x}) \), where \( b \) is some number \( \geq 0 \). Find the associated rate distortion function \( R^*(D) \) in terms of \( R(D) \).

(c) Let \( X \sim \mathcal{N}(0, \sigma^2) \) and \( d(x, \hat{x}) = 5(x - \hat{x})^2 + 3 \). What is \( R(D) \)?

10.19 Rate Distortion with Two Constraints.

Let \( X_i \) be i.i.d. \( \sim p(x) \). We are given two distortion functions, \( d_1(x, \hat{x}) \) and \( d_2(x, \hat{x}) \). We wish to describe \( X^n \) at rate \( R \) and reconstruct it with distortions \( \mathbb{E}d_1(X^n, \hat{X}^n_1) \leq D_1 \) and \( \mathbb{E}d_2(X^n, \hat{X}^n_2) \leq D_2 \), as shown here:

\[
X^n \xrightarrow{i(X^n)} \left( \hat{X}^n_1(i(X^n)), \hat{X}^n_2(i(X^n)) \right)
\]

\[
D_1 = \mathbb{E}d_1(X^n_1, \hat{X}^n_1)
\]

\[
D_2 = \mathbb{E}d_2(X^n_1, \hat{X}^n_2).
\]

Here \( i(\cdot) \) takes on \( 2^{nR} \) values. What is the rate distortion function \( R(D_1, D_2) \)?

10.20 Rate Distortion.

Consider the standard rate distortion problem, \( X_i \) i.i.d. \( \sim p(x) \), \( X^n \to i(X^n) \to \hat{X}^n \), \( |i(\cdot)| = 2^{nR} \). Consider two distortion criteria \( d_1(x, \hat{x}) \) and \( d_2(x, \hat{x}) \). Suppose that \( d_1(x, \hat{x}) \leq d_2(x, \hat{x}) \) for all \( x \in \mathcal{X}, \hat{x} \in \hat{\mathcal{X}} \). Let \( R_1(D) \) and \( R_2(D) \) be the corresponding rate distortion functions.

(a) Find the inequality relationship between \( R_1(D) \) and \( R_2(D) \).

(b) Suppose that we must describe the source \( \{X_i\} \) at the minimum rate \( R \) achieving \( d_1(X^n, \hat{X}^n_1) \leq D \) and \( d_2(X^n, \hat{X}^n_2) \leq D \). Thus,

\[
X^n \xrightarrow{i(X^n)} \left( \hat{X}^n_1(i(X^n)), \hat{X}^n_2(i(X^n)) \right)
\]

and \( |i(\cdot)| = 2^{nR} \). Find the minimum rate \( R \).

HISTORICAL NOTES

The idea of rate distortion was introduced by Shannon in his original paper [472]. He returned to it and dealt with it exhaustively in his 1959 paper [485], which proved the first rate distortion theorem. Meanwhile, Kolmogorov and his school in the Soviet Union began to develop rate distortion theory in 1956. Stronger versions of the rate distortion theorem have been proved for more general sources in the comprehensive book by Berger [52].

The inverse water-filling solution for the rate distortion function for parallel Gaussian sources was established by McDonald and Schultheiss [381]. An iterative algorithm for the calculation of the rate distortion function for a general i.i.d. source and arbitrary distortion measure was described by Blahut [65], Arimoto [25], and Csiszár [139]. This algorithm is a special case of a general alternating minimization algorithm due to Csiszár and Tusnády [155].

CHAPTER 11 INFORMATION THEORY AND STATISTICS

We now explore the relationship between information theory and statistics. We begin by describing the method of types, which is a powerful technique in large deviation theory. We use the method of types to calculate the probability of rare events and to show the existence of universal source codes. We also consider the problem of testing hypotheses and derive the best possible error exponents for such tests (the Chernoff-Stein lemma). Finally, we treat the estimation of the parameters of a distribution and describe the role of Fisher information.

11.1 METHOD OF TYPES

The AEP for discrete random variables (Chapter 3) focuses our attention on a small subset of typical sequences. The method of types is an even more powerful procedure in which we consider sequences that have the same empirical distribution. With this restriction, we can derive strong bounds on the number of sequences with a particular empirical distribution and the probability of each sequence in this set. It is then possible to derive strong error bounds for the channel coding theorem and prove a variety of rate distortion results. The method of types was fully developed by Csiszár and Körner [149], who obtained most of their results from this point of view.

Let \( X_1, X_2, \ldots, X_n \) be a sequence of \( n \) symbols from an alphabet \( \mathcal{X} = \{a_1, a_2, \ldots, a_{|\mathcal{X}|}\} \). We use the notation \( x^n \) and \( x \) interchangeably to denote a sequence \( x_1, x_2, \ldots, x_n \).

DEFINITION

The type \( P_x \) (or empirical probability distribution) of a sequence \( x_1, x_2, \ldots, x_n \) is the relative proportion of occurrences of each symbol of \( \mathcal{X} \) (i.e., \( P_x(a) = \frac{N(a|x)}{n} \) for all \( a \in \mathcal{X} \), where \( N(a|x) \) is the number of times the symbol \( a \) occurs in the sequence \( x \in \mathcal{X}^n \)). The type of a sequence \( x \) is denoted as \( P_x \). It is a probability mass function on \( \mathcal{X} \). (Note that in this chapter, we will use capital letters to denote types and distributions. We also loosely use the word distribution to mean a probability mass function.)

DEFINITION

The probability simplex in \( \mathbb{R}^m \) is the set of points \( x = (x_1, x_2, \ldots, x_m) \in \mathbb{R}^m \) such that \( x_i \geq 0 \), \( \sum_{i=1}^m x_i = 1 \). The probability simplex is an \( (m - 1) \)-dimensional manifold in \( m \)-dimensional space. When \( m = 3 \), the probability simplex is the set of points

\[
\left\{ (x_1, x_2, x_3) : x_1 \geq 0, x_2 \geq 0, x_3 \geq 0, x_1 + x_2 + x_3 = 1 \right\}
\]

(Figure 11.1). Since this is a triangular two-dimensional flat in \( \mathbb{R}^3 \), we use a triangle to represent the probability simplex in later sections of this chapter.

DEFINITION

Let \( \mathcal{P}_n \) denote the set of types with denominator \( n \). For example, if \( \mathcal{X} = \{0, 1\} \), the set of possible types with denominator \( n \) is

\[
\mathcal{P}_n = \left\{ \left( \frac{k}{n}, \frac{n - k}{n} \right) : k = 0, 1, \ldots, n \right\}.
\]

DEFINITION

If \( P \in \mathcal{P}_n \), the set of sequences of length \( n \) and type \( P \) is called the type class of \( P \), denoted \( \mathcal{T}(P) \):

\[
\mathcal{T}(P) = \{ x \in \mathcal{X}^n : P_x = P \}.
\]

The type class is sometimes called the composition class of \( P \).

FIGURE 11.1. Probability simplex in \( \mathbb{R}^3 \).

x2

x3

x1

x1 + x2 + x3 = 1

1

1

1



--- Page 75 ---
11.1
METHOD OF TYPES
349
Example 11.1.1
Let \( X = \{1, 2, 3\} \), a ternary alphabet. Let \( x = 11321 \). Then the type \( P_x \) is
\[ P_x(1) = \frac{3}{5}, \]
\[ P_x(2) = \frac{1}{5}, \]
\[ P_x(3) = \frac{1}{5}. \]
The type class of \( P_x \) is the set of all sequences of length 5 with three 1’s, one 2, and one 3. There are 20 such sequences, and
\[ T(P_x) = \{11123, 11132, 11213, \ldots, 32111\}. \]
The number of elements in \( T(P) \) is
\[ |T(P)| = \binom{n}{nP(a_1), nP(a_2), \ldots, nP(a_{|X|})} = \frac{n!}{nP(a_1)! \, nP(a_2)! \cdots nP(a_{|X|})!}. \]
The essential power of the method of types arises from the following theorem, which shows that the number of types is at most polynomial in \( n \).

Theorem 11.1.1
\[ |P_n| \leq (n + 1)|X|. \]
Proof:
There are \( |X| \) components in the vector that specifies \( P_x \). The numerator in each component can take on only \( n + 1 \) values. So there are at most \( (n + 1)|X| \) choices for the type vector. Of course, these choices are not independent (e.g., the last choice is fixed by the others). But this is a sufficiently good upper bound for our needs.

The crucial point here is that there are only a polynomial number of types of length \( n \). Since the number of sequences is exponential in \( n \), it follows that at least one type has exponentially many sequences in its type class. In fact, the largest type class has essentially the same number of elements as the entire set of sequences, to first order in the exponent.

Now, we assume that the sequence \( X_1, X_2, \ldots, X_n \) is drawn i.i.d. according to a distribution \( Q(x) \). All sequences with the same type have the same probability, as shown in the following theorem. Let \( Q^n(x^n) = \prod_{i=1}^n Q(x_i) \) denote the product distribution associated with \( Q \).

Theorem 11.1.2
If \( X_1, X_2, \ldots, X_n \) are drawn i.i.d. according to \( Q(x) \), the probability of \( x \) depends only on its type and is given by
\[ Q^n(x) = 2^{-n(H(P_x) + D(P_x \| Q))}. \]
Proof:
\[ Q^n(x) = \prod_{i=1}^n Q(x_i) \]
\[ = \prod_{a \in X} Q(a)^{N(a|x)} \]
\[ = \prod_{a \in X} Q(a)^{nP_x(a)} \]
\[ = \prod_{a \in X} 2^{nP_x(a) \log Q(a)} \]
\[ = 2^{\sum_{a \in X} nP_x(a) \log Q(a)} \]
\[ = 2^{\sum_{a \in X} n(P_x(a) \log Q(a) - P_x(a) \log P_x(a) + P_x(a) \log P_x(a))} \]
\[ = 2^{\sum_{a \in X} n(-P_x(a) \log P_x(a) + P_x(a) \log Q(a))} \]
\[ = 2^{-n(H(P_x) + D(P_x \| Q))}. \]

Corollary
If \( x \) is in the type class of \( Q \), then
\[ Q^n(x) = 2^{-nH(Q)}. \]
Proof:
If \( x \in T(Q) \), then \( P_x = Q \), which can be substituted into the above equation.

Example 11.1.2
The probability that a fair die produces a particular sequence of length \( n \) with precisely \( n/6 \) occurrences of each face (assuming \( n \) is a multiple of 6) is \( 2^{-nH(1/6, 1/6, \ldots, 1/6)} = 6^{-n} \). This is obvious. However, if the die has a probability mass function \( (1/3, 1/3, 1/6, 1/12, 1/12, 0) \), the probability of observing a particular sequence with precisely these frequencies is precisely \( 2^{-nH(1/3, 1/3, 1/6, 1/12, 1/12, 0)} \) for \( n \) a multiple of 12. This is more interesting.

We now give an estimate of the size of a type class \( T(P) \).

Theorem 11.1.3 (Size of a type class \( T(P) \))
For any type \( P \in P_n \),
\[ \frac{1}{(n + 1)|X|} 2^{nH(P)} \leq |T(P)| \leq 2^{nH(P)}. \]
Proof:
The exact size of \( T(P) \) is easy to calculate. It is a simple combinatorial problem—the number of ways of arranging \( nP(a_1), nP(a_2), \ldots, nP(a_{|X|}) \) objects in a sequence, which is
\[ |T(P)| = \binom{n}{nP(a_1), nP(a_2), \ldots, nP(a_{|X|})}. \]
This value is hard to manipulate, so we derive simple exponential bounds on its value.

We suggest two alternative proofs for the exponential bounds. The first proof uses Stirling’s formula to bound the factorial function, and after some algebra, we can obtain the bounds of the theorem. We give an alternative proof. We first prove the upper bound. Since a type class must have probability \( \leq 1 \), we have
\[ 1 \geq P^n(T(P)) \]
\[ = \sum_{x \in T(P)} P^n(x) \]
\[ = \sum_{x \in T(P)} 2^{-nH(P)}. \]
Thus,
\[ |T(P)| \leq 2^{nH(P)}. \]
Now for the lower bound. We first prove that the type class \( T(P) \) has the highest probability among all type classes under the probability distribution \( P \):
\[ P^n(T(P)) \geq P^n(T(\hat{P})) \]
for all \( \hat{P} \in P_n \).
We lower bound the ratio of probabilities,
\[ \frac{P^n(T(P))}{P^n(T(\hat{P}))} = \frac{|T(P)| \prod_{a \in X} P(a)^{nP(a)}}{|T(\hat{P})| \prod_{a \in X} P(a)^{n\hat{P}(a)}} \]
\[ = \frac{\binom{n}{nP(a_1), nP(a_2), \ldots, nP(a_{|X|})} \prod_{a \in X} P(a)^{nP(a)}}{\binom{n}{n\hat{P}(a_1), n\hat{P}(a_2), \ldots, n\hat{P}(a_{|X|})} \prod_{a \in X} P(a)^{n\hat{P}(a)}}. \]
Now using the simple bound (easy to prove by separately considering the cases \( m \geq n \) and \( m < n \))
\[ \frac{m!}{n!} \geq n^m-n, \]
we obtain
\[ \frac{P^n(T(P))}{P^n(T(\hat{P}))} \geq \prod_{a \in X} (nP(a))^{n\hat{P}(a)-nP(a)}P(a)^{nP(a)(P(a)-\hat{P}(a))}. \]
\[ = \prod_{a \in X} n^{n(\hat{P}(a)-P(a))} \]
\[ = n^{\sum_{a \in X} n(\hat{P}(a)-P(a))} \]
\[ = n^{n(1-1)} \]
\[ = 1. \]
Hence, \( P^n(T(P)) \geq P^n(T(\hat{P})) \). The lower bound now follows easily from this result, since
\[ 1 = \sum_{Q \in P_n} P^n(T(Q)) \]
\[ \leq \sum_{Q \in P_n} \max_Q P^n(T(Q)) \]
\[ = \sum_{Q \in P_n} P^n(T(P)) \]
\[ \leq (n + 1)|X| P^n(T(P)) \]
\[ = (n + 1)|X| \sum_{x \in T(P)} 2^{-nH(P)}. \]
\[ = (n + 1)|X||T(P)|2^{-nH(P)}. \]
Thus,
\[ |T(P)| \geq \frac{1}{(n + 1)|X|} 2^{nH(P)}. \]

--- Page 76 ---
INFORMATION THEORY AND STATISTICS
We give a slightly better approximation for the binary case.

Example 11.1.3 (Binary alphabet)
In this case, the type is defined by the number of 1’s in the sequence, and the size of the type class is therefore
\[ \binom{n}{k}. \]
We show that
\[ \frac{1}{n + 1} 2^{nH\left(\frac{k}{n}\right)} \leq \binom{n}{k} \leq 2^{nH\left(\frac{k}{n}\right)}. \]
These bounds can be proved using Stirling’s approximation for the factorial function (Lemma 17.5.1). But we provide a more intuitive proof below.

We first prove the upper bound. From the binomial formula, for any \( p \),
\[ \sum_{k=0}^n \binom{n}{k} p^k (1-p)^{n-k} = 1. \]
Since all the terms of the sum are positive for \( 0 \leq p \leq 1 \), each of the terms is less than 1. Setting \( p = \frac{k}{n} \) and taking the \( k \)-th term, we get
\[ 1 \geq \binom{n}{k} \left(\frac{k}{n}\right)^k \left(\frac{n-k}{n}\right)^{n-k} \]
\[ = \binom{n}{k} 2^{k \log \frac{k}{n} + (n-k) \log \frac{n-k}{n}} \]
\[ = \binom{n}{k} 2^{-nH\left(\frac{k}{n}\right)}. \]
Hence,
\[ \binom{n}{k} \leq 2^{nH\left(\frac{k}{n}\right)}. \]
For the lower bound, let \( S \) be a random variable with a binomial distribution with parameters \( n \) and \( p \). The most likely value of \( S \) is \( \langle np \rangle \). This can easily be verified from the fact that
\[ \frac{P(S = i + 1)}{P(S = i)} = \frac{n-i}{i+1} \frac{p}{1-p}. \]
Considering the cases when \( i < np \) and when \( i > np \), we find that the most likely value is indeed \( \langle np \rangle \). Then, since there are \( n + 1 \) terms in the binomial sum,
\[ 1 = \sum_{k=0}^n \binom{n}{k} p^k (1-p)^{n-k} \leq (n + 1) \max_k \binom{n}{k} p^k (1-p)^{n-k}. \]
Now let \( p = \frac{k}{n} \). Then we have
\[ 1 \leq (n + 1) \binom{n}{k} \left(\frac{k}{n}\right)^k \left(\frac{n-k}{n}\right)^{n-k}, \]
which by the arguments above is equivalent to
\[ 1 \leq (n + 1) \binom{n}{k} 2^{-nH\left(\frac{k}{n}\right)}. \]
Thus,
\[ \binom{n}{k} \geq \frac{1}{n + 1} 2^{nH\left(\frac{k}{n}\right)}. \]
Combining the two results, we see that
\[ \binom{n}{k} = 2^{nH\left(\frac{k}{n}\right)}. \]
A more precise bound can be found in theorem 17.5.1 when \( k \neq 0 \) or \( n \).

Theorem 11.1.4 (Probability of type class)
For any \( P \in P_n \) and any distribution \( Q \), the probability of the type class \( T(P) \) under \( Q^n \) is \( 2^{-nD(P \| Q)} \) to first order in the exponent. More precisely,
\[ \frac{1}{(n + 1)|X|} 2^{-nD(P \| Q)} \leq Q^n(T(P)) \leq 2^{-nD(P \| Q)}. \]
Proof:
We have
\[ Q^n(T(P)) = \sum_{x \in T(P)} Q^n(x) \]
\[ = \sum_{x \in T(P)} 2^{-n(D(P \| Q) + H(P))} \]
\[ = |T(P)| 2^{-n(D(P \| Q) + H(P))}. \]
Using the bounds on \( |T(P)| \) derived in Theorem 11.1.3, we have
\[ \frac{1}{(n + 1)|X|} 2^{-nD(P \| Q)} \leq Q^n(T(P)) \leq 2^{-nD(P \| Q)}. \]

--- Page 81 ---
11.2
LAW OF LARGE NUMBERS
We can summarize the basic theorems concerning types in four equations:
\[ |P_n| \leq (n + 1)|X|, \]
\[ Q^n(x) = 2^{-n(D(P_x \| Q) + H(P_x))}, \]
\[ |T(P)| = 2^{nH(P)}, \]
\[ Q^n(T(P)) = 2^{-nD(P \| Q)}. \]
These equations state that there are only a polynomial number of types and that there are an exponential number of sequences of each type. We also have an exact formula for the probability of any sequence of type \( P \) under distribution \( Q \) and an approximate formula for the probability of a type class.

These equations allow us to calculate the behavior of long sequences based on the properties of the type of the sequence. For example, for long sequences drawn i.i.d. according to some distribution, the type of the sequence is close to the distribution generating the sequence, and we can use the properties of this distribution to estimate the properties of the sequence. Some of the applications that will be dealt with in the next few sections are as follows:
• The law of large numbers
• Universal source coding
• Sanov’s theorem
• The Chernoff–Stein lemma and hypothesis testing
• Conditional probability and limit theorems

11.2
LAW OF LARGE NUMBERS
The concept of type and type classes enables us to give an alternative statement of the law of large numbers. In fact, it can be used as a proof of a version of the weak law in the discrete case. The most important property of types is that there are only a polynomial number of types, and...



INFORMATION THEORY AND STATISTICS

An exponential number of sequences of each type. Since the probability of each type class depends exponentially on the relative entropy distance between the type P and the distribution Q, type classes that are far from the true distribution have exponentially smaller probability.

Given an ε > 0, we can define a typical set T_ε^Q of sequences for the distribution Q^n as  
T_ε^Q = {x^n : D(P_{x^n} || Q) ≤ ε}.  
(11.63)  

Then the probability that x^n is not typical is  
1 - Q^n(T_ε^Q) = \sum_{P:D(P||Q)>ε} Q^n(T(P))  
(11.64)  
≤ \sum_{P:D(P||Q)>ε} 2^{-nD(P||Q)}  
(Theorem 11.1.4)  
(11.65)  
≤ \sum_{P:D(P||Q)>ε} 2^{-nε}  
(11.66)  
≤ (n + 1)|X|2^{-nε}  
(Theorem 11.1.1)  
(11.67)  
= 2^{-n(ε - |X| log(n+1)/n)},  
(11.68)  
which goes to 0 as n → ∞. Hence, the probability of the typical set T_ε^Q goes to 1 as n → ∞. This is similar to the AEP proved in Chapter 3, which is a form of the weak law of large numbers. We now prove that the empirical distribution P_{X^n} converges to P.

**Theorem 11.2.1**  
Let X_1, X_2, ..., X_n be i.i.d. ∼ P(x). Then  
Pr{D(P_{X^n} || P) > ε} ≤ 2^{-n(ε - |X| log(n+1)/n)},  
(11.69)  
and consequently, D(P_{X^n} || P) → 0 with probability 1.  

**Proof:**  
The inequality (11.69) was proved in (11.68). Summing over n, we find that  
\sum_{n=1}^∞ Pr{D(P_{X^n} || P) > ε} < ∞.  
(11.70)  

Thus, the expected number of occurrences of the event {D(P_{X^n} || P) > ε} for all n is finite, which implies that the actual number of such occurrences is also finite with probability 1 (Borel–Cantelli lemma). Hence, D(P_{X^n} || P) → 0 with probability 1.  

We now define a stronger version of typicality than in Chapter 3.  

**Definition**  
We define the strongly typical set A_*^n(ε) to be the set of sequences in X^n for which the sample frequencies are close to the true values:  
A_*^n(ε) = {  
x ∈ X^n :  
|1/n N(a|x) - P(a)| < ε / |X|, if P(a) > 0  
N(a|x) = 0, if P(a) = 0  
}.  
(11.71)  

Hence, the typical set consists of sequences whose type does not differ from the true probabilities by more than ε / |X| in any component. By the strong law of large numbers, it follows that the probability of the strongly typical set goes to 1 as n → ∞. The additional power afforded by strong typicality is useful in proving stronger results, particularly in universal coding, rate distortion theory, and large deviation theory.

### 11.3 Universal Source Coding  

Huffman coding compresses an i.i.d. source with a known distribution p(x) to its entropy limit H(X). However, if the code is designed for some incorrect distribution q(x), a penalty of D(p||q) is incurred. Thus, Huffman coding is sensitive to the assumed distribution.  

What compression can be achieved if the true distribution p(x) is unknown? Is there a universal code of rate R, say, that suffices to describe every i.i.d. source with entropy H(X) < R? The surprising answer is yes.  

The idea is based on the method of types. There are 2^{nH(P)} sequences of type P. Since there are only a polynomial number of types with denominator n, an enumeration of all sequences x^n with type P_{x^n} such that H(P_{x^n}) < R will require roughly nR bits. Thus, by describing all such sequences, we are prepared to describe any sequence that is likely to arise from any distribution Q having entropy H(Q) < R. We begin with a definition.  

**Definition**  
A fixed-rate block code of rate R for a source X_1, X_2, ..., X_n which has an unknown distribution Q consists of two mappings: the encoder,  
f_n : X^n → {1, 2, ..., 2^{nR}},  
(11.72)  

and the decoder,  
φ_n : {1, 2, ..., 2^{nR}} → X^n.  
(11.73)  

Here, R is called the rate of the code. The probability of error for the code with respect to the distribution Q is  
P_e^{(n)} = Q^n({x^n : φ_n(f_n(x^n)) ≠ x^n}).  
(11.74)  

**Definition**  
A rate R block code for a source will be called universal if the functions f_n and φ_n do not depend on the distribution Q and if P_e^{(n)} → 0 as n → ∞ if R > H(Q).  

We now describe one such universal encoding scheme, due to Csiszár and Körner [149], that is based on the fact that the number of sequences of type P increases exponentially with the entropy and the fact that there are only a polynomial number of types.  

**Theorem 11.3.1**  
There exists a sequence of (2^{nR}, n) universal source codes such that P_e^{(n)} → 0 for every source Q such that H(Q) < R.  

**Proof:**  
Fix the rate R for the code. Let  
R_n = R - |X| log(n + 1)/n.  
(11.75)  

Consider the set of sequences  
A = {x ∈ X^n : H(P_{x^n}) ≤ R_n}.  
(11.76)  

Then  
|A| = \sum_{P ∈ P_n : H(P) ≤ R_n} |T(P)|  
(11.77)  
≤ \sum_{P ∈ P_n : H(P) ≤ R_n} 2^{nH(P)}  
(11.78)  
≤ \sum_{P ∈ P_n : H(P) ≤ R_n} 2^{nR_n}  
(11.79)  
≤ (n + 1)|X|2^{nR_n}  
(11.80)  
= 2^{n(R_n + |X| log(n + 1)/n)}  
(11.81)  
= 2^{nR}.  
(11.82)  

By indexing the elements of A, we define the encoding function f_n as  
f_n(x) = index of x in A if x ∈ A, otherwise 0.  
(11.83)  

The decoding function maps each index onto the corresponding element of A. Hence, all the elements of A are recovered correctly, and all the remaining sequences result in an error. The set of sequences that are recovered correctly is illustrated in Figure 11.2.  

We now show that this encoding scheme is universal. Assume that the distribution of X_1, X_2, ..., X_n is Q and H(Q) < R. Then the probability of decoding error is given by  
P_e^{(n)} = 1 - Q^n(A)  
(11.84)  
= \sum_{P : H(P) > R_n} Q^n(T(P))  
(11.85)  
≤ (n + 1)|X| max_{P : H(P) > R_n} Q^n(T(P))  
(11.86)  
≤ (n + 1)|X| 2^{-n min_{P : H(P) > R_n} D(P || Q)}.  
(11.87)  

Since R_n ↑ R and H(Q) < R, there exists n_0 such that for all n ≥ n_0, R_n > H(Q). Then for n ≥ n_0, min_{P : H(P) > R_n} D(P || Q) must be greater than 0, and the probability of error P_e^{(n)} converges to 0 exponentially fast as n → ∞.  

H(P) = R  
A  

**FIGURE 11.2.** Universal code and the probability simplex. Each sequence with type that lies outside the circle is encoded by its index. There are fewer than 2^{nR} such sequences. Sequences with types within the circle are encoded by 0.  

---

### Error Exponent  

H(Q)  
Rate of code  

**FIGURE 11.3.** Error exponent for the universal code.  

On the other hand, if the distribution Q is such that the entropy H(Q) is greater than the rate R, then with high probability the sequence will have a type outside the set A. Hence, in such cases, the probability of error is close to 1.  

The exponent in the probability of error is  
D_*^{R,Q} = min_{P : H(P) > R} D(P || Q),  
(11.88)  
which is illustrated in Figure 11.3.  

The universal coding scheme described here is only one of many such schemes. It is universal over the set of i.i.d. distributions. There are other schemes, such as the Lempel–Ziv algorithm, which is a variable-rate universal code for all ergodic sources. The Lempel–Ziv algorithm, discussed in Section 13.4, is often used in practice to compress data that cannot be modeled simply, such as English text or computer source code.  

One may wonder why it is ever necessary to use Huffman codes, which are specific to a probability distribution. What do we lose in using a universal code? Universal codes need a longer block length to obtain the same performance as a code designed specifically for the probability distribution. We pay the penalty for this increase in block length by the increased complexity of the encoder and decoder. Hence, a distribution-specific code is best if one knows the distribution of the source.  

### 11.4 Large Deviation Theory  

The subject of large deviation theory can be illustrated by an example. What is the probability that 1/n \sum_{i=1}^n X_i is near 1/3 if X_1, X_2, ..., X_n are drawn i.i.d. Bernoulli(1/3)? This is a small deviation (from the expected outcome) and the probability is near 1. Now, what is the probability that 1/n \sum_{i=1}^n X_i is greater than 3/4 given that X_1, X_2, ..., X_n are Bernoulli(1/3)? This is a large deviation, and the probability is exponentially small. We might estimate the exponent using the central limit theorem, but this is a poor approximation for more than a few standard deviations. We note that 1/n \sum_{i=1}^n X_i = 3/4 is equivalent to P_{X^n} = (1/4, 3/4). Thus, the probability that X^n is near 3/4 is the probability that P_{X^n} is near (3/4, 1/4). The probability of this large deviation will turn out to be ≈ 2^{-nD((3/4, 1/4) || (1/3, 2/3))}. In this section, we estimate the probability of a set of nontypical types.  

Let E be a subset of the set of probability mass functions. For example, E may be the set of probability mass functions with mean μ. With a slight abuse of notation, we write  
Q^n(E) = Q^n(E ∩ P_n) = \sum_{x : P_{x^n} ∈ E ∩ P_n} Q^n(x^n).  
(11.89)  

If E contains a relative entropy neighborhood of Q, then by the weak law of large numbers (Theorem 11.2.1), Q^n(E) → 1. On the other hand, if E does not contain Q or a neighborhood of Q, then by the weak law of large numbers, Q^n(E) → 0 exponentially fast. We will use the method of types to calculate the exponent.  

Let us first give some examples of the kinds of sets E that we are considering. For example, assume that by observation we find that the sample average of g(X) is greater than or equal to α [i.e., 1/n \sum_{i=1}^n g(X_i) ≥ α]. This event is equivalent to the event P_{X^n} ∈ E ∩ P_n, where  
E = { P : \sum_{a ∈ X} g(a)P(a) ≥ α }.  
(11.90)  

Because  
1/n \sum_{i=1}^n g(X_i) ≥ α ⇨ \sum_{a ∈ X} P_{X^n}(a)g(a) ≥ α  
(11.91)  
⇨ P_{X^n} ∈ E ∩ P_n.  
(11.92)  

Thus,  
Pr{1/n \sum_{i=1}^n g(X_i) ≥ α} = Q^n(E ∩ P_n) = Q^n(E).  
(11.93)  

---

**FIGURE 11.4.** Probability simplex and Sanov’s theorem.  

Here, E is a half-space in the space of probability vectors, as illustrated in Figure 11.4.  

**Theorem 11.4.1 (Sanov’s Theorem)**  
Let X_1, X_2, ..., X_n be i.i.d. ∼ Q(x). Let E ⊆ P be a set of probability distributions. Then  
Q^n(E) = Q^n(E ∩ P_n) ≤ (n + 1)|X|2^{-nD(P_* || Q)},  
(11.94)  
where  
P_* = arg min_{P ∈ E} D(P || Q)  
(11.95)  
is the distribution in E that is closest to Q in relative entropy.  

If, in addition, the set E is the closure of its interior, then  
1/n log Q^n(E) → -D(P_* || Q).  
(11.96)  

**Proof:**  
We first prove the upper bound:  
Q^n(E) = \sum_{P ∈ E ∩ P_n} Q^n(T(P))  
(11.97)  
≤ \sum_{P ∈ E ∩ P_n} 2^{-nD(P || Q)}  
(11.98)  
≤ \sum_{P ∈ E ∩ P_n} max_{P ∈ E ∩ P_n} 2^{-nD(P || Q)}  
(11.99)  
= \sum_{P ∈ E ∩ P_n} 2^{-n min_{P ∈ E ∩ P_n} D(P || Q)}  
(11.100)  
≤ \sum_{P ∈ E ∩ P_n} 2^{-n min_{P ∈ E} D(P || Q)}  
(11.101)  
= \sum_{P ∈ E ∩ P_n} 2^{-nD(P_* || Q)}  
(11.102)  
≤ (n + 1)|X|2^{-nD(P_* || Q)},  
(11.103)  
where the last inequality follows from Theorem 11.1.1. Note that P_* need not be a member of P_n.  

We now come to the lower bound, for which we need a “nice” set E, so that for all large n, we can find a distribution in E ∩ P_n that is close to P_*. If we now assume that E is the closure of its interior (thus, the interior must be nonempty), then since ∪_n P_n is dense in the set of all distributions, it follows that E ∩ P_n is nonempty for all n ≥ n_0 for some n_0. We can then find a sequence of distributions P_n such that P_n ∈ E ∩ P_n and D(P_n || Q) → D(P_* || Q). For each n ≥ n_0,  
Q^n(E) = \sum_{P ∈ E ∩ P_n} Q^n(T(P))  
(11.104)  
≥ Q^n(T(P_n))  
(11.105)  
≥ 1 / ((n + 1)|X|) 2^{-nD(P_n || Q)}.  
(11.106)  

Consequently,  
lim inf 1/n log Q^n(E) ≥ lim inf [ -|X| log(n + 1)/n - D(P_n || Q) ]  
(11.107)  
= -D(P_* || Q).  

Combining this with the upper bound establishes the theorem.  

This argument can be extended to continuous distributions using quantization.



**INFORMATION THEORY AND STATISTICS**

### 11.5 EXAMPLES OF SANOV’S THEOREM

Suppose that we wish to find \( \Pr\left\{ \frac{1}{n} \sum_{i=1}^{k} g_j(X_i) \geq \alpha_j, j = 1, 2, \ldots, k \right\} \). Then the set \( E \) is defined as

\[
E = \left\{ P : \sum_{a} P(a) g_j(a) \geq \alpha_j, j = 1, 2, \ldots, k \right\}.
\]

To find the closest distribution in \( E \) to \( Q \), we minimize \( D(P || Q) \) subject to the constraints in (11.108). Using Lagrange multipliers, we construct the functional

\[
J(P) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)} + \sum_{i} \lambda_i \sum_{x} P(x) g_i(x) + \nu \sum_{x} P(x).
\]

We then differentiate and calculate the closest distribution to \( Q \) to be of the form

\[
P^*(x) = \frac{Q(x) e^{\sum_{i} \lambda_i g_i(x)}}{\sum_{a \in \mathcal{X}} Q(a) e^{\sum_{i} \lambda_i g_i(a)}},
\]

where the constants \( \lambda_i \) are chosen to satisfy the constraints. Note that if \( Q \) is uniform, \( P^* \) is the maximum entropy distribution. Verification that \( P^* \) is indeed the minimum follows from the same kinds of arguments as given in Chapter 12.

Let us consider some specific examples:

#### Example 11.5.1 (Dice)
Suppose that we toss a fair die \( n \) times; what is the probability that the average of the throws is greater than or equal to 4? From Sanov’s theorem, it follows that

\[
Q^n(E) = 2^{-n D(P^* || Q)},
\]

where \( P^* \) minimizes \( D(P || Q) \) over all distributions \( P \) that satisfy

\[
\sum_{i=1}^{6} i P(i) \geq 4.
\]

From (11.110), it follows that \( P^* \) has the form

\[
P^*(x) = \frac{2^{\lambda x}}{\sum_{i=1}^{6} 2^{\lambda i}},
\]

with \( \lambda \) chosen so that \( \sum_{i=1}^{6} i P^*(i) = 4 \). Solving numerically, we obtain \( \lambda = 0.2519 \), \( P^* = (0.1031, 0.1227, 0.1461, 0.1740, 0.2072, 0.2468) \), and therefore \( D(P^* || Q) = 0.0624 \) bit. Thus, the probability that the average of 10000 throws is greater than or equal to 4 is approximately \( 2^{-624} \).

#### Example 11.5.2 (Coins)
Suppose that we have a fair coin and want to estimate the probability of observing more than 700 heads in a series of 1000 tosses. The problem is like Example 11.5.1. The probability is

\[
P(X_n \geq 0.7) = 2^{-n D(P^* || Q)},
\]

where \( P^* \) is the \( (0.7, 0.3) \) distribution and \( Q \) is the \( (0.5, 0.5) \) distribution. In this case, \( D(P^* || Q) = 1 - H(P^*) = 1 - H(0.7) = 0.119 \). Thus, the probability of 700 or more heads in 1000 trials is approximately \( 2^{-119} \).

#### Example 11.5.3 (Mutual Dependence)
Let \( Q(x, y) \) be a given joint distribution and let \( Q_0(x, y) = Q(x)Q(y) \) be the associated product distribution formed from the marginals of \( Q \). We wish to know the likelihood that a sample drawn according to \( Q_0 \) will “appear” to be jointly distributed according to \( Q \). Accordingly, let \( (X_i, Y_i) \) be i.i.d. \( \sim Q_0(x, y) = Q(x)Q(y) \). We define joint typicality as we did in Section 7.6; that is, \( (x^n, y^n) \) is jointly typical with respect to a joint distribution \( Q(x, y) \) if the sample entropies are close to their true values:

\[
\left| \frac{1}{n} \log Q(x^n) - H(X) \right| \leq \epsilon,
\]

\[
\left| \frac{1}{n} \log Q(y^n) - H(Y) \right| \leq \epsilon,
\]

\[
\left| \frac{1}{n} \log Q(x^n, y^n) - H(X, Y) \right| \leq \epsilon.
\]

We wish to calculate the probability (under the product distribution) of seeing a pair \( (x^n, y^n) \) that looks jointly typical of \( Q \) [i.e., \( (x^n, y^n) \) satisfies (11.115)–(11.117)]. Thus, \( (x^n, y^n) \) are jointly typical with respect to \( Q(x, y) \) if \( P_{x^n, y^n} \in E \cap P_n(X, Y) \), where

\[
E = \left\{ P(x, y) : \left| \sum_{x, y} P(x, y) \log Q(x) - H(X) \right| \leq \epsilon, \left| \sum_{x, y} P(x, y) \log Q(y) - H(Y) \right| \leq \epsilon, \left| \sum_{x, y} P(x, y) \log Q(x, y) - H(X, Y) \right| \leq \epsilon \right\}.
\]

Using Sanov’s theorem, the probability is

\[
Q_0^n(E) = 2^{-n D(P^* || Q_0)},
\]

where \( P^* \) is the distribution satisfying the constraints that is closest to \( Q_0 \) in relative entropy. In this case, as \( \epsilon \to 0 \), it can be verified (Problem 11.10) that \( P^* \) is the joint distribution \( Q \), and \( Q_0 \) is the product distribution, so that the probability is \( 2^{-n D(Q(x, y) || Q(x)Q(y))} = 2^{-n I(X; Y)} \), which is the same as the result derived in Chapter 7 for the joint AEP.

In the next section, we consider the empirical distribution of the sequence of outcomes given that the type is in a particular set of distributions \( E \). We will show that not only is the probability of the set \( E \) essentially determined by \( D(P^* || Q) \), the distance of the closest element of \( E \) to \( Q \), but also that the conditional type is essentially \( P^* \), so that given that we are in set \( E \), the type is very likely to be close to \( P^* \).

### 11.6 CONDITIONAL LIMIT THEOREM

It has been shown that the probability of a set of types under a distribution \( Q \) is determined essentially by the probability of the closest element of the set to \( Q \); the probability is \( 2^{-n D^*} \) to first order in the exponent, where \( D^* = \min_{P \in E} D(P || Q) \). This follows because the probability of the set of types is the sum of the probabilities of each type, which is bounded by the largest term times the number of terms. Since the number of terms is polynomial in the length of the sequences, the sum is equal to the largest term to first order in the exponent.

We now strengthen the argument to show that not only is the probability of the set \( E \) essentially the same as the probability of the closest type \( P^* \) but also that the total probability of other types that are far away from \( P^* \) is negligible. This implies that with very high probability, the type observed is close to \( P^* \). We call this a conditional limit theorem.

Before we prove this result, we prove a “Pythagorean” theorem, which gives some insight into the geometry of \( D(P || Q) \). Since \( D(P || Q) \) is not a metric, many of the intuitive properties of distance are not valid for \( D(P || Q) \). The next theorem shows a sense in which \( D(P || Q) \) behaves like the square of the Euclidean metric (Figure 11.5).

#### Theorem 11.6.1
For a closed convex set \( E \subset \mathcal{P} \) and distribution \( Q \notin E \), let \( P^* \in E \) be the distribution that achieves the minimum distance to \( Q \); that is,

\[
D(P^* || Q) = \min_{P \in E} D(P || Q).
\]

Then

\[
D(P || Q) \geq D(P || P^*) + D(P^* || Q)
\]

for all \( P \in E \).

**Note:** The main use of this theorem is as follows: Suppose that we have a sequence \( P_n \in E \) that yields \( D(P_n || Q) \to D(P^* || Q) \). Then from the Pythagorean theorem, \( D(P_n || P^*) \to 0 \) as well.

**Proof:**
Consider any \( P \in E \). Let

\[
P_\lambda = \lambda P + (1 - \lambda) P^*.
\]

Then \( P_\lambda \to P^* \) as \( \lambda \to 0 \). Also, since \( E \) is convex, \( P_\lambda \in E \) for \( 0 \leq \lambda \leq 1 \). Since \( D(P^* || Q) \) is the minimum of \( D(P_\lambda || Q) \) along the path \( P^* \to P \), the derivative of \( D(P_\lambda || Q) \) as a function of \( \lambda \) is nonnegative at \( \lambda = 0 \).

Now

\[
D_\lambda = D(P_\lambda || Q) = \sum_{x} P_\lambda(x) \log \frac{P_\lambda(x)}{Q(x)},
\]

and

\[
\frac{dD_\lambda}{d\lambda} = \sum_{x} \left( (P(x) - P^*(x)) \log \frac{P_\lambda(x)}{Q(x)} + (P(x) - P^*(x)) \right).
\]

Setting \( \lambda = 0 \), so that \( P_\lambda = P^* \) and using the fact that \( \sum_{x} P(x) = \sum_{x} P^*(x) = 1 \), we have

\[
0 \leq \left. \frac{dD_\lambda}{d\lambda} \right|_{\lambda = 0} = \sum_{x} (P(x) - P^*(x)) \log \frac{P^*(x)}{Q(x)}.
\]

This simplifies to

\[
D(P || Q) - D(P || P^*) - D(P^* || Q),
\]

which proves the theorem.

**Note:** The relative entropy \( D(P || Q) \) behaves like the square of the Euclidean distance. Suppose that we have a convex set \( E \) in \( \mathbb{R}^n \). Let \( A \) be a point outside the set, \( B \) the point in the set closest to \( A \), and \( C \) any other point in the set. Then the angle between the lines \( BA \) and \( BC \) must be obtuse, which implies that \( |AC|^2 \geq |AB|^2 + |BC|^2 \), which is of the same form as Theorem 11.6.1. This is illustrated in Figure 11.6.

We now prove a useful lemma which shows that convergence in relative entropy implies convergence in the \( L_1 \) norm.

**Definition:**
The \( L_1 \) distance between any two distributions is defined as

\[
\|P_1 - P_2\|_1 = \sum_{a \in \mathcal{X}} |P_1(a) - P_2(a)|.
\]

Let \( A \) be the set on which \( P_1(x) > P_2(x) \). Then

\[
\|P_1 - P_2\|_1 = \sum_{x \in A} (P_1(x) - P_2(x)) + \sum_{x \in A^c} (P_2(x) - P_1(x)),
\]

which simplifies to

\[
2(P_1(A) - P_2(A)).
\]

Also note that

\[
\max_{B \subseteq \mathcal{X}} (P_1(B) - P_2(B)) = P_1(A) - P_2(A) = \frac{1}{2} \|P_1 - P_2\|_1.
\]

The left-hand side of this equation is called the variational distance between \( P_1 \) and \( P_2 \).

**Lemma 11.6.1:**
\[
D(P_1 || P_2) \geq \frac{1}{2 \ln 2} \|P_1 - P_2\|_1^2.
\]

**Proof:**
We first prove it for the binary case. Consider two binary distributions with parameters \( p \) and \( q \) with \( p \geq q \). We will show that

\[
p \log \frac{p}{q} + (1 - p) \log \frac{1 - p}{1 - q} \geq \frac{4}{2 \ln 2} (p - q)^2.
\]

The difference \( g(p, q) \) between the two sides is

\[
g(p, q) = p \log \frac{p}{q} + (1 - p) \log \frac{1 - p}{1 - q} - \frac{4}{2 \ln 2} (p - q)^2.
\]

Then

\[
\frac{\partial g(p, q)}{\partial q} = -\frac{p}{q} \ln 2 + \frac{1 - p}{1 - q} \ln 2 - \frac{4}{\ln 2} (q - p),
\]

which simplifies to

\[
\frac{q - p}{q(1 - q) \ln 2} - \frac{4}{\ln 2} (q - p) \leq 0,
\]

since \( q(1 - q) \leq \frac{1}{4} \) and \( q \leq p \). For \( q = p \), \( g(p, q) = 0 \), and hence \( g(p, q) \geq 0 \) for \( q \leq p \), which proves the lemma for the binary case.

For the general case, for any two distributions \( P_1 \) and \( P_2 \), let

\[
A = \{x : P_1(x) > P_2(x)\}.
\]

Define a new binary random variable \( Y = \phi(X) \), the indicator of the set \( A \), and let \( \hat{P}_1 \) and \( \hat{P}_2 \) be the distributions of \( Y \). Then by the data-processing inequality applied to relative entropies, we have

\[
D(P_1 || P_2) \geq D(\hat{P}_1 || \hat{P}_2),
\]

which is greater than or equal to

\[
\frac{4}{2 \ln 2} (P_1(A) - P_2(A))^2 = \frac{1}{2 \ln 2} \|P_1 - P_2\|_1^2,
\]

by (11.137), and the lemma is proved.

We can now begin the proof of the conditional limit theorem. We first outline the method used. As stated at the beginning of the chapter, the essential idea is that the probability of a type under \( Q \) depends exponentially on the distance of the type from \( Q \), and hence types that are farther away are exponentially less likely to occur. We divide the set of types in \( E \) into two categories: those at about the same distance from \( Q \) as \( P^* \) and those a distance \( 2\delta \) farther away. The second set has exponentially less probability than the first, and hence the first set has a conditional probability tending to 1. We then use the Pythagorean theorem to establish that all the elements in the first set are close to \( P^* \), which will establish the theorem.

The following theorem is an important strengthening of the maximum entropy principle.

**Theorem 11.6.2 (Conditional Limit Theorem):**
Let \( E \) be a closed convex subset of \( \mathcal{P} \) and let \( Q \) be a distribution not in \( E \). Let \( X_1, X_2, \ldots, X_n \) be discrete random variables drawn i.i.d. \( \sim Q \). Let \( P^* \) achieve \( \min_{P \in E} D(P || Q) \). Then

\[
\Pr(X_1 = a | P_{X^n} \in E) \to P^*(a)
\]

in probability as \( n \to \infty \), i.e., the conditional distribution of \( X_1 \), given that the type of the sequence is in \( E \), is close to \( P^* \) for large \( n \).

**Example 11.6.1:**
If \( X_i \) i.i.d. \( \sim Q \), then

\[
\Pr\left( X_1 = a \left| \frac{1}{n} \sum_{i=2}^{n} g(X_i) \geq \alpha \right. \right) \to P^*(a).
\]



INFORMATION THEORY AND STATISTICS

where \( P^*(a) \) minimizes \( D(P || Q) \) over \( P \) satisfying \( \mathbb{E}[a^2] \geq \alpha \). This minimization results in

\[
P^*(a) = \frac{Q(a)e^{\lambda a^2}}{\sum_{a} Q(a)e^{\lambda a^2}},
\]

where \( \lambda \) is chosen to satisfy \( \mathbb{E}[P^*(a)a^2] = \alpha \). Thus, the conditional distribution on \( X_1 \) given a constraint on the sum of the squares is a (normalized) product of the original probability mass function and the maximum entropy probability mass function (which in this case is Gaussian).

**Proof of Theorem:**

Define the sets

\[
S_t = \{P \in \mathcal{P} : D(P || Q) \leq t\}.
\]

The set \( S_t \) is convex since \( D(P || Q) \) is a convex function of \( P \). Let

\[
D^* = D(P^* || Q) = \min_{P \in \mathcal{E}} D(P || Q).
\]

Then \( P^* \) is unique, since \( D(P || Q) \) is strictly convex in \( P \). Now define the set

\[
A = S_{D^* + 2\delta} \cap \mathcal{E}
\]

and

\[
B = \mathcal{E} \setminus S_{D^* + 2\delta} \cap \mathcal{E}.
\]

Thus, \( A \cup B = \mathcal{E} \). These sets are illustrated in Figure 11.7. Then

\[
Q^n(B) = \sum_{P \in \mathcal{E} \cap \mathcal{P}_n: D(P || Q) > D^* + 2\delta} Q^n(T(P))
\]

\[
\leq \sum_{P \in \mathcal{E} \cap \mathcal{P}_n: D(P || Q) > D^* + 2\delta} 2^{-nD(P || Q)}
\]

\[
\leq \sum_{P \in \mathcal{E} \cap \mathcal{P}_n: D(P || Q) > D^* + 2\delta} 2^{-n(D^* + 2\delta)}
\]

\[
\leq (n + 1)|\mathcal{X}| 2^{-n(D^* + 2\delta)}
\]

since there are only a polynomial number of types. On the other hand,

\[
Q^n(A) \geq Q^n(S_{D^* + \delta} \cap \mathcal{E})
\]

\[
= \sum_{P \in \mathcal{E} \cap \mathcal{P}_n: D(P || Q) \leq D^* + \delta} Q^n(T(P))
\]

\[
\geq \sum_{P \in \mathcal{E} \cap \mathcal{P}_n: D(P || Q) \leq D^* + \delta} \frac{1}{(n + 1)|\mathcal{X}|} 2^{-nD(P || Q)}
\]

\[
\geq \frac{1}{(n + 1)|\mathcal{X}|} 2^{-n(D^* + \delta)}
\]

for \( n \) sufficiently large, since the sum is greater than one of the terms, and for sufficiently large \( n \), there exists at least one type in \( S_{D^* + \delta} \cap \mathcal{E} \cap \mathcal{P}_n \). Then, for \( n \) sufficiently large,

\[
\Pr(P_{X^n} \in B | P_{X^n} \in \mathcal{E}) = \frac{Q^n(B \cap \mathcal{E})}{Q^n(\mathcal{E})}
\]

\[
\leq \frac{Q^n(B)}{Q^n(A)}
\]

\[
\leq \frac{(n + 1)|\mathcal{X}| 2^{-n(D^* + 2\delta)}}{\frac{1}{(n + 1)|\mathcal{X}|} 2^{-n(D^* + \delta)}}
\]

\[
= (n + 1)^2 |\mathcal{X}|^2 2^{-n\delta},
\]

which goes to 0 as \( n \to \infty \). Hence, the conditional probability of \( B \) goes to 0 as \( n \to \infty \), which implies that the conditional probability of \( A \) goes to 1.

We now show that all the members of \( A \) are close to \( P^* \) in relative entropy. For all members of \( A \),

\[
D(P || Q) \leq D^* + 2\delta.
\]

Hence, by the “Pythagorean” theorem (Theorem 11.6.1),

\[
D(P || P^*) + D(P^* || Q) \leq D(P || Q) \leq D^* + 2\delta,
\]

which in turn implies that

\[
D(P || P^*) \leq 2\delta,
\]

since \( D(P^* || Q) = D^* \). Thus, \( P_{X^n} \in A \) implies that \( D(P_{X^n} || Q) \leq D^* + 2\delta \), and therefore that \( D(P_{X^n} || P^*) \leq 2\delta \). Consequently, since \( \Pr(P_{X^n} \in A | P_{X^n} \in \mathcal{E}) \to 1 \), it follows that

\[
\Pr(D(P_{X^n} || P^*) \leq 2\delta | P_{X^n} \in \mathcal{E}) \to 1
\]

as \( n \to \infty \). By Lemma 11.6.1, the fact that the relative entropy is small implies that the \( L_1 \) distance is small, which in turn implies that \( \max_{a \in \mathcal{X}} |P_{X^n}(a) - P^*(a)| \) is small. Thus,

\[
\Pr(\max_{a \in \mathcal{X}} |P_{X^n}(a) - P^*(a)| \geq \varepsilon | P_{X^n} \in \mathcal{E}) \to 0
\]

as \( n \to \infty \). Alternatively, this can be written as

\[
\Pr(X_1 = a | P_{X^n} \in \mathcal{E}) \to P^*(a)
\]

in probability, \( a \in \mathcal{X} \).

In this theorem, we have only proved that the marginal distribution goes to \( P^* \) as \( n \to \infty \). Using a similar argument, we can prove a stronger version of this theorem:

\[
\Pr(X_1 = a_1, X_2 = a_2, \ldots, X_m = a_m | P_{X^n} \in \mathcal{E}) \to \prod_{i=1}^m P^*(a_i)
\]

in probability. This holds for fixed \( m \) as \( n \to \infty \). The result is not true for \( m = n \), since there are end effects; given that the type of the sequence is in \( \mathcal{E} \), the last elements of the sequence can be determined from the remaining elements, and the elements are no longer independent. The conditional limit theorem is illustrated in Figure 11.7.



# Hypothesis Testing

## 11.7 Hypothesis Testing

One of the standard problems in statistics is to decide between two alternative explanations for the data observed. For example, in medical testing, one may wish to test whether or not a new drug is effective. Similarly, a sequence of coin tosses may reveal whether or not the coin is biased. These problems are examples of the general hypothesis-testing problem.

In the simplest case, we have to decide between two i.i.d. distributions. The general problem can be stated as follows:

**Problem 11.7.1**  
Let \( X_1, X_2, \ldots, X_n \) be i.i.d. \( \sim Q(x) \). We consider two hypotheses:  
- \( H_1: Q = P_1 \).  
- \( H_2: Q = P_2 \).  

Consider the general decision function \( g(x_1, x_2, \ldots, x_n) \), where \( g(x_1, x_2, \ldots, x_n) = 1 \) means that \( H_1 \) is accepted and \( g(x_1, x_2, \ldots, x_n) = 2 \) means that \( H_2 \) is accepted. Since the function takes on only two values, the test can also be specified by specifying the set \( A \) over which \( g(x_1, x_2, \ldots, x_n) \) is 1; the complement of this set is the set where \( g(x_1, x_2, \ldots, x_n) \) has the value 2. We define the two probabilities of error:  
\[
\alpha = \Pr(g(X_1, X_2, \ldots, X_n) = 2 | H_1 \text{ true}) = P_1^n(A^c)
\]  
and  
\[
\beta = \Pr(g(X_1, X_2, \ldots, X_n) = 1 | H_2 \text{ true}) = P_2^n(A).
\]  

In general, we wish to minimize both probabilities, but there is a trade-off. Thus, we minimize one of the probabilities of error subject to a constraint on the other probability of error. The best achievable error exponent in the probability of error for this problem is given by the Chernoff–Stein lemma.

We first prove the Neyman–Pearson lemma, which derives the form of the optimum test between two hypotheses. We derive the result for discrete distributions; the same results can be derived for continuous distributions as well.

**Theorem 11.7.1 (Neyman–Pearson lemma)**  
Let \( X_1, X_2, \ldots, X_n \) be drawn i.i.d. according to probability mass function \( Q \). Consider the decision problem corresponding to hypotheses \( Q = P_1 \) vs. \( Q = P_2 \). For \( T \geq 0 \), define a region  
\[
A_n(T) = \left\{ x_1, x_2, \ldots, x_n : \frac{P_1(x_1, x_2, \ldots, x_n)}{P_2(x_1, x_2, \ldots, x_n)} > T \right\}.
\]  
Let  
\[
\alpha^* = P_1^n(A_n(T)^c), \quad \beta^* = P_2^n(A_n(T))
\]  
be the corresponding probabilities of error corresponding to decision region \( A_n \). Let \( B_n \) be any other decision region with associated probabilities of error \( \alpha \) and \( \beta \). If \( \alpha \leq \alpha^* \), then \( \beta \geq \beta^* \).

*Proof:*  
Let \( A = A_n(T) \) be the region defined in (11.176) and let \( B \subseteq X^n \) be any other acceptance region. Let \( \phi_A \) and \( \phi_B \) be the indicator functions of the decision regions \( A \) and \( B \), respectively. Then for all \( x = (x_1, x_2, \ldots, x_n) \in X^n \),  
\[
(\phi_A(x) - \phi_B(x))\left(P_1(x) - T P_2(x)\right) \geq 0.
\]  
This can be seen by considering separately the cases \( x \in A \) and \( x \notin A \). Multiplying out and summing this over the entire space, we obtain  
\[
0 \leq \sum_{x} (\phi_A(x)P_1(x) - T \phi_A(x)P_2(x) - P_1(x)\phi_B(x) + T P_2(x)\phi_B(x)).
\]  
This simplifies to  
\[
0 \leq (1 - \alpha^*) - T\beta^* - (1 - \alpha) + T\beta,
\]  
which reduces to  
\[
T(\beta - \beta^*) - (\alpha^* - \alpha).
\]  
Since \( T \geq 0 \), we have proved the theorem.  

The Neyman–Pearson lemma indicates that the optimum test for two hypotheses is of the form  
\[
\frac{P_1(X_1, X_2, \ldots, X_n)}{P_2(X_1, X_2, \ldots, X_n)} > T.
\]  
This is the likelihood ratio test, and the quantity \( \frac{P_1(X_1, X_2, \ldots, X_n)}{P_2(X_1, X_2, \ldots, X_n)} \) is called the likelihood ratio. For example, in a test between two Gaussian distributions, the likelihood ratio becomes  
\[
\frac{f_1(X_1, X_2, \ldots, X_n)}{f_2(X_1, X_2, \ldots, X_n)} = e^{\frac{2}{\sigma^2} \sum_{i=1}^n X_i} = e^{\frac{2}{\sigma^2} n \bar{X}_n}.
\]  
Hence, the likelihood ratio test consists of comparing the sample mean \( \bar{X}_n \) with a threshold. If we want the two probabilities of error to be equal, we should set \( T = 1 \). This is illustrated in Figure 11.8.

In Theorem 11.7.1, we have shown that the optimum test is a likelihood ratio test. We can rewrite the log-likelihood ratio as  
\[
L(X_1, X_2, \ldots, X_n) = \log \frac{P_1(X_1, X_2, \ldots, X_n)}{P_2(X_1, X_2, \ldots, X_n)} = \sum_{i=1}^n \log \frac{P_1(X_i)}{P_2(X_i)}.
\]  
This can be expressed as  
\[
L(X_1, X_2, \ldots, X_n) = \sum_{a \in \mathcal{X}} n P_n(a) \log \frac{P_1(a)}{P_2(a)},
\]  
where \( P_n(a) \) is the empirical distribution of the sample. The likelihood ratio test  
\[
\frac{P_1(X_1, X_2, \ldots, X_n)}{P_2(X_1, X_2, \ldots, X_n)} > T
\]  
is equivalent to  
\[
D(P_n || P_2) - D(P_n || P_1) > \frac{1}{n} \log T.
\]  

We can consider the test to be equivalent to specifying a region of the simplex of types that corresponds to choosing hypothesis \( H_1 \). The optimum region is of the form  
\[
D(P_n || P_2) - D(P_n || P_1) > \frac{1}{n} \log T.
\]  
The boundary of the region is the set of types for which the difference between the distances is a constant. This boundary is the analog of the perpendicular bisector in Euclidean geometry. The test is illustrated in Figure 11.9.

We now offer some informal arguments based on Sanov’s theorem to show how to choose the threshold to obtain different probabilities of error. Let \( B \) denote the set on which hypothesis 1 is accepted. The probability of error of the first kind is  
\[
\alpha_n = P_1^n(P_n \in B^c).
\]  
Since the set \( B^c \) is convex, we can use Sanov’s theorem to show that the probability of error is determined essentially by the relative entropy of the closest member of \( B^c \) to \( P_1 \). Therefore,  
\[
\alpha_n \approx 2^{-n D(P_1^* || P_1)},
\]  
where \( P_1^* \) is the closest element of \( B^c \) to distribution \( P_1 \). Similarly,  
\[
\beta_n \approx 2^{-n D(P_2^* || P_2)},
\]  
where \( P_2^* \) is the closest element in \( B \) to the distribution \( P_2 \).

Now minimizing \( D(P || P_2) \) subject to the constraint \( D(P || P_2) - D(P || P_1) \geq \frac{1}{n} \log T \) will yield the type in \( B \) that is closest to \( P_2 \). Setting up the minimization of \( D(P || P_2) \) subject to \( D(P || P_2) - D(P || P_1) = \frac{1}{n} \log T \) using Lagrange multipliers, we have  
\[
J(P) = \sum_{x} P(x) \log \frac{P(x)}{P_2(x)} + \lambda \sum_{x} P(x) \log \frac{P_1(x)}{P_2(x)} + \nu \sum_{x} P(x).
\]  
Differentiating with respect to \( P(x) \) and setting to 0, we have  
\[
\log \frac{P(x)}{P_2(x)} + 1 + \lambda \log \frac{P_1(x)}{P_2(x)} + \nu = 0.
\]  

Solving this set of equations, we obtain the minimizing \( P \) of the form  
\[
P_2^* = P_\lambda^* = \frac{P_1(x) P_2(x)^{1-\lambda}}{\sum_{a \in \mathcal{X}} P_1(a) P_2(a)^{1-\lambda}},
\]  
where \( \lambda \) is chosen so that \( D(P_\lambda^* || P_1) - D(P_\lambda^* || P_2) = \frac{1}{n} \log T \). From the symmetry of expression (11.200), it is clear that \( P_1^* = P_2^* \) and that the probabilities of error behave exponentially with exponents given by the relative entropies \( D(P^* || P_1) \) and \( D(P^* || P_2) \). Also note from the equation that as \( \lambda \to 1 \), \( P_\lambda \to P_1 \) and as \( \lambda \to 0 \), \( P_\lambda \to P_2 \). The curve that \( P_\lambda \) traces out as \( \lambda \) varies is a geodesic in the simplex. Here \( P_\lambda \) is a normalized convex combination, where the combination is in the exponent (Figure 11.9).

In the next section, we calculate the best error exponent when one of the two types of error goes to zero arbitrarily slowly (the Chernoff–Stein lemma). We will also minimize the weighted sum of the two probabilities of error and obtain the Chernoff information bound.

---

## 11.8 Chernoff–Stein Lemma

We consider hypothesis testing in the case when one of the probabilities of error is held fixed and the other is made as small as possible. We will show that the other probability of error is exponentially small, with an exponential rate equal to the relative entropy between the two distributions. The method of proof uses a relative entropy version of the AEP.

**Theorem 11.8.1 (AEP for relative entropy)**  
Let \( X_1, X_2, \ldots, X_n \) be a sequence of random variables drawn i.i.d. according to \( P_1(x) \), and let \( P_2(x) \) be any other distribution on \( \mathcal{X} \). Then  
\[
\frac{1}{n} \log \frac{P_1(X_1, X_2, \ldots, X_n)}{P_2(X_1, X_2, \ldots, X_n)} \to D(P_1 || P_2)
\]  
in probability.

*Proof:*  
This follows directly from the weak law of large numbers.  
\[
\frac{1}{n} \log \frac{P_1(X_1, X_2, \ldots, X_n)}{P_2(X_1, X_2, \ldots, X_n)} = \frac{1}{n} \log \prod_{i=1}^n \frac{P_1(X_i)}{P_2(X_i)} = \frac{1}{n} \sum_{i=1}^n \log \frac{P_1(X_i)}{P_2(X_i)}.
\]  
By the weak law of large numbers,  
\[
\frac{1}{n} \sum_{i=1}^n \log \frac{P_1(X_i)}{P_2(X_i)} \to E_{P_1} \left[ \log \frac{P_1(X)}{P_2(X)} \right] = D(P_1 || P_2).
\]  

Just as for the regular AEP, we can define a relative entropy typical sequence as one for which the empirical relative entropy is close to its expected value.

**Definition**  
For a fixed \( n \) and \( \epsilon > 0 \), a sequence \( (x_1, x_2, \ldots, x_n) \in \mathcal{X}^n \) is said to be relative entropy typical if and only if  
\[
D(P_1 || P_2) - \epsilon \leq \frac{1}{n} \log \frac{P_1(x_1, x_2, \ldots, x_n)}{P_2(x_1, x_2, \ldots, x_n)} \leq D(P_1 || P_2) + \epsilon.
\]  
The set of relative entropy typical sequences is called the relative entropy typical set \( A(n)_\epsilon(P_1 || P_2) \).

As a consequence of the relative entropy AEP, we can show that the relative entropy typical set satisfies the following properties:

**Theorem 11.8.2**  
1. For \( (x_1, x_2, \ldots, x_n) \in A(n)_\epsilon(P_1 || P_2) \),  
\[
P_1(x_1, x_2, \ldots, x_n) 2^{-n(D(P_1 || P_2) + \epsilon)} \leq P_2(x_1, x_2, \ldots, x_n) \leq P_1(x_1, x_2, \ldots, x_n) 2^{-n(D(P_1 || P_2) - \epsilon)}.
\]  
2. \( P_1(A(n)_\epsilon(P_1 || P_2)) > 1 - \epsilon \), for \( n \) sufficiently large.  
3. \( P_2(A(n)_\epsilon(P_1 || P_2)) < 2^{-n(D(P_1 || P_2) - \epsilon)} \).  
4. \( P_2(A(n)_\epsilon(P_1 || P_2)) > (1 - \epsilon) 2^{-n(D(P_1 || P_2) + \epsilon)} \), for \( n \) sufficiently large.

*Proof:*  
The proof follows the same lines as the proof of Theorem 3.1.2, with the counting measure replaced by probability measure \( P_2 \). The proof of property 1 follows directly from the definition of the relative entropy typical set.



INFORMATION THEORY AND STATISTICS

typical set. The second property follows from the AEP for relative entropy (Theorem 11.8.1). To prove the third property, we write

P₂(Aⁿ_ε(P₁||P₂)) = ∑_{xⁿ ∈ Aⁿ_ε(P₁||P₂)} P₂(x₁, x₂, ..., xₙ)  
(11.208)  
≤ ∑_{xⁿ ∈ Aⁿ_ε(P₁||P₂)} P₁(x₁, x₂, ..., xₙ)2^{-n(D(P₁||P₂)-ε)}  
(11.209)  
= 2^{-n(D(P₁||P₂)-ε)} ∑_{xⁿ ∈ Aⁿ_ε(P₁||P₂)} P₁(x₁, x₂, ..., xₙ)  
(11.210)  
= 2^{-n(D(P₁||P₂)-ε)}P₁(Aⁿ_ε(P₁||P₂))  
(11.211)  
≤ 2^{-n(D(P₁||P₂)-ε)},  
(11.212)  
where the first inequality follows from property 1, and the second inequality follows from the fact that the probability of any set under P₁ is less than 1.

To prove the lower bound on the probability of the relative entropy typical set, we use a parallel argument with a lower bound on the probability:

P₂(Aⁿ_ε(P₁||P₂)) = ∑_{xⁿ ∈ Aⁿ_ε(P₁||P₂)} P₂(x₁, x₂, ..., xₙ)  
(11.213)  
≥ ∑_{xⁿ ∈ Aⁿ_ε(P₁||P₂)} P₁(x₁, x₂, ..., xₙ)2^{-n(D(P₁||P₂)+ε)}  
(11.214)  
= 2^{-n(D(P₁||P₂)+ε)} ∑_{xⁿ ∈ Aⁿ_ε(P₁||P₂)} P₁(x₁, x₂, ..., xₙ)  
(11.215)  
= 2^{-n(D(P₁||P₂)+ε)}P₁(Aⁿ_ε(P₁||P₂))  
(11.216)  
≥ (1 - ε)2^{-n(D(P₁||P₂)+ε)},  
(11.217)  
where the second inequality follows from the second property of Aⁿ_ε(P₁||P₂).  

With the standard AEP in Chapter 3, we also showed that any set that has a high probability has a high intersection with the typical set, and therefore has about 2^{nH} elements. We now prove the corresponding result for relative entropy.

CHERNOFF–STEIN LEMMA  

Lemma 11.8.1  
Let Bⁿ ⊂ Xⁿ be any set of sequences x₁, x₂, ..., xₙ such that P₁(Bⁿ) > 1 - ε. Let P₂ be any other distribution such that D(P₁||P₂) < ∞. Then P₂(Bⁿ) > (1 - 2ε)2^{-n(D(P₁||P₂)+ε)}.  

Proof:  
For simplicity, we will denote Aⁿ_ε(P₁||P₂) by Aₙ. Since P₁(Bⁿ) > 1 - ε and P(Aₙ) > 1 - ε (Theorem 11.8.2), we have, by the union of events bound, P₁(Aₙᶜ ∪ Bⁿᶜ) < 2ε, or equivalently, P₁(Aₙ ∩ Bⁿ) > 1 - 2ε.  

Thus,  
P₂(Bⁿ) ≥ P₂(Aₙ ∩ Bⁿ)  
(11.218)  
= ∑_{xⁿ ∈ Aₙ ∩ Bⁿ} P₂(xⁿ)  
(11.219)  
≥ ∑_{xⁿ ∈ Aₙ ∩ Bⁿ} P₁(xⁿ)2^{-n(D(P₁||P₂)+ε)}  
(11.220)  
= 2^{-n(D(P₁||P₂)+ε)} ∑_{xⁿ ∈ Aₙ ∩ Bⁿ} P₁(xⁿ)  
(11.221)  
= 2^{-n(D(P₁||P₂)+ε)}P₁(Aₙ ∩ Bⁿ)  
(11.222)  
≥ 2^{-n(D(P₁||P₂)+ε)}(1 - 2ε),  
(11.223)  
where the second inequality follows from the properties of the relative entropy typical sequences (Theorem 11.8.2) and the last inequality follows from the union bound above.  

We now consider the problem of testing two hypotheses, P₁ vs. P₂. We hold one of the probabilities of error fixed and attempt to minimize the other probability of error. We show that the relative entropy is the best exponent in probability of error.

Theorem 11.8.3 (Chernoff–Stein Lemma)  
Let X₁, X₂, ..., Xₙ be i.i.d. ∼ Q. Consider the hypothesis test between two alternatives, Q = P₁ and Q = P₂, where D(P₁||P₂) < ∞. Let Aⁿ ⊆ Xⁿ be an acceptance region for hypothesis H₁. Let the probabilities of error be  

αₙ = P_{1}^{n}(Aⁿᶜ),  
βₙ = P_{2}^{n}(Aⁿ).  
(11.224)  

and for 0 < ε < 1/2, define  

β_{ε,n} = min_{Aⁿ ⊆ Xⁿ} βₙ  
subject to αₙ < ε.  
(11.225)  

Then  
lim_{n→∞} (1/n) log β_{ε,n} = -D(P₁||P₂).  
(11.226)  

Proof:  
We prove this theorem in two parts. In the first part, we exhibit a sequence of sets Aⁿ for which the probability of error βₙ goes exponentially to zero as D(P₁||P₂). In the second part, we show that no other sequence of sets can have a lower exponent in the probability of error.  

For the first part, we choose as the sets Aⁿ = Aⁿ_ε(P₁||P₂). As proved in Theorem 11.8.2, this sequence of sets has P₁(Aⁿᶜ) < ε for n large enough. Also,  
lim_{n→∞} (1/n) log P₂(Aⁿ) ≤ - (D(P₁||P₂) - ε)  
(11.227)  
from property 3 of Theorem 11.8.2. Thus, the relative entropy typical set satisfies the bounds of the lemma.  

To show that no other sequence of sets can do better, consider any sequence of sets Bⁿ with P₁(Bⁿ) > 1 - ε. By Lemma 11.8.1, we have  
P₂(Bⁿ) > (1 - 2ε)2^{-n(D(P₁||P₂)+ε)}, and therefore  
lim_{n→∞} (1/n) log P₂(Bⁿ) > - (D(P₁||P₂) + ε) + lim_{n→∞} (1/n) log(1 - 2ε)  
= - (D(P₁||P₂) + ε).  
(11.228)  
Thus, no other sequence of sets has a probability of error exponent better than D(P₁||P₂). Thus, the set sequence Aⁿ = Aⁿ_ε(P₁||P₂) is asymptotically optimal in terms of the exponent in the probability.  

Note that the relative entropy typical set, although asymptotically optimal (i.e., achieving the best asymptotic rate), is not the optimal set for any fixed hypothesis-testing problem. The optimal set that minimizes the probabilities of error is that given by the Neyman–Pearson lemma.

CHERNOFF INFORMATION  

We have considered the problem of hypothesis testing in the classical setting, in which we treat the two probabilities of error separately. In the derivation of the Chernoff–Stein lemma, we set αₙ ≤ ε and achieved βₙ = 2^{-nD}. But this approach lacks symmetry. Instead, we can follow a Bayesian approach, in which we assign prior probabilities to both hypotheses. In this case, we wish to minimize the overall probability of error given by the weighted sum of the individual probabilities of error. The resulting error exponent is the Chernoff information.  

The setup is as follows: X₁, X₂, ..., Xₙ i.i.d. ∼ Q. We have two hypotheses: Q = P₁ with prior probability π₁ and Q = P₂ with prior probability π₂. The overall probability of error is  

P_e^{(n)} = π₁αₙ + π₂βₙ.  
(11.229)  

Let  

D* = lim_{n→∞} - (1/n) log min_{Aⁿ ⊆ Xⁿ} P_e^{(n)}.  
(11.230)  

Theorem 11.9.1 (Chernoff)  
The best achievable exponent in the Bayesian probability of error is D*, where  

D* = D(P_{λ*}||P₁) = D(P_{λ*}||P₂),  
(11.231)  

with  

P_λ(x) = P_λ₁(x)P_{1-λ}₂(x) / ∑_{a ∈ X} P_λ₁(a)P_{1-λ}₂(a),  
(11.232)  

and λ* the value of λ such that  

D(P_{λ*}||P₁) = D(P_{λ*}||P₂).  
(11.233)  

Proof:  
The basic details of the proof were given in Section 11.8. We have shown that the optimum test is a likelihood ratio test, which can be considered to be of the form  

D(P_{Xⁿ}||P₂) - D(P_{Xⁿ}||P₁) > (1/n) log T.  
(11.234)  

The test divides the probability simplex into regions corresponding to hypothesis H₁ and hypothesis H₂, respectively. This is illustrated in Figure 11.10.  

Let A be the set of types associated with hypothesis H₁. From the discussion preceding (11.200), it follows that the closest point in the set Aᶜ to P₁ is on the boundary of A and is of the form given by (11.232). Then from the discussion in Section 11.8, it is clear that P_λ is the distribution in A that is closest to P₂; it is also the distribution in Aᶜ that is closest to P₁. By Sanov’s theorem, we can calculate the associated probabilities of error,  

αₙ = P_{1}^{n}(Aᶜ) = 2^{-nD(P_{λ*}||P₁)},  
(11.235)  

and  

βₙ = P_{2}^{n}(A) = 2^{-nD(P_{λ*}||P₂)}.  
(11.236)  

In the Bayesian case, the overall probability of error is the weighted sum of the two probabilities of error,  

P_e^{(n)} = π₁2^{-nD(P_{λ*}||P₁)} + π₂2^{-nD(P_{λ*}||P₂)}.  
(11.237)  

The resulting error exponent is the Chernoff information.  

The definition D* = D(P_{λ*}||P₁) = D(P_{λ*}||P₂) is equivalent to the standard definition of Chernoff information,  

C(P₁, P₂) △= - min_{0 ≤ λ ≤ 1} log ∑_{x} P_λ₁(x)P_{1-λ}₂(x).  
(11.238)  

It is left as an exercise to the reader to show the equivalence of (11.231) and (11.238).  

We outline briefly the usual derivation of the Chernoff information bound. The maximum a posteriori probability decision rule minimizes the Bayesian probability of error. The decision region A for hypothesis H₁ for the maximum a posteriori rule is  

A = {x : π₁P₁(x) / π₂P₂(x) > 1}.  
(11.239)  

The probability of error for this rule is  

P_e = π₁αₙ + π₂βₙ.  
(11.240)  

Now for any two positive numbers a and b, we have  

min{a, b} ≤ a^{λ}b^{1-λ} for all 0 ≤ λ ≤ 1.  
(11.241)  

Using this to continue the chain, we have  

P_e = ∑_{Aᶜ} π₁P₁ + ∑_{A} π₂P₂  
(11.242)  
= ∑ min{π₁P₁, π₂P₂}.  
(11.243)  

For a sequence of i.i.d. observations, P_k(x) = ∏_{i=1}^{n} P_k(x_i), and  

P_e^{(n)} ≤ ∑_{xⁿ} (π₁P₁(xⁿ))^{λ}(π₂P₂(xⁿ))^{1-λ}  
(11.244)  
= π₁^{λ}π₂^{1-λ} ∑_{xⁿ} P_λ₁(xⁿ)P_{1-λ}₂(xⁿ)  
(11.245)  
= π₁^{λ}π₂^{1-λ} (∑_{x} P_λ₁(x)P_{1-λ}₂(x))^{n}.  
(11.246)  

Hence,  

(1/n) log P_e^{(n)} ≤ log ∑_{x} P_λ₁(x)P_{1-λ}₂(x).  
(11.247)  

Since this is true for all λ, we can take the minimum over 0 ≤ λ ≤ 1, resulting in the Chernoff information bound. This proves that the exponent is no better than C(P₁, P₂). Achievability follows from Theorem 11.9.1.  

Note that the Bayesian error exponent does not depend on the actual value of π₁ and π₂, as long as they are nonzero. Essentially, the effect of the prior is washed out for large sample sizes. The optimum decision rule is to choose the hypothesis with the maximum a posteriori probability, which corresponds to the test  

π₁P₁(X₁, X₂, ..., Xₙ) / π₂P₂(X₁, X₂, ..., Xₙ) > 1.  
(11.248)



--- Page 15 ---
11.9
CHERNOFF INFORMATION
389
Taking the log and dividing by n, this test can be rewritten as
1
n log π1
π2
+ 1
n
i
log P1(Xi)
P2(Xi)
<> 0,
(11.254)
where the second term tends to D(P1||P2) or −D(P2||P1) accordingly as
P1 or P2 is the true distribution. The first term tends to 0, and the effect
of the prior distribution washes out.
Finally, to round off our discussion of large deviation theory and hypothesis
testing, we consider an example of the conditional limit theorem.
Example 11.9.1
Suppose that major league baseball players have a batting
average of 260 with a standard deviation of 15 and suppose that minor
league ballplayers have a batting average of 240 with a standard deviation
of 15. A group of 100 ballplayers from one of the leagues (the league is
chosen at random) are found to have a group batting average greater than
250 and are therefore judged to be major leaguers. We are now told that
we are mistaken; these players are minor leaguers. What can we say about
the distribution of batting averages among these 100 players? The conditional
limit theorem can be used to show that the distribution of batting averages
among these players will have a mean of 250 and a standard deviation of 15.
To see this, we abstract the problem as follows.
Let us consider an example of testing between two Gaussian distributions,
f1 = N(1, σ²) and f2 = N(−1, σ²), with different means and the same
variance. As discussed in Section 11.8, the likelihood ratio test in this case
is equivalent to comparing the sample mean with a threshold. The Bayes test
is "Accept the hypothesis f = f1 if 1
n
i=1 Xi > 0." Now assume that we
make an error of the first kind (we say that f = f1 when indeed f = f2) in
this test. What is the conditional distribution of the samples given that we
have made an error?
We might guess at various possibilities:
• The sample will look like a (1/2, 1/2) mix of the two normal distributions.
Plausible as this is, it is incorrect.
• Xi ≈ 0 for all i. This is quite clearly very unlikely, although it is conditionally
likely that Xn is close to 0.
• The correct answer is given by the conditional limit theorem. If the true
distribution is f2 and the sample type is in the set A, the conditional distribution
is close to f*, the distribution in A that is closest to f2. By symmetry, this
corresponds to λ = 1/2 in (11.232). Calculating

--- Page 16 ---
390
INFORMATION THEORY AND STATISTICS
the distribution, we get
f*(x) =
1/2
√
2πσ² e−(x−1)²
2σ²
+
1/2
√
2πσ² e−(x+1)²
2σ²
/
1/2
√
2πσ² e−(x−1)²
2σ²
+
1/2
√
2πσ² e−(x+1)²
2σ²
dx
(11.255)
=
1/√
2πσ² e−(x²+1)
2σ²
/
1/√
2πσ² e−(x²+1)
2σ²
dx
(11.256)
=
1/√
2πσ² e−x²
2σ²
(11.257)
= N(0, σ²).
(11.258)
It is interesting to note that the conditional distribution is normal with
mean 0 and with the same variance as the original distributions. This is
strange but true; if we mistake a normal population for another, the "shape"
of this population still looks normal with the same variance and a different
mean. Apparently, this rare event does not result from bizarre-looking data.
Example 11.9.2
(Large deviation theory and football)
Consider a very simple
version of football in which the score is directly related to the number of
yards gained. Assume that the coach has a choice between two strategies:
running or passing. Associated with each strategy is a distribution on the
number of yards gained. For example, in general, running results in a gain of
a few yards with very high probability, whereas passing results in huge gains
with low probability. Examples of the distributions are illustrated in Figure 11.12.
At the beginning of the game, the coach uses the strategy that promises the
greatest expected gain. Now assume that we are in the closing minutes of the
game and one of the teams is leading by a large margin. (Let us ignore first
downs and adaptable defenses.) So the trailing team will win only if it is very
lucky. If luck is required to win, we might as well assume that we will be lucky
and play accordingly. What is the appropriate strategy?
Assume that the team has only n plays left and it must gain l yards, where l
is much larger than n times the expected gain under each play. The probability
that the team succeeds in achieving l yards is exponentially small; hence,
we can use the large deviation results and Sanov’s theorem to calculate the
probability of this event. To be precise, we wish to calculate the probability that
n
i=1 Zi ≥ nα, where Zi are independent random variables and Zi has a
distribution corresponding to the strategy chosen. The situation is illustrated
in Figure 11.13. Let E be the set of types corresponding to the constraint,
E = {
P : a∈X
P(a)a ≥ α
}.
(11.259)
If P1 is the distribution corresponding to passing all the time, the probability
of winning is the probability that the sample type is in E, which by Sanov’s
theorem is 2−nD(P*1 ||P1), where P*1 is the distribution in E that is closest
to P1. Similarly, if the coach uses the running game all the time, the probability
of winning is 2−nD(P*2 ||P2). What if he uses a mixture of strategies? Is it
possible that 2−nD(P*λ ||Pλ), the probability of winning with a mixed strategy,
Pλ = λP1 + (1 − λ)P2, is better than the probability of winning with either
pure passing or pure running? The somewhat surprising answer is yes, as can
be shown by example. This provides a reason to use a mixed strategy other
than the fact that it confuses the defense.
We end this section with another inequality due to Chernoff, which is a special
version of Markov’s inequality. This inequality is called the Chernoff bound.
Lemma 11.9.1
Let Y be any random variable and let ψ(s) be the moment
generating function of Y,
ψ(s) = E[s^Y].
(11.260)
Then for all s ≥ 0,
Pr(Y ≥ a) ≤ e−saψ(s),
(11.261)
and thus
Pr(Y ≥ a) ≤ min
s≥0 e−saψ(s).
(11.262)
Proof:
Apply Markov’s inequality to the nonnegative random variable e^{sY}.
□
11.10
FISHER INFORMATION AND THE CRAMÉR–RAO
INEQUALITY
A standard problem in statistical estimation is to determine the parameters
of a distribution from a sample of data drawn from that distribution. For
example, let X1, X2, ..., Xn be drawn i.i.d. ∼ N(θ, 1). Suppose that we wish
to estimate θ from a sample of size n. There are a number of functions of
the data that we can use to estimate θ. For example, we can use the first sample
X1. Although the expected value of X1 is θ, it is clear that we can do better by
using more of the data. We guess that the best estimate of θ is the sample mean
Xn = 1/n
i=1 Xi. Indeed, it can be shown that Xn is the minimum mean-squared-error
unbiased estimator.
We begin with a few definitions. Let {f(x; θ)}, θ ∈ Θ, denote an indexed
family of densities, f(x; θ) ≥ 0,
∫
f(x; θ) dx = 1 for all θ ∈ Θ.
Here Θ is called the parameter set.
Definition
An estimator for θ for sample size n is a function T: X^n → Θ.

--- Page 19 ---
11.10
FISHER INFORMATION AND THE CRAMÉR–RAO INEQUALITY
393
An estimator is meant to approximate the value of the parameter. It is therefore
desirable to have some idea of the goodness of the approximation. We will call
the difference T − θ the error of the estimator. The error is a random variable.
Definition
The bias of an estimator T(X1, X2, ..., Xn) for the parameter θ is the
expected value of the error of the estimator [i.e., the bias is E_θ[T(X1, X2, ..., Xn) − θ]. The subscript θ means that the expectation is with respect to
the density f(·; θ). The estimator is said to be unbiased if the bias is zero for
all θ ∈ Θ (i.e., the expected value of the estimator is equal to the parameter).
Example 11.10.1
Let X1, X2, ..., Xn drawn i.i.d. ∼ f(x) = (1/λ) e^{−x/λ}, x ≥ 0 be a
sequence of exponentially distributed random variables. Estimators of λ
include X1 and Xn. Both estimators are unbiased.
The bias is the expected value of the error, and the fact that it is zero does not
guarantee that the error is low with high probability. We need to look at some
loss function of the error; the most commonly chosen loss function is the expected
square of the error. A good estimator should have a low expected squared error
and should have an error that approaches 0 as the sample size goes to infinity. This
motivates the following definition:
Definition
An estimator T(X1, X2, ..., Xn) for θ is said to be consistent in probability
if T(X1, X2, ..., Xn) → θ in probability as n → ∞. Consistency is a desirable
asymptotic property, but we are interested in the behavior for small sample sizes
as well. We can then rank estimators on the basis of their mean-squared error.
Definition
An estimator T1(X1, X2, ..., Xn) is said to dominate another estimator
T2(X1, X2, ..., Xn) if, for all θ,
E[(T1(X1, X2, ..., Xn) − θ)^2] ≤ E[(T2(X1, X2, ..., Xn) − θ)^2].
(11.263)
This raises a natural question: Is there a best estimator of θ that dominates every
other estimator? To answer this question, we derive the Cramér–Rao lower bound
on the mean-squared error of any estimator.
We first define the score function of the distribution f(x; θ). We then use the
Cauchy–Schwarz inequality to prove the Cramér–Rao lower bound on the variance
of all unbiased estimators.
Definition
The score V is a random variable defined by
V = ∂/∂θ ln f(X; θ) = ∂/∂θ f(X; θ) / f(X; θ),
(11.264)
where X ∼ f(x; θ).
The mean value of the score is
E[V] = ∫ ∂/∂θ f(x; θ) / f(x; θ) f(x; θ) dx
(11.265)
= ∫ ∂/∂θ f(x; θ) dx
(11.266)
= ∂/∂θ ∫ f(x; θ) dx
(11.267)
= ∂/∂θ 1
(11.268)
= 0,
(11.269)
and therefore Var(V) = E[V^2] = E[(∂/∂θ ln f(X; θ))^2].
The variance of the score has a special significance.
Definition
The Fisher information J(θ) is the variance of the score:
J(θ) = E_θ[(∂/∂θ ln f(X; θ))^2].
(11.270)
If we consider a sample of n random variables X1, X2, ..., Xn drawn i.i.d. ∼ f(x; θ),
we have
f(x1, x2, ..., xn; θ) = ∏_{i=1}^n f(xi; θ),
(11.271)
and the score function is the sum of the individual score functions,
V(X1, X2, ..., Xn) = ∂/∂θ ln f(X1, X2, ..., Xn; θ)
(11.272)
= ∑_{i=1}^n ∂/∂θ ln f(Xi; θ)
(11.273)
= ∑_{i=1}^n V(Xi),
(11.274)

--- Page 21 ---
11.10
FISHER INFORMATION AND THE CRAMÉR–RAO INEQUALITY
395
where the V(Xi) are independent, identically distributed with zero mean.
Hence, the n-sample Fisher information is
Jn(θ) = E_θ[V(X1, X2, ..., Xn)^2]
(11.275)
= E_θ[(∑_{i=1}^n V(Xi))^2]
(11.276)
= E_θ[∑_{i=1}^n V(Xi)^2 + 2∑_{i≠j} V(Xi)V(Xj)]
(11.277)
= ∑_{i=1}^n E_θ[V(Xi)^2] + 2∑_{i≠j} E_θ[V(Xi)V(Xj)]
(11.278)
= nJ(θ).
(11.279)
Consequently, the Fisher information for n i.i.d. samples is n times the individual
Fisher information. The significance of the Fisher information is shown in the
following theorem.
Theorem 11.10.1
(Cramér–Rao Inequality)
The mean-squared error of any unbiased estimator T(X) of the parameter θ is
lower bounded by the reciprocal of the Fisher information:
var(T) ≥ 1/J(θ).
(11.280)
Proof:
Let V be the score function and T be the estimator. By the Cauchy–Schwarz
inequality, we have
[E_θ[(V − E_θV)(T − E_θT)]]^2 ≤ E_θ[(V − E_θV)^2] E_θ[(T − E_θT)^2].
(11.281)
Since T is unbiased, E_θT = θ for all θ. By (11.269), E_θV = 0 and hence
E_θ(V − E_θV)(T − E_θT) = E_θ[VT]. Also, by definition, Var(V) = J(θ).
Substituting these conditions in (11.281), we have
[E_θ[VT]]^2 ≤ J(θ) Var(T).
(11.282)
Now,
E_θ[VT] = ∫ ∂/∂θ f(x; θ) / f(x; θ) T(x) f(x; θ) dx
(11.283)
= ∫ ∂/∂θ f(x; θ) T(x) dx
(11.284)
= ∫ ∂/∂θ [f(x; θ) T(x)] dx − ∫ f(x; θ) ∂/∂θ T(x) dx
(11.285)
= ∂/∂θ ∫ f(x; θ) T(x) dx − E_θ[∂/∂θ T(X)]
(11.286)
= ∂/∂θ E_θ[T(X)] − E_θ[∂/∂θ T(X)]
(11.287)
= ∂/∂θ θ − E_θ[∂/∂θ T(X)]
(11.288)
= 1 − E_θ[∂/∂θ T(X)].
(11.289)
If T is unbiased, then E_θ[T(X)] = θ, so ∂/∂θ E_θ[T(X)] = 1. Therefore,
E_θ[VT] = 1 − E_θ[∂/∂θ T(X)].
(11.290)
To ensure that the right-hand side of (11.282) is valid, we require that
E_θ[VT] = 1. This implies that E_θ[∂/∂θ T(X)] = 0, which is satisfied if T(X)
is a sufficient statistic for θ. Assuming this condition holds, we have
1 ≤ J(θ) Var(T),
(11.291)
which implies
Var(T) ≥ 1/J(θ).
(11.292)
Thus, the Cramér–Rao lower bound is established.



INFORMATION THEORY AND STATISTICS

∂/∂θ f(x; θ)T(x) dx  
(11.284)  
= ∂/∂θ ∫ f(x; θ)T(x) dx  
(11.285)  
= ∂/∂θ Eθ[T]  
(11.286)  
= ∂/∂θ θ  
(11.287)  
= 1,  
(11.288)  
where the interchange of differentiation and integration in (11.285) can be justified using the bounded convergence theorem for appropriately well-behaved f(x; θ), and (11.287) follows from the fact that the estimator T is unbiased. Substituting this in (11.282), we obtain  
var(T) ≥ 1/J(θ),  
(11.289)  
which is the Cramér–Rao inequality for unbiased estimators.  

By essentially the same arguments, we can show that for any estimator  
E(T − θ)² ≥ (1 + b'_T(θ))² / J(θ) + b_T(θ),  
(11.290)  
where b_T(θ) = Eθ[T] − θ and b'_T(θ) is the derivative of b_T(θ) with respect to θ. The proof of this is left as a problem at the end of the chapter.  

**Example 11.10.2**  
Let X₁, X₂, ..., Xₙ be i.i.d. ∼ N(θ, σ²), σ² known. Here J(θ) = n/σ². Let T(X₁, X₂, ..., Xₙ) = X̄ = 1/n ∑ Xi. Then Eθ(X̄ − θ)² = σ²/n = 1/J(θ). Thus, X̄ is the minimum variance unbiased estimator of θ, since it achieves the Cramér–Rao lower bound.  

The Cramér–Rao inequality gives us a lower bound on the variance for all unbiased estimators. When this bound is achieved, we call the estimator efficient.  

**Definition**  
An unbiased estimator T is said to be efficient if it meets the Cramér–Rao bound with equality [i.e., if var(T) = 1/J(θ)].  

---

**SUMMARY**  
The Fisher information is therefore a measure of the amount of "information" about θ that is present in the data. It gives a lower bound on the error in estimating θ from the data. However, it is possible that there does not exist an estimator meeting this lower bound.  

We can generalize the concept of Fisher information to the multiparameter case, in which case we define the Fisher information matrix J(θ) with elements  
Jᵢᵢ(θ) = ∫ f(x; θ) ∂/∂θᵢ ln f(x; θ) ∂/∂θⱼ ln f(x; θ) dx.  
(11.291)  

The Cramér–Rao inequality becomes the matrix inequality  
Σ ≥ J⁻¹(θ),  
(11.292)  
where Σ is the covariance matrix of a set of unbiased estimators for the parameters θ and Σ ≥ J⁻¹(θ) in the sense that the difference Σ − J⁻¹ is a nonnegative definite matrix. We will not go into the details of the proof for multiple parameters; the basic ideas are similar.  

Is there a relationship between the Fisher information J(θ) and quantities such as entropy defined earlier? Note that Fisher information is defined with respect to a family of parametric distributions, unlike entropy, which is defined for all distributions. But we can parametrize any distribution f(x) by a location parameter θ and define Fisher information with respect to the family of densities f(x − θ) under translation. We explore the relationship in greater detail in Section 17.8, where we show that while entropy is related to the volume of the typical set, the Fisher information is related to the surface area of the typical set. Further relationships of Fisher information to relative entropy are developed in the problems.  

**SUMMARY**  
Basic identities  
Qⁿ(x) = 2⁻ⁿ(D(Pₓ||Q) + H(Pₓ)),  
(11.293)  
|Pⁿ| ≤ (n + 1)|X|,  
(11.294)  
|T(P)| = 2ⁿH(P),  
(11.295)  
Qⁿ(T(P)) = 2⁻ⁿD(P||Q).  
(11.296)  

---

**Universal data compression**  
Pⁿ(e) ≤ 2⁻ⁿD(P*||Q) for all Q,  
(11.297)  
where  
D(P*||Q) = min{P: H(P) ≥ R} D(P||Q).  
(11.298)  

**Large deviations (Sanov’s theorem)**  
Qⁿ(E) = Qⁿ(E ∩ Pⁿ) ≤ (n + 1)|X|2⁻ⁿD(P*||Q),  
(11.299)  
D(P*||Q) = min{P ∈ E} D(P||Q).  
(11.300)  

If E is the closure of its interior, then  
Qⁿ(E) = 2⁻ⁿD(P*||Q).  
(11.301)  

**L₁ bound on relative entropy**  
D(P₁||P₂) ≥ (1/2 ln 2)||P₁ − P₂||₂.  
(11.302)  

**Pythagorean theorem.** If E is a convex set of types, distribution Q ∉ E, and P* achieves D(P*||Q) = min{P ∈ E} D(P||Q), we have  
D(P||Q) ≥ D(P||P*) + D(P*||Q)  
(11.303)  
for all P ∈ E.  

**Conditional limit theorem.** If X₁, X₂, ..., Xₙ i.i.d. ∼ Q, then  
Pr(X₁ = a | Pₓₙ ∈ E) → P*(a) in probability,  
(11.304)  
where P* minimizes D(P||Q) over P ∈ E. In particular,  
Pr[ X₁ = a | 1/n ∑ Xi ≥ α ] → Q(a)e^{λa} / ∑x Q(x)e^{λx}.  
(11.305)  

**Neyman–Pearson lemma.** The optimum test between two densities P₁ and P₂ has a decision region of the form "accept P = P₁ if P₁(x₁,x₂,...,xₙ) / P₂(x₁,x₂,...,xₙ) > T."  

---

**PROBLEMS**  

**11.1 Chernoff–Stein lemma.**  
Consider the two-hypothesis test  
H₁: f = f₁ vs. H₂: f = f₂. Find D(f₁ || f₂) if  

(a) fᵢ(x) = N(0, σᵢ²), i = 1, 2.  
(b) fᵢ(x) = λᵢe^{-λᵢx}, x ≥ 0, i = 1, 2.  
(c) f₁(x) is the uniform density over the interval [0, 1] and f₂(x) is the uniform density over [a, a + 1]. Assume that 0 < a < 1.  
(d) f₁(x) corresponds to a fair coin and f₂(x) corresponds to a two-headed coin.  

**11.2 Relation between D(P || Q) and chi-square.**  
Show that the χ² statistic  
χ² = ∑x (P(x) − Q(x))² / Q(x)  
is (twice) the first term in the Taylor series expansion of D(P || Q) about Q. Thus, D(P || Q) = (1/2)χ² + · · ·. [Suggestion: Write P/Q = 1 + P−Q / Q and expand the log.]  

**11.3 Error exponent for universal codes.**  
A universal source code of rate R achieves a probability of error Pⁿ(e) = e^{-nD(P*||Q)}, where Q is the true distribution and P* achieves min D(P || Q) over all P such that H(P) ≥ R.  

(a) Find P* in terms of Q and R.  
(b) Now let X be binary. Find the region of source probabilities Q(x), x ∈ {0, 1}, for which rate R is sufficient for the universal source code to achieve Pⁿ(e) → 0.  

**11.4 Sequential projection.**  
We wish to show that projecting Q onto P₁ and then projecting the projection ˆQ onto P₁ ∩ P₂ is the same as projecting Q directly onto P₁ ∩ P₂. Let P₁ be the set of probability mass functions on X satisfying  
∑x p(x) = 1,  
(11.313)  
∑x p(x)hᵢ(x) ≥ αᵢ, i = 1, 2, ..., r.  
(11.314)  

Let P₂ be the set of probability mass functions on X satisfying  
∑x p(x) = 1,  
(11.315)  
∑x p(x)gⱼ(x) ≥ βⱼ, j = 1, 2, ..., s.  
(11.316)  

Suppose that Q ∉ P₁ ∩ P₂. Let P* minimize D(P || Q) over all P ∈ P₁. Let R* minimize D(R || Q) over all R ∈ P₁ ∩ P₂. Argue that R* minimizes D(R || P*) over all R ∈ P₁ ∩ P₂.  

**11.5 Counting.**  
Let X = {1, 2, ..., m}. Show that the number of sequences xⁿ ∈ Xⁿ satisfying 1/n ∑ g(xᵢ) ≥ α is approximately equal to 2ⁿH*, to first order in the exponent, for n sufficiently large, where  
H* = max{P: ∑ P(i)g(i) ≥ α} H(P).  
(11.317)  

**11.6 Biased estimates may be better.**  
Consider the problem of estimating µ and σ² from n samples of data drawn i.i.d. from a N(µ, σ²) distribution.  

(a) Show that X̄ is an unbiased estimator of µ.  
(b) Show that the estimator  
S²ₙ = 1/n ∑ (Xᵢ − X̄)²  
(11.318)  
is a biased estimator of σ² and the estimator  
S²_{n−1} = 1/(n − 1) ∑ (Xᵢ − X̄)²  
(11.319)  
is unbiased.  
(c) Show that S²ₙ has a lower mean-squared error than that of S²_{n−1}. This illustrates the idea that a biased estimator may be "better" than an unbiased estimator.  

**11.7 Fisher information and relative entropy.**  
Show for a parametric family {pθ(x)} that  
lim_{θ'→θ} 1/(θ − θ')² D(pθ || pθ') = 1/(4 ln 4) J(θ).  
(11.320)  

**11.8 Examples of Fisher information.**  
The Fisher information J(θ) for the family fθ(x), θ ∈ ℝ is defined by  
J(θ) = Eθ [(∂fθ(x)/∂θ) / fθ(x)]² = ∫ (∂fθ(x)/∂θ)² / fθ(x) dx.  

(a) fθ(x) = N(0, θ) = 1/√(2πθ) e^{-x²/(2θ)}.  
(b) fθ(x) = θe^{-θx}, x ≥ 0.  
(c) What is the Cramér–Rao lower bound on Eθ(θ(X) − θ)², where θ(X) is an unbiased estimator of θ for parts (a) and (b)?  

**11.9 Two conditionally independent looks double the Fisher information.**  
Let gθ(x₁, x₂) = fθ(x₁)fθ(x₂). Show that Jg(θ) = 2Jf(θ).  

**11.10 Joint distributions and product distributions.**  
Consider a joint distribution Q(x, y) with marginals Q(x) and Q(y). Let E be the set of types that look jointly typical with respect to Q:  
E = {P(x, y): ∑x,y P(x, y) log Q(x) − H(X) = 0,  
∑x,y P(x, y) log Q(y) − H(Y) = 0,  
∑x,y P(x, y) log Q(x, y) − H(X, Y) = 0}.  
(11.321)  

(a) Let Q₀(x, y) be another distribution on X × Y. Argue that the distribution P* in E that is closest to Q₀ is of the form  
P*(x, y) = Q₀(x, y) e^{λ₀ + λ₁ log Q(x) + λ₂ log Q(y) + λ₃ log Q(x,y)},  
(11.322)  
where λ₀, λ₁, λ₂, and λ₃ are chosen to satisfy the constraints. Argue that this distribution is unique.  
(b) Now let Q₀(x, y) = Q(x)Q(y). Verify that Q(x, y) is of the form (11.322) and satisfies the constraints. Thus, P*(x, y) = Q(x, y) (i.e., the distribution in E closest to the product distribution is the joint distribution).  

**11.11 Cramér–Rao inequality with a bias term.**  
Let X ∼ f(x; θ) and let T(X) be an estimator for θ. Let b_T(θ) = Eθ[T] − θ be the bias of the estimator. Show that  
E(T − θ)² ≥ [1 + b'_T(θ)]² / J(θ) + b_T(θ).  
(11.323)



--- Page 29 ---
PROBLEMS
403
11.12
Hypothesis testing.
Let \( X_1, X_2, \ldots, X_n \) be i.i.d. \( \sim p(x) \). Consider the hypothesis test \( H_1 : p = p_1 \) vs. \( H_2 : p = p_2 \). Let
\[ p_1(x) = \begin{cases} 
\frac{1}{2}, & x = -1 \\
\frac{1}{4}, & x = 0 \\
\frac{1}{4}, & x = 1 
\end{cases} \]
and
\[ p_2(x) = \begin{cases} 
\frac{1}{4}, & x = -1 \\
\frac{1}{4}, & x = 0 \\
\frac{1}{2}, & x = 1.
\end{cases} \]
Find the error exponent for \( \Pr\{\text{Decide } H_2 | H_1 \text{ true}\} \) in the best hypothesis test of \( H_1 \) vs. \( H_2 \) subject to \( \Pr\{\text{Decide } H_1 | H_2 \text{ true}\} \leq \frac{1}{2} \).

11.13
Sanov’s theorem.
Prove a simple version of Sanov’s theorem for Bernoulli(\( q \)) random variables. Let the proportion of 1’s in the sequence \( X_1, X_2, \ldots, X_n \) be
\[ \hat{X}_n = \frac{1}{n} \sum_{i=1}^n X_i. \]
By the law of large numbers, we would expect \( \hat{X}_n \) to be close to \( q \) for large \( n \). Sanov’s theorem deals with the probability that \( \hat{X}_n \) is far away from \( q \). In particular, for concreteness, if we take \( p > q > \frac{1}{2} \), Sanov’s theorem states that
\[ -\frac{1}{n} \log \Pr\left( (X_1, X_2, \ldots, X_n) : \hat{X}_n \geq p \right) \to D((p, 1 - p) || (q, 1 - q)). \]
Justify the following steps:
• Show that
\[ \Pr\left( (X_1, X_2, \ldots, X_n) : \hat{X}_n \geq p \right) \leq \sum_{i = \lfloor np \rfloor}^n \binom{n}{i} q^i (1 - q)^{n - i}. \]
• Argue that the term corresponding to \( i = \lfloor np \rfloor \) is the largest term in the sum on the right-hand side of the last equation.
• Show that this term is approximately \( 2^{-nD} \).
• Prove an upper bound on the probability in Sanov’s theorem using the steps above. Use similar arguments to prove a lower bound and complete the proof of Sanov’s theorem.

--- Page 30 ---
404
INFORMATION THEORY AND STATISTICS
11.14
Sanov.
Let \( X_i \) be i.i.d. \( \sim N(0, \sigma^2) \).
(a) Find the exponent in the behavior of \( \Pr\left\{ \frac{1}{n} \sum_{i=1}^n X_i^2 \geq \alpha^2 \right\} \). This can be done from first principles (since the normal distribution is nice) or by using Sanov’s theorem.
(b) What do the data look like if \( \frac{1}{n} \sum_{i=1}^n X_i^2 \geq \alpha \)? That is, what is the \( P^* \) that minimizes \( D(P || Q) \)?

11.15
Counting states.
Suppose that an atom is equally likely to be in each of six states, \( X \in \{s_1, s_2, s_3, \ldots, s_6\} \). One observes \( n \) atoms \( X_1, X_2, \ldots, X_n \) independently drawn according to this uniform distribution. It is observed that the frequency of occurrence of state \( s_1 \) is twice the frequency of occurrence of state \( s_2 \).
(a) To first order in the exponent, what is the probability of observing this event?
(b) Assuming \( n \) is large, find the conditional distribution of the state of the first atom \( X_1 \), given this observation.

11.16
Hypothesis testing.
Let \( \{X_i\} \) be i.i.d. \( \sim p(x) \), \( x \in \{1, 2, \ldots\} \). Consider two hypotheses, \( H_0 : p(x) = p_0(x) \) vs. \( H_1 : p(x) = p_1(x) \), where \( p_0(x) = \frac{1}{2} \) for \( x \) and \( p_1(x) = q^x p^{x - 1} \) for \( x = 1, 2, 3, \ldots \).
(a) Find \( D(p_0 || p_1) \).
(b) Let \( \Pr\{H_0\} = \frac{1}{2} \). Find the minimal probability of error test for \( H_0 \) vs. \( H_1 \) given data \( X_1, X_2, \ldots, X_n \sim p(x) \).

11.17
Maximum likelihood estimation.
Let \( \{f_\theta(x)\} \) denote a parametric family of densities with parameter \( \theta \in \mathbb{R} \). Let \( X_1, X_2, \ldots, X_n \) be i.i.d. \( \sim f_\theta(x) \). The function
\[ l_\theta(x^n) = \ln \prod_{i=1}^n f_\theta(x_i) \]
is known as the log likelihood function. Let \( \theta_0 \) denote the true parameter value.
(a) Let the expected log likelihood be
\[ E_{\theta_0} l_\theta(X^n) = \int \ln \prod_{i=1}^n f_\theta(x_i) \prod_{i=1}^n f_{\theta_0}(x_i) dx^n, \]
and show that
\[ E_{\theta_0}(l(X^n)) = (-h(f_{\theta_0}) - D(f_{\theta_0} || f_\theta))n. \]
(b) Show that the maximum over \( \theta \) of the expected log likelihood is achieved by \( \theta = \theta_0 \).

--- Page 31 ---
PROBLEMS
405
11.18
Large deviations.
Let \( X_1, X_2, \ldots \) be i.i.d. random variables drawn according to the geometric distribution
\[ \Pr\{X = k\} = p^{k - 1}(1 - p), \quad k = 1, 2, \ldots. \]
Find good estimates (to first order in the exponent) of:
(a) \( \Pr\left\{ \frac{1}{n} \sum_{i=1}^n X_i \geq \alpha \right\} \).
(b) \( \Pr\left\{ X_1 = k \left| \frac{1}{n} \sum_{i=1}^n X_i \geq \alpha \right.\right\} \).
(c) Evaluate parts (a) and (b) for \( p = \frac{1}{2} \), \( \alpha = 4 \).

11.19
Another expression for Fisher information.
Use integration by parts to show that
\[ J(\theta) = -E \frac{\partial^2 \ln f_\theta(x)}{\partial \theta^2}. \]

11.20
Stirling’s approximation.
Derive a weak form of Stirling’s approximation for factorials; that is, show that
\[ \frac{n^n e^{-n}}{n!} \leq 1 \leq \frac{n^n e^{-n}}{n!} e^{n + 1}, \]
using the approximation of integrals by sums. Justify the following steps:
\[ \ln(n!) = \sum_{i=2}^{n - 1} \ln(i) + \ln(n) \leq \int_{2}^{n} \ln(x) dx + \ln(n) = \cdots. \]
and
\[ \ln(n!) = \sum_{i=1}^{n} \ln(i) \geq \int_{1}^{n} \ln(x) dx = \cdots. \]

--- Page 32 ---
406
INFORMATION THEORY AND STATISTICS
11.21
Asymptotic value of \( \binom{n}{k} \).
Use the simple approximation of Problem 11.20 to show that if \( 0 \leq p \leq 1 \), and \( k = \lfloor np \rfloor \) (i.e., \( k \) is the largest integer less than or equal to \( np \)), then
\[ \lim_{n \to \infty} \frac{1}{n} \log \binom{n}{k} = -p \log p - (1 - p) \log(1 - p) = H(p). \]
Now let \( p_i, i = 1, \ldots, m \) be a probability distribution on \( m \) symbols (i.e., \( p_i \geq 0 \) and \( \sum_{i=1}^m p_i = 1 \)). What is the limiting value of
\[ \frac{1}{n} \log \binom{n}{\lfloor np_1 \rfloor} \binom{n}{\lfloor np_2 \rfloor} \cdots \binom{n}{\lfloor np_{m - 1} \rfloor} \binom{n}{n - \sum_{j=0}^{m - 1} \lfloor np_j \rfloor} \]
\[ = \frac{1}{n} \log \frac{n!}{\lfloor np_1 \rfloor! \lfloor np_2 \rfloor! \cdots \lfloor np_{m - 1} \rfloor! (n - \sum_{j=0}^{m - 1} \lfloor np_j \rfloor)!} \]

11.22
Running difference.
Let \( X_1, X_2, \ldots, X_n \) be i.i.d. \( \sim Q_1(x) \), and \( Y_1, Y_2, \ldots, Y_n \) be i.i.d. \( \sim Q_2(y) \). Let \( X_n \) and \( Y_n \) be independent. Find an expression for \( \Pr\left\{ \sum_{i=1}^n X_i - \sum_{i=1}^n Y_i \geq nt \right\} \) good to first order in the exponent. Again, this answer can be left in parametric form.

11.23
Large likelihoods.
Let \( X_1, X_2, \ldots \) be i.i.d. \( \sim Q(x) \), \( x \in \{1, 2, \ldots, m\} \). Let \( P(x) \) be some other probability mass function. We form the log likelihood ratio
\[ \frac{1}{n} \log \frac{P_n(X_1, X_2, \ldots, X_n)}{Q_n(X_1, X_2, \ldots, X_n)} = \frac{1}{n} \sum_{i=1}^n \log \frac{P(X_i)}{Q(X_i)} \]
of the sequence \( X^n \) and ask for the probability that it exceeds a certain threshold. Specifically, find (to first order in the exponent)
\[ Q^n\left( \frac{1}{n} \log \frac{P(X_1, X_2, \ldots, X_n)}{Q(X_1, X_2, \ldots, X_n)} > 0 \right). \]
There may be an undetermined parameter in the answer.

--- Page 33 ---
PROBLEMS
407
11.24
Fisher information for mixtures.
Let \( f_1(x) \) and \( f_0(x) \) be two given probability densities. Let \( Z \) be Bernoulli(\( \theta \)), where \( \theta \) is unknown. Let \( X \sim f_1(x) \) if \( Z = 1 \) and \( X \sim f_0(x) \) if \( Z = 0 \).
(a) Find the density \( f_\theta(x) \) of the observed \( X \).
(b) Find the Fisher information \( J(\theta) \).
(c) What is the Cramér–Rao lower bound on the mean-squared error of an unbiased estimate of \( \theta \)?
(d) Can you exhibit an unbiased estimator of \( \theta \)?

11.25
Bent coins.
Let \( \{X_i\} \) be iid \( \sim Q \), where
\[ Q(k) = \Pr(X_i = k) = \binom{m}{k} q^k (1 - q)^{m - k} \quad \text{for} \quad k = 0, 1, 2, \ldots, m. \]
Thus, the \( X_i \)’s are iid \( \sim \) Binomial(\( m, q \)). Show that as \( n \to \infty \),
\[ \Pr\left( X_1 = k \left| \frac{1}{n} \sum_{i=1}^n X_i \geq \alpha \right. \right) \to P^*(k), \]
where \( P^* \) is Binomial(\( m, \lambda \)) (i.e., \( P^*(k) = \binom{m}{k} \lambda^k (1 - \lambda)^{m - k} \) for some \( \lambda \in [0, 1] \)). Find \( \lambda \).

11.26
Conditional limiting distribution
(a) Find the exact value of
\[ \Pr\left( X_1 = 1 \left| \frac{1}{n} \sum_{i=1}^n X_i = \frac{1}{4} \right. \right) \]
if \( X_1, X_2, \ldots \) are Bernoulli(\( \frac{2}{3} \)) and \( n \) is a multiple of 4.
(b) Now let \( X_i \in \{-1, 0, +1\} \) and let \( X_1, X_2, \ldots \) be i.i.d. uniform over \( \{-1, 0, +1\} \). Find the limit of
\[ \Pr\left( X_1 = +1 \left| \frac{1}{n} \sum_{i=1}^n X_i^2 = \frac{1}{2} \right. \right) \]
for \( n = 2k \), \( k \to \infty \).

--- Page 34 ---
408
INFORMATION THEORY AND STATISTICS
11.27
Variational inequality.
Verify for positive random variables \( X \) that
\[ \log E_P(X) = \sup_Q \left( E_Q(\log X) - D(Q || P) \right), \]
where \( E_P(X) = \sum x P(x) \) and \( D(Q || P) = \sum Q(x) \log \frac{Q(x)}{P(x)} \), and the supremum is over all \( Q(x) \geq 0 \), \( \sum Q(x) = 1 \). It is enough to extremize
\[ J(Q) = E_Q \ln X - D(Q || P) + \lambda \left( \sum Q(x) - 1 \right). \]

11.28
Type constraints
(a) Find constraints on the type \( P_{X^n} \) such that the sample variance \( \frac{1}{n} \sum_{i=1}^n X_i^2 - \left( \frac{1}{n} \sum_{i=1}^n X_i \right)^2 \leq \alpha \), where
\[ \overline{X}_n = \frac{1}{n} \sum_{i=1}^n X_i. \]
(b) Find the exponent in the probability \( Q^n(P_{X^n}: \overline{X}_n^2 - (\overline{X}_n)^2 \leq \alpha) \). You can leave the answer in parametric form.

11.29
Uniform distribution on the simplex.
Which of these methods will generate a sample from the uniform distribution on the simplex \( \{x \in \mathbb{R}^n : x_i \geq 0, \sum_{i=1}^n x_i = 1\} \)?
(a) Let \( Y_i \) be iid uniform [0, 1] with \( X_i = Y_i / \sum_{j=1}^n Y_j \).
(b) Let \( Y_i \) be iid exponentially distributed \( \sim \lambda e^{-\lambda y} \), \( y \geq 0 \), with \( X_i = Y_i / \sum_{j=1}^n Y_j \).
(c) (Break stick into \( n \) parts) Let \( Y_1, Y_2, \ldots, Y_{n - 1} \) be iid uniform [0, 1], and let \( X_i \) be the length of the \( i \)-th interval.

HISTORICAL NOTES
The method of types evolved from notions of strong typicality; some of the ideas were used by Wolfowitz [566] to prove channel capacity theorems. The method was fully developed by Csiszár and Körner [149], who derived the main theorems of information theory from this viewpoint. The method of types described in Section 11.1 follows the development in Csiszár and Körner. The \( L_1 \) lower bound on relative entropy is due to Csiszár [138], Kullback [336], and Kemperman [309]. Sanov’s theorem [455] was generalized by Csiszár [141] using the method of types.

--- Page 35 ---
CHAPTER 12
MAXIMUM ENTROPY
The temperature of a gas corresponds to the average kinetic energy of the molecules in the gas. What can we say about the distribution of velocities in the gas at a given temperature? We know from physics that this distribution is the maximum entropy distribution under the temperature constraint, otherwise known as the Maxwell–Boltzmann distribution. The maximum entropy distribution corresponds to the macrostate (as indexed by the empirical distribution) that has the most microstates (the individual gas velocities). Implicit in the use of maximum entropy methods in physics is a sort of AEP which says that all microstates are equally probable.

12.1
MAXIMUM ENTROPY DISTRIBUTIONS
Consider the following problem: Maximize the entropy \( h(f) \) over all probability densities \( f \) satisfying
1. \( f(x) \geq 0 \), with equality outside the support set \( S \)
2. \( \int_S f(x) dx = 1 \)
3. \( \int_S f(x) r_i(x) dx = \alpha_i \) for \( 1 \leq i \leq m \).
Thus, \( f \) is a density on support set \( S \) meeting certain moment constraints \( \alpha_1, \alpha_2, \ldots, \alpha_m \).

Approach 1 (Calculus)
The differential entropy \( h(f) \) is a concave function over a convex set. We form the functional
\[ J(f) = -\int f \ln f + \lambda_0 \int f + \sum_{i=1}^m \lambda_i \int f r_i \]
and “differentiate” with respect to \( f(x) \), the \( x \)-th component of \( f \), to obtain
\[ \frac{\partial J}{\partial f(x)} = -\ln f(x) - 1 + \lambda_0 + \sum_{i=1}^m \lambda_i r_i(x). \]



MAXIMUM ENTROPY

Setting this equal to zero, we obtain the form of the maximizing density  
f(x) = e^{λ₀ - 1 + ∑_{i=1}^m λ_i r_i(x)}, x ∈ S,  
where λ₀, λ₁, ..., λ_m are chosen so that f satisfies the constraints.  

The approach using calculus only suggests the form of the density that maximizes the entropy. To prove that this is indeed the maximum, we can take the second variation. It is simpler to use the information inequality D(g||f) ≥ 0.  

**Approach 2 (Information inequality)**  
If g satisfies (12.1) and if f* is of the form (12.4), then 0 ≤ D(g||f*) = -h(g) + h(f*). Thus, h(g) ≤ h(f*) for all g satisfying the constraints. We prove this in the following theorem.  

**Theorem 12.1.1 (Maximum entropy distribution)**  
Let f*(x) = f_λ(x) = e^{λ₀ + ∑_{i=1}^m λ_i r_i(x)}, x ∈ S, where λ₀, ..., λ_m are chosen so that f* satisfies (12.1). Then f* uniquely maximizes h(f) over all probability densities f satisfying constraints (12.1).  

**Proof:**  
Let g satisfy the constraints (12.1). Then  
h(g) = -∫_S g ln g d(x)  
= -∫_S g ln g / f* d(x)  
= -D(g||f*) - ∫_S g ln f* d(x) (a)  
≤ -∫_S g [λ₀ + ∑_{i=1}^m λ_i r_i(x)] d(x) (b)  
= -∫_S f* [λ₀ + ∑_{i=1}^m λ_i r_i(x)] d(x) (c)  
= -∫_S f* ln f* d(x)  
= h(f*),  

where (a) follows from the nonnegativity of relative entropy, (b) follows from the definition of f*, and (c) follows from the fact that both f* and g satisfy the constraints. Note that equality holds in (a) if and only if g(x) = f*(x) for all x, except for a set of measure 0, thus proving uniqueness.  

The same approach holds for discrete entropies and for multivariate distributions.  

### 12.2 EXAMPLES  

**Example 12.2.1 (One-dimensional gas with a temperature constraint)**  
Let the constraints be EX = 0 and EX² = σ². Then the form of the maximizing distribution is  
f(x) = e^{λ₀ + λ₁ x + λ₂ x²}.  

To find the appropriate constants, we first recognize that this distribution has the same form as a normal distribution. Hence, the density that satisfies the constraints and also maximizes the entropy is the N(0, σ²) distribution:  
f(x) = 1 / √(2πσ²) e^{-x² / (2σ²)}.  

**Example 12.2.2 (Dice, no constraints)**  
Let S = {1, 2, 3, 4, 5, 6}. The distribution that maximizes the entropy is the uniform distribution, p(x) = 1/6 for x ∈ S.  

**Example 12.2.3 (Dice, with EX = ∑_{i=1}^6 i p_i = α)**  
This important example was used by Boltzmann. Suppose that n dice are thrown on the table, and we are told that the total number of spots showing is nα. What proportion of the dice are showing face i, i = 1, 2, ..., 6?  

One way of going about this is to count the number of ways that n dice can fall so that n_i dice show face i. There are  
(∑_{i=1}^6 n_i)! / (∏_{i=1}^6 n_i!)  
such ways. This is a macrostate indexed by (n₁, n₂, ..., n₆) corresponding to  
(∑_{i=1}^6 n_i)! / (∏_{i=1}^6 n_i!)  
microstates, each having probability 1 / 6ⁿ. To find the most probable macrostate, we wish to maximize  
(∑_{i=1}^6 n_i)! / (∏_{i=1}^6 n_i!)  
under the constraint observed on the total number of spots,  
∑_{i=1}^6 i n_i = nα.  

Using a crude Stirling’s approximation, n! ≈ (n / e)^n, we find that  
(∑_{i=1}^6 n_i)! / (∏_{i=1}^6 n_i!) ≈ (∑_{i=1}^6 n_i / e)^{∑_{i=1}^6 n_i} / (∏_{i=1}^6 (n_i / e)^{n_i})  
= e^{nH(n₁/n, n₂/n, ..., n₆/n)},  

where H is the entropy function. Thus, maximizing  
(∑_{i=1}^6 n_i)! / (∏_{i=1}^6 n_i!)  
under the constraint (12.15) is almost equivalent to maximizing H(p₁, p₂, ..., p₆) under the constraint ∑_{i=1}^6 i p_i = α. Using Theorem 12.1.1 under this constraint, we find the maximum entropy probability mass function to be  
p_i* = e^{λ_i} / ∑_{i=1}^6 e^{λ_i},  

where λ is chosen so that ∑_{i=1}^6 i p_i* = α. Thus, the most probable macrostate is (n p₁*, n p₂*, ..., n p₆*), and we expect to find n* i = n p_i* dice showing face i.  

In Chapter 11, we show that the reasoning and the approximations are essentially correct. In fact, we show that not only is the maximum entropy macrostate the most likely, but it also contains almost all of the probability. Specifically, for rational α,  
Pr(∑_{i=1}^6 |N_i - n p_i*| < ε, i = 1, 2, ..., 6 | ∑_{i=1}^6 X_i = nα) → 1,  

as n → ∞ along the subsequence such that nα is an integer.  

**Example 12.2.4**  
Let S = [a, b], with no other constraints. Then the maximum entropy distribution is the uniform distribution over this range.  

**Example 12.2.5**  
S = [0, ∞) and EX = µ. Then the entropy-maximizing distribution is  
f(x) = 1 / µ e^{-x / µ}, x ≥ 0.  

This problem has a physical interpretation. Consider the distribution of the height X of molecules in the atmosphere. The average potential energy of the molecules is fixed, and the gas tends to the distribution that has the maximum entropy subject to the constraint that E(mgX) is fixed. This is the exponential distribution with density f(x) = λ e^{-λ x}, x ≥ 0. The density of the atmosphere does indeed have this distribution.  

**Example 12.2.6**  
S = (-∞, ∞), and EX = µ. Here the maximum entropy is infinite, and there is no maximum entropy distribution. (Consider normal distributions with larger and larger variances.)  

**Example 12.2.7**  
S = (-∞, ∞), EX = α₁, and EX² = α₂. The maximum entropy distribution is N(α₁, α₂ - α₁²).  

**Example 12.2.8**  
S = ℝⁿ, EX_i X_j = K_ij, 1 ≤ i, j ≤ n. This is a multivariate example, but the same analysis holds, and the maximum entropy density is of the form  
f(x) = e^{λ₀ + ∑_{i,j} λ_ij x_i x_j}.  

Since the exponent is a quadratic form, it is clear by inspection that the density is a multivariate normal with zero mean. Since we have to satisfy the second moment constraints, we must have a multivariate normal with covariance K_ij, and hence the density is  
f(x) = 1 / (√(2π)^n |K|^{1/2}) e^{-1/2 x^T K^{-1} x},  

which has an entropy  
h(N_n(0, K)) = 1/2 log(2π e)^n |K|,  

as derived in Chapter 8.  

**Example 12.2.9**  
Suppose that we have the same constraints as in Example 12.2.8, but EX_i X_j = K_ij only for some restricted set of (i, j) ∈ A. For example, we might know only K_ij for i = j ± 2. Then, by comparing (12.22) and (12.23), we can conclude that (K^{-1})_{ij} = 0 for (i, j) ∈ A^c (i.e., the entries in the inverse of the covariance matrix are 0 when (i, j) is outside the constraint set).  

### 12.3 ANOMALOUS MAXIMUM ENTROPY PROBLEM  

We have proved that the maximum entropy distribution subject to the constraints  
∫_S h_i(x) f(x) dx = α_i  

is of the form  
f(x) = e^{λ₀ + ∑ λ_i h_i(x)}  

if λ₀, λ₁, ..., λ_p satisfying the constraints (12.25) exist.  

We now consider a tricky problem in which the λ_i cannot be chosen to satisfy the constraints. Nonetheless, the "maximum" entropy can be found. We consider the following problem: Maximize the entropy subject to the constraints  
∫_{-∞}^∞ f(x) dx = 1,  
∫_{-∞}^∞ x f(x) dx = α₁,  
∫_{-∞}^∞ x² f(x) dx = α₂,  
∫_{-∞}^∞ x³ f(x) dx = α₃.  

Here, the maximum entropy distribution, if it exists, must be of the form  
f(x) = e^{λ₀ + λ₁ x + λ₂ x² + λ₃ x³}.  

But if λ₃ is nonzero, ∫_{-∞}^∞ f(x) dx = ∞, and the density cannot be normalized. So λ₃ must be 0. But then we have four equations and only three variables, so that in general it is not possible to choose the appropriate constants.  

The method seems to have failed in this case.  

The reason for the apparent failure is simple: The entropy has a least upper bound under these constraints, but it is not possible to attain it. Consider the corresponding problem with only first and second moment constraints. In this case, the results of Example 12.2.1 show that the entropy-maximizing distribution is the normal with the appropriate moments. With the additional third moment constraint, the maximum entropy cannot be higher. Is it possible to achieve this value?  

We cannot achieve it, but we can come arbitrarily close. Consider a normal distribution with a small "wiggle" at a very high value of x. The moments of the new distribution are almost the same as those of the old one, the biggest change being in the third moment. We can bring the first and second moments back to their original values by adding new wiggles to balance out the changes caused by the first. By choosing the position of the wiggles, we can get any value of the third moment without reducing the entropy significantly below that of the associated normal.  

Using this method, we can come arbitrarily close to the upper bound for the maximum entropy distribution. We conclude that  
sup h(f) = h(N(0, α₂ - α₁²)) = 1/2 ln(2π e)(α₂ - α₁²).  

This example shows that the maximum entropy may only be ε-achievable.  

### 12.4 SPECTRUM ESTIMATION  

Given a stationary zero-mean stochastic process {X_i}, we define the autocorrelation function as  
R(k) = E[X_i X_{i+k}].  

The Fourier transform of the autocorrelation function for a zero-mean process is the power spectral density S(λ):  
S(λ) = ∑_{m=-∞}^∞ R(m) e^{-i m λ}, -π < λ ≤ π,  

where i = √-1. Since the power spectral density is indicative of the structure of the process, it is useful to form an estimate from a sample of the process.  

There are many methods to estimate the power spectrum. The simplest way is to estimate the autocorrelation function by taking sample averages for a sample of length n,  
R̂(k) = 1 / (n - k) ∑_{i=1}^{n - k} X_i X_{i+k}.  

If we use all the values of the sample correlation function R̂(·) to calculate the spectrum, the estimate that we obtain from (12.34) does not converge to the true power spectrum for large n. Hence, this method, the periodogram method, is rarely used. One of the reasons for the problem with the periodogram method is that the estimates of the autocorrelation function from the data have different accuracies. The estimates for low values of k (called the lags) are based on a large number of samples, and those for high k on very few samples. So the estimates are more accurate at low k. The method can be modified so that it depends only on the autocorrelations at low k by setting the higher lag autocorrelations to 0. However, this introduces some artifacts because of the sudden transition to zero autocorrelation. Various windowing schemes have been suggested to smooth out the transition. However, windowing reduces spectral resolution and can give rise to negative power spectral estimates.  

In the late 1960s, while working on the problem of spectral estimation for geophysical applications, Burg suggested an alternative method. Instead of setting the autocorrelations at high lags to zero, he set them to values that make the fewest assumptions about the data (i.e., values that maximize the entropy rate of the process). This is consistent with the maximum entropy principle as articulated by Jaynes. Burg assumed the process to be stationary and Gaussian and found that the process which maximizes the entropy subject to the correlation constraints is an autoregressive Gaussian process of the appropriate order. In some applications where we can assume an underlying autoregressive model for the data, this method has proved useful in determining the parameters of the model (e.g., linear predictive coding for speech). This method (known as the maximum entropy method or Burg’s method) is a popular method for estimation of spectral densities.  

We prove Burg’s theorem in Section 12.6.  

### 12.5 ENTROPY RATES OF A GAUSSIAN PROCESS  

In Chapter 8, we defined the differential entropy of a continuous random variable. We can now extend the definition of entropy rates to real-valued stochastic processes.  

**Definition**  
The differential entropy rate of a stochastic process {X_i}, X_i ∈ ℝ, is defined to be  
h(X) = lim_{n→∞} h(X₁, X₂, ..., X_n) / n  

if the limit exists.  

Just as in the discrete case, we can show that the limit exists for stationary processes and that the limit is given by the two expressions  
h(X) = lim_{n→∞} h(X₁, X₂, ..., X_n) / n  
= lim_{n→∞} h(X_n | X_{n-1}, ..., X₁).  

For a stationary Gaussian stochastic process, we have  
h(X₁, X₂, ..., X_n) = 1/2 log(2π e)^n |K(n)|,  

where the covariance matrix K(n) is Toeplitz with entries R(0), R(1), ..., R(n - 1) along the top row. Thus, K(n)_{i,j} = R(i - j) = E[(X_i - E[X_i])(X_j - E[X_j])].



BURG’S MAXIMUM ENTROPY THEOREM

The entropy rate of a stationary Gaussian stochastic process can be expressed as:

\[ h(X) = \frac{1}{2} \log 2\pi e + \frac{1}{4\pi} \int_{-\pi}^{\pi} \log S(\lambda) \, d\lambda. \]

The entropy rate is also \(\lim_{n \to \infty} h(X_n | X_{n-1})\). For a Gaussian process, the conditional distribution is Gaussian, and the conditional entropy is:

\[ \frac{1}{2} \log 2\pi e \sigma^2_{\infty}, \]

where \(\sigma^2_{\infty}\) is the variance of the error in the best estimate of \(X_n\) given the infinite past. Thus,

\[ \sigma^2_{\infty} = \frac{1}{2\pi e^{2}} h(X), \]

where \(h(X)\) is given by the above equation. Hence, the entropy rate corresponds to the minimum mean-squared error of the best estimator of a sample of the process given the infinite past.

---

### Theorem 12.6.1

The maximum entropy rate stochastic process \(\{X_i\}\) satisfying the constraints:

\[ E X_i X_{i+k} = \alpha_k, \quad k = 0, 1, \ldots, p \]

for all \(i\), is the \(p\)th-order Gauss–Markov process of the form:

\[ X_i = -\sum_{k=1}^p a_k X_{i-k} + Z_i, \]

where the \(Z_i\) are i.i.d. \(\sim N(0, \sigma^2)\) and \(a_1, a_2, \ldots, a_p, \sigma^2\) are chosen to satisfy the constraints.

---

### Remark

We do not assume that \(\{X_i\}\) is (a) zero mean, (b) Gaussian, or (c) wide-sense stationary.

---

### Proof

Let \(X_1, X_2, \ldots, X_n\) be any stochastic process satisfying the constraints. Let \(Z_1, Z_2, \ldots, Z_n\) be a Gaussian process with the same covariance matrix as \(X_1, X_2, \ldots, X_n\). Since the multivariate normal distribution maximizes entropy over all vector-valued random variables under a covariance constraint,

\[ h(X_1, X_2, \ldots, X_n) \leq h(Z_1, Z_2, \ldots, Z_n). \]

This entropy is bounded above by the entropy of the minimal order Gauss–Markov process satisfying the given covariance constraints. Such a process exists and has a convenient characterization by means of the Yule–Walker equations.

---

### Note on the Choice of \(a_1, \ldots, a_p\) and \(\sigma^2\)

Given a sequence of covariances \(R(0), R(1), \ldots, R(p)\), the Yule–Walker equations are:

\[ R(0) = -\sum_{k=1}^p a_k R(-k) + \sigma^2, \]
\[ R(l) = -\sum_{k=1}^p a_k R(l-k), \quad l = 1, 2, \ldots. \]

These equations provide a convenient set of linear equations for calculating the coefficients \(a_k\) and \(\sigma^2\) from the \(R(k)\)'s. The spectrum of the maximum entropy process is:

\[ S(\lambda) = \frac{\sigma^2}{\left|1 + \sum_{k=1}^p a_k e^{-ik\lambda}\right|^2}, \quad -\pi \leq \lambda \leq \pi. \]

---

### Summary

- **Maximum entropy distribution**: The density \(f\) that maximizes entropy subject to constraints is of the form \(f(x) = e^{\lambda_0 + \sum \lambda_i r_i(x)}\).
- **Maximum entropy spectral density estimation**: The entropy rate is maximized by the \(p\)th-order Gauss–Markov process, with entropy rate:

\[ h^* = \frac{1}{2} \log(2\pi e) \frac{|K_p|}{|K_{p-1}|}, \]

and spectral density:

\[ S(\lambda) = \frac{\sigma^2}{\left|1 + \sum_{k=1}^p a_k e^{-ik\lambda}\right|^2}. \]

---

### Problems

12.1 **Maximum entropy**: Find the maximum entropy density \(f\) defined for \(x \geq 0\), satisfying \(EX = \alpha_1\), \(E \ln X = \alpha_2\). What family of densities is this?

12.2 **Min \(D(P \parallel Q)\) under constraints on \(P\)**: Find the parametric form of \(P(x)\) that minimizes the relative entropy \(D(P \parallel Q)\) subject to \(\sum P(x)g_i(x) = \alpha_i\).

12.3 **Maximum entropy processes**: Find the maximum entropy rate stochastic process \(\{X_i\}\) subject to constraints on \(EX_i^2\) and \(EX_i X_{i+1}\).

12.4 **Maximum entropy with marginals**: What is the maximum entropy distribution \(p(x, y)\) that has fixed marginals?

12.5 **Processes with fixed marginals**: Show that the maximum entropy process with fixed pairwise marginals is the first-order Markov process with these marginals.

12.6 **Every density is a maximum entropy density**: Show that any density \(f_0(x)\) is a maximum entropy density under the constraint \(\int f \ln f_0 = \alpha_0\).

12.7 **Mean-squared error**: Find a lower bound on the conditional variance in terms of the conditional differential entropy.

12.8 **Maximum entropy characteristic functions**: Find the maximum entropy density \(f(x)\) satisfying constraints on the characteristic function.

12.9 **Maximum entropy rate binary process**: Find the maximum entropy rate binary stochastic process \(\{X_i\}\) satisfying \(\Pr(X_i = X_{i+1}) = \frac{1}{3}\).

12.10 **Maximum entropy of sums**: Find the maximum entropy density for \(Y = X_1 + X_2\) under constraints on \(EX_1^2\) and \(EX_2^2\).

12.11 **Maximum entropy Markov chain**: Find the maximum entropy rate process satisfying \(I(X_n; X_{n+2}) = 0\) or \(I(X_n; X_{n+2}) = \alpha\).

12.12 **Entropy bound on prediction error**: Find a lower bound on the conditional variance in terms of the conditional differential entropy.

12.13 **Maximum entropy rate**: What is the maximum entropy rate stochastic process over the symbol set \(\{0, 1\}\) where the probability of "00" is zero?

12.14 **Maximum entropy**: Find the parametric-form maximum entropy density satisfying \(EX^8 = a\), \(EX^{16} = b\), and \(E(X^8 + X^{16}) = a + b\). Which entropy is higher?



--- Page 50 ---
424  
MAXIMUM ENTROPY  
12.15  
Maximum entropy.  
Find the parametric form of the maximum entropy density \( f \) satisfying the Laplace transform condition  
\[
\int f(x)e^{-x} dx = \alpha,
\]  
and give the constraints on the parameter.  

12.16  
Maximum entropy processes.  
Consider the set of all stochastic processes with \(\{X_i\}\), \(X_i \in \mathbb{R}\), with  
\(R_0 = E[X_i^2] = 1\),  
\(R_1 = E[X_i X_{i+1}] = \frac{1}{2}\).  
Find the maximum entropy rate.  

12.17  
Binary maximum entropy.  
Consider a binary process \(\{X_i\}\), \(X_i \in \{-1, +1\}\), with \(R_0 = E[X_i^2] = 1\) and \(R_1 = E[X_i X_{i+1}] = \frac{1}{2}\).  
(a) Find the maximum entropy process with these constraints.  
(b) What is the entropy rate?  
(c) Is there a Bernoulli process satisfying these constraints?  

12.18  
Maximum entropy.  
Maximize \(h(Z, V_x, V_y, V_z)\) subject to the energy constraint  
\[
E\left(\frac{1}{2m}\|V\|^2 + mgZ\right) = E_0.
\]  
Show that the resulting distribution yields  
\[
E\left(\frac{1}{2m}\|V\|^2\right) = \frac{3}{5}E_0,
\]  
\[
E(mgZ) = \frac{2}{5}E_0.
\]  
Thus, \(\frac{2}{5}\) of the energy is stored in the potential field, regardless of its strength \(g\).  

12.19  
Maximum entropy discrete processes  
(a) Find the maximum entropy rate binary stochastic process \(\{X_i\}_{i=-\infty}^{\infty}\), \(X_i \in \{0, 1\}\), satisfying \(Pr\{X_i = X_{i+1}\} = \frac{1}{3}\) for all \(i\).  
(b) What is the resulting entropy rate?  

12.20  
Maximum entropy of sums.  
Let \(Y = X_1 + X_2\). Find the maximum entropy of \(Y\) under the constraint \(E[X_1^2] = P_1\), \(E[X_2^2] = P_2\):  
(a) If \(X_1\) and \(X_2\) are independent.  
(b) If \(X_1\) and \(X_2\) are allowed to be dependent.  

--- Page 51 ---  
HISTORICAL NOTES  
425  

12.21  
Entropy rate  
(a) Find the maximum entropy rate stochastic process \(\{X_i\}\) with \(E[X_i^2] = 1\), \(E[X_i X_{i+2}] = \alpha\), \(i = 1, 2, \ldots\). Be careful.  
(b) What is the maximum entropy rate?  
(c) What is \(E[X_i X_{i+1}]\) for this process?  

12.22  
Minimum expected value  
(a) Find the minimum value of \(E[X]\) over all probability density functions \(f(x)\) satisfying the following three constraints:  
(i) \(f(x) = 0\) for \(x \leq 0\).  
(ii) \(\int_{-\infty}^{\infty} f(x) dx = 1\).  
(iii) \(h(f) = h\).  
(b) Solve the same problem if (i) is replaced by (i') \(f(x) = 0\) for \(x \leq a\).  

HISTORICAL NOTES  
The maximum entropy principle arose in statistical mechanics in the nineteenth century and has been advocated for use in a broader context by Jaynes [294]. It was applied to spectral estimation by Burg [80]. The information-theoretic proof of Burg’s theorem is from Choi and Cover [98].  

--- Page 52 ---  

--- Page 53 ---  
CHAPTER 13  
UNIVERSAL SOURCE CODING  

Here we develop the basics of universal source coding. Minimax regret data compression is defined, and the descriptive cost of universality is shown to be the information radius of the relative entropy ball containing all the source distributions. The minimax theorem shows this radius to be the channel capacity for the associated channel given by the source distribution. Arithmetic coding enables the use of a source distribution that is learned on the fly. Finally, individual sequence compression is defined and achieved by a succession of Lempel–Ziv parsing algorithms.  

In Chapter 5, we introduced the problem of finding the shortest representation of a source and showed that the entropy is the fundamental lower limit on the expected length of any uniquely decodable representation. We also showed that if we know the probability distribution for the source, we can use the Huffman algorithm to construct the optimal (minimal expected length) code for that distribution.  

For many practical situations, however, the probability distribution underlying the source may be unknown, and we cannot apply the methods of Chapter 5 directly. Instead, all we know is a class of distributions. One possible approach is to wait until we have seen all the data, estimate the distribution from the data, use this distribution to construct the best code, and then go back to the beginning and compress the data using this code. This two-pass procedure is used in some applications where there is a fairly small amount of data to be compressed. But there are many situations in which it is not feasible to make two passes over the data, and it is desirable to have a one-pass (or online) algorithm to compress the data that “learns” the probability distribution of the data and uses it to compress the incoming symbols. We show the existence of such algorithms that do well for any distribution within a class of distributions.  

In yet other cases, there is no probability distribution underlying the data—all we are given is an individual sequence of outcomes. Examples of such data sources include text and music. We can then ask: How well can we compress the sequence? If we do not put any restrictions on the class of algorithms, we get a meaningless answer—there always exists a function that compresses a particular sequence to one bit while leaving every other sequence uncompressed. This function is clearly “overfitted” to the data. However, if we compare our performance to that achievable by optimal word assignments with respect to Bernoulli distributions or kth-order Markov processes, we obtain more interesting answers that are in many ways analogous to the results for the probabilistic or average case analysis. The ultimate answer for compressibility for an individual sequence is the Kolmogorov complexity of the sequence, which we discuss in Chapter 14.  

We begin the chapter by considering the problem of source coding as a game in which the coder chooses a code that attempts to minimize the average length of the representation and nature chooses a distribution on the source sequence. We show that this game has a value that is related to the capacity of a channel with rows of its transition matrix that are the possible distributions on the source sequence. We then consider algorithms for encoding the source sequence given a known or “estimated” distribution on the sequence. In particular, we describe arithmetic coding, which is an extension of the Shannon–Fano–Elias code of Section 5.9 that permits incremental encoding and decoding of sequences of source symbols.  

We then describe two basic versions of the class of adaptive dictionary compression algorithms called Lempel–Ziv, based on the papers by Ziv and Lempel [603, 604]. We provide a proof of asymptotic optimality for these algorithms, showing that in the limit they achieve the entropy rate for any stationary ergodic source. In Chapter 16, we extend the notion of universality to investment in the stock market and describe online portfolio selection procedures that are analogous to the universal methods for data compression.  

13.1  
UNIVERSAL CODES AND CHANNEL CAPACITY  

Assume that we have a random variable \(X\) drawn according to a distribution from the family \(\{p_\theta\}\), where the parameter \(\theta \in \{1, 2, \ldots, m\}\) is unknown. We wish to find an efficient code for this source.  

From the results of Chapter 5, if we know \(\theta\), we can construct a code with codeword lengths \(l(x) = \log \frac{1}{p_\theta(x)}\), achieving an average codeword length equal to the entropy \(H_\theta(x) = -\sum_x p_\theta(x) \log p_\theta(x)\), and this is the best that we can do. For the purposes of this section, we will ignore the integer constraints on \(l(x)\), knowing that applying the integer constraint will cost at most one bit in expected length. Thus,  
\[
\min_l \mathbb{E}_\theta [l(X)] = \mathbb{E}_\theta \left[ \log \frac{1}{p_\theta(X)} \right] = H_\theta(x).
\]  

What happens if we do not know the true distribution \(p_\theta\), yet wish to code as efficiently as possible? In this case, using a code with codeword lengths \(l(x)\) and implied probability \(q(x) = 2^{-l(x)}\), we define the redundancy of the code as the difference between the expected length of the code and the lower limit for the expected length:  
\[
R(p_\theta, q) = \mathbb{E}_\theta [l(X)] - \mathbb{E}_\theta \left[ \log \frac{1}{p_\theta(X)} \right].  
\]  
This can be written as:  
\[
R(p_\theta, q) = \sum_x p_\theta(x) \left( l(x) - \log \frac{1}{p_\theta(x)} \right),  
\]  
\[
R(p_\theta, q) = \sum_x p_\theta(x) \log \frac{p_\theta(x)}{q(x)},  
\]  
\[
R(p_\theta, q) = D(p_\theta \| q).  
\]  

We wish to find a code that does well irrespective of the true distribution \(p_\theta\), and thus we define the minimax redundancy as:  
\[
R^* = \min_q \max_\theta R(p_\theta, q) = \min_q \max_\theta D(p_\theta \| q).  
\]  

This minimax redundancy is achieved by a distribution \(q\) that is at the “center” of the information ball containing the distributions \(p_\theta\), that is, the distribution \(q\) whose maximum distance from any of the distributions \(p_\theta\) is minimized.  

To find the distribution \(q\) that is as close as possible to all the possible \(p_\theta\) in relative entropy, consider the following channel:  

\[
\theta \to \begin{bmatrix}  
\cdots & p_1 & \cdots \\  
\cdots & p_2 & \cdots \\  
& \vdots & \\  
\cdots & p_\theta & \cdots \\  
& \vdots &  
\end{bmatrix} \to X.  
\]  

This is a channel \(\{p(x|\theta)\}\) with the rows of the transition matrix equal to the different \(p_\theta\), the possible distributions of the source. We will show that the minimax redundancy \(R^*\) is equal to the capacity of this channel, and the corresponding optimal coding distribution is the output distribution of this channel induced by the capacity-achieving input distribution. The capacity of this channel is given by:  
\[
C = \max_\pi I(\theta; X),  
\]  
where  
\[
I(\theta; X) = \sum_\theta \pi(\theta) D(p_\theta \| q_\pi(x)),  
\]  
and  
\[
q_\pi(x) = \sum_\theta \pi(\theta) p_\theta(x).  
\]  

The equivalence of \(R^*\) and \(C\) is expressed in the following theorem:  

**Theorem 13.1.1** (Gallager [229], Ryabko [450])  
The capacity of a channel \(p(x|\theta)\) with rows \(p_1, p_2, \ldots, p_m\) is given by:  
\[
C = R^* = \min_q \max_\theta D(p_\theta \| q).  
\]  

The distribution \(q\) that achieves the minimum in (13.11) is the output distribution \(q^*(x)\) induced by the capacity-achieving input distribution \(\pi^*(\theta)\):  
\[
q^*(x) = q_{\pi^*}(x) = \sum_\theta \pi^*(\theta) p_\theta(x).  
\]  

*Proof:*  
Let \(\pi(\theta)\) be an input distribution on \(\theta \in \{1, 2, \ldots, m\}\), and let the induced output distribution be \(q_\pi\):  
\[
q_\pi(x) = \sum_{i=1}^m \pi(i) p_i(x).  
\]  

For any distribution \(q\) on the output, we have:  
\[
I_\pi(\theta; X) = \sum_{i,j} \pi(i) p_i(x) \log \frac{p_i(x)}{q(x)},  
\]  
\[
I_\pi(\theta; X) = \sum_i \pi(i) D(p_i \| q).  
\]  

Thus,  
\[
I_\pi(\theta; X) = \min_q \sum_i \pi(i) D(p_i \| q).  
\]  

The channel capacity can now be written as:  
\[
C = \max_\pi I_\pi(\theta; X).  
\]  

By the minimax theorem,  
\[
C = \min_q \max_i D(p_i \| q).  
\]  

Thus, the channel capacity of the channel from \(\theta\) to \(X\) is the minimax expected redundancy in source coding.  

*Example 13.1.1*  
Consider the case when \(X = \{1, 2, 3\}\) and \(\theta\) takes only two values, 1 and 2, and the corresponding distributions are \(p_1 = (1 - \alpha, \alpha, 0)\) and \(p_2 = (0, \alpha, 1 - \alpha)\). We would like to encode a sequence of symbols from \(X\) without knowing whether the distribution is \(p_1\) or \(p_2\). The arguments above indicate that the worst-case optimal code uses the codeword lengths corresponding to the distribution that has a minimal relative entropy distance from both distributions, in this case, the midpoint of the two distributions. Using this distribution,  
\[
q = \left( \frac{1 - \alpha}{2}, \alpha, \frac{1 - \alpha}{2} \right),  
\]  
we achieve a redundancy of  
\[
D(p_1 \| q) = D(p_2 \| q) = (1 - \alpha) \log \frac{1 - \alpha}{(1 - \alpha)/2} + \alpha \log \frac{\alpha}{\alpha} = 1 - \alpha.  
\]  

The channel with transition matrix rows equal to \(p_1\) and \(p_2\) is equivalent to the erasure channel, and the capacity of this channel can easily be calculated to be \(1 - \alpha\), achieved with a uniform distribution on the inputs. The output distribution corresponding to the capacity-achieving input distribution is equal to:  

--- Page 59 ---  
13.2  
UNIVERSAL CODING FOR BINARY SEQUENCES  
433  

--- Page 56 ---  
430  
UNIVERSAL SOURCE CODING  

--- Page 57 ---  
13.1  
UNIVERSAL CODES AND CHANNEL CAPACITY  
431  

--- Page 58 ---  
432  
UNIVERSAL SOURCE CODING  

--- Page 59 ---  
13.2  
UNIVERSAL CODING FOR BINARY SEQUENCES  
433  

--- Page 55 ---  
13.1  
UNIVERSAL CODES AND CHANNEL CAPACITY  
429  

--- Page 54 ---  
428  
UNIVERSAL SOURCE CODING  

--- Page 53 ---  
CHAPTER 13  
UNIVERSAL SOURCE CODING  

--- Page 52 ---  

--- Page 51 ---  
HISTORICAL NOTES  
425  

--- Page 50 ---  
424  
MAXIMUM ENTROPY  

--- Page 49 ---



1−α  
2, α, 1−α  
2  
(i.e., the same as the distribution q above). Thus, if we don’t know the distribution for this class of sources, we code using the distribution q rather than p1 or p2, and incur an additional cost of 1−α bits per source symbol above the ideal entropy bound.

13.2 UNIVERSAL CODING FOR BINARY SEQUENCES  
Now we consider an important special case of encoding a binary sequence xn ∈ {0, 1}n. We do not make any assumptions about the probability distribution for x1, x2, ..., xn.  

We begin with bounds on the size of  
〈n〉  
k  
, taken from Wozencraft and Reiffen [567] proved in Lemma 17.5.1: For k ≠ 0 or n,  
8k(n −k) ≤  
〈n〉  
k  
2−nH(k/n) ≤  
〈n〉  
πk(n −k).  
(13.31)  

We first describe an offline algorithm to describe the sequence; we count the number of 1’s in the sequence, and after we have seen the entire sequence, we send a two-stage description of the sequence. The first stage is a count of the number of 1’s in the sequence [i.e., k = ∑ xi (using ⌈log(n + 1)⌉ bits)], and the second stage is the index of this sequence among all sequences that have k 1’s (using ⌈log〈n〉k⌉ bits). This two-stage description requires total length  
l(xn) ≤ log(n + 1) + log〈n〉k⌉ + 2  
(13.32)  
≤ log n + nH(k/n) − 1 2 log n − 1 2 log〈π k n (n −k) n⌉ + 3  
(13.33)  

= nH(k/n) + 1 2 log n − 1 2 log〈π k n (n −k) n⌉ + 3.  
(13.34)  

Thus, the cost of describing the sequence is approximately 1 2 log n bits above the optimal cost with the Shannon code for a Bernoulli distribution corresponding to k/n. The last term is unbounded at k = 0 or k = n, so the bound is not useful for these cases (the actual description length is log(n + 1) bits, whereas the entropy H(k/n) = 0 when k = 0 or k = n).  

This counting approach requires the compressor to wait until he has seen the entire sequence. We now describe a different approach using a mixture distribution that achieves the same result on the fly. We choose the coding distribution q(x1, x2, ..., xn) = 2−l(x1,x2,...,xn) to be a uniform mixture of all Bernoulli(θ) distributions on x1, x2, ..., xn. We will analyze the performance of a code using this distribution and show that such codes perform well for all input sequences.  

We construct this distribution by assuming that θ, the parameter of the Bernoulli distribution, is drawn according to a uniform distribution on [0, 1]. The probability of a sequence x1, x2, ..., xn with k ones is θk(1 − θ)n−k under the Bernoulli(θ) distribution. Thus, the mixture probability of the sequence is  
p(x1, x2, ..., xn) = ∫ 0 1 θk(1 − θ)n−k dθ = A(n, k).  
(13.35)  

Integrating by parts, setting u = (1 − θ)n−k and dv = θk dθ, we have  
∫ 0 1 θk(1 − θ)n−k dθ = [1 k + 1 θk+1(1 − θ)n−k] 0 1 + n −k k + 1 ∫ 0 1 θk+1(1 − θ)n−k−1 dθ,  
(13.36)  
or  
A(n, k) = n −k k + 1 A(n, k + 1).  
(13.37)  

Now A(n, n) = ∫ 0 1 θn dθ = 1 n + 1, and we can easily verify from the recursion that  
p(x1, x2, ..., xn) = A(n, k) = 1 n + 1 〈n〉k⌉.  
(13.38)  

The codeword length with respect to the mixture distribution is  
log 1 q(xn) ≤ log(n + 1) + log〈n〉k⌉ + 1,  
(13.39)  
which is within one bit of the length of the two-stage description above. Thus, we have a similar bound on the codeword length  
l(x1, x2, ..., xn) ≤ H(k/n) + 1 2 log n − 1 2 log〈π k n (n −k) n⌉ + 3  
(13.40)  
for all sequences x1, x2, ..., xn. This mixture distribution achieves a codeword length within 1 2 log n bits of the optimal code length nH(k/n) that would be required if the source were really Bernoulli(k/n), without any assumptions about the distribution of the source.  

This mixture distribution yields a nice expression for the conditional probability of the next symbol given the previous symbols of x1, x2, ..., xn. Let ki be the number of 1’s in the first i symbols of x1, x2, ..., xn. Using (13.38), we have  
q(xi+1 = 1 | xi) = q(xi, 1) q(xi)  
(13.41)  
= [1 i + 2 1 〈i + 1〉ki+1⌉] [1 i + 1 1 〈i〉ki⌉]  
(13.42)  
= [1 i + 2 (ki + 1)! (n − ki)! (i + 1)! (i + 1) ki! (i − ki)! i!]  
(13.43)  
= ki + 1 i + 2.  
(13.44)  

This is the Bayesian posterior probability of 1 given the uniform prior on θ, and is called the Laplace estimate for the probability of the next symbol. We can use this posterior probability as the probability of the next symbol for arithmetic coding, and achieve the codeword length log 1 q(xn) in a sequential manner with finite-precision arithmetic. This is a horizon-free result, in that the procedure does not depend on the length of the sequence.  

One issue with the uniform mixture approach or the two-stage approach is that the bound does not apply for k = 0 or k = n. The only uniform bound that we can give on the extra redundancy is log n, which we can obtain by using the bounds of (11.40). The problem is that we are not assigning enough probability to sequences with k = 0 or k = n. If instead of using a uniform distribution on θ, we used the Dirichlet〈1 2, 1 2〉 distribution, also called the Beta〈1 2, 1 2〉 distribution, the probability of a sequence x1, x2, ..., xn becomes  
q 1 2 (xn) = ∫ 0 1 θk(1 − θ)n−k 1 π√θ(1 − θ) dθ  
(13.45)  
and it can be shown that this achieves a description length  
log 1 q 1 2(xn) ≤ H(k/n) + 1 2 log n + log π 8  
(13.46)  
for all xn ∈ {0, 1}n, achieving a uniform bound on the redundancy of the universal mixture code. As in the case of the uniform prior, we can calculate the conditional distribution of the next symbol, given the previous observations, as  
q 1 2(xi+1 = 1 | xi) = ki + 1 2 i + 1,  
(13.47)  
which can be used with arithmetic coding to provide an online algorithm to encode the sequence. We will analyze the performance of the mixture algorithm in greater detail when we analyze universal portfolios in Section 16.7.  

13.3 ARITHMETIC CODING  
The Huffman coding procedure described in Chapter 5 is optimal for encoding a random variable with a known distribution that has to be encoded symbol by symbol. However, due to the fact that the codeword lengths for a Huffman code were restricted to be integral, there could be a loss of up to 1 bit per symbol in coding efficiency. We could alleviate this loss by using blocks of input symbols—however, the complexity of this approach increases exponentially with block length. We now describe a method of encoding without this inefficiency. In arithmetic coding, instead of using a sequence of bits to represent a symbol, we represent it by a subinterval of the unit interval.  

The code for a sequence of symbols is an interval whose length decreases as we add more symbols to the sequence. This property allows us to have a coding scheme that is incremental (the code for an extension to a sequence can be calculated simply from the code for the original sequence) and for which the codeword lengths are not restricted to be integral. The motivation for arithmetic coding is based on Shannon–Fano–Elias coding (Section 5.9) and the following lemma:  

Lemma 13.3.1  
Let Y be a random variable with continuous probability distribution function F(y). Let U = F(Y) (i.e., U is a function of Y defined by its distribution function). Then U is uniformly distributed on [0, 1].  

Proof:  
Since F(y) ∈ [0, 1], the range of U is [0, 1]. Also, for u ∈ [0, 1],  
FU(u) = Pr(U ≤ u)  
(13.48)  
= Pr(F(Y) ≤ u)  
(13.49)  
= Pr(Y ≤ F −1(u))  
(13.50)  
= F(F −1(u))  
(13.51)  
= u,  
(13.52)  
which proves that U has a uniform distribution in [0, 1].  
□  

Now consider an infinite sequence of random variables X1, X2, ... from a finite alphabet X = {0, 1, 2, ..., m}. For any sequence x1, x2, ..., from this alphabet, we can place 0. in front of the sequence and consider it as a real number (base m + 1) between 0 and 1. Let X be the real-valued random variable X = 0.X1X2.... Then X has the following distribution function:  
FX(x) = Pr{X ≤ x = 0.x1x2...}  
(13.53)  
= Pr{0.X1X2... ≤ 0.x1x2...}  
(13.54)  
= Pr{X1 < x1} + Pr{X1 = x1, X2 < x2} + ... .  
(13.55)  

Now let U = FX(X) = FX(0.X1X2...). If the distribution on infinite sequences X∞ has no atoms, then, by the lemma above, U has a uniform distribution on [0, 1], and therefore the bits in the binary expansion of U are Bernoulli(1 2) (i.e., they are independent and uniformly distributed on {0, 1}). These bits are therefore incompressible, and form a compressed representation of the sequence 0.X1X2.... For Bernoulli or Markov models, it is easy to calculate the cumulative distribution function, as illustrated in the following example.  

Example 13.3.1  
Let X1, X2, ..., Xn be Bernoulli(p). Then the sequence xn = 110101 maps into  
F(xn) = Pr(X1 < 1) + Pr(X1 = 1, X2 < 1)  
+ Pr(X1 = 1, X2 = 1, X3 < 0)  
+ Pr(X1 = 1, X2 = 1, X3 = 0, X4 < 1)  
+ Pr(X1 = 1, X2 = 1, X3 = 0, X4 = 1, X5 < 0)  
+ Pr(X1 = 1, X2 = 1, X3 = 0, X4 = 1, X5 = 0, X6 < 1)  
(13.56)  
= q + pq + p2·0 + p2q·q + p2qp·0 + p2qpqq  
(13.57)  
= q + pq + p2q2 + p3q3.  
(13.58)  

Note that each term is easily computed from the previous terms. In general, for an arbitrary binary process {Xi},  
F(xn) = ∑ k=1 n p(xk−10)xk.  
(13.59)  

The probability transform thus forms an invertible mapping from infinite source sequences to incompressible infinite binary sequences. We now consider the compression achieved by this transformation on finite sequences. Let X1, X2, ..., Xn be a sequence of binary random variables of length n, and let x1, x2, ..., xn be a particular outcome. We can treat this sequence as representing an interval [0.x1x2...xn000..., 0.x1x2...xn1111...), or equivalently, [0.x1x2...xn, 0.x1x2...xn + (1 2)n). This is the set of infinite sequences that start with 0.x1x2...xn. Under the probability transform, this interval gets mapped into another interval, [FY(0.x1x2...xn), FY(0.x1x2...xn + (1 2)n)), whose length is equal to PX(x1, x2, ..., xn), the sum of the probabilities of all infinite sequences that start with 0.x1x2...xn. Under the probability inverse transform, any real number u within this interval maps into a sequence that starts with x1, x2, ..., xn, and therefore given u and n, we can reconstruct x1, x2, ..., xn. The Shannon–Fano–Elias coding scheme described earlier allows one to construct a prefix-free code of length log 1 p(x1,x2,...,xn) + 2 bits, and therefore it is possible to encode the sequence x1, x2, ..., xn with this length. Note that log 1 p(x1,...,xn) is the ideal codeword length for xn.  

The process of encoding the sequence with the cumulative distribution function described above assumes arbitrary accuracy for the computation. In practice, though, we have to implement all numbers with finite precision, and we describe such an implementation. The key is to consider...



--- Page 65 ---
13.3
ARITHMETIC CODING
439
not inﬁnite-precision points for the cumulative distribution function but intervals in the unit interval. Any ﬁnite-length sequence of symbols can be said to correspond to a subinterval of the unit interval. The objective of the arithmetic coding algorithm is to represent a sequence of random variables by a subinterval in [0, 1]. As the algorithm observes more input symbols, the length of the subinterval corresponding to the input sequence decreases. As the top end of the interval and the bottom end of the interval get closer, they begin to agree in the ﬁrst few bits. These will be the ﬁrst few bits of the output sequence. As soon as the two ends of the interval agree, we can output the corresponding bits. We can therefore shift these bits out of the calculation and effectively scale the remaining intervals so that the entire calculation can be done with ﬁnite precision. We will not go into the details here—there is a very good description of the algorithm and performance considerations in Bell et al. [41].

Example 13.3.2 (Arithmetic coding for a ternary input alphabet)
Consider a random variable X with a ternary alphabet {A, B, C}, which are assumed to have probabilities 0.4, 0.4, and 0.2, respectively. Let the sequence to be encoded be ACAA. Thus, F_l(·) = (0, 0.4, 0.8) and F_h(·) = (0.4, 0.8, 1.0). Initially, the input sequence is empty, and the corresponding interval is [0, 1). The cumulative distribution function after the ﬁrst input symbol is shown in Figure 13.2. It is easy to calculate that the interval in the algorithm without scaling after the ﬁrst symbol A is [0, 0.4); after the second symbol, C, it is [0.32, 0.4) (Figure 13.3); after the third symbol A, it is [0.32, 0.352); and after the fourth symbol A, it is [0.32, 0.3328). Since the probability of this sequence is 0.0128, we will use log(1/0.0128) + 2 (i.e., 9 bits) to encode the midpoint of the interval sequence using Shannon–Fano–Elias coding (0.3264, which is 0.010100111 binary).

In summary, the arithmetic coding procedure, given any length n and probability mass function q(x1x2 · · · xn), enables one to encode the sequence x1x2 · · · xn in a code of length log 1 q(x1x2···xn) + 2 bits. If the source is i.i.d. and the assumed distribution q is equal to the true distribution p of the data, this procedure achieves an average length for the block that is within 2 bits of the entropy. Although this is not necessarily optimal for any ﬁxed block length (a Huffman code designed for the distribution could have a lower average codeword length), the procedure is incremental and can be used for any blocklength.

13.4
LEMPEL–ZIV CODING
In Section 13.3, we discussed the basic ideas of arithmetic coding and mentioned some results on worst-case redundancy for coding a sequence from an unknown distribution. We now discuss a popular class of techniques for source coding that are universally optimal (their asymptotic compression rate approaches the entropy rate of the source for any stationary ergodic source) and simple to implement. This class of algorithms is termed Lempel–Ziv, named after the authors of two seminal papers [603, 604] that describe the two basic algorithms that underlie this class. The algorithms could also be described as adaptive dictionary compression algorithms.

The notion of using dictionaries for compression dates back to the invention of the telegraph. At the time, companies were charged by the number of letters used, and many large companies produced codebooks for frequently used phrases and used the codewords for their telegraphic communication. Another example is the notion of greetings telegrams that are popular in India—there is a set of standard greetings such as “25: Merry Christmas” and “26: May Heaven’s choicest blessings be showered on the newly married couple.” A person wishing to send a greeting only needs to specify the number, which is used to generate the actual greeting at the destination.

The idea of adaptive dictionary-based schemes was not explored until Ziv and Lempel wrote their papers in 1977 and 1978. The two papers describe two distinct versions of the algorithm. We refer to these versions as LZ77 or sliding window Lempel–Ziv and LZ78 or tree-structured Lempel–Ziv. (They are sometimes called LZ1 and LZ2, respectively.) We ﬁrst describe the basic algorithms in the two cases and describe some simple variations. We later prove their optimality and end with some practical issues. The key idea of the Lempel–Ziv algorithm is to parse the string into phrases and to replace phrases by pointers to where the same string has occurred in the past. The differences between the algorithms are based on differences in the set of possible match locations (and match lengths) the algorithm allows.

13.4.1
Sliding Window Lempel–Ziv Algorithm
The algorithm described in the 1977 paper encodes a string by ﬁnding the longest match anywhere within a window of past symbols and represents the string by a pointer to the location of the match within the window and the length of the match. There are many variations of this basic algorithm, and we describe one due to Storer and Szymanski [507].

We assume that we have a string x1, x2, . . . to be compressed from a ﬁnite alphabet. A parsing S of a string x1x2 · · · xn is a division of the string into phrases, separated by commas. Let W be the length of the window. Then the algorithm can be described as follows: Assume that we have compressed the string until time i −1. Then to ﬁnd the next phrase, ﬁnd the largest k such that for some j, i −1 −W ≤ j ≤ i −1, the string of length k starting at xj is equal to the string (of length k) starting at xi (i.e., xj+l = xi+l for all 0 ≤ l < k). The next phrase is then of length k (i.e., xi . . . xi+k−1) and is represented by the pair (P, L), where P is the location of the beginning of the match and L is the length of the match. If a match is not found in the window, the next character is sent uncompressed. To distinguish between these two cases, a ﬂag bit is needed, and hence the phrases are of two types: (F, P, L) or (F, C), where C represents an uncompressed character.

Note that the target of a (pointer, length) pair could extend beyond the window, so that it overlaps with the new phrase. In theory, this match could be arbitrarily long; in practice, though, the maximum phrase length is restricted to be less than some parameter.

For example, if W = 4 and the string is ABBABBABBBAABABA and the initial window is empty, the string will be parsed as follows: A, B, B, ABBABB, BA, A, BA, BA, which is represented by the sequence of “pointers”: (0, A), (0, B), (1, 1, 1), (1, 3, 6), (1, 4, 2), (1, 1, 1), (1, 3, 2), (1, 2, 2), where the ﬂag bit is 0 if there is no match and 1 if there is a match, and the location of the match is measured backward from the end of the window. [In the example, we have represented every match within the window using the (P, L) pair; however, it might be more efﬁcient to represent short matches as uncompressed characters. See Problem 13.8 for details.]

We can view this algorithm as using a dictionary that consists of all substrings of the string in the window and of all single characters. The algorithm ﬁnds the longest match within the dictionary and sends a pointer to that match. We later show that a simple variation on this version of LZ77 is asymptotically optimal. Most practical implementations of LZ77, such as gzip and pkzip, are also based on this version of LZ77.

13.4.2
Tree-Structured Lempel–Ziv Algorithms
In the 1978 paper, Ziv and Lempel described an algorithm that parses a string into phrases, where each phrase is the shortest phrase not seen earlier. This algorithm can be viewed as building a dictionary in the form of a tree, where the nodes correspond to phrases seen so far. The algorithm is particularly simple to implement and has become popular as one of the early standard algorithms for ﬁle compression on computers because of its speed and efﬁciency. It is also used for data compression in high-speed modems.

The source sequence is sequentially parsed into strings that have not appeared so far. For example, if the string is ABBABBABBBAABABAA . . . , we parse it as A, B, BA, BB, AB, BBA, ABA, BAA . . . . After every comma, we look along the input sequence until we come to the shortest string that has not been marked off before. Since this is the shortest such string, all its preﬁxes must have occurred earlier. (Thus, we can build up a tree of these phrases.) In particular, the string consisting of all but the last bit of this string must have occurred earlier. We code this phrase by giving the location of the preﬁx and the value of the last symbol. Thus, the string above would be represented as (0, A), (0, B), (2, A), (2, B), (1, B), (4, A), (5, A), (3, A), . . . .

Sending an uncompressed character in each phrase results in a loss of efﬁciency. It is possible to get around this by considering the extension character (the last character of the current phrase) as part of the next phrase. This variation, due to Welch [554], is the basis of most practical implementations of LZ78, such as compress on Unix, in compression in modems, and in the image ﬁles in the GIF format.

13.5
OPTIMALITY OF LEMPEL–ZIV ALGORITHMS
13.5.1
Sliding Window Lempel–Ziv Algorithms
In the original paper of Ziv and Lempel [603], the authors described the basic LZ77 algorithm and proved that it compressed any string as well as any ﬁnite-state compressor acting on that string. However, they did not prove that this algorithm achieved asymptotic optimality (i.e., that the compression ratio converged to the entropy for an ergodic source). This result was proved by Wyner and Ziv [591].

The proof relies on a simple lemma due to Kac: the average length of time that you need to wait to see a particular symbol is the reciprocal of the probability of a symbol. Thus, we are likely to see the high-probability strings within the window and encode these strings efﬁciently. The strings that we do not ﬁnd within the window have low probability, so that asymptotically, they do not inﬂuence the compression achieved.

Instead of proving the optimality of the practical version of LZ77, we will present a simpler proof for a different version of the algorithm, which, though not practical, captures some of the basic ideas. This algorithm assumes that both the sender and receiver have access to the inﬁnite past of the string, and represents a string of length n by pointing to the last time it occurred in the past. We assume that we have a stationary and ergodic process deﬁned for time from −∞ to ∞, and that both the encoder and decoder have access to . . . , X−2, X−1, the inﬁnite past of the sequence. Then to encode X0, X1, . . . , Xn−1 (a block of length n), we ﬁnd the last time we have seen these n symbols in the past. Let

Rn(X0, X1, . . . , Xn−1) = max{j < 0 : (X−j, X−j+1 . . . X−j+n−1) = (X0, . . . , Xn−1)}. (13.60)

Then to represent X0, . . . , Xn−1, we need only to send Rn to the receiver, who can then look back Rn bits into the past and recover X0, . . . , Xn−1. Thus, the cost of the encoding is the cost of representing Rn. We will show that this cost is approximately log Rn and that asymptotically (1/n)E log Rn → H(X), thus proving the asymptotic optimality of this algorithm.

We will need the following lemmas.

Lemma 13.5.1
There exists a preﬁx-free code for the integers such that the length of the codeword for integer k is log k + 2 log log k + O(1).

Proof:
If we knew that k ≤ m, we could encode k with log m bits. However, since we don’t have an upper limit for k, we need to tell the receiver the length of the encoding of k (i.e., we need to specify log k). Consider the following encoding for the integer k: We ﬁrst represent ⌈log k⌉ in unary, followed by the binary representation of k:

C1(k) = 00 · · · 0
                        1
                        x · · · x
                        k in binary
.
(13.61)

It is easy to see that the length of this representation is 2⌈log k⌉ + 1 ≤ 2 log k + 3. This is more than the length we are looking for since we are using the very inefﬁcient unary code to send log k. However, if we use C1 to represent log k, it is now easy to see that this representation has a length less than log k + 2 log log k + 4, which proves the lemma. A similar method is presented in the discussion following Theorem 14.2.3.

□

The key result that underlies the proof of the optimality of LZ77 is Kac’s lemma, which relates the average recurrence time to the probability of a symbol for any stationary ergodic process. For example, if X1, X2, . . . , Xn is an i.i.d. process, we ask what is the expected waiting time to see the symbol a again, conditioned on the fact that X1 = a. In this case, the waiting time has a geometric distribution with parameter p = p(X0 = a), and thus the expected waiting time is 1/p(X0 = a). The somewhat surprising result is that the same is true even if the process is not i.i.d., but stationary and ergodic. A simple intuitive reason for this is that in a long sample of length n, we would expect to see a about np(a) times, and the average distance between these occurrences of a is n/(np(a)) (i.e., 1/p(a)).

Lemma 13.5.2 (Kac)
Let . . . , U2, U1, U0, U1, . . . be a stationary ergodic process on a countable alphabet. For any u such that p(u) > 0 and for i = 1, 2, . . . , let

Qu(i) = Pr(U−i = u; Uj ̸= u for −i < j < 0 | U0 = u).

(13.62)

[i.e., Qu(i) is the conditional probability that the most recent previous occurrence of the symbol u is i, given that U0 = u]. Then

E(R1(U) | X0 = u) = ∑ i Qu(i) = 1/p(u).

(13.63)

Thus, the conditional expected waiting time to see the symbol u again, looking backward from zero, is 1/p(u).

Note the amusing fact that the expected recurrence time ER1(U) = ∑ p(u) 1/p(u) = m, where m is the alphabet size.

Proof:
Let U0 = u. Deﬁne the events for j = 1, 2, . . . and k = 0, 1, 2, . . . :

Ajk = ...



U−j = u, Ul ≠ u, −j < l < k, Uk = u.

Event Ajk corresponds to the event where the last time before zero at which the process is equal to u is at −j, the first time after zero at which the process equals u is k. These events are disjoint, and by ergodicity, the probability Pr{∪j,kAjk} = 1. Thus,

1 = Pr(∪j,kAjk)  
= ∞∑j=1 ∞∑k=0 Pr(Ajk)  
= ∞∑j=1 ∞∑k=0 Pr(Uk = u) Pr(U−j = u, Ul ≠ u, −j < l < k | Uk = u)  
= ∞∑j=1 ∞∑k=0 Pr(Uk = u) Qu(j + k)  
= ∞∑j=1 ∞∑k=0 Pr(U0 = u) Qu(j + k)  
= Pr(U0 = u) ∞∑j=1 ∞∑k=0 Qu(j + k)  
= Pr(U0 = u) ∞∑i=1 iQu(i),  

where (a) follows from the fact that the Ajk are disjoint, (b) follows from the definition of Qu(·), (c) follows from stationarity, and (d) follows from the fact that there are i pairs (j, k) such that j + k = i in the sum. Kac’s lemma follows directly from this equation.

Corollary  
Let ..., X−1, X0, X1, ... be a stationary ergodic process and let Rn(X0, ..., Xn−1) be the recurrence time looking backward as defined in (13.60). Then  

E[Rn(X0, ..., Xn−1) | (X0, ..., Xn−1) = xn−1]  
= 1 / p(xn−1).  

Proof:  
Define a new process with Ui = (Xi, Xi+1, ..., Xi+n−1). The U process is also stationary and ergodic, and thus by Kac’s lemma, the average recurrence time for U conditioned on U0 = u is 1/p(u). Translating this to the X process proves the corollary.

We are now in a position to prove the main result, which shows that the compression ratio for the simple version of Lempel–Ziv using recurrence time approaches the entropy. The algorithm describes Xn−1 by describing Rn(Xn−1), which by Lemma 13.5.1 can be done with log Rn + 2 log log Rn + 4 bits. We now prove the following theorem.

Theorem 13.5.1  
Let Ln(Xn−1) = log Rn + 2 log log Rn + O(1) be the description length for Xn−1 in the simple algorithm described above. Then  

1/n ELn(Xn−1) → H(X)  

as n → ∞, where H(X) is the entropy rate of the process {Xi}.

Proof:  
We will prove upper and lower bounds for ELn. The lower bound follows directly from standard source coding results (i.e., ELn ≥ nH for any prefix-free code). To prove the upper bound, we first show that  

lim 1/n E log Rn ≤ H  

and later bound the other terms in the expression for Ln. To prove the bound for E log Rn, we expand the expectation by conditioning on the value of Xn−1 and then applying Jensen’s inequality. Thus,  

1/n E log Rn = 1/n ∑xn−1 p(xn−1) E[log Rn(Xn−1) | Xn−1 = xn−1]  
≤ 1/n ∑xn−1 p(xn−1) log E[Rn(Xn−1) | Xn−1 = xn−1]  
= 1/n ∑xn−1 p(xn−1) log(1 / p(xn−1))  
= 1/n H(Xn−1)  
→ H(X).  

The second term in the expression for Ln is log log Rn, and we wish to show that  

1/n E[log log Rn(Xn−1)] → 0.  

Again, we use Jensen’s inequality,  

1/n E log log Rn ≤ 1/n log E[log Rn(Xn−1)]  
≤ 1/n log H(Xn−1),  

where the last inequality follows from (13.79). For any ε > 0, for large enough n,  

H(Xn−1) < n(H + ε),  

and therefore  

1/n log log Rn < 1/n log(n) + 1/n log(H + ε) → 0.  

This completes the proof of the theorem.

Thus, a compression scheme that represents a string by encoding the last time it was seen in the past is asymptotically optimal. Of course, this scheme is not practical, since it assumes that both sender and receiver have access to the infinite past of a sequence. For longer strings, one would have to look further and further back into the past to find a match. For example, if the entropy rate is 1/2 and the string has length 200 bits, one would have to look an average of 2^100 ≈ 10^30 bits into the past to find a match. Although this is not feasible, the algorithm illustrates the basic idea that matching the past is asymptotically optimal. The proof of the optimality of the practical version of LZ77 with a finite window is based on similar ideas. We will not present the details here but refer the reader to the original proof in [591].

13.5.2 Optimality of Tree-Structured Lempel–Ziv Compression  
We now consider the tree-structured version of Lempel–Ziv, where the input sequence is parsed into phrases, each phrase being the shortest string that has not been seen so far. The proof of the optimality of this algorithm has a very different flavor from the proof for LZ77; the essence of the proof is a counting argument that shows that the number of phrases cannot be too large if they are all distinct, and the probability of any sequence of symbols can be bounded by a function of the number of distinct phrases in the parsing of the sequence.

The algorithm described in Section 13.4.2 requires two passes over the string—in the first pass, we parse the string and calculate c(n), the number of phrases in the parsed string. We then use that to decide how many bits [log c(n)] to allot to the pointers in the algorithm. In the second pass, we calculate the pointers and produce the coded string as indicated above. The algorithm can be modified so that it requires only one pass over the string and also uses fewer bits for the initial pointers. These modifications do not affect the asymptotic efficiency of the algorithm. Some of the implementation details are discussed by Welch [554] and Bell et al. [41].

We will show that, like the sliding window version of Lempel–Ziv, this algorithm asymptotically achieves the entropy rate for the unknown ergodic source. We first define a parsing of the string to be a decomposition into phrases.

Definition  
A parsing S of a binary string x1x2 · · · xn is a division of the string into phrases, separated by commas. A distinct parsing is a parsing such that no two phrases are identical. For example, 0,111,1 is a distinct parsing of 01111, but 0,11,11 is a parsing that is not distinct.

The LZ78 algorithm described above gives a distinct parsing of the source sequence. Let c(n) denote the number of phrases in the LZ78 parsing of a sequence of length n. Of course, c(n) depends on the sequence Xn. The compressed sequence (after applying the Lempel–Ziv algorithm) consists of a list of c(n) pairs of numbers, each pair consisting of a pointer to the previous occurrence of the prefix of the phrase and the last bit of the phrase. Each pointer requires log c(n) bits, and hence the total length of the compressed sequence is c(n)[log c(n) + 1] bits. We now show that c(n)(log c(n) + 1)/n → H(X) for a stationary ergodic sequence X1, X2, ..., Xn.

Our proof is based on the simple proof of asymptotic optimality of LZ78 coding due to Wyner and Ziv [575].

Before we proceed to the details of the proof, we provide an outline of the main ideas. The first lemma shows that the number of phrases in a distinct parsing of a sequence is less than n / log n; the main argument in the proof is based on the fact that there are not enough distinct short phrases. This bound holds for any distinct parsing of the sequence, not just the LZ78 parsing.

The second key idea is a bound on the probability of a sequence based on the number of distinct phrases. To illustrate this, consider an i.i.d. sequence of random variables X1, X2, X3, X4 that take on four possible values, {A, B, C, D}, with probabilities pA, pB, pC, and pD, respectively. Now consider the probability of a sequence P(D, A, B, C) = pDpApBpC. Since pA + pB + pC + pD = 1, the product pDpApBpC is maximized when the probabilities are equal (i.e., the maximum value of the probability of a sequence of four distinct symbols is 1/256). On the other hand, if we consider a sequence A, B, A, B, the probability of this sequence is maximized if pA = pB = 1/2, pC = pD = 0, and the maximum probability for A, B, A, B is 1/16. A sequence of the form A, A, A, A could have a probability of 1. All these examples illustrate a basic point—sequences with a large number of distinct symbols (or phrases) cannot have a large probability. Ziv’s inequality (Lemma 13.5.5) is the extension of this idea to the Markov case, where the distinct symbols are the phrases of the distinct parsing of the source sequence.

Since the description length of a sequence after the parsing grows as c log c, the sequences that have very few distinct phrases can be compressed efficiently and correspond to strings that could have a high probability. On the other hand, strings that have a large number of distinct phrases do not compress as well; but the probability of these sequences could not be too large by Ziv’s inequality. Thus, Ziv’s inequality enables us to connect the logarithm of the probability of the sequence with the number of phrases in its parsing, and this is finally used to show that the tree-structured Lempel–Ziv algorithm is asymptotically optimal.

We first prove a few lemmas that we need for the proof of the theorem.

Lemma 13.5.3 (Lempel and Ziv [604])  
The number of phrases c(n) in a distinct parsing of a binary sequence X1, X2, ..., Xn satisfies  

c(n) ≤ n(1 − εn) log n,  

where εn = min{1, log(log n) + 4 / log n} → 0 as n → ∞.

Proof:  
Let  

n_k = ∑_{j=1}^k 2^j = (k − 1)2^{k+1} + 2  

be the sum of the lengths of all distinct strings of length less than or equal to k. The number of phrases c in a distinct parsing of a sequence of length n is maximized when all the phrases are as short as possible. If n = n_k, this occurs when all the phrases are of length ≤ k, and thus  

c(n_k) ≤ ∑_{j=1}^k 2^j = 2^{k+1} − 2 < 2^{k+1} ≤ n_k / (k − 1).  

If n_k ≤ n < n_{k+1}, we write n = n_k + t, where t < (k + 1)2^{k+1}. Then the parsing into shortest phrases has each of the phrases of length ≤ k and t / (k + 1) phrases of length k + 1. Thus,  

c(n) ≤ n_k / (k − 1) + t / (k + 1) ≤ n_k + t / (k − 1) = n / (k − 1).  

We now bound the size of k for a given n. Let n_k ≤ n < n_{k+1}. Then  

n ≥ n_k = (k − 1)2^{k+1} + 2 ≥ 2^k,  

and therefore  

k ≤ log n.  

Moreover,  

n ≤ n_{k+1} = k 2^{k+2} + 2 ≤ (k + 2)2^{k+2},  

by (13.89), and therefore  

k + 2 ≥ log n / log n + 2,  

or for all n ≥ 4,  

k − 1 ≥ log n − log(log n + 2) − 3  

= [1 − log(log n + 2) + 3] / log n  

≥ [1 − log(log n) + 4] / log n.  

Note that εn = min{1, log(log n) + 4 / log n}. Combining (13.96) with (13.87), we obtain the lemma.

Lemma 13.5.4  
Let Z be a nonnegative integer-valued random variable with mean µ. Then the entropy H(Z) is bounded by  

H(Z) ≤ (µ + 1) log(µ + 1) − µ log µ.  

Proof:  
The lemma follows directly from the results of Theorem 12.1.1, which show that the geometric distribution maximizes the entropy of a nonnegative integer-valued random variable subject to a mean constraint.

Let {Xi}_{i=−∞}^∞ be a binary stationary ergodic process with probability mass function P(x1, x2, ..., xn). (Ergodic processes are discussed in greater detail in Section 16.8.) For a fixed integer k, define the kth-order Markov approximation to P as  

Q_k(x_{−(k−1)}, ..., x0, x1, ..., xn)  
△= P(x_{0−(k−1)}) ∏_{j=1}^n P(xj | x_{j−1}^{j−k}),  

where x_j^i △= (xi, xi+1, ..., xj), i ≤ j, and the initial state x0_{−(k−1)} will be part of the specification of Q_k. Since P(Xn | Xn−1^{n−k}) is itself an ergodic process, we can apply the results of Theorem 13.5.1 to the kth-order Markov approximation.



UNIVERSAL SOURCE CODING

process, we have

−1
n log Qk(X1, X2, . . . , Xn|X0
−(k−1)) = −1
n
n
\[
\sum_{j=1}^{n} \log P (Xj|Xj−1, \ldots, Xj−k) \quad (13.99)
\]
→−E log P (Xj|Xj−1, \ldots, Xj−k) \quad (13.100)
= H(Xj|Xj−1, \ldots, Xj−k). \quad (13.101)

We will bound the rate of the LZ78 code by the entropy rate of the kth-order Markov approximation for all k. The entropy rate of the Markov approximation H(Xj|Xj−1, \ldots, Xj−k) converges to the entropy rate of the process as k → ∞, and this will prove the result.

Suppose that Xn−(k−1) = xn−(k−1), and suppose that xn1 is parsed into c distinct phrases, y1, y2, \ldots, yc. Let νi be the index of the start of the ith phrase (i.e., yi = xνi+1−1
νi
). For each i = 1, 2, \ldots, c, define si = xνi−1
νi−k. Thus, si is the k bits of x preceding yi. Of course, s1 = x0−(k−1).

Let cls be the number of phrases yi with length l and preceding state si = s for l = 1, 2, \ldots and s ∈ Xk. We then have

\[
\sum_{l,s} cls = c \quad (13.102)
\]
and

\[
\sum_{l,s} lcls = n. \quad (13.103)
\]

We now prove a surprising upper bound on the probability of a string based on the parsing of the string.

**Lemma 13.5.5 (Ziv’s inequality)**

For any distinct parsing (in particular, the LZ78 parsing) of the string x1x2 \cdots xn, we have

\[
\log Qk(x1, x2, \ldots, xn|s1) \leq - \sum_{l,s} cls \log cls. \quad (13.104)
\]

Note that the right-hand side does not depend on Qk.

**Proof:**

We write

\[
Qk(x1, x2, \ldots, xn|s1) = Qk(y1, y2, \ldots, yc|s1) \quad (13.105)
\]
= \[
\prod_{i=1}^{c} P(yi|si) \quad (13.106)
\]
or

\[
\log Qk(x1, x2, \ldots, xn|s1) = \sum_{i=1}^{c} \log P(yi|si) \quad (13.107)
\]
= \[
\sum_{l,s} \sum_{i:|yi|=l,si=s} \log P(yi|si) \quad (13.108)
\]
= \[
\sum_{l,s} cls \sum_{i:|yi|=l,si=s} \frac{1}{cls} \log P(yi|si) \quad (13.109)
\]
≤ \[
\sum_{l,s} cls \log \left( \sum_{i:|yi|=l,si=s} \frac{1}{cls} P(yi|si) \right), \quad (13.110)
\]

where the inequality follows from Jensen’s inequality and the concavity of the logarithm.

Now, since the yi are distinct, we have

\[
\sum_{i:|yi|=l,si=s} P(yi|si) \leq 1. \quad \text{Thus,}
\]

\[
\log Qk(x1, x2, \ldots, xn|s1) \leq \sum_{l,s} cls \log 1 \quad (13.111)
\]

proving the lemma.

We can now prove the main theorem.

**Theorem 13.5.2**

Let {Xn} be a binary stationary ergodic process with entropy rate H(X), and let c(n) be the number of phrases in a distinct parsing of a sample of length n from this process. Then

\[
\limsup_{n \to \infty} \frac{c(n) \log c(n)}{n} \leq H(X) \quad (13.112)
\]

with probability 1.

**Proof:**

We begin with Ziv’s inequality, which we rewrite as

\[
\log Qk(x1, x2, \ldots, xn|s1) \leq - \sum_{l,s} cls \log cls \quad (13.113)
\]
= \[
- c \log c - c \sum_{l,s} \frac{cls}{c} \log cls \quad (13.114)
\]

Writing πls = cls/c, we have

\[
\sum_{l,s} πls = 1, \quad (13.115)
\]
\[
\sum_{l,s} lπls = \frac{n}{c}, \quad (13.116)
\]

from (13.102) and (13.103). We now define random variables U, V such that

\[
\Pr(U = l, V = s) = πls. \quad (13.117)
\]

Thus, EU = n/c and

\[
\log Qk(x1, x2, \ldots, xn|s1) \leq cH(U, V) - c \log c \quad (13.118)
\]

or

\[
-\frac{1}{n} \log Qk(x1, x2, \ldots, xn|s1) \geq \frac{c}{n} \log c - \frac{c}{n} H(U, V). \quad (13.119)
\]

Now,

\[
H(U, V) \leq H(U) + H(V) \quad (13.120)
\]

and H(V) ≤ log |X|k = k. By Lemma 13.5.4, we have

\[
H(U) \leq (EU + 1) \log (EU + 1) - EU \log EU \quad (13.121)
\]
= \[
\log \left( \frac{n}{c} + 1 \right) + \left( \frac{n}{c} + 1 \right) \log \left( \frac{c}{n + 1} \right). \quad (13.122)
\]

Thus,

\[
\frac{c}{n} H(U, V) \leq \frac{c}{n} k + \frac{c}{n} \log \frac{n}{c} + o(1). \quad (13.123)
\]

For a given n, the maximum of c/n log n/c is attained for the maximum value of c (for c/n ≤ 1/e). But from Lemma 13.5.3, c ≤ n log n(1 + o(1)). Thus,

\[
\frac{c}{n} \log \frac{n}{c} \leq O \left( \frac{\log \log n}{\log n} \right), \quad (13.124)
\]

and therefore c/n H(U, V) → 0 as n → ∞. Therefore,

\[
\frac{c(n) \log c(n)}{n} \leq -\frac{1}{n} \log Qk(x1, x2, \ldots, xn|s1) + ǫk(n), \quad (13.125)
\]

where ǫk(n) → 0 as n → ∞. Hence, with probability 1,

\[
\limsup_{n \to \infty} \frac{c(n) \log c(n)}{n} \leq \lim_{n \to \infty} -\frac{1}{n} \log Qk(X1, X2, \ldots, Xn|X0−(k−1)) \quad (13.126)
\]
= \[
H(X0|X−1, \ldots, X−k) \quad (13.127)
\]
→ H(X) as k → ∞. \quad (13.128)

We now prove that LZ78 coding is asymptotically optimal.

**Theorem 13.5.3**

Let {Xi} i=−∞ ∞ be a binary stationary ergodic stochastic process. Let l(X1, X2, \ldots, Xn) be the LZ78 codeword length associated with X1, X2, \ldots, Xn. Then

\[
\limsup_{n \to \infty} \frac{1}{n} l(X1, X2, \ldots, Xn) \leq H(X) \quad (13.129)
\]

with probability 1, where H(X) is the entropy rate of the process.

**Proof:**

We have shown that l(X1, X2, \ldots, Xn) = c(n)(log c(n) + 1), where c(n) is the number of phrases in the LZ78 parsing of the string X1, X2, \ldots, Xn. By Lemma 13.5.3, limsup c(n)/n = 0, and thus Theorem 13.5.2 establishes that

\[
\limsup_{n \to \infty} \frac{l(X1, X2, \ldots, Xn)}{n} = \limsup_{n \to \infty} \left( \frac{c(n) \log c(n)}{n} + \frac{c(n)}{n} \right) \leq H(X) \quad (13.130)
\]

with probability 1. \quad \Box

Thus, the length per source symbol of the LZ78 encoding of an ergodic source is asymptotically no greater than the entropy rate of the source.

There are some interesting features of the proof of the optimality of LZ78 that are worth noting. The bounds on the number of distinct phrases and Ziv’s inequality apply to any distinct parsing of the string, not just the incremental parsing version used in the algorithm. The proof can be extended in many ways with variations on the parsing algorithm; for example, it is possible to use multiple trees that are context or state-dependent [218, 426]. Ziv’s inequality (Lemma 13.5.5) remains particularly intriguing since it relates a probability on one side with a purely deterministic function of the parsing of a sequence on the other.

The Lempel–Ziv codes are simple examples of a universal code (i.e., a code that does not depend on the distribution of the source). This code can be used without knowledge of the source distribution and yet will achieve an asymptotic compression equal to the entropy rate of the source.

**SUMMARY**

Ideal word length

\[
l^*(x) = \log \frac{1}{p(x)}. \quad (13.131)
\]

Average description length

\[
E p l^*(x) = H(p). \quad (13.132)
\]

Estimated probability distribution \hat{p}(x). If \hat{l}(x) = \log \frac{1}{\hat{p}(x)}, then

\[
E \hat{p} l(x) = H(p) + D(p \| \hat{p}). \quad (13.133)
\]

Average redundancy

\[
R_p = E p l(X) − H(p). \quad (13.134)
\]

Minimax redundancy. For X ∼ pθ(x), θ ∈ θ,

\[
D^* = \min_p \max_{θ} D(pθ \| p). \quad (13.135)
\]

Minimax theorem. D^* = C, where C is the capacity of the channel {θ, pθ(x), X}.

Bernoulli sequences. For Xn ∼ Bernoulli(θ), the redundancy is

\[
D^*_n = \min_q \max_{θ} D(pθ(xn) \| q(xn)) \approx \frac{1}{2} \log n + o(\log n). \quad (13.136)
\]

Arithmetic coding. nH bits of F(xn) reveal approximately n bits of xn.

**PROBLEMS**

**13.1 Minimax regret data compression and channel capacity.**

First consider universal data compression with respect to two source distributions. Let the alphabet V = {1, e, 0} and let p1(v) put mass 1 − α on v = 1 and mass α on v = e. Let p2(v) put mass 1 − α on 0 and mass α on v = e. We assign word lengths to V according to l(v) = log 1/p(v), the ideal codeword length with respect to a cleverly chosen probability mass function p(v). The worst-case excess description length (above the entropy of the true distribution) is

\[
\max_i \left( E_{p_i} \log \frac{1}{p(V)} − E_{p_i} \log \frac{1}{p_i(V)} \right) = \max_i D(p_i \| p). \quad (13.138)
\]

Thus, the minimax regret is D^* = min_p maxi D(pi \| p).

(a) Find D^*.

(b) Find the p(v) achieving D^*.

(c) Compare D^* to the capacity of the binary erasure channel

\[
\begin{pmatrix}
1 − α & α \\
α & 1 − α
\end{pmatrix}
\]

and comment.

**13.2 Universal data compression.**

Consider three possible source distributions on X,

Pa = (0.7, 0.2, 0.1),

Pb = (0.1, 0.7, 0.2),

Pc = (0.2, 0.1, 0.7).

(a) Find the minimum incremental cost of compression

\[
D^* = \min_P \max_{θ} D(Pθ \| P),
\]

the associated mass function P = (p1, p2, p3), and ideal codeword lengths li = log(1/pi).

(b) What is the channel capacity of a channel matrix with rows Pa, Pb, Pc?

**13.3 Arithmetic coding.**

Let {Xi} i=0 ∞ be a stationary binary Markov chain with transition matrix

\[
\begin{pmatrix}
\frac{3}{4} & \frac{1}{4} \\
\frac{1}{4} & \frac{3}{4}
\end{pmatrix}. \quad (13.139)
\]

Calculate the first 3 bits of F(X∞) = 0.F1F2 \cdots when X∞ = 1010111 \cdots. How many bits of X∞ does this specify?

**13.4 Arithmetic coding.**

Let Xi be binary stationary Markov with transition matrix

\[
\begin{pmatrix}
\frac{1}{3} & \frac{2}{3} \\
\frac{2}{3} & \frac{1}{3}
\end{pmatrix}.
\]

(a) Find F(01110) = Pr{.X1X2X3X4X5 < .01110}.

(b) How many bits .F1F2 \cdots can be known for sure if it is not known how X = 01110 continues?

**13.5 Lempel–Ziv.**

Give the LZ78 parsing and encoding of 00000011010100000110101.

**13.6 Compression of constant sequence.**

We are given the constant sequence xn = 11111 \ldots.

(a) Give the LZ78 parsing for this sequence.

(b) Argue that the number of encoding bits per symbol for this sequence goes to zero as n → ∞.

**13.7 Another idealized version of Lempel–Ziv coding.**

An idealized version of LZ was shown to be optimal: The encoder and decoder both have available to them the “infinite past” generated by the process, \ldots, X−1, X0, and the encoder describes the string (X1, X2, \ldots, Xn) by telling the decoder the position Rn in the past where the same string was last seen.



PROBLEMS  
459  
of the first recurrence of that string. This takes roughly log Rₙ + 2 log log Rₙ bits. Now consider the following variant: Instead of describing Rₙ, the encoder describes Rₙ−1 plus the last symbol, Xₙ. From these two, the decoder can reconstruct the string (X₁, X₂, . . . , Xₙ).  

(a) What is the number of bits per symbol used in this case to encode (X₁, X₂, . . . , Xₙ)?  

(b) Modify the proof given in the text to show that this version is also asymptotically optimal: namely, that the expected number of bits per symbol converges to the entropy rate.  

13.8  
Length of pointers in LZ77.  
In the version of LZ77 due to Storer and Szymanski [507] described in Section 13.4.1, a short match can be represented by either (F, P, L) (flag, pointer, length) or by (F, C) (flag, character). Assume that the window length is W, and assume that the maximum match length is M.  

(a) How many bits are required to represent P? To represent L?  

(b) Assume that C, the representation of a character, is 8 bits long. If the representation of P plus L is longer than 8 bits, it would be better to represent a single character match as an uncompressed character rather than as a match within the dictionary. As a function of W and M, what is the shortest match that one should represent as a match rather than as uncompressed characters?  

(c) Let W = 4096 and M = 256. What is the shortest match that one would represent as a match rather than uncompressed characters?  

13.9  
Lempel–Ziv 78.  

(a) Continue the Lempel–Ziv parsing of the sequence 0,00,001,00000011010111.  

(b) Give a sequence for which the number of phrases in the LZ parsing grows as fast as possible.  

(c) Give a sequence for which the number of phrases in the LZ parsing grows as slowly as possible.  

13.10  
Two versions of fixed-database Lempel–Ziv.  
Consider a source (A, P). For simplicity, assume that the alphabet is finite |A| = A < ∞ and the symbols are i.i.d. ∼ P. A fixed database D is given and is revealed to the decoder. The encoder parses the target sequence Xₙ₁ into blocks of length l and subsequently encodes them by giving the binary description of their last appearance in the database. If a match is not found, the entire block is sent uncompressed, requiring l log A bits. A flag is used to tell the decoder whether a match location is being described or the sequence itself. Parts (a) and (b) give some preliminaries you will need in showing the optimality of fixed-database LZ in part (c).  

(a) Let Xₗ be a δ-typical sequence of length l starting at 0, and let Rₗ(Xₗ) be the corresponding recurrence index in the infinite past . . . , X₋₂, X₋₁. Show that  
E(Rₗ(Xₗ)|Xₗ = xₗ) ≤ 2l(H + δ),  
where H is the entropy rate of the source.  

(b) Prove that for any ǫ > 0,  
Pr(Rₗ(Xₗ) > 2l(H + ǫ)) → 0 as l → ∞. (Hint: Expand the probability by conditioning on strings xₗ, and break things up into typical and nontypical. Markov’s inequality and the AEP should prove handy as well.)  

(c) Consider the following two fixed databases: (i) D₁ is formed by taking all δ-typical l-vectors; and (ii) D₂ is formed by taking the most recent ˜L = 2l(H + δ) symbols in the infinite past (i.e., X₋˜L, . . . , X₋₁). Argue that the algorithm described above is asymptotically optimal: namely, that the expected number of bits per symbol converges to the entropy rate when used in conjunction with either database D₁ or D₂.  

13.11  
Tunstall coding.  
The normal setting for source coding maps a symbol (or a block of symbols) from a finite alphabet onto a variable-length string. An example of such a code is the Huffman code, which is the optimal (minimal expected length) mapping from a set of symbols to a prefix-free set of codewords. Now consider the dual problem of variable-to-fixed length codes, where we map a variable-length sequence of source symbols into a fixed-length binary (or D-ary) representation. A variable-to-fixed length code for an i.i.d. sequence of random variables X₁, X₂, . . . , Xₙ, Xᵢ ∼ p(x), x ∈ X = {0, 1, . . . , m − 1}, is defined by a prefix-free set of phrases AD ⊂ X∗, where X∗ is the set of finite-length strings of symbols of X, and |AD| = D. Given any sequence X₁, X₂, . . . , Xₙ, the string is parsed into phrases from AD (unique because of the prefix-free property of AD) and represented by a sequence of symbols from a D-ary alphabet. Define the efficiency of this coding scheme by  
R(AD) = log D / EL(AD),  
where EL(AD) is the expected length of a phrase from AD.  

(a) Prove that R(AD) ≥ H(X).  

(b) The process of constructing AD can be considered as a process of constructing an m-ary tree whose leaves are the phrases in AD. Assume that D = 1 + k(m − 1) for some integer k ≥ 1. Consider the following algorithm due to Tunstall:  
(i) Start with A = {0, 1, . . . , m − 1} with probabilities p₀, p₁, . . . , pₘ₋₁. This corresponds to a complete m-ary tree of depth 1.  
(ii) Expand the node with the highest probability. For example, if p₀ is the node with the highest probability, the new set is A = {00, 01, . . . , 0(m − 1), 1, . . . , (m − 1)}.  
(iii) Repeat step 2 until the number of leaves (number of phrases) reaches the required value.  
Show that the Tunstall algorithm is optimal in the sense that it constructs a variable-to-fixed code with the best R(AD) for a given D [i.e., the largest value of EL(AD) for a given D].  

(c) Show that there exists a D such that R(A₊D) < H(X) + 1.  

HISTORICAL NOTES  
The problem of encoding a source with an unknown distribution was analyzed by Fitingof [211] and Davisson [159], who showed that there were classes of sources for which the universal coding procedure was asymptotically optimal. The result relating the average redundancy of a universal code and channel capacity is due to Gallager [229] and Ryabko [450]. Our proof follows that of Csiszár. This result was extended to show that the channel capacity was the lower bound for the redundancy for "most" sources in the class by Merhav and Feder [387], extending the results obtained by Rissanen [444, 448] for the parametric case.  

The arithmetic coding procedure has its roots in the Shannon–Fano code developed by Elias (unpublished), which was analyzed by Jelinek [297]. The procedure for the construction of a prefix-free code described in the text is due to Gilbert and Moore [249]. Arithmetic coding itself was developed by Rissanen [441] and Pasco [414]; it was generalized by Langdon and Rissanen [343]. See also the enumerative methods in Cover [120]. Tutorial introductions to arithmetic coding can be found in Langdon [342] and Witten et al. [564]. Arithmetic coding combined with the context-tree weighting algorithm due to Willems et al. [560, 561] achieves the Rissanen lower bound [444] and therefore has the optimal rate of convergence to the entropy for tree sources with unknown parameters.  

The class of Lempel–Ziv algorithms was first described in the seminal papers of Lempel and Ziv [603, 604]. The original results were theoretically interesting, but people implementing compression algorithms did not take notice until the publication of a simple efficient version of the algorithm due to Welch [554]. Since then, multiple versions of the algorithms have been described, many of them patented. Versions of this algorithm are now used in many compression products, including GIF files for image compression and the CCITT standard for compression in modems. The optimality of the sliding window version of Lempel–Ziv (LZ77) is due to Wyner and Ziv [575]. An extension of the proof of the optimality of LZ78 [426] shows that the redundancy of LZ78 is on the order of 1 / log(n), as opposed to the lower bounds of log(n) / n. Thus, even though LZ78 is asymptotically optimal for all stationary ergodic sources, it converges to the entropy rate very slowly compared to the lower bounds for finite-state Markov sources. However, for the class of all ergodic sources, lower bounds on the redundancy of a universal code do not exist, as shown by examples due to Shields [492] and Shields and Weiss [494]. A lossless block compression algorithm based on sorting the blocks and using simple run-length encoding due to Burrows and Wheeler [81] has been analyzed by Effros et al. [181]. Universal methods for prediction are discussed in Feder, Merhav, and Gutman [204, 386, 388].  

CHAPTER 14  
KOLMOGOROV COMPLEXITY  

The great mathematician Kolmogorov culminated a lifetime of research in mathematics, complexity, and information theory with his definition in 1965 of the intrinsic descriptive complexity of an object. In our treatment so far, the object X has been a random variable drawn according to a probability mass function p(x). If X is random, there is a sense in which the descriptive complexity of the event X = x is log 1 / p(x), because ⌈log 1 / p(x)⌉ is the number of bits required to describe x by a Shannon code.  

One notes immediately that the descriptive complexity of such an object depends on the probability distribution. Kolmogorov went further. He defined the algorithmic (descriptive) complexity of an object to be the length of the shortest binary computer program that describes the object. (Apparently, a computer, the most general form of data decompressor, will, after a finite amount of computation, use this description to exhibit the object described.) Thus, the Kolmogorov complexity of an object dispenses with the probability distribution. Kolmogorov made the crucial observation that the definition of complexity is essentially computer-independent. It is an amazing fact that the expected length of the shortest binary computer description of a random variable is approximately equal to its entropy. Thus, the shortest computer description acts as a universal code which is uniformly good for all probability distributions. In this sense, algorithmic complexity is a conceptual precursor to entropy.  

Perhaps a good point of view of the role of this chapter is to consider Kolmogorov complexity as a way to think. One does not use the shortest computer program in practice because it may take infinitely long to find such a minimal program. But one can use very short, not necessarily minimal programs in practice; and the idea of finding such short programs leads to universal codes, a good basis for inductive inference, a formalization of Occam’s razor ("The simplest explanation is best") and to fundamental understanding in physics, computer science, and communication theory.  

Elements of Information Theory, Second Edition,  
By Thomas M. Cover and Joy A. Thomas  
Copyright © 2006 John Wiley & Sons, Inc.  

464  
KOLMOGOROV COMPLEXITY  

Before formalizing the notion of Kolmogorov complexity, let us give three strings as examples:  
1. 0101010101010101010101010101010101010101010101010101010101010101  
2. 0110101000001001111001100110011111110011101111001100100100001000  
3. 1101111001110101111101101111101110101101111000101110010100111011  

What are the shortest binary computer programs for each of these sequences? The first sequence is definitely simple. It consists of thirty-two 01’s. The second sequence looks random and passes most tests for randomness, but it is in fact the initial segment of the binary expansion of √2 − 1. Again, this is a simple sequence. The third again looks random, except that the proportion of 1’s is not near 1/2. We shall assume that it is otherwise random. It turns out that by describing the number k of 1’s in the sequence, then giving the index of the sequence in a lexicographic ordering of those with this number of 1’s, one can give a description of the sequence in roughly log n + nH(k / n) bits. This again is substantially fewer than the n bits in the sequence. Again, we conclude that the sequence, random though it is, is simple. In this case, however, it is not as simple as the other two sequences, which have constant-length programs. In fact, its complexity is proportional to n. Finally, we can imagine a truly random sequence generated by pure coin flips. There are 2ⁿ such sequences, and they are all equally probable. It is highly likely that such a random sequence cannot be compressed (i.e., there is no better program for such a sequence than simply saying "Print the following: 0101100111010. . . 0"). The reason for this is that there are not enough short programs to go around. Thus, the descriptive complexity of a truly random binary sequence is as long as the sequence itself.  

These are the basic ideas. It will remain to be shown that this notion of intrinsic complexity is computer-independent (i.e., that the length of the shortest program does not depend on the computer). At first, this seems like nonsense. But it turns out to be true, up to an additive constant. And for long sequences of high complexity, this additive constant (which is the length of the preprogram that allows one computer to mimic the other) is negligible.  

14.1  
MODELS OF COMPUTATION  
To formalize the notions of algorithmic complexity, we first discuss acceptable models for computers. All but the most trivial computers are universal, in the sense that they can mimic the actions of other computers.



MODELS OF COMPUTATION

We touch briefly on a certain canonical universal computer, the universal Turing machine, the conceptually simplest universal computer.

In 1936, Turing was obsessed with the question of whether the thoughts in a living brain could be held equally well by a collection of inanimate parts. In short, could a machine think? By analyzing the human computational process, he posited some constraints on such a computer. Apparently, a human thinks, writes, thinks some more, writes, and so on. Consider a computer as a finite-state machine operating on a finite symbol set. (The symbols in an infinite symbol set cannot be distinguished in finite space.) A program tape, on which a binary program is written, is fed left to right into this finite-state machine. At each unit of time, the machine inspects the program tape, writes some symbols on a work tape, changes its state according to its transition table, and calls for more program. The operations of such a machine can be described by a finite list of transitions. Turing argued that this machine could mimic the computational ability of a human being.

After Turing’s work, it turned out that every new computational system could be reduced to a Turing machine, and conversely. In particular, the familiar digital computer with its CPU, memory, and input-output devices could be simulated by and could simulate a Turing machine. This led Church to state what is now known as Church’s thesis, which states that all (sufficiently complex) computational models are equivalent in the sense that they can compute the same family of functions. The class of functions they can compute agrees with our intuitive notion of effectively computable functions, that is, functions for which there is a finite prescription or program that will lead in a finite number of mechanically specified computational steps to the desired computational result.

We shall have in mind throughout this chapter the computer illustrated in Figure 14.1. At each step of the computation, the computer reads a symbol from the input tape, changes state according to its state transition table, possibly writes something on the work tape or output tape, and moves the program read head to the next cell of the program read tape. This machine reads the program from right to left only, never going back, and therefore the programs form a prefix-free set. No program leading to a halting computation can be the prefix of another such program. The restriction to prefix-free programs leads immediately to a theory of Kolmogorov complexity which is formally analogous to information theory.

We can view the Turing machine as a map from a set of finite-length binary strings to the set of finite- or infinite-length binary strings. In some cases, the computation does not halt, and in such cases, the value of the function is said to be undefined. The set of functions f: {0, 1}* → {0, 1}* ∪ {0, 1}^∞ computable by Turing machines is called the set of partial recursive functions.

KOLMOGOROV COMPLEXITY: DEFINITIONS AND EXAMPLES

Let x be a finite-length binary string and let U be a universal computer. Let l(x) denote the length of the string x. Let U(p) denote the output of the computer U when presented with a program p.

We define the Kolmogorov (or algorithmic) complexity of a string x as the minimal description length of x.

Definition: The Kolmogorov complexity K_U(x) of a string x with respect to a universal computer U is defined as

K_U(x) = min { l(p) | p: U(p) = x }

(14.1)

the minimum length over all programs that print x and halt. Thus, K_U(x) is the shortest description length of x over all descriptions interpreted by computer U.

A useful technique for thinking about Kolmogorov complexity is the following—if one person can describe a sequence to another person in such a manner as to lead unambiguously to a computation of that sequence in a finite amount of time, the number of bits in that communication is an upper bound on the Kolmogorov complexity. For example, one can say “Print out the first 1,239,875,981,825,931 bits of the square root of e.” Allowing 8 bits per character (ASCII), we see that the unambiguous 73-symbol program above demonstrates that the Kolmogorov complexity of this huge number is no greater than (8)(73) = 584 bits. Most numbers of this length (more than a quadrillion bits) have a Kolmogorov complexity of nearly 1,239,875,981,825,931 bits. The fact that there is a simple algorithm to calculate the square root of e provides the saving in descriptive complexity.

In the definition above, we have not mentioned anything about the length of x. If we assume that the computer already knows the length of x, we can define the conditional Kolmogorov complexity knowing l(x) as

K_U(x | l(x)) = min { l(p) | p: U(p, l(x)) = x }

(14.2)

This is the shortest description length if the computer U has the length of x made available to it.

It should be noted that K_U(x | y) is usually defined as K_U(x | y, y*), where y* is the shortest program for y. This is to avoid certain slight asymmetries, but we will not use this definition here.

We first prove some of the basic properties of Kolmogorov complexity and then consider various examples.

Theorem 14.2.1 (Universality of Kolmogorov complexity): If U is a universal computer, for any other computer A there exists a constant c_A such that

K_U(x) ≤ K_A(x) + c_A

(14.3)

for all strings x ∈ {0, 1}*, and the constant c_A does not depend on x.

Proof: Assume that we have a program p_A for computer A to print x. Thus, A(p_A) = x. We can precede this program by a simulation program s_A which tells computer U how to simulate computer A. Computer U will then interpret the instructions in the program for A, perform the corresponding calculations, and print out x. The program for U is p = s_A p_A and its length is

l(p) = l(s_A) + l(p_A) = c_A + l(p_A),

(14.4)

where c_A is the length of the simulation program. Hence,

K_U(x) = min { l(p) | p: U(p) = x } ≤ min { l(p) | p: A(p) = x } (l(p) + c_A) = K_A(x) + c_A

(14.5)

for all strings x.

The constant c_A in the theorem may be very large. For example, A may be a large computer with a large number of functions built into the system. The computer U can be a simple microprocessor. The simulation program will contain the details of the implementation of all these functions, in fact, all the software available on the large computer. The crucial point is that the length of this simulation program is independent of the length of x, the string to be compressed. For sufficiently long x, the length of this simulation program can be neglected, and we can discuss Kolmogorov complexity without talking about the constants.

If A and U are both universal, we have

|K_U(x) − K_A(x)| < c

(14.6)

for all x. Hence, we will drop all mention of U in all further definitions. We will assume that the unspecified computer U is a fixed universal computer.

Theorem 14.2.2 (Conditional complexity is less than the length of the sequence): K(x | l(x)) ≤ l(x) + c.

(14.7)

Proof: A program for printing x is “Print the following l-bit sequence: x_1 x_2 . . . x_l(x).” Note that no bits are required to describe l since l is given. The program is self-delimiting because l(x) is provided and the end of the program is thus clearly defined. The length of this program is l(x) + c.

Without knowledge of the length of the string, we will need an additional stop symbol or we can use a self-punctuating scheme like the one described in the proof of the next theorem.

Theorem 14.2.3 (Upper bound on Kolmogorov complexity): K(x) ≤ K(x | l(x)) + 2 log l(x) + c.

(14.8)

Proof: If the computer does not know l(x), the method of Theorem 14.2.2 does not apply. We must have some way of informing the computer when it has come to the end of the string of bits that describes the sequence. We describe a simple but inefficient method that uses a sequence 01 as a “comma.”

Suppose that l(x) = n. To describe l(x), repeat every bit of the binary expansion of n twice; then end the description with a 01 so that the computer knows that it has come to the end of the description of n. For example, the number 5 (binary 101) will be described as 11001101. This description requires 2⌈log n⌉ + 2 bits. Thus, inclusion of the binary representation of l(x) does not add more than 2 log l(x) + c bits to the length of the program, and we have the bound in the theorem.

A more efficient method for describing n is to do so recursively. We first specify the number (log n) of bits in the binary representation of n and then specify the actual bits of n. To specify log n, the length of the binary representation of n, we can use the inefficient method (2 log log n) or the efficient method (log log n + · · ·). If we use the efficient method at each level, until we have a small number to specify, we can describe n in log n + log log n + log log log n + · · · bits, where we continue the sum until the last positive term. This sum of iterated logarithms is sometimes written log* n. Thus, Theorem 14.2.3 can be improved to

K(x) ≤ K(x | l(x)) + log* l(x) + c.

(14.9)

We now prove that there are very few sequences with low complexity.

Theorem 14.2.4 (Lower bound on Kolmogorov complexity): The number of strings x with complexity K(x) < k satisfies

|{x ∈ {0, 1}* : K(x) < k}| < 2k.

(14.10)

Proof: There are not very many short programs. If we list all the programs of length < k, we have

1, 0, 1,

00, 01, 10, 11,

000, 001, 010, 011, 100, 101, 110, 111,

. . . ,

11 . . . 1

(14.11)

and the total number of such programs is

1 + 2 + 4 + · · · + 2^{k−1} = 2k − 1 < 2k.

(14.12)

Since each program can produce only one possible output sequence, the number of sequences with complexity < k is less than 2k.

To avoid confusion and to facilitate exposition in the rest of this chapter, we shall need to introduce a special notation for the binary entropy function

H_0(p) = −p log p − (1 − p) log(1 − p).

(14.13)

Thus, when we write H_0(1 n ∑_{i=1}^n X_i), we will mean −X_n log X_n − (1 − X_n) log(1 − X_n) and not the entropy of random variable X_n. When there is no confusion, we shall simply write H(p) for H_0(p).

Now let us consider various examples of Kolmogorov complexity. The complexity will depend on the computer, but only up to an additive constant. To be specific, we consider a computer that can accept unambiguous commands in English (with numbers given in binary notation). We will use the inequality

∑_{n}^{k} 8k(n − k)2^{nH(k/n)} ≤ ∑_{n}^{k} πk(n − k)2^{nH(k/n)}, k ≠ 0, n,

(14.14)

which is proved in Lemma 17.5.1.

Example 14.2.1 (A sequence of n zeros): If we assume that the computer knows n, a short program to print this string is “Print the specified number of zeros.” The length of this program is a constant number of bits. This program length does not depend on n. Hence, the Kolmogorov complexity of this sequence is c, and

K(000 . . . 0 | n) = c

(14.15)

for all n.

Example 14.2.2 (Kolmogorov complexity of π): The first n bits of π can be calculated using a simple series expression. This program has a small constant length if the computer already knows n. Hence,

K(π_1 π_2 · · · π_n | n) = c.

(14.16)

Example 14.2.3 (Gotham weather): Suppose that we want the computer to print out the weather in Gotham for n days. We can write a program that contains the entire sequence x = x_1 x_2 · · · x_n, where x_i = 1 indicates rain on day i. But this is inefficient, since the weather is quite dependent. We can devise various coding schemes for the sequence to take the dependence into account. A simple one is to find a Markov model to approximate the sequence (using the empirical transition probabilities) and then code the sequence using the Shannon code for this probability distribution. We can describe the empirical Markov transitions in O(log n) bits and then use log 1/p(x) bits to describe x, where p is the



KOLMOGOROV COMPLEXITY: DEFINITIONS AND EXAMPLES

14.2 KOLMOGOROV COMPLEXITY: DEFINITIONS AND EXAMPLES

471  
Specified Markov probability. Assuming that the entropy of the weather is 15 bits per day, we can describe the weather for n days using about n/5 bits, and hence  
K(Gotham weather|n) ≈ n/5 + O(log n) + c.  
(14.17)  

Example 14.2.4 (Repeating sequence of the form 01010101...01)  
A short program suffices. Simply print the specified number of 01 pairs. Hence,  
K(010101010...01|n) = c.  
(14.18)  

Example 14.2.5 (Fractal)  
A fractal is part of the Mandelbrot set and is generated by a simple computer program. For different points c in the complex plane, one calculates the number of iterations of the map z_{n+1} = z^2_n + c (starting with z_0 = 0) needed for |z| to cross a particular threshold. The point c is then colored according to the number of iterations needed. Thus, the fractal is an example of an object that looks very complex but is essentially very simple. Its Kolmogorov complexity is essentially zero.  

Example 14.2.6 (Mona Lisa)  
We can make use of the many structures and dependencies in the painting. We can probably compress the image by a factor of 3 or so by using some existing easily described image compression algorithm. Hence, if n is the number of pixels in the image of the Mona Lisa,  
K(Mona Lisa|n) ≤ n/3 + c.  
(14.19)  

Example 14.2.7 (Integer n)  
If the computer knows the number of bits in the binary representation of the integer, we need only provide the values of these bits. This program will have length c + log n. In general, the computer will not know the length of the binary representation of the integer. So we must inform the computer in some way when the description ends. Using the method to describe integers used to derive (14.9), we see that the Kolmogorov complexity of an integer is bounded by  
K(n) ≤ log∗n + c.  
(14.20)  

Example 14.2.8 (Sequence of n bits with k ones)  
Can we compress a sequence of n bits with k ones? Our first guess is no, since we have a series of bits that must be reproduced exactly. But consider the following program:  

Generate, in lexicographic order, all sequences with k ones; of these sequences, print the ith sequence. This program will print out the required sequence. The only variables in the program are k (with known range {0, 1, ..., n}) and i (with conditional range {1, 2, ..., C(n, k)}). The total length of this program is  
l(p) = c + log n (to express k) + log C(n, k) (to express i).  
(14.21)  
≤ c' + log n + nH(k/n) - (1/2) log n,  
(14.22)  
since C(n, k) ≤ 2^{nH(k/n)} / √(π npq) by (14.14) for p = k/n and q = 1 - p and k ≠ 0 and k ≠ n. We have used log n bits to represent k. Thus, if ∑_{i=1}^n x_i = k, then  
K(x1, x2, ..., xn|n) ≤ nH(k/n) + (1/2) log n + c.  
(14.23)  

We can summarize Example 14.2.8 in the following theorem.  

Theorem 14.2.5  
The Kolmogorov complexity of a binary string x is bounded by  
K(x1x2 · · · xn|n) ≤ nH_0(∑_{i=1}^n x_i / n) + (1/2) log n + c.  
(14.24)  

Proof: Use the program described in Example 14.2.8.  

Remark  
Let x ∈ {0, 1}^* be the data that we wish to compress, and consider the program p to be the compressed data. We will have succeeded in compressing the data only if l(p) < l(x), or K(x) < l(x).  

In general, when the length l(x) of the sequence x is small, the constants that appear in the expressions for the Kolmogorov complexity will overwhelm the contributions due to l(x). Hence, the theory is useful primarily when l(x) is very large. In such cases, we can safely neglect the terms that do not depend on l(x).  

14.3 KOLMOGOROV COMPLEXITY AND ENTROPY  

We now consider the relationship between the Kolmogorov complexity of a sequence of random variables and its entropy. In general, we show that the expected value of the Kolmogorov complexity of a random sequence is close to the Shannon entropy. First, we prove that the program lengths satisfy the Kraft inequality.  

Lemma 14.3.1  
For any computer U,  
∑_{p: U(p) halts} 2^{-l(p)} ≤ 1.  
(14.26)  

Proof: If the computer halts on any program, it does not look any further for input. Hence, there cannot be any other halting program with this program as a prefix. Thus, the halting programs form a prefix-free set, and their lengths satisfy the Kraft inequality (Theorem 5.2.1).  

We now show that  
(1/n) E[K(X^n|n)] ≈ H(X) for i.i.d. processes with a finite alphabet.  

Theorem 14.3.1 (Relationship of Kolmogorov complexity and entropy)  
Let the stochastic process {X_i} be drawn i.i.d. according to the probability mass function f(x), x ∈ X, where X is a finite alphabet. Let f(x^n) = ∏_{i=1}^n f(x_i). Then there exists a constant c such that  
H(X) ≤ (1/n) ∑_{x^n} f(x^n) K(x^n|n) ≤ H(X) + (|X| - 1) log n + c.  
(14.27)  
for all n. Consequently,  
E[(1/n) K(X^n|n)] → H(X).  
(14.28)  

Proof: Consider the lower bound. The allowed programs satisfy the prefix property, and thus their lengths satisfy the Kraft inequality. We assign to each x^n the length of the shortest program p such that U(p, n) = x^n. These shortest programs also satisfy the Kraft inequality. We know from the theory of source coding that the expected codeword length must be greater than the entropy. Hence,  
∑_{x^n} f(x^n) K(x^n|n) ≥ H(X_1, X_2, ..., X_n) = nH(X).  
(14.29)  

We first prove the upper bound when X is binary (i.e., X_1, X_2, ..., X_n are i.i.d. ∼ Bernoulli(θ)). Using the method of Theorem 14.2.5, we can bound the complexity of a binary string by  
K(x_1x_2 · · · x_n|n) ≤ nH_0(∑_{i=1}^n x_i / n) + (1/2) log n + c.  
(14.30)  

Hence,  
E[K(X_1X_2 · · · X_n|n)] ≤ nE[H_0(∑_{i=1}^n X_i / n)] + (1/2) log n + c.  
(14.31)  
(a) ≤ nH_0(θ) + (1/2) log n + c,  
(14.32)  
where (a) follows from Jensen’s inequality and the concavity of the entropy. Thus, we have proved the upper bound in the theorem for binary processes.  

We can use the same technique for the case of a nonbinary finite alphabet. We first describe the type of the sequence (the empirical frequency of occurrence of each of the alphabet symbols as defined in Section 11.1) using (|X| - 1) log n bits (the frequency of the last symbol can be calculated from the frequencies of the rest). Then we describe the index of the sequence within the set of all sequences having the same type. The type class has less than 2^{nH(P_x)} elements (where P_x is the type of the sequence x^n) as shown in Chapter 11, and therefore the two-stage description of a string x^n has length  
K(x^n|n) ≤ nH(P_x) + (|X| - 1) log n + c.  
(14.34)  

Again, taking the expectation and applying Jensen’s inequality as in the binary case, we obtain  
E[K(X^n|n)] ≤ nH(X) + (|X| - 1) log n + c.  
(14.35)  

Dividing this by n yields the upper bound of the theorem.



KOLMOGOROV COMPLEXITY OF INTEGERS

Removing the conditioning on the length of the sequence is straightforward. By similar arguments, we can show that

\[
H(X) \leq \frac{1}{n} \sum_{i=1}^{n} x_i f(x_i) K(x_i) \leq H(X) + \frac{|X| + 1}{n} \log n + \frac{c}{n}
\]

for all \( n \). The lower bound follows from the fact that \( K(x_n) \) is also a prefix-free code for the source, and the upper bound can be derived from the fact that \( K(x_n) \leq K(x_n|n) + 2 \log n + c \). Thus,

\[
\frac{1}{n} E[K(X_n)] \to H(X),
\]

and the compressibility achieved by the computer goes to the entropy limit.

---

KOLMOGOROV COMPLEXITY OF INTEGERS

In Section 14.3, we defined the Kolmogorov complexity of a binary string as the length of the shortest program for a universal computer that prints out that string. We can extend that definition to define the Kolmogorov complexity of an integer to be the Kolmogorov complexity of the corresponding binary string.

**Definition**  
The Kolmogorov complexity of an integer \( n \) is defined as

\[
K(n) = \min_{p: U(p) = n} l(p).
\]

The properties of the Kolmogorov complexity of integers are very similar to those of the Kolmogorov complexity of bit strings. The following properties are immediate consequences of the corresponding properties for strings.

**Theorem 14.4.1**  
For universal computers \( A \) and \( U \),

\[
K_U(n) \leq K_A(n) + c_A.
\]

Also, since any number can be specified by its binary expansion, we have the following theorem.

**Theorem 14.4.2**  
\[
K(n) \leq \log^* n + c.
\]

---

KOLMOGOROV COMPLEXITY

**Theorem 14.4.3**  
There are an infinite number of integers \( n \) such that \( K(n) > \log n \).

*Proof:*  
We know from Lemma 14.3.1 that

\[
\sum_{n=1}^{\infty} 2^{-K(n)} \leq 1
\]

and

\[
\sum_{n=1}^{\infty} 2^{-\log n} = \sum_{n=1}^{\infty} \frac{1}{n} = \infty.
\]

But if \( K(n) < \log n \) for all \( n > n_0 \), then

\[
\sum_{n=n_0}^{\infty} 2^{-K(n)} > \sum_{n=n_0}^{\infty} 2^{-\log n} = \infty,
\]

which is a contradiction. \(\Box\)

---

ALGORITHMICALLY RANDOM AND INCOMPRESSIBLE SEQUENCES

From the examples in Section 14.2, it is clear that there are some long sequences that are simple to describe, like the first million bits of \( \pi \). By the same token, there are also large integers that are simple to describe, such as \( 2222222 \) or \( (100!)! \).

We now show that although there are some simple sequences, most sequences do not have simple descriptions. Similarly, most integers are not simple. Hence, if we draw a sequence at random, we are likely to draw a complex sequence. The next theorem shows that the probability that a sequence can be compressed by more than \( k \) bits is no greater than \( 2^{-k} \).

**Theorem 14.5.1**  
Let \( X_1, X_2, \ldots, X_n \) be drawn according to a Bernoulli \( (1/2) \) process. Then

\[
P(K(X_1X_2 \cdots X_n|n) < n - k) < 2^{-k}.
\]

*Proof:*  
\[
P(K(X_1X_2 \cdots X_n|n) < n - k) = \sum_{x_1x_2\cdots x_n: K(x_1x_2\cdots x_n|n) < n - k} p(x_1, x_2, \ldots, x_n)
\]

\[
= \sum_{x_1x_2\cdots x_n: K(x_1x_2\cdots x_n|n) < n - k} 2^{-n}
\]

\[
= |\{x_1x_2 \cdots x_n: K(x_1x_2 \cdots x_n|n) < n - k\}| 2^{-n}
\]

\[
< 2^{n - k} 2^{-n} = 2^{-k}.
\]

\(\Box\)

Thus, most sequences have a complexity close to their length. For example, the fraction of sequences of length \( n \) that have complexity less than \( n - 5 \) is less than \( 1/32 \). This motivates the following definition:

**Definition**  
A sequence \( x_1, x_2, \ldots, x_n \) is said to be algorithmically random if

\[
K(x_1x_2 \cdots x_n|n) \geq n.
\]

Note that by the counting argument, there exists, for each \( n \), at least one sequence \( x_n \) such that

\[
K(x_n|n) \geq n.
\]

**Definition**  
We call an infinite string \( x \) incompressible if

\[
\lim_{n \to \infty} \frac{K(x_1x_2x_3 \cdots x_n|n)}{n} = 1.
\]

**Theorem 14.5.2**  
(Strong law of large numbers for incompressible sequences)  
If a string \( x_1x_2 \cdots \) is incompressible, it satisfies the law of large numbers in the sense that

\[
\frac{1}{n} \sum_{i=1}^{n} x_i \to \frac{1}{2}.
\]

*Proof:*  
Let \( \theta_n = \frac{1}{n} \sum_{i=1}^{n} x_i \) denote the proportion of 1’s in \( x_1x_2 \cdots x_n \). Then using the method of Example 14.2, one can write a program of length \( nH_0(\theta_n) + 2 \log(n\theta_n) + c \) to print \( x_n \). Thus,

\[
\frac{K(x_n|n)}{n} < H_0(\theta_n) + \frac{2 \log n}{n} + \frac{c'}{n}.
\]

By the incompressibility assumption, we also have the lower bound for large enough \( n \),

\[
1 - \epsilon \leq \frac{K(x_n|n)}{n} \leq H_0(\theta_n) + \frac{2 \log n}{n} + \frac{c'}{n}.
\]

Thus,

\[
H_0(\theta_n) > 1 - \frac{2 \log n}{n} - \frac{c'}{n} - \epsilon.
\]

Inspection of the graph of \( H_0(p) \) (Figure 14.2) shows that \( \theta_n \) is close to \( 1/2 \) for large \( n \). Specifically, the inequality above implies that

\[
\theta_n \in \left[\frac{1}{2} - \delta_n, \frac{1}{2} + \delta_n\right],
\]

where \( \delta_n \) is chosen so that

\[
H_0\left(\frac{1}{2} - \delta_n\right) = 1 - \frac{2 \log n}{n} - \frac{c'}{n} - \epsilon,
\]

which implies that \( \delta_n \to 0 \) as \( n \to \infty \). Thus, \( \frac{1}{n} \sum_{i=1}^{n} x_i \to \frac{1}{2} \) as \( n \to \infty \). \(\Box\)

We have now proved that incompressible sequences look random in the sense that the proportion of 0’s and 1’s are almost equal. In general, we can show that if a sequence is incompressible, it will satisfy all computable statistical tests for randomness. (Otherwise, identification of the test that \( x \) fails will reduce the descriptive complexity of \( x \), yielding a contradiction.) In this sense, the algorithmic test for randomness is the ultimate test, including within it all other computable tests for randomness.

We now prove a related law of large numbers for the Kolmogorov complexity of Bernoulli(\( \theta \)) sequences. The Kolmogorov complexity of a sequence of binary random variables drawn i.i.d. according to a Bernoulli(\( \theta \)) process is close to the entropy \( H_0(\theta) \). In Theorem 14.3.1, we proved that the expected value of the Kolmogorov complexity of a random Bernoulli sequence converges to the entropy [i.e., \( E \frac{1}{n} K(X_1X_2 \cdots X_n|n) \to H_0(\theta) \)]. Now we remove the expectation.

**Theorem 14.5.3**  
Let \( X_1, X_2, \ldots, X_n \) be drawn i.i.d. \( \sim \) Bernoulli(\( \theta \)). Then

\[
\frac{1}{n} K(X_1X_2 \cdots X_n|n) \to H_0(\theta)
\]

in probability.

*Proof:*  
Let \( X_n = \frac{1}{n} \sum_{i=1}^{n} X_i \) be the proportion of 1’s in \( X_1, X_2, \ldots, X_n \). Then using the method described in (14.23), we have

\[
K(X_1X_2 \cdots X_n|n) \leq nH_0(X_n) + 2 \log n + c,
\]

and since by the weak law of large numbers, \( X_n \to \theta \) in probability, we have

\[
\Pr\left(\left|\frac{1}{n} K(X_1X_2 \cdots X_n|n) - H_0(\theta)\right| \geq \epsilon\right) \to 0.
\]

Conversely, we can bound the number of sequences with complexity significantly lower than the entropy. From the AEP, we can divide the set of sequences into the typical set and the nontypical set. There are at least \( (1 - \epsilon)2^n H_0(\theta) - \epsilon \) sequences in the typical set. At most \( 2^n H_0(\theta) - c \) of these typical sequences can have a complexity less than \( n(H_0(\theta) - c) \). The probability that the complexity of the random sequence is less than \( n(H_0(\theta) - c) \) is

\[
\Pr(K(X_n|n) < n(H_0(\theta) - c)) \leq \Pr(X_n \notin A(n)_\epsilon) + \Pr(X_n \in A(n)_\epsilon, K(X_n|n) < n(H_0(\theta) - c))
\]

\[
\leq \epsilon + \sum_{x_n \in A(n)_\epsilon, K(x_n|n) < n(H_0(\theta) - c)} p(x_n)
\]

\[
\leq \epsilon + \sum_{x_n \in A(n)_\epsilon, K(x_n|n) < n(H_0(\theta) - c)} 2^{-n H_0(\theta) + \epsilon}
\]

\[
\leq \epsilon + 2^n H_0(\theta) - c) 2^{-n H_0(\theta) + \epsilon}
\]

\[
= \epsilon + 2^{-n c + \epsilon},
\]

which is arbitrarily small for appropriate choice of \( \epsilon, n, \) and \( c \). Hence, with high probability, the Kolmogorov complexity of the random sequence is close to the entropy, and we have

\[
\frac{1}{n} K(X_1, X_2, \ldots, X_n|n) \to H_0(\theta)
\]

in probability. \(\Box\)

---

UNIVERSAL PROBABILITY

Suppose that a computer is fed a random program. Imagine a monkey sitting at a keyboard and typing the keys at random. Equivalently, feed a series of fair coin flips into a universal Turing machine. In either case, most strings will not make sense to the computer. If a person sits at a terminal and types keys at random, he will probably get an error message (i.e., the computer will print the null string and halt). But with a certain probability, she will hit on something that makes sense. The computer will then print out something meaningful. Will this output sequence look random?

From our earlier discussions, it is clear that most sequences of length \( n \) have complexity close to \( n \). Since the probability of an input program \( p \) is \( 2^{-l(p)} \), shorter programs are much more probable than longer ones; and when they produce long strings, shorter programs do not produce random strings; they produce strings with simply described structure.

The probability distribution on the output strings is far from uniform. Under the computer-induced distribution, simple strings are more likely than complicated strings of the same length. This motivates us to define a universal probability distribution on strings as follows:

**Definition**  
The universal probability of a string \( x \) is

\[
P_U(x) = \sum_{p: U(p) = x} 2^{-l(p)} = \Pr(U(p) = x),
\]

which is the probability that a program randomly drawn as a sequence of fair coin flips \( p_1, p_2, \ldots \) will print out the string \( x \).

This probability is universal in many senses. We can consider it as the probability of observing such a string in nature; the implicit belief is that simpler strings are more likely than complicated strings. For example, if we wish to describe the laws of physics, we might consider the simplest string describing the laws as the most likely. This principle, known as Occam’s Razor, has been a general principle guiding scientific research for centuries: If there are many explanations consistent with the observed data, choose the simplest. In our framework, Occam’s Razor is equivalent to choosing the shortest program that produces a given string.

This probability mass function is called universal because of the following theorem.

**Theorem 14.6.1**  
For every computer \( A \),

\[
P_U(x) \geq c'_A P_A(x)
\]

for every string \( x \in \{0, 1\}^* \), where the constant \( c'_A \) depends only on \( U \) and \( A \).

*Proof:*  
From the discussion of Section 14.2, we recall that for every program \( p' \) for \( A \) that prints \( x \), there exists a program \( p \) for \( U \) of length not more than \( l(p') + c_A \) produced by prefixing a simulation program for \( A \). Hence,

\[
P_U(x) = \sum_{p: U(p) = x} 2^{-l(p)} \geq \sum_{p': A(p') = x} 2^{-l(p') - c_A} = c'_A P_A(x).
\]

\(\Box\)

Any sequence drawn according to a computable probability mass function on binary strings can be considered to be produced by some computer \( A \) acting on a random input (via the probability inverse transformation acting on a random input). Hence, the universal probability distribution includes a mixture of all computable probability distributions.



KOLMOGOROV COMPLEXITY

Remark (Bounded likelihood ratio). In particular, Theorem 14.6.1 guarantees that a likelihood ratio test of the hypothesis that X is drawn according to PU versus the hypothesis that it is drawn according to PA will have a bounded likelihood ratio. If U and A are universal, the ratio PU(x)/PA(x) is bounded away from 0 and infinity for all x. This is in contrast to other simple hypothesis-testing problems (like Bernoulli(θ1) versus Bernoulli(θ2)), where the likelihood ratio goes to 0 or ∞ as the sample size goes to infinity. Apparently, PU, which is a mixture of all computable distributions, can never be rejected completely as the true distribution of any data drawn according to some computable probability distribution. In that sense, we cannot reject the possibility that the universe is the output of monkeys typing at a computer. However, we can reject the hypothesis that the universe is random (monkeys with no computer).

In Section 14.11, we prove that PU(x) ≈ 2^{-K(x)}, thus showing that K(x) and log_{2} PU(x) have equal status as universal algorithmic complexity measures. This is especially interesting since log_{2} PU(x) is the ideal codeword length (the Shannon codeword length) with respect to the universal probability distribution PU(x).

We conclude this section with an example of a monkey at a typewriter vs. a monkey at a computer keyboard. If the monkey types at random on a typewriter, the probability that it types out all the works of Shakespeare (assuming that the text is 1 million bits long) is 2^{-1,000,000}. If the monkey sits at a computer terminal, however, the probability that it types out Shakespeare is now 2^{-K(Shakespeare)} ≈ 2^{-250,000}, which, although extremely small, is still exponentially more likely than when the monkey sits at a dumb typewriter. The example indicates that a random input to a computer is much more likely to produce "interesting" outputs than a random input to a typewriter. We all know that a computer is an intelligence amplifier. Apparently, it creates sense from nonsense as well.

14.7 THE HALTING PROBLEM AND THE NONCOMPUTABILITY OF KOLMOGOROV COMPLEXITY

Consider the following paradoxical statement: "This statement is false." This paradox is sometimes stated in a two-statement form: "The next statement is false. The preceding statement is true." These paradoxes are versions of what is called the Epimenides liar paradox and illustrate the pitfalls involved in self-reference. In 1931, Gödel used this idea of self-reference to show that any interesting system of mathematics is not complete; there are statements in the system that are true but that cannot be proved within the system. To accomplish this, he translated theorems and proofs into integers and constructed a statement of the above form, which can therefore not be proved true or false.

The halting problem in computer science is very closely connected with Gödel’s incompleteness theorem. In essence, it states that for any computational model, there is no general algorithm to decide whether a program will halt or not (go on forever). Note that it is not a statement about any specific program. Quite clearly, there are many programs that can easily be shown to halt or go on forever. The halting problem says that we cannot answer this question for all programs. The reason for this is again the idea of self-reference.

To a practical person, the halting problem may not be of any immediate significance, but it has great theoretical importance as the dividing line between things that can be done on a computer (given unbounded memory and time) and things that cannot be done at all (such as proving all true statements in number theory). Gödel’s incompleteness theorem is one of the most important mathematical results of the twentieth century, and its consequences are still being explored. The halting problem is an essential example of Gödel’s incompleteness theorem.

One of the consequences of the nonexistence of an algorithm for the halting problem is the noncomputability of Kolmogorov complexity. The only way to find the shortest program in general is to try all short programs and see which of them can do the job. However, at any time, some of the short programs may not have halted, and there is no effective (finite mechanical) way to tell whether or not they will halt and what they will print out. Hence, there is no effective way to find the shortest program to print a given string.

The noncomputability of Kolmogorov complexity is an example of the Berry paradox. The Berry paradox asks for the shortest number not nameable in under 10 words. A number like 1,101,121 cannot be a solution since the defining expression itself is less than 10 words long. This illustrates the problems with the terms nameable and describable; they are too powerful to be used without a strict meaning. If we restrict ourselves to the meaning "can be described for printing out on a computer," we can resolve Berry’s paradox by saying that the smallest number not describable in less than 10 words exists but is not computable. This "description" is not a program for computing the number. E. F. Beckenbach pointed out a similar problem in the classification of numbers as dull or interesting; the smallest dull number must be interesting.

As stated at the beginning of the chapter, one does not really anticipate that practitioners will find the shortest computer program for a given string. The shortest program is not computable, although as more and more programs are shown to produce the string, the estimates from above of the Kolmogorov complexity converge to the true Kolmogorov complexity. (The problem, of course, is that one may have found the shortest program and never know that no shorter program exists.) Even though Kolmogorov complexity is not computable, it provides a framework within which to consider questions of randomness and inference.

14.8 CHAITIN'S MYSTICAL, MAGICAL NUMBER, Ω

In this section, we introduce Chaitin’s mystical, magical number, Ω, which has some extremely interesting properties.

Definition: Ω = Σ_{p: U(p) halts} 2^{-l(p)}. Note that Ω = Pr(U(p) halts), the probability that the given universal computer halts when the input to the computer is a binary string drawn according to a Bernoulli(1/2) process. Since the programs that halt are prefix-free, their lengths satisfy the Kraft inequality, and hence the sum above is always between 0 and 1. Let Ω_n = ω₁ω₂ · · · ω_n denote the first n bits of Ω. The properties of Ω are as follows:

1. Ω is noncomputable. There is no effective (finite, mechanical) way to check whether arbitrary programs halt (the halting problem), so there is no effective way to compute Ω.

2. Ω is a "philosopher’s stone." Knowing Ω to an accuracy of n bits will enable us to decide the truth of any provable or finitely refutable mathematical theorem that can be written in less than n bits. Actually, all that this means is that given n bits of Ω, there is an effective procedure to decide the truth of n-bit theorems; the procedure may take an arbitrarily long (but finite) time. Of course, without knowing Ω, it is not possible to check the truth or falsity of every theorem by an effective procedure (Gödel’s incompleteness theorem).

The basic idea of the procedure using n bits of Ω is simple: We run all programs until the sum of the masses 2^{-l(p)} contributed by programs that halt equals or exceeds Ω_n = 0.ω₁ω₂ · · · ω_n, the truncated version of Ω that we are given. Then, since Ω - Ω_n < 2^{-n}, we know that the sum of all further contributions of the form 2^{-l(p)} to Ω from programs that halt must also be less than 2^{-n}. This implies that no program of length ≤n that has not yet halted will ever halt, which enables us to decide the halting or nonhalting of all programs of length ≤n.

To complete the proof, we must show that it is possible for a computer to run all possible programs in "parallel" in such a way that any program that halts will eventually be found to halt. First, list all possible programs, starting with the null program, λ: λ, 0, 1, 00, 01, 10, 11, 000, 001, 010, 011, . . . . Then let the computer execute one clock cycle of λ for the first cycle. In the next cycle, let the computer execute two clock cycles of λ and two clock cycles of the program 0. In the third cycle, let it execute three clock cycles of each of the first three programs, and so on. In this way, the computer will eventually run all possible programs and run them for longer and longer times, so that if any program halts, it will eventually be discovered to halt. The computer keeps track of which program is being executed and the cycle number so that it can produce a list of all the programs that halt. Thus, we will ultimately know whether or not any program of less than n bits will halt. This enables the computer to find any proof of the theorem or a counterexample to the theorem if the theorem can be stated in less than n bits. Knowledge of Ω turns previously unprovable theorems into provable theorems. Here Ω acts as an oracle.

Although Ω seems magical in this respect, there are other numbers that carry the same information. For example, if we take the list of programs and construct a real number in which the ith bit indicates whether program i halts, this number can also be used to decide any finitely refutable question in mathematics. This number is very dilute (in information content) because one needs approximately 2n bits of this indicator function to decide whether or not an n-bit program halts. Given 2n bits, one can tell immediately without any computation whether or not any program of length less than n halts. However, Ω is the most compact representation of this information since it is algorithmically random and incompressible.

What are some of the questions that we can resolve using Ω? Many of the interesting problems in number theory can be stated as a search for a counterexample. For example, it is straightforward to write a program that searches over the integers x, y, z, and n and halts only if it finds a counterexample to Fermat’s last theorem, which states that x^n + y^n = z^n has no solution in integers for n ≥3. Another example is Goldbach’s conjecture, which states that any even number is a sum of two primes. Our program would search through all the even numbers starting with 2, check all prime numbers less than it, and find a decomposition as a sum of two primes. It will halt if it comes across an even number that does not have such a decomposition. Knowing whether this program halts is equivalent to knowing the truth of Goldbach’s conjecture.

We can also design a program that searches through all proofs and halts only when it finds a proof of the theorem required. This program will eventually halt if the theorem has a finite proof. Hence, knowing n bits of Ω, we can find the truth or falsity of all theorems that have a finite proof or are finitely refutable and which can be stated in less than n bits.

3. Ω is algorithmically random. Theorem 14.8.1: Ω cannot be compressed by more than a constant; that is, there exists a constant c such that K(ω₁ω₂ . . . ω_n) ≥n −c for all n.

Proof: We know that if we are given n bits of Ω, we can determine whether or not any program of length ≤n halts. Using K(ω₁ω₂ · · · ω_n) bits, we can calculate n bits of Ω, and then we can generate a list of all programs of length ≤n that halt, together with their corresponding outputs. We find the first string x₀ that is not on this list. The string x₀ is then the shortest string with Kolmogorov complexity K(x₀) > n. The complexity of this program to print x₀ is K(Ω_n) + c, which must be at least as long as the shortest program for x₀. Consequently, K(Ω_n) + c ≥K(x₀) > n for all n. Thus, K(ω₁ω₂ · · · ω_n) > n −c, and Ω cannot be compressed by more than a constant.

14.9 UNIVERSAL GAMBLING

Suppose that a gambler is asked to gamble sequentially on sequences x ∈{0, 1}^*. He has no idea of the origin of the sequence. He is given fair odds (2-for-1) on each bit. How should he gamble? If he knew the distribution of the elements of the string, he might use proportional betting because of its optimal growth-rate properties, as shown in Chapter 6. If he believed that the string occurred naturally, it seems intuitive that simpler strings are more likely than complex ones. Hence, if he were to extend the idea of proportional betting, he might bet according to the universal probability of the string. For reference, note that if the gambler knew the string x in advance, he could increase his wealth by a factor of 2^{l(x)} simply by betting all his wealth each time on the next symbol of x. Let the wealth S(x) associated with betting scheme b(x), where ∑ b(x) = 1, be given by S(x) = 2^{l(x)}b(x).

Suppose that the gambler bets b(x) = 2^{-K(x)} on a string x. This betting strategy can be called universal gambling. We note that the sum of the bets ∑ b(x) = ∑ 2^{-K(x)} ≤ ∑_{p: p halts} 2^{-l(p)} = Ω ≤1, and he will not have used all his money. For simplicity, let us assume that he throws the rest away. For example, the amount of wealth resulting from a bet b(0110) on a sequence x = 0110 is 2^{l(x)}b(x) = 2^{4}b(0110) plus the amount won on all bets b(0110...) on sequences that extend x. Then we have the following theorem:

Theorem 14.9.1: The logarithm of the wealth a gambler achieves on a sequence using universal gambling plus the complexity of the sequence is no smaller than the length of the sequence, or log S(x) + K(x) ≥ l(x).



--- Page 14 ---
488
KOLMOGOROV COMPLEXITY
Remark
This is the counterpart of the gambling conservation theorem \( W^* + H = \log m \) from Chapter 6.

Proof:
The proof follows directly from the universal gambling scheme, \( b(x) = 2^{-K(x)} \), since
\[
S(x) = \sum_{x' \supseteq x} 2^{l(x)}b(x') \geq 2^{l(x)}2^{-K(x)}, 
\]
where \( x' \supseteq x \) means that \( x \) is a prefix of \( x' \). Taking logarithms establishes the theorem.
□

The result can be understood in many ways. For infinite sequences \( x \) with finite Kolmogorov complexity,
\[
S(x_1 x_2 \cdots x_l) \geq 2^l - K(x) = 2^l - c 
\]
for all \( l \). Since \( 2^l \) is the most that can be won in \( l \) gambles at fair odds, this scheme does asymptotically as well as the scheme based on knowing the sequence in advance. For example, if \( x = \pi_1 \pi_2 \cdots \pi_n \cdots \), the digits in the expansion of \( \pi \), the wealth at time \( n \) will be \( S_n = S(x_n) \geq 2^n - c \) for all \( n \).

If the string is actually generated by a Bernoulli process with parameter \( p \), then
\[
S(X_1 \cdots X_n) \geq 2^n - nH_0(X_n) - 2 \log n - c \approx 2^n(1 - H_0(p) - 2 \log n / n - c / n),
\]
which is the same to first order as the rate achieved when the gambler knows the distribution in advance, as in Chapter 6.

From the examples we see that the universal gambling scheme on a random sequence does asymptotically as well as a scheme that uses prior knowledge of the true distribution.

14.10
OCCAM’S RAZOR

In many areas of scientific research, it is important to choose among various explanations of data observed. After choosing the explanation, we wish to assign a confidence level to the predictions that ensue from the laws that have been deduced. For example, Laplace considered the probability that the sun will rise again tomorrow given that it has risen every day in recorded history. Laplace’s solution was to assume that the rising of the sun was a Bernoulli(\( \theta \)) process with unknown parameter \( \theta \). He assumed that \( \theta \) was uniformly distributed on the unit interval. Using the observed data, he calculated the posterior probability that the sun will rise again tomorrow and found that it was
\[
P(X_{n+1} = 1 | X_n = 1, X_{n-1} = 1, \ldots, X_1 = 1) = \frac{P(X_{n+1} = 1, X_n = 1, X_{n-1} = 1, \ldots, X_1 = 1)}{P(X_n = 1, X_{n-1} = 1, \ldots, X_1 = 1)} = \frac{\int_0^1 \theta^{n+1} d\theta}{\int_0^1 \theta^n d\theta} = \frac{n + 1}{n + 2},
\]
which he put forward as the probability that the sun will rise on day \( n + 1 \) given that it has risen on days 1 through \( n \).

Using the ideas of Kolmogorov complexity and universal probability, we can provide an alternative approach to the problem. Under the universal probability, let us calculate the probability of seeing a 1 next after having observed \( n \) 1’s in the sequence so far. The conditional probability that the next symbol is a 1 is the ratio of the probability of all sequences with initial segment \( 1^n \) and next bit equal to 1 to the probability of all sequences with initial segment \( 1^n \). The simplest programs carry most of the probability; hence we can approximate the probability that the next bit is a 1 with the probability of the program that says “Print 1’s forever.”
\[
p(1^n 1 y) \approx p(1^\infty) = c > 0.
\]
Estimating the probability that the next bit is 0 is more difficult. Since any program that prints \( 1^n 0 \cdots \) yields a description of \( n \), its length should at least be \( K(n) \), which for most \( n \) is about \( \log n + O(\log \log n) \), and hence, ignoring second-order terms, we have
\[
p(1^n 0 y) \approx p(1^n 0) \approx 2^{-\log n} \approx \frac{1}{n}.
\]
Hence, the conditional probability of observing a 0 next is
\[
p(0 | 1^n) = \frac{p(1^n 0)}{p(1^n 0) + p(1^\infty)} \approx \frac{1 / n}{c + 1 / n} \approx \frac{1}{c n + 1},
\]
which is similar to the result \( p(0 | 1^n) = 1 / (n + 1) \) derived by Laplace.

--- Page 15 ---
490
KOLMOGOROV COMPLEXITY

This type of argument is a special case of Occam’s Razor, a general principle governing scientific research, weighting possible explanations by their complexity. William of Occam said “Nunquam ponenda est pluralitas sine necessitate”: Explanations should not be multiplied beyond necessity [516]. In the end, we choose the simplest explanation that is consistent with the data observed. For example, it is easier to accept the general theory of relativity than it is to accept a correction factor of \( c / r^3 \) to the gravitational law to explain the precession of the perihelion of Mercury, since the general theory explains more with fewer assumptions than does a “patched” Newtonian theory.

14.11
KOLMOGOROV COMPLEXITY AND UNIVERSAL PROBABILITY

We now prove an equivalence between Kolmogorov complexity and universal probability. We begin by repeating the basic definitions.
\[
K(x) = \min_{p: U(p) = x} l(p),
\]
\[
PU(x) = \sum_{p: U(p) = x} 2^{-l(p)}.
\]

Theorem 14.11.1 (Equivalence of \( K(x) \) and \( \log^{-1} PU(x) \)).

There exists a constant \( c \), independent of \( x \), such that
\[
2^{-K(x)} \leq PU(x) \leq c 2^{-K(x)}
\]
for all strings \( x \). Thus, the universal probability of a string \( x \) is determined essentially by its Kolmogorov complexity.

Remark:
This implies that \( K(x) \) and \( \log^{-1} PU(x) \) have equal status as universal complexity measures, since
\[
K(x) - c' \leq \log^{-1} PU(x) \leq K(x).
\]
Recall that the complexity defined with respect to two different computers \( K_U \) and \( K_{U'} \) are essentially equivalent complexity measures if \( |K_U(x) - K_{U'}(x)| \) is bounded. Theorem 14.11.1 shows that \( K_U(x) \) and \( \log^{-1} PU(x) \) are essentially equivalent complexity measures.

Notice the striking similarity between the relationship of \( K(x) \) and \( \log^{-1} PU(x) \) in Kolmogorov complexity and the relationship of \( H(X) \) and \( \log^{-1} p(x) \) in information theory. The ideal Shannon code length assignment \( l(x) = \log^{-1} p(x) \) achieves an average description length \( H(X) \), while in Kolmogorov complexity theory, the ideal description length \( \log^{-1} PU(x) \) is almost equal to \( K(X) \). Thus, \( \log^{-1} p(x) \) is the natural notion of descriptive complexity of \( x \) in algorithmic as well as probabilistic settings.

The upper bound in (14.90) is obvious from the definitions, but the lower bound is more difficult to prove. The result is very surprising, since there are an infinite number of programs that print \( x \). From any program, it is possible to produce longer programs by padding the program with irrelevant instructions. The theorem proves that although there are an infinite number of such programs, the universal probability is essentially determined by the largest term, which is \( 2^{-K(x)} \). If \( PU(x) \) is large, \( K(x) \) is small, and vice versa.

However, there is another way to look at the upper bound that makes it less surprising. Consider any computable probability mass function on strings \( p(x) \). Using this mass function, we can construct a Shannon–Fano code (Section 5.9) for the source and then describe each string by the corresponding codeword, which will have length \( \log^{-1} p(x) \). Hence, for any computable distribution, we can construct a description of a string using not more than \( \log^{-1} p(x) + c \) bits, which is an upper bound on the Kolmogorov complexity \( K(x) \). Even though \( PU(x) \) is not a computable probability mass function, we are able to finesse the problem using the rather involved tree construction procedure described below.

Proof:
(of Theorem 14.11.1). The first inequality is simple. Let \( p^* \) be the shortest program for \( x \). Then
\[
PU(x) = \sum_{p: U(p) = x} 2^{-l(p)} \geq 2^{-l(p^*)} = 2^{-K(x)},
\]
as we wished to show.

We can rewrite the second inequality as
\[
K(x) \leq \log^{-1} PU(x) + c.
\]

Our objective in the proof is to find a short program to describe the strings that have high \( PU(x) \). An obvious idea is some kind of Huffman coding based on \( PU(x) \), but \( PU(x) \) cannot be calculated effectively, hence a procedure using Huffman coding is not implementable on a computer. Similarly, the process using the Shannon–Fano code also cannot be implemented.

However, if we have the Shannon–Fano code tree, we can reconstruct the string by looking for the corresponding node in the tree. This is the basis for the following tree construction procedure.

To overcome the problem of noncomputability of \( PU(x) \), we use a modified approach, trying to construct a code tree directly. Unlike Huffman coding, this approach is not optimal in terms of minimum expected codeword length. However, it is good enough for us to derive a code for which each codeword for \( x \) has a length that is within a constant of \( \log^{-1} PU(x) \).

Before we get into the details of the proof, let us outline our approach. We want to construct a code tree in such a way that strings with high probability have low depth. Since we cannot calculate the probability of a string, we do not know a priori the depth of the string on the tree. Instead, we assign \( x \) successively to the nodes of the tree, assigning \( x \) to nodes closer and closer to the root as our estimate of \( PU(x) \) improves. We want the computer to be able to recreate the tree and use the lowest depth node corresponding to the string \( x \) to reconstruct the string.

We now consider the set of programs and their corresponding outputs \( \{(p, x)\} \). We try to assign these pairs to the tree. But we immediately come across a problem—there are an infinite number of programs for a given string, and we do not have enough nodes of low depth. However, as we shall show, if we trim the list of program-output pairs, we will be able to define a more manageable list that can be assigned to the tree.

Next, we demonstrate the existence of programs for \( x \) of length \( \log^{-1} PU(x) \).

Tree construction procedure: For the universal computer \( U \), we simulate all programs using the technique explained in Section 14.8. We list all binary programs:
\[
\epsilon, 0, 1, 00, 01, 10, 11, 000, 001, 010, 011, \ldots.
\]
Then let the computer execute one clock cycle of \( \epsilon \) for the first stage. In the next stage, let the computer execute two clock cycles of \( \epsilon \) and two clock cycles of the program \( 0 \). In the third stage, let the computer execute three clock cycles of each of the first three programs, and so on. In this way, the computer will eventually run all possible programs and run them for longer and longer times, so that if any program halts, it will be discovered to halt eventually. We use this method to produce a list of all programs that halt in the order in which they halt, together with their associated outputs. For each program and its corresponding output, \( (p_k, x_k) \), we calculate \( n_k \), which is chosen so that it corresponds to the current estimate of \( PU(x) \). Specifically,
\[
n_k = \left\lfloor \log \hat{PU}(x_k) \right\rfloor,
\]
where
\[
\hat{PU}(x_k) = \sum_{(p_i, x_i): x_i = x_k, i \leq k} 2^{-l(p_i)}.
\]
Note that \( \hat{PU}(x_k) \uparrow PU(x) \) on the subsequence of times \( k \) such that \( x_k = x \).

We are now ready to construct a tree. As we add to the list of triplets, \( (p_k, x_k, n_k) \), of programs that halt, we map some of them onto nodes of a binary tree. For purposes of the construction, we must ensure that all the \( n_i \)’s corresponding to a particular \( x_k \) are distinct. To ensure this, we remove from the list all triplets that have the same \( x \) and \( n \) as a previous triplet. This will ensure that there is at most one node at each level of the tree that corresponds to a given \( x \).

Let \( \{(p'_i, x'_i, n'_i): i = 1, 2, 3, \ldots\} \) denote the new list. On the winnowed list, we assign the triplet \( (p'_k, x'_k, n'_k) \) to the first available node at level \( n'_k + 1 \). As soon as a node is assigned, all of its descendants become unavailable for assignment. (This keeps the assignment prefix-free.)

We illustrate this by means of an example:
\[
(p_1, x_1, n_1) = (10111, 1110, 5), \quad n_1 = 5 \text{ because } PU(x_1) \geq 2^{-5},
\]
\[
(p_2, x_2, n_2) = (11, 10, 2), \quad n_2 = 2 \text{ because } PU(x_2) \geq 2^{-2},
\]
\[
(p_3, x_3, n_3) = (0, 1110, 1), \quad n_3 = 1 \text{ because } PU(x_3) \geq 2^{-1} + 2^{-5} \geq 2^{-1},
\]
\[
(p_4, x_4, n_4) = (1010, 1111, 4), \quad n_4 = 4 \text{ because } PU(x_4) \geq 2^{-4},
\]
\[
(p_5, x_5, n_5) = (101101, 1110, 1), \quad n_5 = 1 \text{ because } PU(x_5) \geq 2^{-1} + 2^{-5} + 2^{-5} \geq 2^{-1},
\]
\[
(p_6, x_6, n_6) = (100, 1, 3), \quad n_6 = 3 \text{ because } PU(x_6) \geq 2^{-3}.
\]
We note that the string \( x = 1110 \) appears in positions 1, 3, and 5 in the list, but \( n_3 = n_5 \). The estimate of the probability \( \hat{PU}(1110) \) has not jumped sufficiently for \( (p_5, x_5, n_5) \) to survive the cut. Thus, the winnowed list becomes
\[
(p'_1, x'_1, n'_1) = (10111, 1110, 5),
\]
\[
(p'_2, x'_2, n'_2) = (11, 10, 2),
\]
\[
(p'_3, x'_3, n'_3) = (0, 1110, 1),
\]
\[
(p'_4, x'_4, n'_4) = (1010, 1111, 4),
\]
\[
(p'_5, x'_5, n'_5) = (100, 1, 3),
\]
and so on.

The assignment of the winnowed list to nodes of the tree is illustrated in Figure 14.3.



KOLMOGOROV COMPLEXITY

(p3, x3, n3), x3 = 110  
(p2, x2, n2), x2 = 10  
(p5, x5, n5), x5 = 100  
(p4, x4, n4), x4 = 1111  
(p1, x1, n1), x1 = 1110  

FIGURE 14.3. Assignment of nodes.  

In the example, we are able to find nodes at level nk + 1 to which we can assign the triplets. Now we shall prove that there are always enough nodes so that the assignment can be completed. We can perform the assignment of triplets to nodes if and only if the Kraft inequality is satisfied.  

We now drop the primes and deal only with the winnowed list illustrated in (14.97). We start with the infinite sum in the Kraft inequality and split it according to the output strings:  

\[
\sum_{k=1}^{\infty} 2^{-(nk+1)} = \sum_{x \in \{0,1\}^*} \sum_{k: xk=x} 2^{-(nk+1)}.  
\]

(14.98)  

We then write the inner sum as  

\[
\sum_{k: xk=x} 2^{-(nk+1)} = 2^{-1} \sum_{k: xk=x} 2^{-nk}.  
\]

(14.99)  

KOLMOGOROV COMPLEXITY AND UNIVERSAL PROBABILITY  

\[
\leq 2^{-1} \left(2^{\lfloor \log PU(x) \rfloor + 2^{\lfloor \log PU(x) \rfloor - 1} + 2^{\lfloor \log PU(x) \rfloor - 2} + \cdots} \right).  
\]

(14.100)  

\[
= 2^{-1} \left( \frac{1}{2^{\lfloor \log PU(x) \rfloor}} \left(1 + \frac{1}{2} + \frac{1}{4} + \cdots \right) \right).  
\]

(14.101)  

\[
= 2^{-1} \cdot 2^{-\lfloor \log PU(x) \rfloor} \leq PU(x),  
\]

(14.102)  

where (14.100) is true because there is at most one node at each level that prints out a particular x. More precisely, the nk’s on the winnowed list for a particular output string x are all different integers. Hence,  

\[
\sum_{k: xk=x} 2^{-(nk+1)} \leq PU(x) \leq 1,  
\]

(14.104)  

and we can construct a tree with the nodes labeled by the triplets.  

If we are given the tree constructed above, it is easy to identify a given x by the path to the lowest depth node that prints x. Call this node \tilde{p}. (By construction, l(\tilde{p}) \leq \log_{1/PU}(x) + 2.) To use this tree in a program to print x, we specify \tilde{p} and ask the computer to execute the foregoing simulation of all programs. Then the computer will construct the tree as described above and wait for the particular node \tilde{p} to be assigned. Since the computer executes the same construction as the sender, eventually the node \tilde{p} will be assigned. At this point, the computer will halt and print out the x assigned to that node.  

This is an effective (finite, mechanical) procedure for the computer to reconstruct x. However, there is no effective procedure to find the lowest depth node corresponding to x. All that we have proved is that there is an (infinite) tree with a node corresponding to x at level \lceil \log_{1/PU}(x) \rceil + 1. But this accomplishes our purpose.  

With reference to the example, the description of x = 1110 is the path to the node (p3, x3, n3) (i.e., 01), and the description of x = 1111 is the path 00001. If we wish to describe the string 1110, we ask the computer to perform the (simulation) tree construction until node 01 is assigned. Then we ask the computer to execute the program corresponding to node 01 (i.e., p3). The output of this program is the desired string, x = 1110.  

The length of the program to reconstruct x is essentially the length of the description of the position of the lowest depth node \tilde{p} corresponding to x in the tree. The length of this program for x is l(\tilde{p}) + c, where  

\[
l(\tilde{p}) \leq \log_{1/PU}(x) + 1,  
\]

(14.105)  

and hence the complexity of x satisfies  

\[
K(x) \leq \log_{1/PU}(x) + c.  
\]

(14.106)  

KOLMOGOROV SUFFICIENT STATISTIC  

Suppose that we are given a sample sequence from a Bernoulli(\theta) process. What are the regularities or deviations from randomness in this sequence? One way to address the question is to find the Kolmogorov complexity K(x_n|n), which we discover to be roughly nH_0(\theta) + \log n + c. Since, for \theta \neq 1/2, this is much less than n, we conclude that x_n has structure and is not randomly drawn Bernoulli(1/2). But what is the structure? The first attempt to find the structure is to investigate the shortest program p* for x_n. But the shortest description of p* is about as long as p* itself; otherwise, we could further compress the description of x_n, contradicting the minimality of p*. So this attempt is fruitless.  

A hint at a good approach comes from an examination of the way in which p* describes x_n. The program “The sequence has k 1’s; of such sequences, it is the ith” is optimal to first order for Bernoulli(\theta) sequences. We note that it is a two-stage description, and all of the structure of the sequence is captured in the first stage. Moreover, x_n is maximally complex given the first stage of the description. The first stage, the description of k, requires \log(n + 1) bits and defines a set S = \{x \in \{0, 1\}^n : \sum x_i = k\}. The second stage requires \log |S| = \log \binom{n}{k} \approx nH_0(x_n) \approx nH_0(\theta) bits and reveals nothing extraordinary about x_n.  

We mimic this process for general sequences by looking for a simple set S that contains x_n. We then follow it with a brute-force description of x_n in S using \log |S| bits. We begin with a definition of the smallest set containing x_n that is describable in no more than k bits.  

DEFINITION  

The Kolmogorov structure function K_k(x_n|n) of a binary string x \in \{0, 1\}^n is defined as  

\[
K_k(x_n|n) = \min_{p : l(p) \leq k} \log |S|,  
\]

(14.107)  

where S is the smallest set that can be described with no more than k bits and which includes x_n. By U(p, n) = S, we mean that running the program p with data n on the universal computer U will print out the indicator function of the set S.  

DEFINITION  

For a given small constant c, let k* be the least k such that  

\[
K_k(x_n|n) + k \leq K(x_n|n) + c.  
\]

(14.108)  

Let S** be the corresponding set and let p** be the program that prints out the indicator function of S**. Then we shall say that p** is a Kolmogorov minimal sufficient statistic for x_n.  

Consider the programs p* describing sets S* such that  

\[
K_k(x_n|n) + k = K(x_n|n).  
\]

(14.109)  

All the programs p* are “sufficient statistics” in that the complexity of x_n given S* is maximal. But the minimal sufficient statistic is the shortest “sufficient statistic.”  

The equality in the definition above is up to a large constant depending on the computer U. Then k* corresponds to the least k for which the two-stage description of x_n is as good as the best single-stage description of x_n. The second stage of the description merely provides the index of x_n within the set S**; this takes K_k(x_n|n) bits if x_n is conditionally maximally complex given the set S**. Hence the set S** captures all the structure within x_n. The remaining description of x_n within S** is essentially the description of the randomness within the string. Hence S** or p** is called the Kolmogorov sufficient statistic for x_n.  

This is parallel to the definition of a sufficient statistic in mathematical statistics. A statistic T is said to be sufficient for a parameter \theta if the distribution of the sample given the sufficient statistic is independent of the parameter; that is,  

\[
\theta \rightarrow T(X) \rightarrow X  
\]

(14.110)  

forms a Markov chain in that order. For the Kolmogorov sufficient statistic, the program p** is sufficient for the “structure” of the string x_n; the remainder of the description of x_n is essentially independent of the “structure” of x_n. In particular, x_n is maximally complex given S**.  

A typical graph of the structure function is illustrated in Figure 14.4.  

When k = 0, the only set that can be described is the entire set \{0, 1\}^n, so that the corresponding log set size is n. As we increase k, the size of the set drops rapidly until  

\[
k + K_k(x_n|n) \approx K(x_n|n).  
\]

(14.111)  

After this, each additional bit of k reduces the set by half, and we proceed along the line of slope -1 until k = K(x_n|n). For k \geq K(x_n|n), the smallest set that can be described that includes x_n is the singleton \{x_n\}, and hence K_k(x_n|n) = 0.  

We will now illustrate the concept with a few examples.  

1. Bernoulli(\theta) sequence. Consider a sample of length n from a Bernoulli sequence with an unknown parameter \theta. As discussed in Example 14.2, we can describe this sequence with nH_0(\theta) + \frac{1}{2} \log n + c bits using a two-stage description where we describe k in the first stage (using \log n bits) and then describe the sequence within all sequences with k ones (using \log \binom{n}{k} bits). However, we can use an even shorter first-stage description. Instead of describing k exactly, we divide the range of k into bins and describe k only to an accuracy of \sqrt{n \choose k} n using \frac{1}{2} \log n bits. Then we describe the actual sequence among all sequences whose type is in the same bin as k.  

The size of the set of all sequences with l ones, l \in k \pm \sqrt{n \choose k} n, is \binom{n}{k} \approx nH_0(\theta) + o(n) by Stirling’s formula, so the total description length is still nH_0(\theta) + \frac{1}{2} \log n + o(n), but the description length of the Kolmogorov sufficient statistics is k* \approx \frac{1}{n} \log n.  

2. Sample from a Markov chain. In the same vein as the preceding example, consider a sample from a first-order binary Markov chain. In this case again, p** will correspond to describing the Markov type of the sequence (the number of occurrences of 00’s, 01’s, 10’s, and 11’s in the sequence); this conveys all the structure in the sequence. The remainder of the description will be the index of the sequence in the set of all sequences of this Markov type. Hence, in this case, k* \approx 2(\frac{1}{2} \log n) = \log n, corresponding to describing two elements of the conditional joint type to appropriate accuracy. (The other elements of the conditional joint type can be determined from these two.)  

3. Mona Lisa.  

Consider an image that consists of a gray circle on a white background. The circle is not uniformly gray but Bernoulli with parameter \theta. This is illustrated in Figure 14.6. In this case, the best two-stage description is first to describe the size and position of the circle and its average gray level and then to describe the index of the circle among all the circles with the same gray level. Assuming an n-pixel image (of size \sqrt{n} by \sqrt{n}), there are about n + 1 possible gray levels, and there are about (\sqrt{n})^3 distinguishable circles. Hence, k* \approx \frac{5}{2} \log n in this case.  

MINIMUM DESCRIPTION LENGTH PRINCIPLE  

A natural extension of Occam’s razor occurs when we need to describe data drawn from an unknown distribution. Let X_1, X_2, \ldots, X_n be drawn i.i.d. according to probability mass function p(x). We assume that we do not know p(x), but know that p(x) \in P, a class of probability mass functions. Given the data, we can estimate the probability mass function in P that best fits the data. For simple classes P (e.g., if P has only finitely many distributions), the problem is straightforward, and the maximum likelihood procedure [i.e., find \hat{p} \in P that maximizes \hat{p}(X_1, X_2, \ldots, X_n)] works well. However, if the class P is rich enough, there is a problem of overfitting the data. For example, if X_1, X_2, \ldots, X_n are continuous random variables, and if P is the set of all probability distributions, the maximum likelihood estimator given X_1, X_2, \ldots, X_n is a distribution that places a single mass point of weight 1/n at each observed value. Clearly, this estimate is too closely tied to actual observed data and does not capture any of the structure of the underlying distribution.  

To get around this problem, various methods have been applied. In the simplest case, the data are assumed to come from some parametric distribution (e.g., the normal distribution), and the parameters of the distribution are estimated from the data. To validate this method, the data should be tested to check whether the distribution “looks” normal, and if the data pass the test, we could use this description of the data. A more general procedure is to take the maximum likelihood estimate and smooth it out to obtain a smooth density. With enough data, and appropriate smoothness.



**Kolmogorov Complexity**

---

### Summary

Under certain conditions, it is possible to make good estimates of the original density. This process is called kernel density estimation. However, the theory of Kolmogorov complexity (or the Kolmogorov sufficient statistic) suggests a different procedure: Find the \( p \in P \) that minimizes

\[
L_p(X_1, X_2, \ldots, X_n) = K(p) + \log \frac{1}{p(X_1, X_2, \ldots, X_n)}.
\]

This is the length of a two-stage description of the data, where we first describe the distribution \( p \) and then, given the distribution, construct the Shannon code and describe the data using \( \log \frac{1}{p(X_1, X_2, \ldots, X_n)} \) bits. This procedure is a special case of what is termed the minimum description length (MDL) principle: Given data and a choice of models, choose the model such that the description of the model plus the conditional description of the data is as short as possible.

---

### Summary

**Definition.** The Kolmogorov complexity \( K(x) \) of a string \( x \) is

\[
K(x) = \min_{p: U(p)=x} l(p),
\]

and

\[
K(x|l(x)) = \min_{p: U(p,l(x))=x} l(p).
\]

**Universality of Kolmogorov complexity.** There exists a universal computer \( U \) such that for any other computer \( A \),

\[
K_U(x) \leq K_A(x) + c_A
\]

for any string \( x \), where the constant \( c_A \) does not depend on \( x \). If \( U \) and \( A \) are universal, \( |K_U(x) - K_A(x)| \leq c \) for all \( x \).

**Upper bound on Kolmogorov complexity:**

\[
K(x|l(x)) \leq l(x) + c,
\]

and

\[
K(x) \leq K(x|l(x)) + 2 \log l(x) + c.
\]

---

### Kolmogorov Complexity

**Kolmogorov complexity and entropy.** If \( X_1, X_2, \ldots \) are i.i.d. integer-valued random variables with entropy \( H \), there exists a constant \( c \) such that for all \( n \),

\[
H \leq \frac{1}{n} E K(X_n|n) \leq H + \frac{|X| \log n}{n} + \frac{c}{n}.
\]

**Lower bound on Kolmogorov complexity.** There are no more than \( 2^k \) strings \( x \) with complexity \( K(x) < k \). If \( X_1, X_2, \ldots, X_n \) are drawn according to a Bernoulli \( (1/2) \) process,

\[
\Pr(K(X_1 X_2 \ldots X_n|n) \leq n - k) \leq 2^{-k}.
\]

**Definition.** A sequence \( x \) is said to be incompressible if

\[
\frac{K(x_1 x_2 \ldots x_n|n)}{n} \to 1.
\]

**Strong law of large numbers for incompressible sequences:**

\[
\frac{K(x_1, x_2, \ldots, x_n)}{n} \to 1 \implies \frac{1}{n} \sum_{i=1}^n x_i \to \frac{1}{2}.
\]

**Definition.** The universal probability of a string \( x \) is

\[
P_U(x) = \sum_{p: U(p)=x} 2^{-l(p)} = \Pr(U(p) = x).
\]

**Universality of \( P_U(x) \).** For every computer \( A \),

\[
P_U(x) \geq c_A P_A(x)
\]

for every string \( x \in \{0, 1\}^* \), where the constant \( c_A \) depends only on \( U \) and \( A \).

**Definition.** Let \( \Omega \) be the probability that the computer halts when the input \( p \) to the computer is a binary string drawn according to a Bernoulli \( (1/2) \) process. Properties of \( \Omega \):

1. \( \Omega \) is not computable.
2. \( \Omega \) is a "philosopher's stone."
3. \( \Omega \) is algorithmically random (incompressible).

---

### Problems

**14.1. Kolmogorov complexity of two sequences.** Let \( x, y \in \{0, 1\}^* \). Argue that \( K(x, y) \leq K(x) + K(y) + c \).

**14.2. Complexity of the sum.**

(a) Argue that \( K(n) \leq \log n + 2 \log \log n + c \).

(b) Argue that \( K(n_1 + n_2) \leq K(n_1) + K(n_2) + c \).

(c) Give an example in which \( n_1 \) and \( n_2 \) are complex but the sum is relatively simple.

**14.3. Images.** Consider an \( n \times n \) array \( x \) of 0's and 1's. Thus, \( x \) has \( n^2 \) bits. Find the Kolmogorov complexity \( K(x | n) \) (to first order) if:

(a) \( x \) is a horizontal line.

(b) \( x \) is a square.

(c) \( x \) is the union of two lines, each line being vertical or horizontal.

---

### Network Information Theory

A system with many senders and receivers contains many new elements in the communication problem: interference, cooperation, and feedback. These are the issues that are the domain of network information theory. The general problem is easy to state. Given many senders and receivers and a channel transition matrix that describes the effects of the interference and the noise in the network, decide whether or not the sources can be transmitted over the channel. This problem involves distributed source coding (data compression) as well as distributed communication (finding the capacity region of the network). This general problem has not yet been solved, so we consider various special cases in this chapter.

Examples of large communication networks include computer networks, satellite networks, and the phone system. Even within a single computer, there are various components that talk to each other. A complete theory of network information would have wide implications for the design of communication and computer networks.

Suppose that \( m \) stations wish to communicate with a common satellite over a common channel, as shown in Figure 15.1. This is known as a multiple-access channel. How do the various senders cooperate with each other to send information to the receiver? What rates of communication are achievable simultaneously? What limitations does interference among the senders put on the total rate of communication? This is the best understood multiuser channel, and the above questions have satisfying answers.

In contrast, we can reverse the network and consider one TV station sending information to \( m \) TV receivers, as in Figure 15.2. How does the sender encode information meant for different receivers in a common signal? What are the rates at which information can be sent to the different receivers? For this channel, the answers are known only in special cases.

There are other channels, such as the relay channel (where there is one source and one destination, but one or more intermediate sender–receiver pairs that act as relays to facilitate the communication between the source and the destination), the interference channel (two senders and two receivers with crosstalk), and the two-way channel (two sender–receiver pairs sending information to each other). For all these channels, we have only some of the answers to questions about achievable communication rates and the appropriate coding strategies.

All these channels can be considered special cases of a general communication network that consists of \( m \) nodes trying to communicate with each other, as shown in Figure 15.3. At each instant of time, the \( i \)-th node sends a symbol \( x_i \) that depends on the messages that it wants to send.

---

### Historical Notes

The original ideas of Kolmogorov complexity were put forth independently and almost simultaneously by Kolmogorov [321, 322], Solomonoff [504], and Chaitin [89]. These ideas were developed further by students of Kolmogorov such as Martin-Löf [374], who defined the notion of algorithmically random sequences and algorithmic tests for randomness, and by Levin and Zvonkin [353], who explored the ideas of universal probability and its relationship to complexity. A series of papers by Chaitin [90]–[92] developed the relationship between algorithmic complexity and mathematical proofs. C. P. Schnorr studied the universal notion of randomness and related it to gambling in [466]–[468].

The concept of the Kolmogorov structure function was defined by Kolmogorov at a talk at the Tallinn conference in 1973, but these results were not published. V’yugin pursues this in [549], where he shows that there are some very strange sequences \( x_n \) that reveal their structure arbitrarily slowly in the sense that \( K_k(x_n|n) = n - k \) for \( k < K(x_n|n) \). Zurek [606]–[608] addresses the fundamental questions of Maxwell’s demon and the second law of thermodynamics by establishing the physical consequences of Kolmogorov complexity.

Rissanen’s minimum description length (MDL) principle is very close in spirit to the Kolmogorov sufficient statistic. Rissanen [445, 446] finds a low-complexity model that yields a high likelihood of the data. Barron and Cover [32] argue that the density minimizing \( K(f) + \log \frac{1}{f}(X_i) \) yields consistent density estimation.

A nontechnical introduction to the various measures of complexity can be found in a thought-provoking book by Pagels [412]. Additional references to work in the area can be found in a paper by Cover et al. [114] on Kolmogorov’s contributions to information theory and algorithmic complexity. A comprehensive introduction to the field, including applications of the theory to analysis of algorithms and automata, may be found in the book by Li and Vitányi [354]. Additional coverage may be found in the books by Chaitin [86, 93].



NETWORK INFORMATION THEORY  
511  

The simultaneous transmission of the symbols (x₁, x₂, . . . , xₘ) results in random received symbols (y₁, y₂, . . . , yₘ) drawn according to the conditional probability distribution p(y₁, y₂, . . . , yₘ | x₁, x₂, . . . , xₘ). Here, p(·|·) expresses the effects of the noise and interference present in the network. If p(·|·) takes on only the values 0 and 1, the network is deterministic.  

Associated with some of the nodes in the network are stochastic data sources, which are to be communicated to some of the other nodes in the network. If the sources are independent, the messages sent by the nodes are also independent. However, for full generality, we must allow the sources to be dependent. How does one take advantage of the dependence to reduce the amount of information transmitted? Given the probability distribution of the sources and the channel transition function, can one transmit these sources over the channel and recover the sources at the destinations with the appropriate distortion?  

We consider various special cases of network communication. We consider the problem of source coding when the channels are noiseless and without interference. In such cases, the problem reduces to finding the set of rates associated with each source such that the required sources can be decoded at the destination with a low probability of error (or appropriate distortion). The simplest case for distributed source coding is the Slepian–Wolf source coding problem, where we have two sources that must be encoded separately but decoded together at a common node. We consider extensions to this theory when only one of the two sources needs to be recovered at the destination.  

The theory of flow in networks has satisfying answers in such domains as circuit theory and the flow of water in pipes. For example, for the single-source single-sink network of pipes shown in Figure 15.4, the maximum flow from A to B can be computed easily from the Ford–Fulkerson theorem. Assume that the edges have capacities Cᵢ as shown. Clearly, the maximum flow across any cut set cannot be greater than the sum of the capacities of the cut edges. Thus, minimizing the maximum flow across cut sets yields an upper bound on the capacity of the network. The Ford–Fulkerson theorem shows that this capacity can be achieved.  

The theory of information flow in networks does not have the same simple answers as the theory of flow of water in pipes. Although we prove an upper bound on the rate of information flow across any cut set, these bounds are not achievable in general. However, it is gratifying that some problems, such as the relay channel and the cascade channel, admit a simple max-flow min-cut interpretation. Another subtle problem in the search for a general theory is the absence of a source–channel separation theorem, which we touch on briefly in Section 15.10. A complete theory combining distributed source coding and network channel coding is still a distant goal.  

In the next section, we consider Gaussian examples of some of the basic channels of network information theory. The physically motivated Gaussian channel lends itself to concrete and easily interpreted answers. Later, we prove some of the basic results about joint typicality that we use to prove the theorems of multiuser information theory. We then consider various problems in detail: the multiple-access channel, the coding of correlated sources (Slepian–Wolf data compression), the broadcast channel, the relay channel, the coding of a random variable with side information, and the rate distortion problem with side information. We end with an introduction to the general theory of information flow in networks. There are a number of open problems in the area, and there does not yet exist a comprehensive theory of information networks. Even if such a theory is found, it may be too complex for easy implementation. But the theory will be able to tell communication designers how close they are to optimality and perhaps suggest some means of improving the communication rates.  

### 15.1 Gaussian Multiple-User Channels  

Gaussian multiple-user channels illustrate some of the important features of network information theory. The intuition gained in Chapter 9 on the Gaussian channel should make this section a useful introduction. Here, the key ideas for establishing the capacity regions of the Gaussian multiple-access, broadcast, relay, and two-way channels will be given without proof. The proofs of the coding theorems for the discrete memoryless counterparts to these theorems are given in later sections of the chapter.  

The basic discrete-time additive white Gaussian noise channel with input power P and noise variance N is modeled by  
Yᵢ = Xᵢ + Zᵢ, for i = 1, 2, . . . ,  
where the Zᵢ are i.i.d. Gaussian random variables with mean 0 and variance N. The signal X = (X₁, X₂, . . . , Xₙ) has a power constraint  
(1/n) ∑₁ⁿ Xᵢ² ≤ P.  

The Shannon capacity C is obtained by maximizing I(X; Y) over all random variables X such that E[X²] ≤ P and is given by  
C = (1/2) log(1 + P/N) bits per transmission.  

In this chapter, we restrict our attention to discrete-time memoryless channels; the results can be extended to continuous-time Gaussian channels.  

#### 15.1.1 Single-User Gaussian Channel  

We first review the single-user Gaussian channel studied in Chapter 9. Here, Y = X + Z. Choose a rate R < (1/2) log(1 + P/N). Fix a good (2ⁿʳ, n) codebook of power P. Choose an index w in the set {1, 2, . . . , 2ⁿʳ}. Send the wth codeword X(w) from the codebook generated above. The receiver observes Y = X(w) + Z and then finds the index ŵ of the codeword closest to Y. If n is sufficiently large, the probability of error Pr(w ≠ ŵ) will be arbitrarily small. As can be seen from the definition of joint typicality, this minimum-distance decoding scheme is essentially equivalent to finding the codeword in the codebook that is jointly typical with the received vector Y.  

#### 15.1.2 Gaussian Multiple-Access Channel with m Users  

We consider m transmitters, each with a power P. Let  
Y = ∑ᵢ=1ᵐ Xᵢ + Z.  

Let  
C(P/N) = (1/2) log(1 + P/N)  

denote the capacity of a single-user Gaussian channel with signal-to-noise ratio P/N. The achievable rate region for the Gaussian channel takes on the simple form given in the following equations:  
Rᵢ < C(P/N),  
Rᵢ + Rⱼ < C(2P/N),  
Rᵢ + Rⱼ + Rₖ < C(3P/N),  
...  
∑ᵢ=1ᵐ Rᵢ < C(mP/N).  

Note that when all the rates are the same, the last inequality dominates the others.  

Here, we need m codebooks, the ith codebook having 2ⁿʳᵢ codewords of power P. Transmission is simple. Each of the independent transmitters chooses an arbitrary codeword from its own codebook. The users send these vectors simultaneously. The receiver sees these codewords added together with the Gaussian noise Z.  

Optimal decoding consists of looking for the m codewords, one from each codebook, such that the vector sum is closest to Y in Euclidean distance. If (R₁, R₂, . . . , Rₘ) is in the capacity region given above, the probability of error goes to 0 as n tends to infinity.  

**Remarks**  
It is exciting to see in this problem that the sum of the rates of the users C(mP/N) goes to infinity with m. Thus, in a cocktail party with m celebrants of power P in the presence of ambient noise N, the intended listener receives an unbounded amount of information as the number of people grows to infinity. A similar conclusion holds, of course, for ground communications to a satellite. Apparently, the increasing interference as the number of senders m → ∞ does not limit the total received information.  

It is also interesting to note that the optimal transmission scheme here does not involve time-division multiplexing. In fact, each of the transmitters uses all of the bandwidth all of the time.  

#### 15.1.3 Gaussian Broadcast Channel  

Here, we assume that we have a sender of power P and two distant receivers, one with Gaussian noise power N₁ and the other with Gaussian noise power N₂. Without loss of generality, assume that N₁ < N₂. Thus, receiver Y₁ is less noisy than receiver Y₂. The model for the channel is  
Y₁ = X + Z₁,  
Y₂ = X + Z₂,  

where Z₁ and Z₂ are arbitrarily correlated Gaussian random variables with variances N₁ and N₂, respectively. The sender wishes to send independent messages at rates R₁ and R₂ to receivers Y₁ and Y₂, respectively.  

Fortunately, all scalar Gaussian broadcast channels belong to the class of degraded broadcast channels discussed in Section 15.6.2. Specializing that work, we find that the capacity region of the Gaussian broadcast channel is  
R₁ < C(αP/N₁),  
R₂ < C((1 − α)P / (αP + N₂)),  

where α may be arbitrarily chosen (0 ≤ α ≤ 1) to trade off rate R₁ for rate R₂ as the transmitter wishes.  

To encode the messages, the transmitter generates two codebooks, one with power αP at rate R₁, and another codebook with power αP at rate R₂, where R₁ and R₂ lie in the capacity region above. Then, to send an index w₁ ∈ {1, 2, . . . , 2ⁿʳ₁} and w₂ ∈ {1, 2, . . . , 2ⁿʳ₂} to Y₁ and Y₂, respectively, the transmitter takes the codeword X(w₁) from the first codebook and codeword X(w₂) from the second codebook and computes the sum. He sends the sum over the channel.  

The receivers must now decode the messages. First, consider the bad receiver Y₂. He merely looks through the second codebook to find the closest codeword to the received vector Y₂. His effective signal-to-noise ratio is αP / (αP + N₂), since Y₁’s message acts as noise to Y₂. (This can be proved.)  

The good receiver Y₁ first decodes Y₂’s codeword, which he can accomplish because of his lower noise N₁. He subtracts this codeword Ŷ₂ from Y₁. He then looks for the codeword in the first codebook closest to Y₁ − Ŷ₂. The resulting probability of error can be made as low as desired.  

A nice dividend of optimal encoding for degraded broadcast channels is that the better receiver Y₁ always knows the message intended for receiver Y₂ in addition to the message intended for himself.  

#### 15.1.4 Gaussian Relay Channel  

For the relay channel, we have a sender X and an ultimate intended receiver Y. Also present is the relay channel, intended solely to help the receiver. The Gaussian relay channel (Figure 15.31 in Section 15.7) is given by  
Y₁ = X + Z₁,  
Y = X + Z₁ + X₁ + Z₂,  

where Z₁ and Z₂ are independent zero-mean Gaussian random variables with variance N₁ and N₂, respectively. The encoding allowed by the relay is the causal sequence  
X₁ᵢ = fᵢ(Y₁₁, Y₁₂, . . . , Y₁ᵢ₋₁).  

Sender X has power P and sender X₁ has power P₁. The capacity is  
C = max₀ ≤ α ≤ 1 min{C(αP / (N₁ + N₂)), C((1 − α)P / (αP + N₂))}.  

Note that if  
P₁ / N₂ ≥ P / N₁,  

it can be seen that C = C(P / N₁), which is achieved by α = 1. The channel appears to be noise-free after the relay, and the capacity C(P / N₁) from X to the relay can be achieved. Thus, the rate C(P / (N₁ + N₂)) without the relay is increased by the presence of the relay to C(P / N₁). For large N₂ and for P₁ / N₂ ≥ P / N₁, we see that the increment in rate is from C(P / (N₁ + N₂)) ≈ 0 to C(P / N₁).  

Let R₁ < C(αP / N₁). Two codebooks are needed. The first codebook has 2ⁿʳ₁ words of power αP. The second has 2ⁿʳ₀ codewords of power αP. We shall use codewords from these codebooks successively to create the opportunity for cooperation by the relay. We start by sending a codeword from the first codebook. The relay now knows the index of this codeword since R₁ < C(αP / N₁), but the intended receiver has a list of possible codewords of size 2ⁿ(R₁ − C(αP / (N₁ + N₂))). This list calculation involves a result on list codes.  

In the next block, the transmitter and the relay wish to cooperate to resolve the receiver’s uncertainty about the codeword sent previously that is on the receiver’s list. Unfortunately, they cannot be sure what this list is because they do not know the received signal Y. Thus, they randomly partition the first codebook into 2ⁿʳ₀ cells with an equal number of codewords in each cell. The relay, the receiver, and the transmitter agree on this partition. The relay and the transmitter find the cell of the partition in which the codeword from the first codebook lies and cooperatively send the codeword from the second codebook with that index. That is, X and X₁ send the same designated codeword. The relay, of course, must scale this codeword so that it meets his power constraint P₁. They now transmit their codewords simultaneously. An important point to note here is that the cooperative information sent by the relay and the transmitter is sent coherently. So the power of the sum as seen by the receiver Y is (√(αP) + √P₁)².  

However, this does not exhaust what the transmitter does in the second block. He also chooses a fresh codeword from the first codebook, adds it “on paper” to the cooperative codeword from the second codebook, and sends the sum over the channel.  

The reception by the ultimate receiver Y in the second block involves first finding the cooperative index from the second codebook by looking for the closest codeword in the second codebook. He subtracts the codeword from the received sequence and then calculates a list of indices of size 2ⁿʳ₀ corresponding to all codewords of the first codebook that might have been sent in the second block.  

Now it is time for the intended receiver to complete computing the codeword from the first codebook sent in the first block. He takes his list of possible codewords that might have been sent in the first block and intersects it with the cell of the partition that he has learned from the cooperative relay transmission in the second block. The rates and powers have been chosen so that it is highly probable that there is only one codeword in the intersection.



# NETWORK INFORMATION THEORY

---

## 15.1.5 Gaussian Interference Channel

The interference channel involves two senders and two receivers. Sender 1 aims to send information to receiver 1, while sender 2 aims to send information to receiver 2. Each channel interferes with the other, as illustrated in Figure 15.5. This channel is not a broadcast channel because each sender has only one intended receiver, nor is it a multiple-access channel because each receiver is only interested in the message from the corresponding sender.

For symmetric interference, the channel is described by:
\[
Y_1 = X_1 + aX_2 + Z_1
\]
\[
Y_2 = X_2 + aX_1 + Z_2
\]
where \(Z_1, Z_2\) are independent Gaussian noise variables with mean 0 and variance \(N\). Despite the complexity of this channel, it has been shown that in the case of high interference, the capacity region is the same as if there were no interference.

To achieve this, each sender generates a codebook with power \(P\) and rate \(C(P/N)\). If the interference satisfies \(C(a^2P/(P + N)) > C(P/N)\), the first transmitter can perfectly determine the second transmitter's codeword by finding the closest match to the received signal. Once identified, the interference is subtracted, leaving a clean channel for the intended message.

---

## 15.1.6 Gaussian Two-Way Channel

The two-way channel is similar to the interference channel but includes feedback, where senders can use information from previous transmissions to cooperate. The capacity region for this channel is not known in general, but for Gaussian channels, it decomposes into two independent channels.

Let \(P_1\) and \(P_2\) be the powers of transmitters 1 and 2, and \(N_1\) and \(N_2\) be the noise variances. The rates \(R_1 < C(P_1/N_1)\) and \(R_2 < C(P_2/N_2)\) can be achieved. Each sender generates a codebook, and the receiver subtracts the known codeword of the other sender, resulting in a clean channel.

---

## 15.2 Jointly Typical Sequences

The joint typicality of sequences is extended from the joint AEP to analyze network information theory. For a set of random variables \(X_1, X_2, \ldots, X_k\) with a fixed joint distribution, the typical set \(A(n)_\epsilon\) is defined by:
\[
A(n)_\epsilon(X_1, X_2, \ldots, X_k) = \left\{(x_1, x_2, \ldots, x_k) : \frac{1}{n} \log p(s) - H(S) < \epsilon, \forall S \subseteq \{X_1, X_2, \ldots, X_k\}\right\}
\]

### Theorems and Properties

1. **Typical Set Properties**:
   - For any \(\epsilon > 0\), \(P(A(n)_\epsilon(S)) \geq 1 - \epsilon\) for sufficiently large \(n\).
   - Sequences in \(A(n)_\epsilon(S)\) satisfy \(p(s) \approx 2^{-n(H(S) \pm \epsilon)}\).
   - The size of the typical set \(|A(n)_\epsilon(S)| \approx 2^{n(H(S) \pm 2\epsilon)}\).

2. **Conditional Typicality**:
   - For a given typical sequence \(s_2\), the number of conditionally typical sequences \(s_1\) is bounded by \(|A(n)_\epsilon(S_1|s_2)| \leq 2^{n(H(S_1|S_2) + 2\epsilon)}\).
   - The probability of jointly typical sequences is bounded by \(P\{(S'_1, S'_2, S'_3) \in A(n)_\epsilon\} \approx 2^{-n(I(S_1;S_2|S_3) \pm 6\epsilon)}\).

---

## 15.3 Multiple-Access Channel

The multiple-access channel involves two or more senders transmitting to a common receiver, as shown in Figure 15.7. This channel is common in scenarios like satellite communication or cellular networks, where senders must contend with both noise and interference.

### Definition

A discrete memoryless multiple-access channel consists of three alphabets \(X_1, X_2, Y\) and a transition probability matrix \(p(y|x_1, x_2)\). The senders must coordinate their transmissions to minimize interference while maintaining reliable communication.



**Definition**  
A ((2nR1, 2nR2), n) code for the multiple-access channel consists of two sets of integers W1 = {1, 2, ..., 2nR1} and W2 = {1, 2, ..., 2nR2}, called the message sets, two encoding functions,  
X1 : W1 → X1^n (15.52)  
and  
X2 : W2 → X2^n, (15.53)  
and a decoding function,  
g : Y^n → W1 × W2. (15.54)  

There are two senders and one receiver for this channel. Sender 1 chooses an index W1 uniformly from the set {1, 2, ..., 2nR1} and sends the corresponding codeword over the channel. Sender 2 does likewise. Assuming that the distribution of messages over the product set W1 × W2 is uniform (i.e., the messages are independent and equally likely), we define the average probability of error for the ((2nR1, 2nR2), n) code as follows:  
P_e^(n) = 1 / (2n(R1+R2)) ∑_{(w1,w2)∈W1×W2} Pr[g(Y^n) ≠ (w1, w2) | (w1, w2) sent]. (15.55)  

**Definition**  
A rate pair (R1, R2) is said to be achievable for the multiple-access channel if there exists a sequence of ((2nR1, 2nR2), n) codes with P_e^(n) → 0.  

**Definition**  
The capacity region of the multiple-access channel is the closure of the set of achievable (R1, R2) rate pairs.  

An example of the capacity region for a multiple-access channel is illustrated in Figure 15.8. We first state the capacity region in the form of a theorem.  

**Theorem 15.3.1 (Multiple-access channel capacity)**  
The capacity of a multiple-access channel (X1 × X2, p(y|x1, x2), Y) is the closure of the convex hull of all (R1, R2) satisfying  
R1 < I(X1; Y|X2), (15.56)  
R2 < I(X2; Y|X1), (15.57)  
R1 + R2 < I(X1, X2; Y) (15.58)  
for some product distribution p1(x1)p2(x2) on X1 × X2.  

Before we prove that this is the capacity region of the multiple-access channel, let us consider a few examples of multiple-access channels:  

**Example 15.3.1 (Independent binary symmetric channels)**  
Assume that we have two independent binary symmetric channels, one from sender 1 and the other from sender 2, as shown in Figure 15.9. In this case, it is obvious from the results of Chapter 7 that we can send at rate 1 − H(p1) over the first channel and at rate 1 − H(p2) over the second channel.  

**Example 15.3.2 (Binary multiplier channel)**  
Consider a multiple-access channel with binary inputs and output Y = X1X2. (15.59) Such a channel is called a binary multiplier channel. It is easy to see that by setting X2 = 1, we can send at a rate of 1 bit per transmission from sender 1 to the receiver. Similarly, setting X1 = 1, we can achieve R2 = 1. Clearly, since the output is binary, the combined rates R1 + R2 of sender 1 and sender 2 cannot be more than 1 bit. By time-sharing, we can achieve any combination of rates such that R1 + R2 = 1. Hence, the capacity region is as shown in Figure 15.11.  

**Example 15.3.3 (Binary erasure multiple-access channel)**  
This multiple-access channel has binary inputs, X1 = X2 = {0, 1}, and a ternary output, Y = X1 + X2. There is no ambiguity in (X1, X2) if Y = 0 or Y = 2 is received; but Y = 1 can result from either (0,1) or (1,0).  

We now examine the achievable rates on the axes. Setting X2 = 0, we can send at a rate of 1 bit per transmission from sender 1. Similarly, setting X1 = 0, we can send at a rate R2 = 1. This gives us two extreme points of the capacity region. Can we do better? Let us assume that R1 = 1, so that the codewords of X1 must include all possible binary sequences; X1 would look like a Bernoulli(1/2) process. This acts like noise for the transmission from X2. For X2, the channel looks like the channel in Figure 15.12. This is the binary erasure channel of Chapter 7. Recalling the results, the capacity of this channel is 1/2 bit per transmission. Hence, when sending at maximum rate 1 for sender 1, we can send an additional 1/2 bit from sender 2. Later, after deriving the capacity region, we can verify that these rates are the best that can be achieved. The capacity region for a binary erasure multiple-access channel is illustrated in Figure 15.13.  

**15.3.1 Achievability of the Capacity Region for the Multiple-Access Channel**  
We now prove the achievability of the rate region in Theorem 15.3.1; the proof of the converse will be left until the next section. The proof of achievability is very similar to the proof for the single-user channel. We therefore only emphasize the points at which the proof differs from the single-user case. We begin by proving the achievability of rate pairs that satisfy (15.58) for some fixed product distribution p(x1)p(x2). In Section 15.3.3, we extend this to prove that all points in the convex hull of (15.58) are achievable.  

**Proof:**  
(Achievability in Theorem 15.3.1). Fix p(x1, x2) = p1(x1)p2(x2).  

*Codebook generation:* Generate 2nR1 independent codewords X1(i), i ∈ {1, 2, ..., 2nR1}, of length n, generating each element i.i.d. ∼ p1(x1i). Similarly, generate 2nR2 independent codewords X2(j), j ∈ {1, 2, ..., 2nR2}, generating each element i.i.d. ∼ p2(x2i). These codewords form the codebook, which is revealed to the senders and the receiver.  

*Encoding:* To send index i, sender 1 sends the codeword X1(i). Similarly, to send j, sender 2 sends X2(j).  

*Decoding:* Let A(n)_ε denote the set of typical (x1, x2, y) sequences. The receiver Y^n chooses the pair (i, j) such that (x1(i), x2(j), y) ∈ A(n)_ε (15.60) if such a pair (i, j) exists and is unique; otherwise, an error is declared.  

*Analysis of the probability of error:* By the symmetry of the random code construction, the conditional probability of error does not depend on which pair of indices is sent. Thus, the conditional probability of error is the same as the unconditional probability of error. So, without loss of generality, we can assume that (i, j) = (1, 1) was sent.  

We have an error if either the correct codewords are not typical with the received sequence or there is a pair of incorrect codewords that are typical with the received sequence. Define the events  
E_ij = {(X1(i), X2(j), Y) ∈ A(n)_ε}. (15.61)  

By the union of events bound,  
P_e^(n) = Pr[Ec_11 ∪ (i,j)≠(1,1) E_ij] (15.62)  
≤ Pr[Ec_11] + ∑_{i≠1, j=1} Pr[E_i1] + ∑_{i=1, j≠1} Pr[E_1j] + ∑_{i≠1, j≠1} Pr[E_ij]. (15.63)  

From the AEP, Pr[Ec_11] → 0. By Theorems 15.2.1 and 15.2.3, for i ≠ 1, we have  
Pr[E_i1] = Pr[(X1(i), X2(1), Y) ∈ A(n)_ε] (15.64)  
= ∑_{(x1,x2,y)∈A(n)_ε} p(x1)p(x2, y) (15.65)  
≤ |A(n)_ε| 2^{-n(H(X1) - ε)} 2^{-n(H(X2,Y) - ε)} (15.66)  
≤ 2^{-n(H(X1) + H(X2,Y) - H(X1,X2,Y) - 3ε)} (15.67)  
= 2^{-n(I(X1; X2,Y) - 3ε)} (15.68)  
= 2^{-n(I(X1; Y|X2) - 3ε)}, (15.69)  

where the equivalence of (15.68) and (15.69) follows from the independence of X1 and X2, and the consequent I(X1; X2, Y) = I(X1; Y|X2). Similarly, for j ≠ 1,  
Pr[E_1j] ≤ 2^{-n(I(X2; Y|X1) - 3ε)}, (15.70)  
and for i ≠ 1, j ≠ 1,  
Pr[E_ij] ≤ 2^{-n(I(X1,X2; Y) - 4ε)}. (15.71)  

It follows that  
P_e^(n) ≤ Pr[Ec_11] + 2nR1 2^{-n(I(X1; Y|X2) - 3ε)} + 2nR2 2^{-n(I(X2; Y|X1) - 3ε)} + 2n(R1+R2) 2^{-n(I(X1,X2; Y) - 4ε)}. (15.72)  

Since ε > 0 is arbitrary, the conditions of the theorem imply that each term tends to 0 as n → ∞. Thus, the probability of error, conditioned on a particular codeword being sent, goes to zero if the conditions of the theorem are met. The above bound shows that the average probability of error, which by symmetry is equal to the probability for an individual codeword, averaged over all choices of codebooks in the random code construction, is arbitrarily small. Hence, there exists at least one code C* with arbitrarily small probability of error.  

This completes the proof of achievability of the region in (15.58) for a fixed input distribution. Later, in Section 15.3.3, we show that time-sharing allows any (R1, R2) in the convex hull to be achieved, completing the proof of the forward part of the theorem.  

**15.3.2 Comments on the Capacity Region for the Multiple-Access Channel**  
We have now proved the achievability of the capacity region of the multiple-access channel, which is the closure of the convex hull of the set of points (R1, R2) satisfying  
R1 < I(X1; Y|X2), (15.73)  
R2 < I(X2; Y|X1), (15.74)  
R1 + R2 < I(X1, X2; Y) (15.75)  

for some distribution p1(x1)p2(x2) on X1 × X2. For a particular p1(x1)p2(x2), the region is illustrated in Figure 15.14.  

Let us now interpret the corner points in the region. Point A corresponds to the maximum rate achievable from sender 1 to the receiver when sender 2 is not sending any information. This is  
max R1 = max_{p1(x1)p2(x2)} I(X1; Y|X2). (15.76)  

Now for any distribution p1(x1)p2(x2),  
I(X1; Y|X2) = ∑_{x2} p2(x2) I(X1; Y|X2 = x2) (15.77)  
≤ max_{x2} I(X1; Y|X2 = x2), (15.78)  

since the average is less than the maximum. Therefore, the maximum in (15.76) is attained when we set X2 = x2, where x2 is the value that maximizes the conditional mutual information between X1 and Y. The distribution of X1 is chosen to maximize this mutual information. Thus, X2 must facilitate the transmission of X1 by setting X2 = x2.  

The point B corresponds to the maximum rate at which sender 2 can send as long as sender 1 sends at his maximum rate. This is the rate that is obtained if X1 is considered as noise for the channel from X2 to Y. In this case, using the results from single-user channels, X2 can send at a rate I(X2; Y). The receiver now knows which X2 codeword was used and can "subtract" its effect from the channel. We can consider the channel now to be an indexed set of single-user channels, where the index is the X2 symbol used. The X1 rate achieved in this case is the average mutual information, where the average is over these channels, and each channel occurs as many times as the corresponding X2 symbol appears in the codewords. Hence, the rate achieved is  
∑_{x2} p(x2) I(X1; Y|X2 = x2) = I(X1; Y|X2). (15.79)  

Points C and D correspond to B and A, respectively, with the roles of the senders reversed. The non-corner points can be achieved by time-sharing. Thus, we have given a single-user interpretation and justification for the capacity region of a multiple-access channel.  

The idea of considering other signals as part of the noise, decoding one signal, and then "subtracting" it from the received signal is a very useful one. We will come across the same concept again in the capacity calculations for the degraded broadcast channel.



--- Page 60 ---
534
NETWORK INFORMATION THEORY
15.3.3
Convexity of the Capacity Region of the Multiple-Access Channel

We now recast the capacity region of the multiple-access channel to take into account the operation of taking the convex hull by introducing a new random variable. We begin by proving that the capacity region is convex.

**Theorem 15.3.2**  
The capacity region \( \mathcal{C} \) of a multiple-access channel is convex. That is, if \( (R_1, R_2) \in \mathcal{C} \) and \( (R'_1, R'_2) \in \mathcal{C} \), then \( (\lambda R_1 + (1 - \lambda)R'_1, \lambda R_2 + (1 - \lambda)R'_2) \in \mathcal{C} \) for \( 0 \leq \lambda \leq 1 \).

**Proof:**  
The idea is time-sharing. Given two sequences of codes at different rates \( R = (R_1, R_2) \) and \( R' = (R'_1, R'_2) \), we can construct a third codebook at a rate \( \lambda R + (1 - \lambda)R' \) by using the first codebook for the first \( \lambda n \) symbols and the second codebook for the last \( (1 - \lambda)n \) symbols. The number of \( X_1 \) codewords in the new code is  
\[
2^{n\lambda R_1} \cdot 2^{n(1-\lambda)R'_1} = 2^{n(\lambda R_1 + (1 - \lambda)R'_1)},
\]  
and hence the rate of the new code is \( \lambda R + (1 - \lambda)R' \). Since the overall probability of error is less than the sum of the probabilities of error for each of the segments, the probability of error of the new code goes to 0, and the rate is achievable.  
□

We can now recast the statement of the capacity region for the multiple-access channel using a time-sharing random variable \( Q \). Before we prove this result, we need to prove a property of convex sets defined by linear inequalities like those of the capacity region of the multiple-access channel. In particular, we would like to show that the convex hull of two such regions defined by linear constraints is the region defined by the convex combination of the constraints. Initially, the equality of these two sets seems obvious, but on closer examination, there is a subtle difficulty due to the fact that some of the constraints might not be active. This is best illustrated by an example. Consider the following two sets defined by linear inequalities:  
\[
\mathcal{C}_1 = \{(x, y) : x \geq 0, y \geq 0, x \leq 10, y \leq 10, x + y \leq 100\}, 
\]  
\[
\mathcal{C}_2 = \{(x, y) : x \geq 0, y \geq 0, x \leq 20, y \leq 20, x + y \leq 20\}.
\]  
In this case, the \( \frac{1}{2} \) convex combination of the constraints defines the region  
\[
\mathcal{C} = \{(x, y) : x \geq 0, y \geq 0, x \leq 15, y \leq 15, x + y \leq 60\}.
\]  

--- Page 61 ---
15.3  
MULTIPLE-ACCESS CHANNEL  
535  

It is not difficult to see that any point in \( \mathcal{C}_1 \) or \( \mathcal{C}_2 \) has \( x + y < 20 \), so any point in the convex hull of the union of \( \mathcal{C}_1 \) and \( \mathcal{C}_2 \) satisfies this property. Thus, the point \( (15, 15) \), which is in \( \mathcal{C} \), is not in the convex hull of \( \mathcal{C}_1 \cup \mathcal{C}_2 \). This example also hints at the cause of the problem—in the definition for \( \mathcal{C}_1 \), the constraint \( x + y \leq 100 \) is not active. If this constraint were replaced by a constraint \( x + y \leq a \), where \( a \leq 20 \), the above result of the equality of the two regions would be true, as we now prove.

We restrict ourselves to the pentagonal regions that occur as components of the capacity region of a two-user multiple-access channel. In this case, the capacity region for a fixed \( p(x_1)p(x_2) \) is defined by three mutual informations: \( I(X_1; Y|X_2) \), \( I(X_2; Y|X_1) \), and \( I(X_1, X_2; Y) \), which we shall call \( I_1 \), \( I_2 \), and \( I_3 \), respectively. For each \( p(x_1)p(x_2) \), there is a corresponding vector \( I = (I_1, I_2, I_3) \), and a rate region defined by  
\[
\mathcal{C}_I = \{(R_1, R_2) : R_1 \geq 0, R_2 \geq 0, R_1 \leq I_1, R_2 \leq I_2, R_1 + R_2 \leq I_3\}.
\]  

Also, since for any distribution \( p(x_1)p(x_2) \), we have  
\[
I(X_2; Y|X_1) = H(X_2|X_1) - H(X_2|Y, X_1) = H(X_2) - H(X_2|Y, X_1) = I(X_2; Y, X_1) = I(X_2; Y) + I(X_2; X_1|Y) \geq I(X_2; Y),
\]  
and therefore,  
\[
I(X_1; Y|X_2) + I(X_2; Y|X_1) \geq I(X_1; Y|X_2) + I(X_2; Y) = I(X_1, X_2; Y),
\]  
we have for all vectors \( I \) that \( I_1 + I_2 \geq I_3 \). This property will turn out to be critical for the theorem.

**Lemma 15.3.1**  
Let \( I_1, I_2 \in \mathbb{R}^3 \) be two vectors of mutual informations that define rate regions \( \mathcal{C}_{I_1} \) and \( \mathcal{C}_{I_2} \), respectively, as given in (15.84). For \( 0 \leq \lambda \leq 1 \), define \( I_\lambda = \lambda I_1 + (1 - \lambda)I_2 \), and let \( \mathcal{C}_{I_\lambda} \) be the rate region defined by \( I_\lambda \). Then  
\[
\mathcal{C}_{I_\lambda} = \lambda \mathcal{C}_{I_1} + (1 - \lambda)\mathcal{C}_{I_2}.
\]  

**Proof:**  
We shall prove this theorem in two parts. We first show that any point in the \( (\lambda, 1 - \lambda) \) mix of the sets \( \mathcal{C}_{I_1} \) and \( \mathcal{C}_{I_2} \) satisfies the constraints \( I_\lambda \). But this is straightforward, since any point in \( \mathcal{C}_{I_1} \) satisfies the inequalities for \( I_1 \) and a point in \( \mathcal{C}_{I_2} \) satisfies the inequalities for \( I_2 \), so the \( (\lambda, 1 - \lambda) \) mix of these points will satisfy the \( (\lambda, 1 - \lambda) \) mix of the constraints. Thus, it follows that  
\[
\lambda \mathcal{C}_{I_1} + (1 - \lambda)\mathcal{C}_{I_2} \subseteq \mathcal{C}_{I_\lambda}.
\]  

To prove the reverse inclusion, we consider the extreme points of the pentagonal regions. It is not difficult to see that the rate regions defined in (15.84) are always in the form of a pentagon, or in the extreme case when \( I_3 = I_1 + I_2 \), in the form of a rectangle. Thus, the capacity region \( \mathcal{C}_I \) can also be defined as a convex hull of five points:  
\[
(0, 0), (I_1, 0), (I_1, I_3 - I_1), (I_3 - I_2, I_2), (0, I_2).
\]  

Consider the region defined by \( I_\lambda \); it, too, is defined by five points. Take any one of the points, say \( (I_\lambda^{(3)} - I_\lambda^{(2)}, I_\lambda^{(2)}) \). This point can be written as the \( (\lambda, 1 - \lambda) \) mix of the points \( (I_1^{(3)} - I_1^{(2)}, I_1^{(2)}) \) and \( (I_2^{(3)} - I_2^{(2)}, I_2^{(2)}) \), and therefore lies in the convex mixture of \( \mathcal{C}_{I_1} \) and \( \mathcal{C}_{I_2} \). Thus, all extreme points of the pentagon \( \mathcal{C}_{I_\lambda} \) lie in the convex hull of \( \mathcal{C}_{I_1} \) and \( \mathcal{C}_{I_2} \), or  
\[
\mathcal{C}_{I_\lambda} \subseteq \lambda \mathcal{C}_{I_1} + (1 - \lambda)\mathcal{C}_{I_2}.
\]  

Combining the two parts, we have the theorem.  
□  

In the proof of the theorem, we have implicitly used the fact that all the rate regions are defined by five extreme points (at worst, some of the points are equal). All five points defined by the \( I \) vector were within the rate region. If the condition \( I_3 \leq I_1 + I_2 \) is not satisfied, some of the points in (15.87) may be outside the rate region and the proof collapses.

As an immediate consequence of the above lemma, we have the following theorem:

**Theorem 15.3.3**  
The convex hull of the union of the rate regions defined by individual \( I \) vectors is equal to the rate region defined by the convex hull of the \( I \) vectors.

These arguments on the equivalence of the convex hull operation on the rate regions with the convex combinations of the mutual informations can be extended to the general \( m \)-user multiple-access channel. A proof along these lines using the theory of polymatroids is developed in Han [271].

**Theorem 15.3.4**  
The set of achievable rates of a discrete memoryless multiple-access channel is given by the closure of the set of all \( (R_1, R_2) \) pairs satisfying  
\[
R_1 < I(X_1; Y|X_2, Q), \quad R_2 < I(X_2; Y|X_1, Q), \quad R_1 + R_2 < I(X_1, X_2; Y|Q)
\]  
for some choice of the joint distribution \( p(q)p(x_1|q)p(x_2|q)p(y|x_1, x_2) \) with \( |Q| \leq 4 \).

**Proof:**  
We will show that every rate pair lying in the region defined in (15.89) is achievable (i.e., it lies in the convex closure of the rate pairs satisfying Theorem 15.3.1). We also show that every point in the convex closure of the region in Theorem 15.3.1 is also in the region defined in (15.89).

Consider a rate point \( R \) satisfying the inequalities (15.89) of the theorem. We can rewrite the right-hand side of the first inequality as  
\[
I(X_1; Y|X_2, Q) = \sum_{q=1}^m p(q)I(X_1; Y|X_2, Q = q),
\]  
where \( m \) is the cardinality of the support set of \( Q \). We can expand the other mutual informations similarly.

For simplicity in notation, we consider a rate pair as a vector and denote a pair satisfying the inequalities in (15.58) for a specific input product distribution \( p_1(x_1)p_2(x_2) \) as \( R_{p_1,p_2} \). Specifically, let \( R_q = (R_{1q}, R_{2q}) \) be a rate pair satisfying  
\[
R_{1q} < I(X_1; Y|X_2)p_1(x_1)p_2(x_2), \quad R_{2q} < I(X_2; Y|X_1)p_1(x_1)p_2(x_2), \quad R_{1q} + R_{2q} < I(X_1, X_2; Y)p_1(x_1)p_2(x_2).
\]  
Then by Theorem 15.3.1, \( R_q = (R_{1q}, R_{2q}) \) is achievable. Then since \( R \) satisfies (15.89) and we can expand the right-hand sides as in (15.91), there exists a set of pairs \( R_q \) satisfying (15.94) such that  
\[
R = \sum_{q=1}^m p(q)R_q.
\]  
Since a convex combination of achievable rates is achievable, so is \( R \). Hence, we have proven the achievability of the region in the theorem.

The same argument can be used to show that every point in the convex closure of the region in (15.58) can be written as the mixture of points satisfying (15.94) and hence can be written in the form (15.89).

--- Page 64 ---
538  
NETWORK INFORMATION THEORY  

The converse is proved in the next section. The converse shows that all achievable rate pairs are of the form (15.89), and hence establishes that this is the capacity region of the multiple-access channel. The cardinality bound on the time-sharing random variable \( Q \) is a consequence of Carathéodory’s theorem on convex sets. See the discussion below.  
□  

The proof of the convexity of the capacity region shows that any convex combination of achievable rate pairs is also achievable. We can continue this process, taking convex combinations of more points. Do we need to use an arbitrary number of points? Will the capacity region be increased? The following theorem says no.

**Theorem 15.3.5 (Carathéodory)**  
Any point in the convex closure of a compact set \( A \) in a \( d \)-dimensional Euclidean space can be represented as a convex combination of \( d + 1 \) or fewer points in the original set \( A \).

**Proof:**  
The proof may be found in Eggleston [183] and Grünbaum [263].  
□  

This theorem allows us to restrict attention to a certain finite convex combination when calculating the capacity region. This is an important property because without it, we would not be able to compute the capacity region in (15.89), since we would never know whether using a larger alphabet \( Q \) would increase the region.

In the multiple-access channel, the bounds define a connected compact set in three dimensions. Therefore, all points in its closure can be defined as the convex combination of at most four points. Hence, we can restrict the cardinality of \( Q \) to at most 4 in the above definition of the capacity region.

**Remark**  
Many of the cardinality bounds may be slightly improved by introducing other considerations. For example, if we are only interested in the boundary of the convex hull of \( A \) as we are in capacity theorems, a point on the boundary can be expressed as a mixture of \( d \) points of \( A \), since a point on the boundary lies in the intersection of \( A \) with a \( (d - 1) \)-dimensional support hyperplane.

15.3.4  
Converse for the Multiple-Access Channel  

We have so far proved the achievability of the capacity region. In this section, we prove the converse.

**Proof (Converse to Theorems 15.3.1 and 15.3.4):**  
We must show that given any sequence of \( (2^nR_1, 2^nR_2) \) codes with \( P_e^{(n)} \to 0 \), the rates must satisfy  
\[
R_1 \leq I(X_1; Y|X_2, Q), \quad R_2 \leq I(X_2; Y|X_1, Q), \quad R_1 + R_2 \leq I(X_1, X_2; Y|Q)
\]  
for some choice of random variable \( Q \) defined on \( \{1, 2, 3, 4\} \) and joint distribution \( p(q)p(x_1|q)p(x_2|q)p(y|x_1, x_2) \). Fix \( n \). Consider the given code of block length \( n \). The joint distribution on \( W_1 \times W_2 \times X_1^n \times X_2^n \times Y^n \) is well-defined. The only randomness is due to the random uniform choice of indices \( W_1 \) and \( W_2 \) and the randomness induced by the channel. The joint distribution is  
\[
p(w_1, w_2, x_1^n, x_2^n, y^n) = \frac{1}{2^nR_1} \frac{1}{2^nR_2} p(x_1^n|w_1)p(x_2^n|w_2) \prod_{i=1}^n p(y_i|x_{1i}, x_{2i}),
\]  
where \( p(x_1^n|w_1) \) is either 1 or 0, depending on whether \( x_1^n = x_1(w_1) \), the codeword corresponding to \( w_1 \), or not, and similarly, \( p(x_2^n|w_2) = 1 \) or 0, according to whether \( x_2^n = x_2(w_2) \) or not. The mutual informations that follow are calculated with respect to this distribution.

By the code construction, it is possible to estimate \( (W_1, W_2) \) from the received sequence \( Y^n \) with a low probability of error. Hence, the conditional entropy of \( (W_1, W_2) \) given \( Y^n \) must be small. By Fano’s inequality,  
\[
H(W_1, W_2|Y^n) \leq n(R_1 + R_2)P_e^{(n)} + H(P_e^{(n)}) \triangleq n\varepsilon_n,
\]  
where \( \varepsilon_n \to 0 \) as \( P_e^{(n)} \to 0 \). Then we have  
\[
H(W_1|Y^n) \leq H(W_1, W_2|Y^n) \leq n\varepsilon_n,
\]  
\[
H(W_2|Y^n) \leq H(W_1, W_2|Y^n) \leq n\varepsilon_n.
\]  

We can now bound the rate \( R_1 \) as  
\[
nR_1 = H(W_1) = I(W_1; Y^n) + H(W_1|Y^n) \leq I(W_1; Y^n) + n\varepsilon_n.
\]  

--- Page 65 ---  
15.3  
MULTIPLE-ACCESS CHANNEL  
539  

Perform the following actions:  
1. Remove unnecessary formatting, extra spaces, or line breaks.  
2. Fix broken sentences or paragraphs.  
3. Ensure consistent punctuation and capitalization.  
4. Remove any non-standard characters or encoding artifacts.  
5. Preserve the original meaning and structure of the text.  
6. Make the text more readable and well-formatted.  

Return only the cleaned text without additional explanations.



NETWORK INFORMATION THEORY

(b)  
I(X₁ⁿ(W₁); Yⁿ) + nεₙ  
(15.104)  
= H(X₁ⁿ(W₁)) − H(X₁ⁿ(W₁)|Yⁿ) + nεₙ  
(15.105)  

(c)  
≤ H(X₁ⁿ(W₁)|X₂ⁿ(W₂)) − H(X₁ⁿ(W₁)|Yⁿ, X₂ⁿ(W₂)) + nεₙ  
(15.106)  
= I(X₁ⁿ(W₁); Yⁿ|X₂ⁿ(W₂)) + nεₙ  
(15.107)  
= H(Yⁿ|X₂ⁿ(W₂)) − H(Yⁿ|X₁ⁿ(W₁), X₂ⁿ(W₂)) + nεₙ  
(15.108)  

(d)  
= H(Yⁿ|X₂ⁿ(W₂)) − ∑₁ⁿ H(Yᵢ|Yᵢ₋₁, X₁ⁿ(W₁), X₂ⁿ(W₂)) + nεₙ  
(15.109)  

(e)  
= H(Yⁿ|X₂ⁿ(W₂)) − ∑₁ⁿ H(Yᵢ|X₁ᵢ, X₂ᵢ) + nεₙ  
(15.110)  

(f)  
≤ ∑₁ⁿ H(Yᵢ|X₂ⁿ(W₂)) − ∑₁ⁿ H(Yᵢ|X₁ᵢ, X₂ᵢ) + nεₙ  
(15.111)  

(g)  
≤ ∑₁ⁿ H(Yᵢ|X₂ᵢ) − ∑₁ⁿ H(Yᵢ|X₁ᵢ, X₂ᵢ) + nεₙ  
(15.112)  
= ∑₁ⁿ I(X₁ᵢ; Yᵢ|X₂ᵢ) + nεₙ,  
(15.113)  

where  
(a) follows from Fano’s inequality  
(b) follows from the data-processing inequality  
(c) follows from the fact that since W₁ and W₂ are independent, so are X₁ⁿ(W₁) and X₂ⁿ(W₂), and hence H(X₁ⁿ(W₁)|X₂ⁿ(W₂)) = H(X₁ⁿ(W₁)), and H(X₁ⁿ(W₁)|Yⁿ, X₂ⁿ(W₂)) ≤ H(X₁ⁿ(W₁)|Yⁿ) by conditioning  
(d) follows from the chain rule  
(e) follows from the fact that Yᵢ depends only on X₁ᵢ and X₂ᵢ by the memoryless property of the channel  
(f) follows from the chain rule and removing conditioning  
(g) follows from removing conditioning  

Hence, we have  
R₁ ≤ (1/n) ∑₁ⁿ I(X₁ᵢ; Yᵢ|X₂ᵢ) + εₙ.  
(15.114)  

Similarly, we have  
R₂ ≤ (1/n) ∑₁ⁿ I(X₂ᵢ; Yᵢ|X₁ᵢ) + εₙ.  
(15.115)  

To bound the sum of the rates, we have  
n(R₁ + R₂) = H(W₁, W₂)  
(15.116)  
= I(W₁, W₂; Yⁿ) + H(W₁, W₂|Yⁿ)  
(15.117)  
(a) ≤ I(W₁, W₂; Yⁿ) + nεₙ  
(15.118)  
(b) ≤ I(X₁ⁿ(W₁), X₂ⁿ(W₂); Yⁿ) + nεₙ  
(15.119)  
= H(Yⁿ) − H(Yⁿ|X₁ⁿ(W₁), X₂ⁿ(W₂)) + nεₙ  
(15.120)  

(c) = H(Yⁿ) − ∑₁ⁿ H(Yᵢ|Yᵢ₋₁, X₁ⁿ(W₁), X₂ⁿ(W₂)) + nεₙ  
(15.121)  

(d) = H(Yⁿ) − ∑₁ⁿ H(Yᵢ|X₁ᵢ, X₂ᵢ) + nεₙ  
(15.122)  

(e) ≤ ∑₁ⁿ H(Yᵢ) − ∑₁ⁿ H(Yᵢ|X₁ᵢ, X₂ᵢ) + nεₙ  
(15.123)  
= ∑₁ⁿ I(X₁ᵢ, X₂ᵢ; Yᵢ) + nεₙ,  
(15.124)  

where  
(a) follows from Fano’s inequality  
(b) follows from the data-processing inequality  
(c) follows from the chain rule  
(d) follows from the fact that Yᵢ depends only on X₁ᵢ and X₂ᵢ and is conditionally independent of everything else  
(e) follows from the chain rule and removing conditioning  

Hence, we have  
R₁ + R₂ ≤ (1/n) ∑₁ⁿ I(X₁ᵢ, X₂ᵢ; Yᵢ) + εₙ.  
(15.125)  

The expressions in (15.114), (15.115), and (15.125) are the averages of the mutual informations calculated at the empirical distributions in column i of the codebook. We can rewrite these equations with the new variable Q, where Q = i ∈ {1, 2, ..., n} with probability 1/n. The equations become  

R₁ ≤ (1/n) ∑₁ⁿ I(X₁ᵢ; Yᵢ|X₂ᵢ) + εₙ  
(15.126)  
= (1/n) ∑₁ⁿ I(X₁q; Yq|X₂q, Q = i) + εₙ  
(15.127)  
= I(X₁Q; YQ|X₂Q, Q) + εₙ  
(15.128)  
= I(X₁; Y|X₂, Q) + εₙ,  
(15.129)  

where X₁ △= X₁Q, X₂ △= X₂Q, and Y △= YQ are new random variables whose distributions depend on Q in the same way as the distributions of X₁ᵢ, X₂ᵢ, and Yᵢ depend on i. Since W₁ and W₂ are independent, so are X₁ᵢ(W₁) and X₂ᵢ(W₂), and hence  

Pr(X₁ᵢ(W₁) = x₁, X₂ᵢ(W₂) = x₂) △= Pr{X₁Q = x₁|Q = i} Pr{X₂Q = x₂|Q = i}.  
(15.130)  

Hence, taking the limit as n → ∞, P(n)ₑ → 0, we have the following converse:  

R₁ ≤ I(X₁; Y|X₂, Q),  
R₂ ≤ I(X₂; Y|X₁, Q),  
R₁ + R₂ ≤ I(X₁, X₂; Y|Q)  
(15.131)  

for some choice of joint distribution p(q)p(x₁|q)p(x₂|q)p(y|x₁, x₂). As in Section 15.3.3, the region is unchanged if we limit the cardinality of Q to 4.  

This completes the proof of the converse.  

Thus, the achievability of the region of Theorem 15.3.1 was proved in Section 15.3.1. In Section 15.3.3, we showed that every point in the region defined by (15.96) was also achievable. In the converse, we showed that the region in (15.96) was the best we can do, establishing that this is indeed the capacity region of the channel. Thus, the region in (15.58)  

cannot be any larger than the region in (15.96), and this is the capacity region of the multiple-access channel.  

15.3.5 m-User Multiple-Access Channels  

We will now generalize the result derived for two senders to m senders, m ≥ 2. The multiple-access channel in this case is shown in Figure 15.15. We send independent indices w₁, w₂, ..., wₘ over the channel from the senders 1, 2, ..., m, respectively. The codes, rates, and achievability are all defined in exactly the same way as in the two-sender case.  

Let S ⊆ {1, 2, ..., m}. Let Sᶜ denote the complement of S. Let R(S) = ∑_{i∈S} R_i, and let X(S) = {X_i : i ∈ S}. Then we have the following theorem.  

Theorem 15.3.6  
The capacity region of the m-user multiple-access channel is the closure of the convex hull of the rate vectors satisfying  

R(S) ≤ I(X(S); Y|X(Sᶜ))  
(15.132)  

for all S ⊆ {1, 2, ..., m}  

for some product distribution p₁(x₁)p₂(x₂)⋯pₘ(xₘ).  

Proof:  
The proof contains no new ideas. There are now 2m − 1 terms in the probability of error in the achievability proof and an equal number of inequalities in the proof of the converse. Details are left to the reader.  

In general, the region in (15.132) is a beveled box.  

p(y|x₁,x₂,...,xₘ)  
X₁  
X₂  
Xₘ  
...  
Y  

FIGURE 15.15. m-user multiple-access channel.  

15.3.6 Gaussian Multiple-Access Channels  

We now discuss the Gaussian multiple-access channel of Section 15.1.2 in somewhat more detail.  

Two senders, X₁ and X₂, communicate to the single receiver, Y. The received signal at time i is  

Yᵢ = X₁ᵢ + X₂ᵢ + Zᵢ,  
(15.133)  

where {Zᵢ} is a sequence of independent, identically distributed, zero-mean Gaussian random variables with variance N (Figure 15.16). We assume that there is a power constraint Pⱼ on sender j; that is, for each sender, for all messages, we must have  

(1/n) ∑₁ⁿ x₂ⱼ(wⱼ) ≤ Pⱼ,  
wⱼ ∈ {1, 2, ..., 2ⁿʳⱼ}, j = 1, 2.  
(15.134)  

Just as the proof of achievability of channel capacity for the discrete case (Chapter 7) was extended to the Gaussian channel (Chapter 9), we can extend the proof for the discrete multiple-access channel to the Gaussian multiple-access channel. The converse can also be extended similarly, so we expect the capacity region to be the convex hull of the set of rate pairs satisfying  

R₁ ≤ I(X₁; Y|X₂),  
(15.135)  
R₂ ≤ I(X₂; Y|X₁),  
(15.136)  
R₁ + R₂ ≤ I(X₁, X₂; Y)  
(15.137)  

for some input distribution f₁(x₁)f₂(x₂) satisfying E[X₁²] ≤ P₁ and E[X₂²] ≤ P₂.  

Zⁿ  
Xⁿ₁  
W₁  
W₂  
P₁  
Xⁿ₂  
P₂  
Yⁿ  
(W₁, W₂)  
^  
^  

FIGURE 15.16. Gaussian multiple-access channel.  

Now, we can expand the mutual information in terms of relative entropy, and thus  

I(X₁; Y|X₂) = h(Y|X₂) − h(Y|X₁, X₂)  
(15.138)  
= h(X₁ + X₂ + Z|X₂) − h(X₁ + X₂ + Z|X₁, X₂)  
(15.139)  
= h(X₁ + Z|X₂) − h(Z|X₁, X₂)  
(15.140)  
= h(X₁ + Z|X₂) − h(Z)  
(15.141)  
= h(X₁ + Z) − h(Z)  
(15.142)  
= h(X₁ + Z) − (1/2) log(2πe)N  
(15.143)  
≤ (1/2) log(2πe)(P₁ + N) − (1/2) log(2πe)N  
(15.144)  
= (1/2) log(1 + P₁/N),  
(15.145)  

where (15.141) follows from the fact that Z is independent of X₁ and X₂, (15.142) from the independence of X₁ and X₂, and (15.144) from the fact that the normal maximizes entropy for a given second moment. Thus, the maximizing distribution is X₁ ∼ N(0, P₁) and X₂ ∼ N(0, P₂) with X₁ and X₂ independent. This distribution simultaneously maximizes the mutual information bounds in (15.135)–(15.137).  

Definition  
We define the channel capacity function  

C(x) △= (1/2) log(1 + x),  
(15.146)  

corresponding to the channel capacity of a Gaussian white-noise channel with signal-to-noise ratio x (Figure 15.17). Then we write the bound on R₁ as  

R₁ ≤ C(P₁/N).  
(15.147)  

Similarly,  
R₂ ≤ C(P₂/N).  
(15.148)  

R₂  
R₁  
D  
P₂/N  
C  
B  
A  
0  
P₁/N  
C  
P₂  
P₁ + N  
C  
P₁  
P₂ + N  
C  

FIGURE 15.17. Gaussian multiple-access channel capacity.  

and  
R₁ + R₂ ≤ C((P₁ + P₂)/N).  
(15.149)  

These upper bounds are achieved when X₁ ∼ N(0, P₁) and X₂ = N(0, P₂) and define the capacity region. The surprising fact about these inequalities is that the sum of the rates can be as large as C((P₁ + P₂)/N), which is the rate achieved by a single transmitter sending with a power equal to the sum of the powers.  

The interpretation of the corner points is very similar to the interpretation of the achievable rate pairs for a discrete multiple-access channel for a fixed input distribution. In the case of the Gaussian channel, we can consider decoding as a two-stage process: In the first stage, the receiver decodes the second sender, considering the first sender as part of the noise. This decoding will have low probability of error if R₂ < C(P₂/(P₁ + N)). After the second sender has been decoded successfully, it can be subtracted out, and the first sender can be decoded correctly if R₁ < C(P₁/N). Hence, this argument shows that we can achieve the rate pairs at the corner points of the capacity region by means of single-user operations. This process, called onion-peeling, can be extended to any number of users.  

If we generalize this to m senders with equal power, the total rate is C(mP/N), which goes to ∞ as m → ∞. The average rate per sender, (1/m)C(mP/N), goes to 0. Thus, when the total number of senders is very large,



--- Page 73 ---
15.3
MULTIPLE-ACCESS CHANNEL
547
So, even if there is a lot of interference, we can still send a total amount of information that is arbitrarily large, even though the rate per individual sender goes to 0.

The capacity region described above corresponds to code-division multiple access (CDMA), where separate codes are used for the different senders, and the receiver decodes them one by one. In many practical situations, though, simpler schemes, such as frequency-division multiplexing or time-division multiplexing, are used. With frequency-division multiplexing, the rates depend on the bandwidth allotted to each sender. Consider the case of two senders with powers \( P_1 \) and \( P_2 \) using nonintersecting frequency bands with bandwidths \( W_1 \) and \( W_2 \), where \( W_1 + W_2 = W \) (the total bandwidth). Using the formula for the capacity of a single-user bandlimited channel, the following rate pair is achievable:

\[
R_1 = W_1 \log\left(1 + \frac{P_1}{N W_1}\right),
\]
\[
R_2 = W_2 \log\left(1 + \frac{P_2}{N W_2}\right).
\]

As we vary \( W_1 \) and \( W_2 \), we trace out the curve as shown in Figure 15.18. This curve touches the boundary of the capacity region at one point, which corresponds to allotting bandwidth to each channel proportional to the power in that channel. We conclude that no allocation of frequency bands to radio stations can be optimal unless the allocated powers are proportional to the bandwidths.

In time-division multiple access (TDMA), time is divided into slots, and each user is allotted a slot during which only that user will transmit, and every other user remains quiet. If there are two users, each of power \( P \), the rate that each sends when the other is silent is \( C(P/N) \). Now, if time is divided into equal-length slots, and every odd slot is allocated to user 1 and every even slot to user 2, the average rate that each user achieves is \( \frac{1}{2}C(P/N) \). This system is called naive time-division multiple access (TDMA). However, it is possible to do better if we notice that since user 1 is sending only half the time, it is possible for him to use twice the power during his transmissions and still maintain the same average power constraint. With this modification, it is possible for each user to send information at a rate \( \frac{1}{2}C(2P/N) \). By varying the lengths of the slots allotted to each sender (and the instantaneous power used during the slot), we can achieve the same capacity region as FDMA with different bandwidth allocations.

As Figure 15.18 illustrates, in general, the capacity region is larger than that achieved by time- or frequency-division multiplexing. But note that the multiple-access capacity region derived above is achieved by use of a common decoder for all the senders. However, it is also possible to achieve the capacity region by onion-peeling, which removes the need for a common decoder and instead uses a sequence of single-user codes. CDMA achieves the entire capacity region and, in addition, allows new users to be added easily without changing the codes of the current users. On the other hand, TDMA and FDMA systems are usually designed for a fixed number of users, and it is possible that either some slots are empty (if the actual number of users is less than the number of slots) or some users are left out (if the number of users is greater than the number of slots). However, in many practical systems, simplicity of design is an important consideration, and the improvement in capacity due to the multiple-access ideas presented earlier may not be sufficient to warrant the increased complexity.

For a Gaussian multiple-access system with \( m \) sources with powers \( P_1, P_2, \ldots, P_m \) and ambient noise of power \( N \), we can state the equivalent of Gauss’s law for any set \( S \) in the form:

\[
\sum_{i \in S} R_i = \text{total rate of information flow from } S
\]

\[
\leq C\left(\frac{\sum_{i \in S} P_i}{N}\right).
\]

--- Page 74 ---
548
NETWORK INFORMATION THEORY
\( R_2 \)  
\( R_1 \)  
\( P_2 \)  
\( N \)  
\( C \)  
0  
\( P_1 \)  
\( N \)  
\( C \)  
\( P_2 \)  
\( P_1 + N \)  
\( C \)  
\( P_1 \)  
\( P_2 + N \)  
\( C \)  
FIGURE 15.18. Gaussian multiple-access channel capacity with FDMA and TDMA.

--- Page 75 ---
15.4
ENCODING OF CORRELATED SOURCES
549
15.4
ENCODING OF CORRELATED SOURCES
We now turn to distributed data compression. This problem is in many ways the data compression dual to the multiple-access channel problem. We know how to encode a source \( X \). A rate \( R > H(X) \) is sufficient. Now, suppose that there are two sources \( (X, Y) \sim p(x, y) \). A rate \( R = H(X, Y) \) is sufficient if we are encoding them together. But what if the \( X \) and \( Y \) sources must be described separately for some user who wishes to reconstruct both \( X \) and \( Y \)? Clearly, by separately encoding \( X \) and \( Y \), it is seen that a rate \( R = R_x + R_y > H(X) + H(Y) \) is sufficient. However, in a surprising and fundamental paper by Slepian and Wolf [502], it is shown that a total rate \( R = H(X, Y) \) is sufficient even for separate encoding of correlated sources.

Let \( (X_1, Y_1), (X_2, Y_2), \ldots \) be a sequence of jointly distributed random variables i.i.d. \( \sim p(x, y) \). Assume that the \( X \) sequence is available at a location \( A \) and the \( Y \) sequence is available at a location \( B \). The situation is illustrated in Figure 15.19.

Before we proceed to the proof of this result, we will give a few definitions.

**Definition**  
A \( ((2nR_1, 2nR_2), n) \) distributed source code for the joint source \( (X, Y) \) consists of two encoder maps,

\[
f_1 : X^n \to \{1, 2, \ldots, 2nR_1\},
\]

\[
f_2 : Y^n \to \{1, 2, \ldots, 2nR_2\},
\]

and a decoder map,

\[
g : \{1, 2, \ldots, 2nR_1\} \times \{1, 2, \ldots, 2nR_2\} \to X^n \times Y^n.
\]

Here, \( f_1(X^n) \) is the index corresponding to \( X^n \), \( f_2(Y^n) \) is the index corresponding to \( Y^n \), and \( (R_1, R_2) \) is the rate pair of the code.

**Definition**  
The probability of error for a distributed source code is defined as

\[
P_e^{(n)} = P(g(f_1(X^n), f_2(Y^n)) \neq (X^n, Y^n)).
\]

**Definition**  
A rate pair \( (R_1, R_2) \) is said to be achievable for a distributed source if there exists a sequence of \( ((2nR_1, 2nR_2), n) \) distributed source codes with probability of error \( P_e^{(n)} \to 0 \). The achievable rate region is the closure of the set of achievable rates.

**Theorem 15.4.1** (Slepian–Wolf)  
For the distributed source coding problem for the source \( (X, Y) \) drawn i.i.d. \( \sim p(x, y) \), the achievable rate region is given by

\[
R_1 \geq H(X|Y),
\]

\[
R_2 \geq H(Y|X),
\]

\[
R_1 + R_2 \geq H(X, Y).
\]

Let us illustrate the result with some examples.

**Example 15.4.1**  
Consider the weather in Gotham and Metropolis. For the purposes of our example, we assume that Gotham is sunny with probability 0.5 and that the weather in Metropolis is the same as in Gotham with probability 0.89. The joint distribution of the weather is given as follows:

\[
\begin{array}{c|cc}
p(x, y) & \text{Rain} & \text{Shine} \\
\hline
\text{Gotham Rain} & 0.445 & 0.055 \\
\text{Gotham Shine} & 0.055 & 0.445 \\
\end{array}
\]

Assume that we wish to transmit 100 days of weather information to the National Weather Service headquarters in Washington. We could send all the 100 bits of the weather in both places, making 200 bits in all. If we decided to compress the information independently, we would still need 100 \( H(0.5) = 100 \) bits of information from each place, for a total of 200 bits. If, instead, we use Slepian–Wolf encoding, we need only \( H(X) + H(Y|X) = 100 H(0.5) + 100 H(0.89) = 100 + 50 = 150 \) bits total.

**Example 15.4.2**  
Consider the following joint distribution:

\[
\begin{array}{c|cc}
p(u, v) & 0 & 1 \\
\hline
0 & 1 & 3 \\
1 & 3 & 1 \\
\end{array}
\]

In this case, the total rate required for the transmission of this source is \( H(U) + H(V|U) = \log 3 = 1.58 \) bits rather than the 2 bits that would be needed if the sources were transmitted independently without Slepian–Wolf encoding.

15.4.1  
Achievability of the Slepian–Wolf Theorem  
We now prove the achievability of the rates in the Slepian–Wolf theorem. Before we proceed to the proof, we introduce a new coding procedure using random bins. The essential idea of random bins is very similar to hash functions: We choose a large random index for each source sequence. If the set of typical source sequences is small enough (or equivalently, the range of the hash function is large enough), then with high probability, different source sequences have different indices, and we can recover the source sequence from the index.

Let us consider the application of this idea to the encoding of a single source. In Chapter 3, the method that we considered was to index all elements of the typical set and not bother about elements outside the typical set. We will now describe the random binning procedure, which indexes all sequences but rejects untypical sequences at a later stage. Consider the following procedure: For each sequence \( X^n \), draw an index at random from \( \{1, 2, \ldots, 2nR\} \). The set of sequences \( X^n \) which have the same index are said to form a bin, since this can be viewed as first laying down a row of bins and then throwing the \( X^n \)’s at random into the bins. For decoding the source from the bin index, we look for a typical \( X^n \) sequence in the bin. If there is one and only one typical \( X^n \) sequence in the bin, we declare it to be the estimate \( \hat{X}^n \) of the source sequence; otherwise, an error is declared.

The above procedure defines a source code. To analyze the probability of error for this code, we will now divide the \( X^n \) sequences into two types: typical sequences and nontypical sequences. If the source sequence is typical, the bin corresponding to this source sequence will contain at least one typical sequence (the source sequence itself). Hence, there will be an error only if there is more than one typical sequence in this bin. If the source sequence is nontypical, there will always be an error. But if the number of bins is much larger than the number of typical sequences, the probability that there is more than one typical sequence in a bin is very small, and hence the probability that a typical sequence will result in an error is very small.

Formally, let \( f(X^n) \) be the bin index corresponding to \( X^n \). Call the decoding function \( g \). The probability of error (averaged over the random choice of codes \( f \)) is

\[
P(g(f(X)) \neq X) \leq P(X \notin A(n)_\epsilon) + \sum_{x} P(\exists x' \neq x : x' \in A(n)_\epsilon, f(x') = f(x))p(x)
\]

\[
\leq \epsilon + \sum_{x} \sum_{x' \in A(n)_\epsilon, x' \neq x} P(f(x') = f(x))p(x)
\]

\[
\leq \epsilon + \sum_{x} \sum_{x' \in A(n)_\epsilon} 2^{-nR} p(x)
\]

\[
= \epsilon + \sum_{x' \in A(n)_\epsilon} 2^{-nR} \sum_{x} p(x)
\]

\[
\leq \epsilon + \sum_{x' \in A(n)_\epsilon} 2^{-nR}
\]

\[
\leq \epsilon + 2n(H(X) + \epsilon)2^{-nR}
\]

\[
\leq 2\epsilon
\]

if \( R > H(X) + \epsilon \) and \( n \) is sufficiently large. Hence, if the rate of the code is greater than the entropy, the probability of error is arbitrarily small, and the code achieves the same results as the code described in Chapter 3.

The above example illustrates the fact that there are many ways to construct codes with low probabilities of error at rates above the entropy of the source; the universal source code is another example of such a code.

--- Page 79 ---
15.4
ENCODING OF CORRELATED SOURCES
553
Note that the binning scheme does not require an explicit characterization of the typical set at the encoder; it is needed only at the decoder. It is this property that enables this code to continue to work in the case of a distributed source, as illustrated in the proof of the theorem.

We now return to the consideration of the distributed source coding and prove the achievability of the rate region in the Slepian–Wolf theorem.

**Proof:**  
(Achievability in Theorem 15.4.1). The basic idea of the proof is to partition the space of \( X^n \) into \( 2nR_1 \) bins and the space of \( Y^n \) into \( 2nR_2 \) bins.

**Random code generation:** Assign every \( x \in X^n \) to one of \( 2nR_1 \) bins independently according to a uniform distribution on \( \{1, 2, \ldots, 2nR_1\} \). Similarly, randomly assign every \( y \in Y^n \) to one of \( 2nR_2 \) bins. Reveal the assignments \( f_1 \) and \( f_2 \) to both the encoder and the decoder.

**Encoding:** Sender 1 sends the index of the bin to which \( X \) belongs. Sender 2 sends the index of the bin to which \( Y \) belongs.

**Decoding:** Given the received index pair \( (i_0, j_0) \), declare \( (\hat{x}, \hat{y}) = (x, y) \) if there is one and only one pair of sequences \( (x, y) \) such that \( f_1(x) = i_0 \), \( f_2(y) = j_0 \), and \( (x, y) \in A(n)_\epsilon \). Otherwise, declare an error. The scheme is illustrated in Figure 15.20. The set of \( X \) sequences and the set of \( Y \) sequences are divided into bins in such a way that the pair of indices specifies a product bin.

\[
2nH(X, Y) \quad \text{jointly typical pairs} \quad (x^n, y^n)
\]

\[
2nR_1 \text{ bins} \quad y^n \quad x^n \quad 2nR_2 \text{ bins}
\]

FIGURE 15.20. Slepian–Wolf encoding: the jointly typical pairs are isolated by the product bins.



NETWORK INFORMATION THEORY

---

### Page 80

**Probability of Error:**

Let \((X_i, Y_i) \sim p(x, y)\). Define the events:

- \(E_0 = \{(X, Y) \notin A(n)_\varepsilon\}\)
- \(E_1 = \{\exists x' \neq X : f_1(x') = f_1(X) \text{ and } (x', Y) \in A(n)_\varepsilon\}\)
- \(E_2 = \{\exists y' \neq Y : f_2(y') = f_2(Y) \text{ and } (X, y') \in A(n)_\varepsilon\}\)
- \(E_{12} = \{\exists (x', y') : x' \neq X, y' \neq Y, f_1(x') = f_1(X), f_2(y') = f_2(Y) \text{ and } (x', y') \in A(n)_\varepsilon\}\)

Here, \(X, Y, f_1,\) and \(f_2\) are random. An error occurs if \((X, Y)\) is not in \(A(n)_\varepsilon\) or if there is another typical pair in the same bin. Hence, by the union of events bound:

\[
P_e^{(n)} = P(E_0 \cup E_1 \cup E_2 \cup E_{12}) \leq P(E_0) + P(E_1) + P(E_2) + P(E_{12})
\]

First, consider \(E_0\). By the AEP, \(P(E_0) \to 0\), and hence for sufficiently large \(n\), \(P(E_0) < \varepsilon\). To bound \(P(E_1)\):

\[
P(E_1) = P\{\exists x' \neq X : f_1(x') = f_1(X), \text{ and } (x', Y) \in A(n)_\varepsilon\}
\]

\[
= \sum_{(x, y)} p(x, y) P\{\exists x' \neq x : f_1(x') = f_1(x), (x', y) \in A(n)_\varepsilon\}
\]

\[
\leq \sum_{(x, y)} p(x, y) \sum_{x' \neq x} P\{(x', y) \in A(n)_\varepsilon\} P(f_1(x') = f_1(x))
\]

\[
= \sum_{(x, y)} p(x, y) 2^{-nR_1} |A_\varepsilon(X|y)|
\]

\[
\leq 2^{-nR_1} 2^{n(H(X|Y) + \varepsilon)}
\]

which goes to 0 if \(R_1 > H(X|Y)\). Hence, for sufficiently large \(n\), \(P(E_1) < \varepsilon\). Similarly, for sufficiently large \(n\), \(P(E_2) < \varepsilon\) if \(R_2 > H(Y|X)\) and \(P(E_{12}) < \varepsilon\) if \(R_1 + R_2 > H(X, Y)\). Since the average probability of error is \(< 4\varepsilon\), there exists at least one code \((f_1^*, f_2^*, g^*)\) with probability of error \(< 4\varepsilon\). Thus, we can construct a sequence of codes with \(P_e^{(n)} \to 0\), and the proof of achievability is complete.

---

### Page 81

**15.4 Encoding of Correlated Sources**

**15.4.2 Converse for the Slepian–Wolf Theorem**

The converse for the Slepian–Wolf theorem follows obviously from the results for a single source, but we will provide it for completeness.

**Proof:**

(Converse to Theorem 15.4.1). As usual, we begin with Fano’s inequality. Let \(f_1, f_2, g\) be fixed. Let \(I_0 = f_1(X^n)\) and \(J_0 = f_2(Y^n)\). Then:

\[
H(X^n, Y^n | I_0, J_0) \leq n \varepsilon_n \log(|\mathcal{X}| + \mathcal{Y}|) + 1 = n \varepsilon_n
\]

where \(\varepsilon_n \to 0\) as \(n \to \infty\). Now, adding conditioning, we also have:

\[
H(X^n | Y^n, I_0, J_0) \leq n \varepsilon_n
\]

\[
H(Y^n | X^n, I_0, J_0) \leq n \varepsilon_n
\]

We can write a chain of inequalities:

\[
n(R_1 + R_2) \geq H(I_0, J_0)
\]

\[
= I(X^n, Y^n; I_0, J_0) + H(I_0, J_0 | X^n, Y^n)
\]

\[
= I(X^n, Y^n; I_0, J_0)
\]

\[
= H(X^n, Y^n) - H(X^n, Y^n | I_0, J_0)
\]

\[
\geq H(X^n, Y^n) - n \varepsilon_n
\]

\[
= nH(X, Y) - n \varepsilon_n
\]

where:

- (a) follows from the fact that \(I_0 \in \{1, 2, \ldots, 2^{nR_1}\}\) and \(J_0 \in \{1, 2, \ldots, 2^{nR_2}\}\)
- (b) follows from the fact that \(I_0\) is a function of \(X^n\) and \(J_0\) is a function of \(Y^n\)
- (c) follows from Fano’s inequality
- (d) follows from the chain rule and the fact that \((X_i, Y_i)\) are i.i.d.

---

### Page 82

**556 Network Information Theory**

Similarly, using (15.179), we have:

\[
nR_1 \geq H(I_0)
\]

\[
\geq H(I_0 | Y^n)
\]

\[
= I(X^n; I_0 | Y^n) + H(I_0 | X^n, Y^n)
\]

\[
= I(X^n; I_0 | Y^n)
\]

\[
= H(X^n | Y^n) - H(X^n | I_0, J_0, Y^n)
\]

\[
\geq H(X^n | Y^n) - n \varepsilon_n
\]

\[
= nH(X | Y) - n \varepsilon_n
\]

where the reasons are the same as for the equations above. Similarly, we can show that:

\[
nR_2 \geq nH(Y | X) - n \varepsilon_n
\]

Dividing these inequalities by \(n\) and taking the limit as \(n \to \infty\), we have the desired converse.

---

### Page 83

**15.4 Encoding of Correlated Sources**

**15.4.3 Slepian–Wolf Theorem for Many Sources**

The results of Section 15.4.2 can easily be generalized to many sources. The proof follows exactly the same lines.

**Theorem 15.4.2:**

Let \((X_{1i}, X_{2i}, \ldots, X_{mi})\) be i.i.d. \(\sim p(x_1, x_2, \ldots, x_m)\). Then the set of rate vectors achievable for distributed source coding with separate encoders and a common decoder is defined by:

\[
R(S) > H(X(S) | X(S^c))
\]

for all \(S \subseteq \{1, 2, \ldots, m\}\), where:

\[
R(S) = \sum_{i \in S} R_i
\]

and \(X(S) = \{X_j : j \in S\}\).

---

### Page 84

**557 Encoding of Correlated Sources**

**Proof:**

The proof is identical to the case of two variables and is omitted.

---

The achievability of Slepian–Wolf encoding has been proved for an i.i.d. correlated source, but the proof can easily be extended to the case of an arbitrary joint source that satisfies the AEP; in particular, it can be extended to the case of any jointly ergodic source. In these cases, the entropies in the definition of the rate region are replaced by the corresponding entropy rates.

**15.4.4 Interpretation of Slepian–Wolf Coding**

We consider an interpretation of the corner points of the rate region in Slepian–Wolf encoding in terms of graph coloring. Consider the point with rate \(R_1 = H(X)\), \(R_2 = H(Y | X)\). Using \(nH(X)\) bits, we can encode \(X^n\) efficiently, so that the decoder can reconstruct \(X^n\) with arbitrarily low probability of error. But how do we code \(Y^n\) with \(nH(Y | X)\) bits? Looking at the picture in terms of typical sets, we see that associated with every \(X^n\) is a typical “fan” of \(Y^n\) sequences that are jointly typical with the given \(X^n\).

If the \(Y\) encoder knows \(X^n\), the encoder can send the index of the \(Y^n\) within this typical fan. The decoder, also knowing \(X^n\), can then construct this typical fan and hence reconstruct \(Y^n\). But the \(Y\) encoder does not know \(X^n\). So instead of trying to determine the typical fan, it randomly colors all \(Y^n\) sequences with \(2^{nR_2}\) colors. If the number of colors is high enough, then with high probability, all the colors in a particular fan will be different, and the color of the \(Y^n\) sequence will uniquely define the \(Y^n\) sequence within the \(X^n\) fan. If the rate \(R_2 > H(Y | X)\), the number of colors is exponentially larger than the number of elements in the fan, and we can show that the scheme will have an exponentially small probability of error.

---

### Page 85

**15.5 Duality Between Slepian–Wolf Encoding and Multiple-Access Channels**

With multiple-access channels, we considered the problem of sending independent messages over a channel with two inputs and only one output. With Slepian–Wolf encoding, we considered the problem of sending a correlated source over a noiseless channel, with a common decoder for recovery of both sources. In this section, we explore the duality between the two systems.

In Figure 15.23, two independent messages are to be sent over the channel as \(X_1^n\) and \(X_2^n\) sequences. The receiver estimates the messages from the received sequence. In Figure 15.24, the correlated sources are encoded as “independent” messages \(i\) and \(j\). The receiver tries to estimate the source sequences from knowledge of \(i\) and \(j\).

In the proof of the achievability of the capacity region for the multiple-access channel, we used a random map from the set of messages to the sequences \(X_1^n\) and \(X_2^n\). In the proof for Slepian–Wolf coding, we used a random map from the set of sequences \(X^n\) and \(Y^n\) to a set of messages.

In the proof of the coding theorem for the multiple-access channel, the probability of error was bounded by:

\[
P_e^{(n)} \leq \varepsilon + \sum \text{codewords} \Pr(\text{codeword jointly typical with sequence received})
\]

\[
= \varepsilon + \sum 2^{nR_1} \text{terms } 2^{-nI_1} + \sum 2^{nR_2} \text{terms } 2^{-nI_2} + \sum 2^{n(R_1+R_2)} \text{terms } 2^{-nI_3}
\]

where \(\varepsilon\) is the probability the sequences are not typical, \(R_i\) are the rates corresponding to the number of codewords that can contribute to the probability of error, and \(I_i\) is the corresponding mutual information that corresponds to the probability that the codeword is jointly typical with the received sequence.

In the case of Slepian–Wolf encoding, the corresponding expression for the probability of error is:

\[
P_e^{(n)} \leq \varepsilon + \sum \text{jointly typical sequences} \Pr(\text{have the same codeword})
\]

\[
= \varepsilon + \sum 2^{nH_1} \text{terms } 2^{-nR_1} + \sum 2^{nH_2} \text{terms } 2^{-nR_2} + \sum 2^{nH_3} \text{terms } 2^{-n(R_1+R_2)}
\]

where again the probability that the constraints of the AEP are not satisfied is bounded by \(\varepsilon\), and the other terms refer to the various ways in which another pair of sequences could be jointly typical and in the same bin as the given source pair.

The duality of the multiple-access channel and correlated source encoding is now obvious. It is rather surprising that these two systems are duals of each other; one would have expected a duality between the broadcast channel and the multiple-access channel.

---

### Page 86

**560 Network Information Theory**

---

### Page 87

**15.6 Broadcast Channel**

The broadcast channel is a communication channel in which there is one sender and two or more receivers. It is illustrated in Figure 15.25. The basic problem is to find the set of simultaneously achievable rates for communication in a broadcast channel. Before we begin the analysis, let us consider some examples.

**Example 15.6.1 (TV station):**

The simplest example of the broadcast channel is a radio or TV station. But this example is slightly degenerate in the sense that normally the station wants to send the same information to everybody who is tuned in; the capacity is essentially \(\max_p(x) \min I(X; Y_i)\), which may be less than the capacity of the worst receiver. But we may wish to arrange the information in such a way that the better receivers receive extra information, which produces a better picture or sound, while the worst receivers continue to receive more basic information. As TV stations introduce high-definition TV (HDTV), it may be necessary to encode the information so that bad receivers will receive the regular TV signal, while good receivers will receive the extra information for the high-definition signal. The methods to accomplish this will be explained in the discussion of the broadcast channel.

**Example 15.6.2 (Lecturer in classroom):**

A lecturer in a classroom is communicating information to the students in the class. Due to differences among the students, they receive various amounts of information. Some of the students receive most of the information; others receive only a little. In the ideal situation, the lecturer would be able to tailor his or her lecture in such a way that the good students receive more information and the poor students receive at least the minimum amount of information. However, a poorly prepared lecture proceeds at the pace of the weakest student. This situation is another example of a broadcast channel.

**Example 15.6.3 (Orthogonal broadcast channels):**

The simplest broadcast channel consists of two independent channels to the two receivers. Here we can send independent information over both channels, and we can achieve rate \(R_1\) to receiver 1 and rate \(R_2\) to receiver 2 if \(R_1 < C_1\) and \(R_2 < C_2\). The capacity region is the rectangle shown in Figure 15.26.

**Example 15.6.4 (Spanish and Dutch speaker):**

To illustrate the idea of superposition, we will consider a simplified example of a speaker who can speak both Spanish and Dutch. There are two listeners: one understands only Spanish and the other understands only Dutch. Assume for simplicity that the vocabulary of each language is 220 words and that the speaker speaks at the rate of 1 word per second in either language. Then he...



NETWORK INFORMATION THEORY

FIGURE 15.26. Capacity region for two orthogonal broadcast channels.

He can transmit 20 bits of information per second to receiver 1 by speaking to her all the time; in this case, he sends no information to receiver 2. Similarly, he can send 20 bits per second to receiver 2 without sending any information to receiver 1. Thus, he can achieve any rate pair with \( R_1 + R_2 = 20 \) by simple time-sharing. But can he do better?

Recall that the Dutch listener, even though he does not understand Spanish, can recognize when the word is Spanish. Similarly, the Spanish listener can recognize when Dutch occurs. The speaker can use this to convey information; for example, if the proportion of time he uses each language is 50%, then of a sequence of 100 words, about 50 will be Dutch and about 50 will be Spanish. But there are many ways to order the Spanish and Dutch words; in fact, there are about \( \binom{100}{50} \approx 2100 \) ways to order the words. Choosing one of these orderings conveys information to both listeners. This method enables the speaker to send information at a rate of 10 bits per second to the Dutch receiver, 10 bits per second to the Spanish receiver, and 1 bit per second of common information to both receivers, for a total rate of 21 bits per second, which is more than that achievable by simple time-sharing. This is an example of superposition of information.

The results of the broadcast channel can also be applied to the case of a single-user channel with an unknown distribution. In this case, the objective is to get at least the minimum information through when the channel is bad and to get some extra information through when the channel is good. We can use the same superposition arguments as in the case of the broadcast channel to find the rates at which we can send information.

### 15.6 Broadcast Channel

#### 15.6.1 Definitions for a Broadcast Channel

**Definition**  
A broadcast channel consists of an input alphabet \( \mathcal{X} \) and two output alphabets \( \mathcal{Y}_1 \) and \( \mathcal{Y}_2 \), and a probability transition function \( p(y_1, y_2|x) \). The broadcast channel will be said to be memoryless if  
\[
p(y_1^n, y_2^n|x^n) = \prod_{i=1}^n p(y_{1i}, y_{2i}|x_i).
\]  
We define codes, probability of error, achievability, and capacity regions for the broadcast channel as we did for the multiple-access channel. A \( ((2^nR_1, 2^nR_2), n) \) code for a broadcast channel with independent information consists of an encoder,  
\[
X: \{1, 2, \ldots, 2^nR_1\} \times \{1, 2, \ldots, 2^nR_2\} \to \mathcal{X}^n,
\]  
and two decoders,  
\[
g_1: \mathcal{Y}_1^n \to \{1, 2, \ldots, 2^nR_1\},
\]  
\[
g_2: \mathcal{Y}_2^n \to \{1, 2, \ldots, 2^nR_2\}.
\]  
We define the average probability of error as the probability that the decoded message is not equal to the transmitted message; that is,  
\[
P_e^{(n)} = P(g_1(Y_1^n) \neq W_1 \text{ or } g_2(Y_2^n) \neq W_2),
\]  
where \( (W_1, W_2) \) are assumed to be uniformly distributed over \( \{1, 2, \ldots, 2^nR_1\} \times \{1, 2, \ldots, 2^nR_2\} \).

**Definition**  
A rate pair \( (R_1, R_2) \) is said to be achievable for the broadcast channel if there exists a sequence of \( ((2^nR_1, 2^nR_2), n) \) codes with \( P_e^{(n)} \to 0 \).

We will now define the rates for the case where we have common information to be sent to both receivers. A \( ((2^nR_0, 2^nR_1, 2^nR_2), n) \) code for a broadcast channel with common information consists of an encoder,  
\[
X: \{1, 2, \ldots, 2^nR_0\} \times \{1, 2, \ldots, 2^nR_1\} \times \{1, 2, \ldots, 2^nR_2\} \to \mathcal{X}^n,
\]  
and two decoders,  
\[
g_1: \mathcal{Y}_1^n \to \{1, 2, \ldots, 2^nR_0\} \times \{1, 2, \ldots, 2^nR_1\},
\]  
\[
g_2: \mathcal{Y}_2^n \to \{1, 2, \ldots, 2^nR_0\} \times \{1, 2, \ldots, 2^nR_2\}.
\]  

Assuming that the distribution on \( (W_0, W_1, W_2) \) is uniform, we can define the probability of error as the probability that the decoded message is not equal to the transmitted message:  
\[
P_e^{(n)} = P(g_1(Y_1^n) \neq (W_0, W_1) \text{ or } g_2(Y_2^n) \neq (W_0, W_2)).
\]  

**Definition**  
A rate triple \( (R_0, R_1, R_2) \) is said to be achievable for the broadcast channel with common information if there exists a sequence of \( ((2^nR_0, 2^nR_1, 2^nR_2), n) \) codes with \( P_e^{(n)} \to 0 \).

**Definition**  
The capacity region of the broadcast channel is the closure of the set of achievable rates.

We observe that an error for receiver \( Y_1^n \) depends only on the distribution \( p(x^n, y_1^n) \) and not on the joint distribution \( p(x^n, y_1^n, y_2^n) \). Thus, we have the following theorem:

**Theorem 15.6.1**  
The capacity region of a broadcast channel depends only on the conditional marginal distributions \( p(y_1|x) \) and \( p(y_2|x) \).

*Proof:*  
See the problems.

#### 15.6.2 Degraded Broadcast Channels

**Definition**  
A broadcast channel is said to be physically degraded if  
\[
p(y_1, y_2|x) = p(y_1|x)p(y_2|y_1).
\]  

**Definition**  
A broadcast channel is said to be stochastically degraded if its conditional marginal distributions are the same as that of a physically degraded broadcast channel; that is, if there exists a distribution \( p'(y_2|y_1) \) such that  
\[
p(y_2|x) = \sum_{y_1} p(y_1|x)p'(y_2|y_1).
\]  

Note that since the capacity of a broadcast channel depends only on the conditional marginals, the capacity region of the stochastically degraded broadcast channel is the same as that of the corresponding physically degraded channel. In much of the following, we therefore assume that the channel is physically degraded.

#### 15.6.3 Capacity Region for the Degraded Broadcast Channel

We now consider sending independent information over a degraded broadcast channel at rate \( R_1 \) to \( Y_1 \) and rate \( R_2 \) to \( Y_2 \).

**Theorem 15.6.2**  
The capacity region for sending independent information over the degraded broadcast channel \( X \to Y_1 \to Y_2 \) is the convex hull of the closure of all \( (R_1, R_2) \) satisfying  
\[
R_2 \leq I(U; Y_2),
\]  
\[
R_1 \leq I(X; Y_1|U)
\]  
for some joint distribution \( p(u)p(x|u)p(y_1, y_2|x) \), where the auxiliary random variable \( U \) has cardinality bounded by \( |U| \leq \min\{|X|, |Y_1|, |Y_2|\} \).

*Proof:*  
(The cardinality bounds for the auxiliary random variable \( U \) are derived using standard methods from convex set theory and are not dealt with here.) We first give an outline of the basic idea of superposition coding for the broadcast channel. The auxiliary random variable \( U \) will serve as a "cloud center" that can be distinguished by both receivers \( Y_1 \) and \( Y_2 \). Each cloud consists of \( 2^nR_1 \) codewords \( X^n \) distinguishable by the receiver \( Y_1 \). The worst receiver can only see the clouds, while the better receiver can see the individual codewords within the clouds. The formal proof of the achievability of this region uses a random coding argument:

*Random codebook generation:* Generate \( 2^nR_2 \) independent codewords of length \( n \), \( U(w_2) \), \( w_2 \in \{1, 2, \ldots, 2^nR_2\} \), according to \( \prod_{i=1}^n p(u_i) \). For each codeword \( U(w_2) \), generate \( 2^nR_1 \) independent codewords \( X(w_1, w_2) \) according to \( \prod_{i=1}^n p(x_i|u_i) \). Here \( u(i) \) plays the role of the cloud center understandable to both \( Y_1 \) and \( Y_2 \), while \( x(i, j) \) is the \( j \)-th satellite codeword in the \( i \)-th cloud.

*Encoding:* To send the pair \( (W_1, W_2) \), send the corresponding codeword \( X(W_1, W_2) \).

*Decoding:* Receiver 2 determines the unique \( \hat{W}_2 \) such that \( (U(\hat{W}_2), Y_2) \in A_\varepsilon^{(n)} \). If there are none such or more than one such, an error is declared. Receiver 1 looks for the unique \( (\hat{W}_1, \hat{W}_2) \) such that \( (U(\hat{W}_2), X(\hat{W}_1, \hat{W}_2), Y_1) \in A_\varepsilon^{(n)} \). If there are none such or more than one such, an error is declared.

*Analysis of the probability of error:* By the symmetry of the code generation, the probability of error does not depend on which codeword was sent. Hence, without loss of generality, we can assume that the message pair \( (W_1, W_2) = (1, 1) \) was sent. Let \( P(\cdot) \) denote the conditional probability of an event given that \( (1, 1) \) was sent.

Since we have essentially a single-user channel from \( U \) to \( Y_2 \), we will be able to decode the \( U \) codewords with a low probability of error if \( R_2 < I(U; Y_2) \). To prove this, we define the events  
\[
E_{Y_i} = \{(U(i), Y_2) \in A_\varepsilon^{(n)}\}.
\]  
Then the probability of error at receiver 2 is  
\[
P_e^{(n)}(2) = P\left(\neg \bigcup_{i=1} E_{Y_i}\right).
\]  
Using the union bound,  
\[
P_e^{(n)}(2) \leq P\left(\neg E_{Y_1}\right) + \sum_{i \neq 1} P(E_{Y_i}).
\]  
By the AEP,  
\[
P(E_{Y_i}) \leq 2^{-n(I(U; Y_2) - 2\varepsilon)}.
\]  
Thus,  
\[
P_e^{(n)}(2) \leq \varepsilon + 2^nR_2 2^{-n(I(U; Y_2) - 2\varepsilon)} \leq 2\varepsilon
\]  
if \( n \) is large enough and \( R_2 < I(U; Y_2) \).

Similarly, for decoding for receiver 1, we define the events  
\[
\tilde{E}_{Y_i} = \{(U(i), Y_1) \in A_\varepsilon^{(n)}\},
\]  
\[
\tilde{E}_{Y_{i,j}} = \{(U(i), X(i, j), Y_1) \in A_\varepsilon^{(n)}\}.
\]  
Then we can bound the probability of error as  
\[
P_e^{(n)}(1) \leq P\left(\neg \tilde{E}_{Y_1}\right) + P\left(\neg \tilde{E}_{Y_{1,1}}\right) + \sum_{i \neq 1} P(\tilde{E}_{Y_i}) + \sum_{j \neq 1} P(\tilde{E}_{Y_{1,j}}).
\]  
By the same arguments as for receiver 2, we can bound \( P(\tilde{E}_{Y_i}) \leq 2^{-n(I(U; Y_1) - 3\varepsilon)} \). Hence, the third term goes to 0 if \( R_2 < I(U; Y_1) \). But by the data-processing inequality and the degraded nature of the channel, \( I(U; Y_1) \geq I(U; Y_2) \), and hence the conditions of the theorem imply that the third term goes to 0. We can also bound the fourth term in the probability of error as  
\[
P(\tilde{E}_{Y_{1,j}}) = P((U(1), X(1, j), Y_1) \in A_\varepsilon^{(n)}).
\]  
Using the AEP,  
\[
P(\tilde{E}_{Y_{1,j}}) \leq 2^{-n(I(X; Y_1|U) - 4\varepsilon)}.
\]  
Hence, if \( R_1 < I(X; Y_1|U) \), the fourth term in the probability of error goes to 0. Thus, we can bound the probability of error  
\[
P_e^{(n)}(1) \leq 4\varepsilon
\]  
if \( n \) is large enough and \( R_2 < I(U; Y_1) \) and \( R_1 < I(X; Y_1|U) \). The above bounds show that we can decode the messages with total probability of error that goes to 0. Hence, there exists a sequence of good \( ((2^nR_1, 2^nR_2), n) \) codes \( C_n^* \) with probability of error going to 0. With this, we complete the proof of the achievability of the capacity region for the degraded broadcast channel. Gallager’s proof of the converse is outlined in Problem 15.11.

So far we have considered sending independent information to each receiver. But in certain situations, we wish to send common information to both receivers. Let the rate at which we send common information be \( R_0 \). Then we have the following obvious theorem:

**Theorem 15.6.3**  
If the rate pair \( (R_1, R_2) \) is achievable for a broadcast channel with independent information, the rate triple \( (R_0, R_1 - R_0, R_2 - R_0) \) with a common rate \( R_0 \) is achievable, provided that \( R_0 \leq \min(R_1, R_2) \).

In the case of a degraded broadcast channel, we can do even better. Since by our coding scheme the better receiver always decodes all the information that is sent to the worst receiver, one need not reduce the amount of information sent to the better receiver when we have common information. Hence, we have the following theorem:

**Theorem 15.6.4**  
If the rate pair \( (R_1, R_2) \) is achievable for a degraded broadcast channel, the rate triple \( (R_0, R_1, R_2 - R_0) \) is achievable for the channel with common information, provided that \( R_0 < R_2 \).

We end this section by considering the example of the binary symmetric broadcast channel.

**Example 15.6.5**  
Consider a pair of binary symmetric channels with parameters \( p_1 \) and \( p_2 \) that form a broadcast channel as shown in Figure 15.27. Without loss of generality in the capacity calculation, we can recast this channel as a physically degraded channel. We assume that \( p_1 < p_2 < \frac{1}{2} \). Then we can express a binary symmetric channel with parameter \( p_2 \) as a cascade of a binary symmetric channel with parameter \( p_1 \) with another binary symmetric channel. Let the crossover probability of the new channel be \( \alpha \). Then we must have  
\[
p_1(1 - \alpha) + (1 - p_1)\alpha = p_2.
\]  

FIGURE 15.27. Binary symmetric broadcast channel.



--- Page 95 ---
15.6 BROADCAST CHANNEL 569 U 1 −b 1 −b 1 −a 1 −a 1 − p1 1 − p1 X Y1 Y2 b b a a p1 p1 FIGURE 15.28. Physically degraded binary symmetric broadcast channel. or α = p2 − p1 1 − 2p1. (15.230) We now consider the auxiliary random variable in the definition of the capacity region. In this case, the cardinality of U is binary from the bound of the theorem. By symmetry, we connect U to X by another binary symmetric channel with parameter β, as illustrated in Figure 15.28. We can now calculate the rates in the capacity region. It is clear by symmetry that the distribution on U that maximizes the rates is the uniform distribution on {0, 1}, so that I(U; Y2) = H(Y2) − H(Y2|U) (15.231) = 1 − H(β ∗ p2), (15.232) where β ∗ p2 = β(1 − p2) + (1 − β)p2. (15.233) Similarly, I(X; Y1|U) = H(Y1|U) − H(Y1|X, U) (15.234) = H(Y1|U) − H(Y1|X) (15.235) = H(β ∗ p1) − H(p1), (15.236) where β ∗ p1 = β(1 − p1) + (1 − β)p1. (15.237) Plotting these points as a function of β, we obtain the capacity region in Figure 15.29. When β = 0, we have maximum information transfer to Y2 [i.e., R2 = 1 − H(p2) and R1 = 0]. When β = 1/2, we have maximum information transfer to Y1 [i.e., R1 = 1 − H(p1)] and no information transfer to Y2. These values of β give us the corner points of the rate region. --- Page 96 --- 570 NETWORK INFORMATION THEORY R2 R1 I − H(p2) I − H(p1) FIGURE 15.29. Capacity region of binary symmetric broadcast channel. Z1 ~ Y1 Y2 (0,N1) Z′2 ~ (0,N2 − N1) X FIGURE 15.30. Gaussian broadcast channel. Example 15.6.6 (Gaussian broadcast channel) The Gaussian broadcast channel is illustrated in Figure 15.30. We have shown it in the case where one output is a degraded version of the other output. Based on the results of Problem 15.10, it follows that all scalar Gaussian broadcast channels are equivalent to this type of degraded channel. Y1 = X + Z1, (15.238) Y2 = X + Z2 = Y1 + Z′2, (15.239) where Z1 ∼ N(0, N1) and Z′2 ∼ N(0, N2 − N1). Extending the results of this section to the Gaussian case, we can show that the capacity region of this channel is given by R1 < C(αP / N1) (15.240) R2 < C((1 − α)P / (αP + N2)), (15.241) where α may be arbitrarily chosen (0 ≤ α ≤ 1). The coding scheme that achieves this capacity region is outlined in Section 15.1.3. 15.7 RELAY CHANNEL 571 The relay channel is a channel in which there is one sender and one receiver with a number of intermediate nodes that act as relays to help the communication from the sender to the receiver. The simplest relay channel has only one intermediate or relay node. In this case, the channel consists of four finite sets X, X1, Y, and Y1 and a collection of probability mass functions p(y, y1|x, x1) on Y × Y1, one for each (x, x1) ∈ X × X1. The interpretation is that x is the input to the channel and y is the output of the channel, y1 is the relay’s observation, and x1 is the input symbol chosen by the relay, as shown in Figure 15.31. The problem is to find the capacity of the channel between the sender X and the receiver Y. The relay channel combines a broadcast channel (X to Y and Y1) and a multiple-access channel (X and X1 to Y). The capacity is known for the special case of the physically degraded relay channel. We first prove an outer bound on the capacity of a general relay channel and later establish an achievable region for the degraded relay channel. Definition A (2nR, n) code for a relay channel consists of a set of integers W = {1, 2, ..., 2nR}, an encoding function X: {1, 2, ..., 2nR} → Xn, (15.242) a set of relay functions {fi}n i=1 such that x1i = fi(Y11, Y12, ..., Y1i−1), 1 ≤ i ≤ n, (15.243) and a decoding function, g: Yn → {1, 2, ..., 2nR}. (15.244) X Y1: X1 Y FIGURE 15.31. Relay channel. --- Page 98 --- 572 NETWORK INFORMATION THEORY Note that the definition of the encoding functions includes the non-anticipatory condition on the relay. The relay channel input is allowed to depend only on the past observations y11, y12, ..., y1i−1. The channel is memoryless in the sense that (Yi, Y1i) depends on the past only through the current transmitted symbols (Xi, X1i). Thus, for any choice p(w), w ∈ W, and code choice X: {1, 2, ..., 2nR} → Xn i and relay functions {fi}n i=1, the joint probability mass function on W × Xn × Xn1 × Yn × Yn1 is given by p(w, x, x1, y, y1) = p(w) n ∏ i=1 p(xi|w)p(x1i|y11, y12, ..., y1i−1) × p(yi, y1i|xi, x1i). (15.245) If the message w ∈ [1, 2nR] is sent, let λ(w) = Pr{g(Y) ≠ w | w sent} (15.246) denote the conditional probability of error. We define the average probability of error of the code as P(n)e = 1/(2nR) ∑w λ(w). (15.247) The probability of error is calculated under the uniform distribution over the codewords w ∈ {1, ..., 2nR}. The rate R is said to be achievable by the relay channel if there exists a sequence of (2nR, n) codes with P(n)e → 0. The capacity C of a relay channel is the supremum of the set of achievable rates. We first give an upper bound on the capacity of the relay channel. Theorem 15.7.1 For any relay channel (X × X1, p(y, y1|x, x1), Y × Y1), the capacity C is bounded above by C ≤ sup p(x,x1) min {I(X, X1; Y), I(X; Y, Y1|X1)}. (15.248) Proof: The proof is a direct consequence of a more general max-flow min-cut theorem given in Section 15.10. □ This upper bound has a nice max-flow min-cut interpretation. The first term in (15.248) upper bounds the maximum rate of information transfer from senders X and X1 to receiver Y. The second term bounds the rate from X to Y and Y1. We now consider a family of relay channels in which the relay receiver is better than the ultimate receiver Y in the sense defined below. Here the max-flow min-cut upper bound in (15.248) is achieved. Definition The relay channel (X × X1, p(y, y1|x, x1), Y × Y1) is said to be physically degraded if p(y, y1|x, x1) can be written in the form p(y, y1|x, x1) = p(y1|x, x1)p(y|y1, x1). (15.249) Thus, Y is a random degradation of the relay signal Y1. For the physically degraded relay channel, the capacity is given by the following theorem. Theorem 15.7.2 The capacity C of a physically degraded relay channel is given by C = sup p(x,x1) min {I(X, X1; Y), I(X; Y1|X1)}, (15.250) where the supremum is over all joint distributions on X × X1. Proof: Converse: The proof follows from Theorem 15.7.1 and by degradedness, since for the degraded relay channel, I(X; Y, Y1|X1) = I(X; Y1|X1). Achievability: The proof of achievability involves a combination of the following basic techniques: (1) random coding, (2) list codes, (3) Slepian-Wolf partitioning, (4) coding for the cooperative multiple-access channel, (5) superposition coding, and (6) block Markov encoding at the relay and transmitter. We provide only an outline of the proof. Outline of achievability: We consider B blocks of transmission, each of n symbols. A sequence of B − 1 indices, wi ∈ {1, 2, ..., 2nR}, i = 1, 2, ..., B − 1, will be sent over the channel in nB transmissions. (Note that as B → ∞ for a fixed n, the rate R(B − 1)/B is arbitrarily close to R.) We define a doubly indexed set of codewords: C = {x(w|s), x1(s)} : w ∈ {1, 2nR}, s ∈ {1, 2nR0}, x ∈ Xn, x1 ∈ Xn1. (15.251) We will also need a partition S = {S1, S2, ..., S2nR0} of W = {1, 2, ..., 2nR} (15.252) --- Page 100 --- 574 NETWORK INFORMATION THEORY into 2nR0 cells, with Si ∩ Sj = ∅, i ≠ j, and ∪Si = W. The partition will enable us to send side information to the receiver in the manner of Slepian and Wolf [502]. Generation of random code: Fix p(x1)p(x|x1). First generate at random 2nR0 i.i.d. n-sequences in Xn1, each drawn according to p(x1) = ∏n i=1 p(x1i). Index them as x1(s), s ∈ {1, 2, ..., 2nR0}. For each x1(s), generate 2nR conditionally independent n-sequences x(w|s), w ∈ {1, 2, ..., 2nR}, drawn independently according to p(x|x1(s)) = ∏n i=1 p(xi|x1i(s)). This defines the random codebook C = {x(w|s), x1(s)}. The random partition S = {S1, S2, ..., S2nR0} of {1, 2, ..., 2nR} is defined as follows. Let each integer w ∈ {1, 2, ..., 2nR} be assigned independently, according to a uniform distribution over the indices s = 1, 2, ..., 2nR0, to cells Ss. Encoding: Let wi ∈ {1, 2, ..., 2nR} be the new index to be sent in block i, and let si be defined as the partition corresponding to wi−1 (i.e., wi−1 ∈ Ssi). The encoder sends x(wi|si). The relay has an estimate ˆˆwi−1 of the previous index wi−1. (This will be made precise in the decoding section.) Assume that ˆˆwi−1 ∈ Sˆˆsi. The relay encoder sends x1(ˆˆsi) in block i. Decoding: We assume that at the end of block i − 1, the receiver knows (w1, w2, ..., wi−2) and (s1, s2, ..., si−1) and the relay knows (w1, w2, ..., wi−1) and consequently, (s1, s2, ..., si). The decoding procedures at the end of block i are as follows: 1. Knowing si and upon receiving y1(i), the relay receiver estimates the message of the transmitter ˆˆwi = w if and only if there exists a unique w such that (x(w|si), x1(si), y1(i)) are jointly ǫ-typical. Using Theorem 15.2.3, it can be shown that ˆˆwi = wi with an arbitrarily small probability of error if R < I(X; Y1|X1) (15.253) and n is sufficiently large. 2. The receiver declares that ˆsi = s was sent iff there exists one and only one s such that (x1(s), y(i)) are jointly ǫ-typical. From Theorem 15.2.1 we know that si can be decoded with arbitrarily small probability of error if R0 < I(X1; Y) (15.254) and n is sufficiently large.



### Page 1

#### 15.8 Source Coding with Side Information

**Theorem 15.7.2** can also be shown to be the capacity for the following classes of relay channels:

1. Reversely degraded relay channel, i.e.,  
   \( p(y, y_1|x, x_1) = p(y|x, x_1)p(y_1|y, x_1). \)  
   (15.257)  

2. Relay channel with feedback.  

3. Deterministic relay channel,  
   \( y_1 = f(x, x_1), \)  
   \( y = g(x, x_1). \)  
   (15.258)  

---

### Page 2

#### 15.8 Source Coding with Side Information

We now consider the distributed source coding problem where two random variables \( X \) and \( Y \) are encoded separately but only \( X \) is to be recovered. We now ask how many bits \( R_1 \) are required to describe \( X \) if we are allowed \( R_2 \) bits to describe \( Y \). If \( R_2 > H(Y) \), then \( Y \) can be described perfectly, and by the results of Slepian–Wolf coding, \( R_1 = H(X|Y) \) bits suffice to describe \( X \). At the other extreme, if \( R_2 = 0 \), we must describe \( X \) without any help, and \( R_1 = H(X) \) bits are then necessary to describe \( X \). In general, we use \( R_2 = I(Y; \hat{Y}) \) bits to describe an approximate version of \( Y \). This will allow us to describe \( X \) using \( H(X|\hat{Y}) \) bits in the presence of side information \( \hat{Y} \). The following theorem is consistent with this intuition.

---

### Page 3

#### Theorem 15.8.1

Let \( (X, Y) \sim p(x, y) \). If \( Y \) is encoded at rate \( R_2 \) and \( X \) is encoded at rate \( R_1 \), we can recover \( X \) with an arbitrarily small probability of error if and only if  
\[ R_1 \geq H(X|U), \]  
(15.259)  
\[ R_2 \geq I(Y; U) \]  
(15.260)  
for some joint probability mass function \( p(x, y)p(u|y) \), where \( |U| \leq |Y| + 2 \).

**Proof:**  
*(Converse). Consider any source code for Figure 15.32. The source code consists of mappings \( f_n(X^n) \) and \( g_n(Y^n) \) such that the rates of \( f_n \) and \( g_n \) are less than \( R_1 \) and \( R_2 \), respectively, and a decoding mapping \( h_n \) such that*  
\[ P_e^{(n)} = \Pr\{h_n(f_n(X^n), g_n(Y^n)) \neq X^n\} < \varepsilon. \]  
(15.261)  

Define new random variables \( S = f_n(X^n) \) and \( T = g_n(Y^n) \). Then since we can recover \( X^n \) from \( S \) and \( T \) with low probability of error, we have, by Fano’s inequality,  
\[ H(X^n|S, T) \leq n\varepsilon_n. \]  
(15.262)  

Then  
\[ nR_2 \geq H(T) \]  
(15.263)  
\[ \geq I(Y^n; T) \]  
(15.264)  
\[ = \sum_{i=1}^n I(Y_i; T|Y_1, \dots, Y_{i-1}) \]  
(15.265)  
\[ = \sum_{i=1}^n I(Y_i; T, Y_1, \dots, Y_{i-1}) \]  
(15.266)  
\[ = \sum_{i=1}^n I(Y_i; U_i) \]  
(15.267)  
where \( U_i = (T, Y_1, \dots, Y_{i-1}) \).  

We also have another chain for \( R_1 \):  
\[ nR_1 \geq H(S) \]  
(15.268)  
\[ \geq H(S|T) \]  
(15.269)  
\[ = H(S|T) + H(X^n|S, T) - H(X^n|S, T) \]  
(15.270)  
\[ \geq H(X^n, S|T) - n\varepsilon_n \]  
(15.271)  
\[ = H(X^n|T) - n\varepsilon_n \]  
(15.272)  
\[ = \sum_{i=1}^n H(X_i|T, X_1, \dots, X_{i-1}) - n\varepsilon_n \]  
(15.273)  
\[ \geq \sum_{i=1}^n H(X_i|T, X_{i-1}, Y_{i-1}) - n\varepsilon_n \]  
(15.274)  
\[ = \sum_{i=1}^n H(X_i|T, Y_{i-1}) - n\varepsilon_n \]  
(15.275)  
\[ = \sum_{i=1}^n H(X_i|U_i) - n\varepsilon_n, \]  
(15.276)  
where \( U_i = (T, Y_{i-1}) \).  

---

### Page 4

Since \( X_i \) contains no more information about \( U_i \) than is present in \( Y_i \), it follows that \( X_i \to Y_i \to U_i \) forms a Markov chain. Thus, we have the following inequalities:  
\[ R_1 \geq \frac{1}{n} \sum_{i=1}^n H(X_i|U_i), \]  
(15.277)  
\[ R_2 \geq \frac{1}{n} \sum_{i=1}^n I(Y_i; U_i). \]  
(15.278)  

We now introduce a timesharing random variable \( Q \) so that we can rewrite these equations as  
\[ R_1 \geq \frac{1}{n} \sum_{i=1}^n H(X_i|U_i, Q = i) = H(X_Q|U_Q, Q), \]  
(15.279)  
\[ R_2 \geq \frac{1}{n} \sum_{i=1}^n I(Y_i; U_i|Q = i) = I(Y_Q; U_Q|Q). \]  
(15.280)  

Now, since \( Q \) is independent of \( Y_Q \) (the distribution of \( Y_i \) does not depend on \( i \)), we have  
\[ I(Y_Q; U_Q|Q) = I(Y_Q; U_Q, Q) - I(Y_Q; Q) = I(Y_Q; U_Q, Q). \]  
(15.281)  

Since \( X_Q \) and \( Y_Q \) have the joint distribution \( p(x, y) \) in the theorem, defining \( U = (U_Q, Q) \), \( X = X_Q \), and \( Y = Y_Q \), we have shown the existence of a random variable \( U \) such that  
\[ R_1 \geq H(X|U), \]  
(15.282)  
\[ R_2 \geq I(Y; U) \]  
(15.283)  

for any encoding scheme that has a low probability of error. Thus, the converse is proved.  

---

### Page 5

Before we proceed to the proof of the achievability of this pair of rates, we will need a new lemma about strong typicality and Markov chains. Recall the definition of strong typicality for a triple of random variables \( X, Y, \) and \( Z \). A triplet of sequences \( x^n, y^n, z^n \) is said to be \( \varepsilon \)-strongly typical if  

\[ \left| \frac{1}{n}N(a, b, c|x^n, y^n, z^n) - p(a, b, c) \right| < \varepsilon \]  
for all \( a \in \mathcal{X}, b \in \mathcal{Y}, c \in \mathcal{Z} \).  

(15.284)  

In particular, this implies that \( (x^n, y^n) \) are jointly strongly typical and that \( (y^n, z^n) \) are also jointly strongly typical. But the converse is not true: The fact that \( (x^n, y^n) \in A_{\varepsilon}^{(n)}(X, Y) \) and \( (y^n, z^n) \in A_{\varepsilon}^{(n)}(Y, Z) \) does not in general imply that \( (x^n, y^n, z^n) \in A_{\varepsilon}^{(n)}(X, Y, Z) \). But if \( X \to Y \to Z \) forms a Markov chain, this implication is true. We state this as a lemma without proof [53, 149].  

**Lemma 15.8.1**  
Let \( (X, Y, Z) \) form a Markov chain \( X \to Y \to Z \) (i.e., \( p(x, y, z) = p(x, y)p(z|y) \)). If for a given \( (y^n, z^n) \in A_{\varepsilon}^{(n)}(Y, Z) \), \( X^n \) is drawn \( \sim \prod_{i=1}^n p(x_i|y_i) \), then  
\[ \Pr\{(X^n, y^n, z^n) \in A_{\varepsilon}^{(n)}(X, Y, Z)\} > 1 - \varepsilon \]  
for sufficiently large \( n \).  

**Remark**  
The theorem is true from the strong law of large numbers if \( X^n \sim \prod_{i=1}^n p(x_i|y_i, z_i) \). The Markovity of \( X \to Y \to Z \) is used to show that \( X^n \sim \prod_{i=1}^n p(x_i|y_i) \) is sufficient for the same conclusion.  

---

### Page 6

We now outline the proof of achievability in Theorem 15.8.1.  

**Proof:** *(Achievability in Theorem 15.8.1). Fix \( p(u|y) \). Calculate \( p(u) = \sum_{y} p(y)p(u|y) \).  

**Generation of codebooks:** Generate \( 2^{nR_2} \) independent codewords of length \( n \), \( U(w_2) \), \( w_2 \in \{1, 2, \dots, 2^{nR_2}\} \), according to \( \prod_{i=1}^n p(u_i) \). Randomly bin all the \( X^n \) sequences into \( 2^{nR_1} \) bins by independently generating an index \( b \) distributed uniformly on \( \{1, 2, \dots, 2^{nR_1}\} \) for each \( X^n \). Let \( B(i) \) denote the set of \( X^n \) sequences allotted to bin \( i \).  

**Encoding:** The \( X \) sender sends the index \( i \) of the bin in which \( X^n \) falls. The \( Y \) sender looks for an index \( s \) such that \( (Y^n, U^n(s)) \in A_{\varepsilon}^{(n)}(Y, U) \). If there is more than one such \( s \), it sends the least. If there is no such \( U^n(s) \) in the codebook, it sends \( s = 1 \).  

**Decoding:** The receiver looks for a unique \( X^n \in B(i) \) such that \( (X^n, U^n(s)) \in A_{\varepsilon}^{(n)}(X, U) \). If there is none or more than one, it declares an error.  

**Analysis of the probability of error:** The various sources of error are as follows:  

1. The pair \( (X^n, Y^n) \) generated by the source is not typical. The probability of this is small if \( n \) is large. Hence, without loss of generality, we can condition on the event that the source produces a particular typical sequence \( (x^n, y^n) \in A_{\varepsilon}^{(n)}(X, Y) \).  

2. The sequence \( Y^n \) is typical, but there does not exist a \( U^n(s) \) in the codebook that is jointly typical with it. The probability of this is small from the arguments of Section 10.6, where we showed that if there are enough codewords; that is, if  
\[ R_2 > I(Y; U), \]  
(15.285)  
we are very likely to find a codeword that is jointly strongly typical with the given source sequence.  

3. The codeword \( U^n(s) \) is jointly typical with \( y^n \) but not with \( x^n \). But by Lemma 15.8.1, the probability of this is small since \( X \to Y \to U \) forms a Markov chain.  

4. We also have an error if there exists another typical \( X^n \in B(i) \) which is jointly typical with \( U^n(s) \). The probability that any other \( X^n \) is jointly typical with \( U^n(s) \) is less than \( 2^{-n(I(U;X) - 3\varepsilon)} \), and therefore the probability of this kind of error is bounded above by  
\[ |B(i) \cap A_{\varepsilon}^{(n)}(X)|2^{-n(I(X;U) - 3\varepsilon)} \leq 2^{n(H(X) + \varepsilon)}2^{-n(I(X;U) - 3\varepsilon)} \]  
(15.286)  
which goes to 0 if \( R_1 > H(X|U) \).  

Hence, it is likely that the actual source sequence \( X^n \) is jointly typical with \( U^n(s) \) and that no other typical source sequence in the same bin is also jointly typical with \( U^n(s) \). We can achieve an arbitrarily low probability of error with an appropriate choice of \( n \) and \( \varepsilon \), and this completes the proof of achievability.  

---

### Page 7

#### 15.9 Rate Distortion with Side Information

We know that \( R(D) \) bits are sufficient to describe \( X \) within distortion \( D \). We now ask how many bits are required given side information \( Y \).  

We begin with a few definitions. Let \( (X_i, Y_i) \) be i.i.d. \( \sim p(x, y) \) and encoded as shown in Figure 15.33.  

**Definition**  
The rate distortion function with side information \( R_Y(D) \) is defined as the minimum rate required to achieve distortion \( D \) if the side information \( Y \) is available to the decoder. Precisely, \( R_Y(D) \) is the infimum of rates \( R \) such that there exist maps \( \phi_n : X^n \to \{1, \dots, 2^{nR}\} \), \( \gamma_n : Y^n \times \{1, \dots, 2^{nR}\} \to \hat{X}^n \) such that  
\[ \limsup_{n \to \infty} \mathbb{E}[d(X^n, \gamma_n(Y^n, \phi_n(X^n)))] \leq D. \]  
(15.287)  

Clearly, since the side information can only help, we have \( R_Y(D) \leq R(D) \). For the case of zero distortion, this is the Slepian–Wolf problem, and we will need \( H(X|Y) \) bits. Hence, \( R_Y(0) = H(X|Y) \). We wish to determine the entire curve \( R_Y(D) \). The result can be expressed in the following theorem.  

**Theorem 15.9.1** *(Rate distortion with side information (Wyner and Ziv))*  
Let \( (X, Y) \) be drawn i.i.d. \( \sim p(x, y) \) and let \( d(x^n, \hat{x}^n) = \frac{1}{n} \sum_{i=1}^n d(x_i, \hat{x}_i) \) be given. The rate distortion function with side information is  
\[ R_Y(D) = \min_{p(w|x)} \min_{f} [I(X; W) - I(Y; W)] \]  
(15.288)  
where the minimization is over all functions \( f : Y \times W \to \hat{X} \) and conditional probability mass functions \( p(w|x) \), \( |W| \leq |X| + 1 \), such that  
\[ \sum_{x,y,w} p(x, y)p(w|x)d(x, f(y, w)) \leq D. \]  
(15.289)  

The function \( f \) in the theorem corresponds to the decoding map that maps the encoded version of the \( X \) symbols and the side information \( Y \) to the output alphabet. We minimize over all conditional distributions on \( W \) and functions \( f \) such that the expected distortion for the joint distribution is less than \( D \).  

We first prove the converse after considering some of the properties of the function \( R_Y(D) \) defined in (15.288).



# NETWORK INFORMATION THEORY

---

## LEMMA 15.9.1

The rate distortion function with side information, \( R_Y(D) \), defined in (15.288), is a nonincreasing convex function of \( D \).

**Proof:**  
The monotonicity of \( R_Y(D) \) follows immediately from the fact that the domain of minimization in the definition of \( R_Y(D) \) increases with \( D \). As in the case of rate distortion without side information, we expect \( R_Y(D) \) to be convex. However, the proof of convexity is more involved because of the double rather than single minimization in the definition of \( R_Y(D) \) in (15.288). We outline the proof here.

Let \( D_1 \) and \( D_2 \) be two values of the distortion, and let \( W_1, f_1 \) and \( W_2, f_2 \) be the corresponding random variables and functions that achieve the minima in the definitions of \( R_Y(D_1) \) and \( R_Y(D_2) \), respectively. Let \( Q \) be a random variable independent of \( X, Y, W_1, \) and \( W_2 \), which takes on the value 1 with probability \( \lambda \) and the value 2 with probability \( 1 - \lambda \). Define \( W = (Q, W_Q) \) and let \( f(W, Y) = f_Q(W_Q, Y) \). Specifically, \( f(W, Y) = f_1(W_1, Y) \) with probability \( \lambda \) and \( f(W, Y) = f_2(W_2, Y) \) with probability \( 1 - \lambda \). Then the distortion becomes:

\[
D = E[d(X, \hat{X})]
\]

\[
= \lambda E[d(X, f_1(W_1, Y))] + (1 - \lambda) E[d(X, f_2(W_2, Y))]
\]

\[
= \lambda D_1 + (1 - \lambda) D_2,
\]

and (15.288) becomes:

\[
I(W; X) - I(W; Y) = H(X) - H(X|W) - H(Y) + H(Y|W)
\]

\[
= H(X) - \lambda H(X|W_1) - (1 - \lambda) H(X|W_2) - H(Y) + \lambda H(Y|W_1) + (1 - \lambda) H(Y|W_2)
\]

\[
= \lambda (I(W_1; X) - I(W_1; Y)) + (1 - \lambda) (I(W_2; X) - I(W_2; Y)),
\]

and hence:

\[
R_Y(D) = \min_{U: E[d] \leq D} (I(U; X) - I(U; Y))
\]

\[
\leq I(W; X) - I(W; Y)
\]

\[
= \lambda (I(W_1; X) - I(W_1; Y)) + (1 - \lambda) (I(W_2; X) - I(W_2; Y))
\]

\[
= \lambda R_Y(D_1) + (1 - \lambda) R_Y(D_2),
\]

proving the convexity of \( R_Y(D) \).

---

## CONVERSE TO THE CONDITIONAL RATE DISTORTION THEOREM

**Proof:**  
Consider any rate distortion code with side information. Let the encoding function be \( f_n: X^n \to \{1, 2, \dots, 2^{nR}\} \). Let the decoding function be \( g_n: Y^n \times \{1, 2, \dots, 2^{nR}\} \to \hat{X}^n \), and let \( g_{n,i}: Y^n \times \{1, 2, \dots, 2^{nR}\} \to \hat{X} \) denote the \( i \)-th symbol produced by the decoding function. Let \( T = f_n(X^n) \) denote the encoded version of \( X^n \). We must show that if \( E[d(X^n, g_n(Y^n, f_n(X^n)))] \leq D \), then \( R \geq R_Y(D) \).

We have the following chain of inequalities:

\[
nR \geq H(T)
\]

\[
\geq H(T | Y^n)
\]

\[
\geq I(X^n; T | Y^n)
\]

\[
= \sum_{i=1}^n I(X_i; T | Y^n, X^{i-1})
\]

\[
= \sum_{i=1}^n H(X_i | Y^n, X^{i-1}) - H(X_i | T, Y^n, X^{i-1})
\]

\[
= \sum_{i=1}^n H(X_i | Y_i) - H(X_i | T, Y_{i-1}, Y_i, Y_{i+1}^n, X^{i-1})
\]

\[
\geq \sum_{i=1}^n H(X_i | Y_i) - H(X_i | T, Y_{i-1}, Y_i, Y_{i+1}^n)
\]

\[
= \sum_{i=1}^n H(X_i | Y_i) - H(X_i | W_i, Y_i)
\]

\[
= \sum_{i=1}^n I(X_i; W_i | Y_i)
\]

\[
= \sum_{i=1}^n H(W_i | Y_i) - H(W_i | X_i, Y_i)
\]

\[
= \sum_{i=1}^n H(W_i) - H(W_i | X_i) - H(W_i) + H(W_i | Y_i)
\]

\[
= \sum_{i=1}^n I(W_i; X_i) - I(W_i; Y_i)
\]

\[
\geq \sum_{i=1}^n R_Y(E[d(X_i, g_{n,i}(W_i, Y_i)]))
\]

\[
= n \cdot \frac{1}{n} \sum_{i=1}^n R_Y(E[d(X_i, g_{n,i}(W_i, Y_i)]))
\]

\[
\geq n R_Y\left(\frac{1}{n} \sum_{i=1}^n E[d(X_i, g_{n,i}(W_i, Y_i)]}\right)
\]

\[
\geq n R_Y(D),
\]

where:  
(a) follows from the fact that the range of \( T \) is \( \{1, 2, \dots, 2^{nR}\} \);  
(b) follows from the fact that conditioning reduces entropy;  
(c) follows from the chain rule for mutual information;  
(d) follows from the fact that \( X_i \) is independent of the past and future \( Y \)'s and \( X \)'s given \( Y_i \);  
(e) follows from the fact that conditioning reduces entropy;  
(f) follows by defining \( W_i = (T, Y_{i-1}, Y_{i+1}^n) \);  
(g) follows from the definition of mutual information;  
(h) follows from the fact that since \( Y_i \) depends only on \( X_i \) and is conditionally independent of \( T \) and the past and future \( Y \)'s, \( W_i \to X_i \to Y_i \) forms a Markov chain;  
(i) follows from the definition of the conditional rate distortion function since \( \hat{X}_i = g_{n,i}(T, Y^n) \triangleq g_{n,i}(W_i, Y_i) \), and hence \( I(W_i; X_i) - I(W_i; Y_i) \geq \min_{W: E[d] \leq D_i} I(W; X) - I(W; Y) = R_Y(D_i) \);  
(j) follows from Jensen's inequality and the convexity of the conditional rate distortion function (Lemma 15.9.1);  
(k) follows from the definition of \( D = E[\frac{1}{n} \sum_{i=1}^n d(X_i, \hat{X}_i)] \).

---

It is easy to see the parallels between this converse and the converse for rate distortion without side information (Section 10.4). The proof of achievability is also parallel to the proof of the rate distortion theorem using strong typicality. However, instead of sending the index of the codeword that is jointly typical with the source, we divide these codewords into bins and send the bin index instead. If the number of codewords in each bin is small enough, the side information can be used to isolate the particular codeword in the bin at the receiver. Hence, again, we are combining random binning with rate distortion encoding to find a jointly typical reproduction codeword. We outline the details of the proof below.

---

## ACHIEVABILITY OF THEOREM 15.9.1

**Proof:**  
Fix \( p(w|x) \) and the function \( f(w, y) \). Calculate \( p(w) = \sum_x p(x)p(w|x) \).

**Generation of codebook:**  
Let \( R_1 = I(X; W) + \epsilon \). Generate \( 2^{nR_1} \) i.i.d. codewords \( W^n(s) \sim p(w_1)p(w_2)\dots p(w_n) \), and index them by \( s \in \{1, 2, \dots, 2^{nR_1}\} \). Let \( R_2 = I(X; W) - I(Y; W) + 5\epsilon \). Randomly assign the indices \( s \in \{1, 2, \dots, 2^{nR_1}\} \) to one of \( 2^{nR_2} \) bins using a uniform distribution over the bins. Let \( B(i) \) denote the indices assigned to bin \( i \). There are approximately \( 2^{n(R_1 - R_2)} \) indices in each bin.

**Encoding:**  
Given a source sequence \( X^n \), the encoder looks for a codeword \( W^n(s) \) such that \( (X^n, W^n(s)) \in A_{\epsilon}^{(n)} \). If there is no such \( W^n \), the encoder sets \( s = 1 \). If there is more than one such \( s \), the encoder uses the lowest \( s \). The encoder sends the index of the bin in which \( s \) belongs.

**Decoding:**  
The decoder looks for a \( W^n(s) \) such that \( s \in B(i) \) and \( (W^n(s), Y^n) \in A_{\epsilon}^{(n)} \). If he finds a unique \( s \), he then calculates \( \hat{X}^n \), where \( \hat{X}_i = f(W_i, Y_i) \). If he does not find any such \( s \) or more than one such \( s \), he sets \( \hat{X}^n = \hat{x}^n \), where \( \hat{x}^n \) is an arbitrary sequence in \( \hat{X}^n \). It does not matter which default sequence is used; we will show that the probability of this event is small.

**Analysis of the probability of error:**  
As usual, we have various error events:  
1. The pair \( (X^n, Y^n) \notin A_{\epsilon}^{(n)} \). The probability of this event is small for large enough \( n \) by the weak law of large numbers.  
2. The sequence \( X^n \) is typical, but there does not exist an \( s \) such that \( (X^n, W^n(s)) \in A_{\epsilon}^{(n)} \). As in the proof of the rate distortion theorem, the probability of this event is small if \( R_1 > I(W; X) \).  
3. The pair of sequences \( (X^n, W^n(s)) \in A_{\epsilon}^{(n)} \) but \( (W^n(s), Y^n) \notin A_{\epsilon}^{(n)} \) (i.e., the codeword is not jointly typical with the \( Y^n \) sequence). By the Markov lemma (Lemma 15.8.1), the probability of this event is small if \( n \) is large enough.  
4. There exists another \( s' \) with the same bin index such that \( (W^n(s'), Y^n) \in A_{\epsilon}^{(n)} \). Since the probability that a randomly chosen \( W^n \) is jointly typical with \( Y^n \) is \( \approx 2^{-nI(Y; W)} \), the probability that there is another \( W^n \) in the same bin that is typical with \( Y^n \) is bounded by the number of codewords in the bin times the probability of joint typicality, that is,

\[
Pr(\exists s' \in B(i): (W^n(s'), Y^n) \in A_{\epsilon}^{(n)})
\leq 2^{n(R_1 - R_2)} 2^{-n(I(W; Y) - 3\epsilon)},
\]

which goes to zero since \( R_1 - R_2 < I(Y; W) - 3\epsilon \).  
5. If the index \( s \) is decoded correctly, \( (X^n, W^n(s)) \in A_{\epsilon}^{(n)} \). By item 1, we can assume that \( (X^n, Y^n) \in A_{\epsilon}^{(n)} \). Thus, by the Markov lemma, we have \( (X^n, Y^n, W^n) \in A_{\epsilon}^{(n)} \), and therefore the empirical joint distribution is close to the original distribution \( p(x, y)p(w|x) \) that we started with, and hence \( (X^n, \hat{X}^n) \) will have a joint distribution that is close to the distribution that achieves distortion \( D \).

Hence, with high probability, the decoder will produce \( \hat{X}^n \) such that the distortion between \( X^n \) and \( \hat{X}^n \) is close to \( nD \). This completes the proof of the theorem.

---

The reader is referred to Wyner and Ziv [574] for details of the proof.

After the discussion of the various situations of compressing distributed data, it might be expected that the problem is almost completely solved, but unfortunately, this is not true. An immediate generalization of all the above problems is the rate distortion problem for correlated sources, illustrated in Figure 15.34. This is essentially the Slepian-Wolf problem with distortion in both \( X \) and \( Y \). It is easy to see that the three distributed source coding problems considered above are all special cases of this setup. Unlike the earlier problems, though, this problem has not yet been solved, and the general rate distortion region remains unknown.

---

## GENERAL MULTITERMINAL NETWORKS

We conclude this chapter by considering a general multiterminal network of senders and receivers and deriving some bounds on the rates achievable for communication in such a network. A general multiterminal network is illustrated in Figure 15.35. In this section, superscripts denote node indices and subscripts denote time indices. There are \( m \) nodes, and node \( i \) has an associated transmitted variable \( X(i) \) and a received variable \( Y(i) \).

\[
\boxed{(X(1), Y(1)) \quad (X(m), Y(m))}
\]

FIGURE 15.35. General multiterminal network.



NETWORK INFORMATION THEORY

The node \(i\) sends information at rate \(R(ij)\) to node \(j\). We assume that all the messages \(W(ij)\) being sent from node \(i\) to node \(j\) are independent and uniformly distributed over their respective ranges \(\{1, 2, \ldots, 2^{nR(ij)}\}\). The channel is represented by the channel transition function \(p(y(1), \ldots, y(m)|x(1), \ldots, x(m))\), which is the conditional probability mass function of the outputs given the inputs. This probability transition function captures the effects of the noise and the interference in the network. The channel is assumed to be memoryless (i.e., the outputs at any time instant depend only on the current inputs and are conditionally independent of the past inputs).

Corresponding to each transmitter–receiver node pair is a message \(W(ij) \in \{1, 2, \ldots, 2^{nR(ij)}\}\). The input symbol \(X(i)\) at node \(i\) depends on \(W(ij)\), \(j \in \{1, \ldots, m\}\), and also on the past values of the received symbol \(Y(i)\) at node \(i\). Hence, an encoding scheme of block length \(n\) consists of a set of encoding and decoding functions, one for each node:

- **Encoders**: \(X(i)_k = g_k(W(i1), W(i2), \ldots, W(im), Y(i)_1, Y(i)_2, \ldots, Y(i)_{k-1})\), \(k = 1, \ldots, n\). The encoder maps the messages and past received symbols into the symbol \(X(i)_k\) transmitted at time \(k\).

- **Decoders**: \(\hat{W}(ji) = h_j(Y(i)_1, \ldots, Y(i)_n, W(i1), \ldots, W(im))\), \(j = 1, 2, \ldots, m\). The decoder \(j\) at node \(i\) maps the received symbols in each block and his own transmitted information to form estimates of the messages intended for him from node \(j\), \(j = 1, 2, \ldots, m\).

Associated with every pair of nodes is a rate and a corresponding probability of error that the message will not be decoded correctly,

\[
P_e^{(n)}(ij) = \Pr\left(\hat{W}(ij) \neq W(ij) \mid Y(j), W(j1), \ldots, W(jm)\right),
\]

where \(P_e^{(n)}(ij)\) is defined under the assumption that all the messages are independent and distributed uniformly over their respective ranges. A set of rates \(\{R(ij)\}\) is said to be achievable if there exist encoders and decoders with block length \(n\) with \(P_e^{(n)}(ij) \to 0\) as \(n \to \infty\) for all \(i, j \in \{1, 2, \ldots, m\}\). We use this formulation to derive an upper bound on the flow of information in any multiterminal network. We divide the nodes into two sets, \(S\) and the complement \(S^c\). We now bound the rate of flow of information from nodes in \(S\) to nodes in \(S^c\). See [514].

---

### Theorem 15.10.1

If the information rates \(\{R(ij)\}\) are achievable, there exists some joint probability distribution \(p(x(1), x(2), \ldots, x(m))\) such that

\[
\sum_{i \in S, j \in S^c} R(ij) \leq I(X(S); Y(S^c) \mid X(S^c))
\]

for all \(S \subset \{1, 2, \ldots, m\}\). Thus, the total rate of flow of information across cut sets is bounded by the conditional mutual information.

**Proof:**

The proof follows the same lines as the proof of the converse for the multiple access channel. Let \(T = \{(i, j) : i \in S, j \in S^c\}\) be the set of links that cross from \(S\) to \(S^c\), and let \(T^c\) be all the other links in the network. Then

\[
n \sum_{i \in S, j \in S^c} R(ij)
\]

(a) \[
= \sum_{i \in S, j \in S^c} H(W(ij))
\]

(b) \[
= H(W(T))
\]

(c) \[
= H(W(T) \mid W(T^c))
\]

(d) \[
= I(W(T); Y(S^c)_1, \ldots, Y(S^c)_n \mid W(T^c))
\]

(e) \[
= \sum_{k=1}^n I(W(T); Y(S^c)_k \mid Y(S^c)_1, \ldots, Y(S^c)_{k-1}, W(T^c)))
\]

(f) \[
= \sum_{k=1}^n \left[H(Y(S^c)_k \mid Y(S^c)_1, \ldots, Y(S^c)_{k-1}, W(T^c)) - H(Y(S^c)_k \mid Y(S^c)_1, \ldots, Y(S^c)_{k-1}, W(T^c), W(T))\right]
\]

(g) \[
\leq \sum_{k=1}^n \left[H(Y(S^c)_k \mid Y(S^c)_1, \ldots, Y(S^c)_{k-1}, W(T^c), X(S^c)_k) - H(Y(S^c)_k \mid Y(S^c)_1, \ldots, Y(S^c)_{k-1}, W(T^c), W(T), X(S)_k, X(S^c)_k)\right]
\]

(h) \[
\leq \sum_{k=1}^n \left[H(Y(S^c)_k \mid X(S^c)_k) - H(Y(S^c)_k \mid X(S^c)_k, X(S)_k)\right]
\]

(i) \[
= \sum_{k=1}^n I(X(S)_k; Y(S^c)_k \mid X(S^c)_k)
\]

(j) \[
= n I(X(S)_Q; Y(S^c)_Q \mid X(S^c)_Q, Q)
\]

(k) \[
= n \left[H(Y(S^c)_Q \mid X(S^c)_Q, Q) - H(Y(S^c)_Q \mid X(S)_Q, X(S^c)_Q, Q)\right]
\]

(l) \[
\leq n \left[H(Y(S^c)_Q \mid X(S^c)_Q) - H(Y(S^c)_Q \mid X(S)_Q, X(S^c)_Q)\right]
\]

\[
= n I(X(S)_Q; Y(S^c)_Q \mid X(S^c)_Q),
\]

where \(Q\) is a timesharing random variable. Thus, there exist random variables \(X(S)\) and \(X(S^c)\) with some arbitrary joint distribution that satisfy the inequalities of the theorem.

The theorem has a simple max-flow min-cut interpretation. The rate of flow of information across any boundary is less than the mutual information between the inputs on one side of the boundary and the outputs on the other side, conditioned on the inputs on the other side.

The problem of information flow in networks would be solved if the bounds of the theorem were achievable. But unfortunately, these bounds are not achievable even for some simple channels. We now apply these bounds to a few of the channels that we considered earlier.

- **Multiple-access channel.** The multiple-access channel is a network with many input nodes and one output node. For the case of a two-user multiple-access channel, the bounds of Theorem 15.10.1 reduce to

\[
R_1 \leq I(X_1; Y \mid X_2),
\]

\[
R_2 \leq I(X_2; Y \mid X_1),
\]

\[
R_1 + R_2 \leq I(X_1, X_2; Y),
\]

for some joint distribution \(p(x_1, x_2)p(y \mid x_1, x_2)\). These bounds coincide with the capacity region if we restrict the input distribution to be a product distribution and take the convex hull (Theorem 15.3.1).

- **Relay channel.** For the relay channel, these bounds give the upper bound of Theorem 15.7.1 with different choices of subsets as shown in Figure 15.36. Thus,

\[
C \leq \sup_{p(x,x_1)} \min \{I(X, X_1; Y), I(X; Y, Y_1 \mid X_1)\}.
\]

This upper bound is the capacity of a physically degraded relay channel and for the relay channel with feedback [127].

---

### Figure 15.36. Relay channel.

---

To complement our discussion of a general network, we should mention two features of single-user channels that do not apply to a multiuser network.

- **Source–channel separation theorem.** In Section 7.13, we discussed the source–channel separation theorem, which proves that we can transmit the source noiselessly over the channel if and only if the entropy rate is less than the channel capacity. This allows us to characterize a source by a single number (the entropy rate) and the channel by a single number (the capacity). What about the multiuser case? We would expect that a distributed source could be transmitted over a channel if and only if the rate region for the noiseless coding of the source lay within the capacity region of the channel. To be specific, consider the transmission of a distributed source over a multiple-access channel, as shown in Figure 15.37. Combining the results of Slepian–Wolf encoding with the capacity results for the multiple-access channel, we can show that we can transmit the source over the channel and recover it with a low probability of error if

\[
H(U \mid V) \leq I(X_1; Y \mid X_2, Q),
\]

\[
H(V \mid U) \leq I(X_2; Y \mid X_1, Q),
\]

\[
H(U, V) \leq I(X_1, X_2; Y \mid Q),
\]

for some distribution \(p(q)p(x_1 \mid q)p(x_2 \mid q)p(y \mid x_1, x_2)\). This condition is equivalent to saying that the Slepian–Wolf rate region of the source has a nonempty intersection with the capacity region of the multiple-access channel.

But is this condition also necessary? No, as a simple example illustrates. Consider the transmission of the source of Example 15.4.2 over the binary erasure multiple-access channel (Example 15.3.3). The Slepian–Wolf region does not intersect the capacity region, yet it is simple to devise a scheme that allows the source to be transmitted over the channel. We just let \(X_1 = U\) and \(X_2 = V\), and the value of \(Y\) will tell us the pair \((U, V)\) with no error. Thus, the conditions (15.345) are not necessary.

The reason for the failure of the source–channel separation theorem lies in the fact that the capacity of the multiple-access channel increases with the correlation between the inputs of the channel. Therefore, to maximize the capacity, one should preserve the correlation between the inputs of the channel. Slepian–Wolf encoding, on the other hand, gets rid of the correlation. Cover et al. [129] proposed an achievable region for transmission of a correlated source over a multiple-access channel based on the idea of preserving the correlation. Han and Costa [273] have proposed a similar region for the transmission of a correlated source over a broadcast channel.

- **Capacity regions with feedback.** Theorem 7.12.1 shows that feedback does not increase the capacity of a single-user discrete memoryless channel. For channels with memory, on the other hand, feedback enables the sender to predict something about the noise and to combat it more effectively, thus increasing capacity.

What about multiuser channels? Rather surprisingly, feedback does increase the capacity region of multiuser channels, even when the channels are memoryless. This was first shown by Gaarder and Wolf [220], who showed how feedback helps increase the capacity of the binary erasure multiple-access channel. In essence, feedback from the receiver to the two senders acts as a separate channel between the two senders. The senders can decode each other’s transmissions before the receiver does. They then cooperate to resolve the uncertainty at the receiver, sending information at the higher cooperative capacity rather than the noncooperative capacity. Using this scheme, Cover and Leung [133] established an achievable region for a multiple-access channel with feedback. Willems [557] showed that this region was the capacity for a class of multiple-access channels that included the binary erasure multiple-access channel. Ozarow [410] established the capacity region for a two-user Gaussian multiple-access channel. The problem of finding the capacity region for a multiple-access channel with feedback is closely related to the capacity of a two-way channel with a common output.

There is as yet no unified theory of network information flow. But there can be no doubt that a complete theory of communication networks would have wide implications for the theory of communication and computation.

---

### Summary

- **Multiple-access channel.** The capacity of a multiple-access channel \((\mathcal{X}_1 \times \mathcal{X}_2, p(y \mid x_1, x_2), \mathcal{Y})\) is the closure of the convex hull of all \((R_1, R_2)\) satisfying

\[
R_1 < I(X_1; Y \mid X_2),
\]

\[
R_2 < I(X_2; Y \mid X_1),
\]

\[
R_1 + R_2 < I(X_1, X_2; Y),
\]

for some distribution \(p_1(x_1)p_2(x_2)\) on \(\mathcal{X}_1 \times \mathcal{X}_2\).

The capacity region of the \(m\)-user multiple-access channel is the closure of the convex hull of the rate vectors satisfying

\[
R(S) \leq I(X(S); Y \mid X(S^c))
\]

for all \(S \subseteq \{1, 2, \ldots, m\}\) for some product distribution \(p_1(x_1)p_2(x_2) \cdots p_m(x_m)\).

- **Gaussian multiple-access channel.** The capacity region of a two-user Gaussian multiple-access channel is

\[
R_1 \leq \mathcal{C}\left(\frac{P_1}{N}\right),
\]

\[
R_2 \leq \mathcal{C}\left(\frac{P_2}{N}\right),
\]

\[
R_1 + R_2 \leq \mathcal{C}\left(\frac{P_1 + P_2}{N}\right).
\]



SUMMARY

595  
The Slepian–Wolf coding for correlated sources X and Y states that they can be described separately at rates R1 and R2 and recovered with low probability of error if:  
R1 ≥ H(X|Y),  
R2 ≥ H(Y|X),  
R1 + R2 ≥ H(X, Y).  

For broadcast channels with a degraded channel X → Y1 → Y2, the capacity region is the convex hull of all (R1, R2) satisfying:  
R2 ≤ I(U; Y2),  
R1 ≤ I(X; Y1|U),  
for some joint distribution p(u)p(x|u)p(y1, y2|x).  

The capacity of a physically degraded relay channel is given by:  
C = sup min {I(X, X1; Y), I(X; Y1|X1)},  
where the supremum is over all joint distributions on X × X1.  

For source coding with side information, if Y is encoded at rate R2 and X at rate R1, recovery is possible if:  
R1 ≥ H(X|U),  
R2 ≥ I(Y; U),  
for some distribution p(y, u) such that X → Y → U.  

The rate distortion function with side information is:  
RY(D) = min I(X; W) − I(Y; W),  
where the minimization is over functions f and conditional distributions p(w|x) such that E[d(x, f(y, w))] ≤ D.  

PROBLEMS  

15.1 Cooperative capacity of a multiple-access channel  
(a) Find the capacity region when codewords depend on both indices.  
(b) Evaluate for the binary erasure channel and compare to the non-cooperative region.  

15.2 Capacity of multiple-access channels  
(a) Additive modulo 2 channel: Y = X1 ⊕ X2.  
(b) Multiplicative channel: Y = X1 · X2.  

15.3 Cut-set interpretation of multiple-access channel capacity  
Show that I(X1; Y|X2) = I(X1; Y, X2) and interpret the bounds as flow rates across cut sets.  

15.4 Gaussian multiple-access channel capacity  
Prove achievability of rates satisfying:  
R1 < 1/2 log(1 + P1/N),  
R2 < 1/2 log(1 + P2/N),  
R1 + R2 < 1/2 log(1 + P1 + P2/N).  

15.5 Converse for Gaussian multiple-access channel  
Extend the discrete case converse to account for power constraints.  

15.6 Unusual multiple-access channel  
(a) Show (1,0) and (0,1) are achievable.  
(b) Show I(X1, X2; Y) < 1 for non-degenerate distributions.  
(c) Argue that time-sharing enlarges the capacity region strictly.  

15.7 Convexity of broadcast channel capacity region  
Show that if R(1) and R(2) are achievable, then λR(1) + (1 − λ)R(2) is achievable.  

15.8 Slepian–Wolf for deterministically related sources  
Find the rate region for sources where Y is a deterministic function of X.  

15.9 Slepian–Wolf with Bernoulli sources  
Determine the rate region for Xi ∼ Bernoulli(p), Zi ∼ Bernoulli(r), and Y = X ⊕ Z.  

15.10 Broadcast capacity dependence on marginals  
Show the capacity region depends only on p(y1|x) and p(y2|x).  

15.11 Converse for degraded broadcast channel  
Justify the inequalities in the converse proof and show the region equals the convex closure of regions defined by R1 ≤ I(X; Y1|U) and R2 ≤ I(U; Y2).  

15.12 Capacity points  
(a) Find points where the capacity region meets the R1 and R2 axes.  
(b) Show that the R2-axis point is less than the R1-axis point.  

15.13 Degraded broadcast channel capacity  
Find the capacity region for the given channel.  

15.14 Channels with unknown parameters  
Devise codes for a binary symmetric channel with unknown p, achieving rates ≈C(p1) or C(p2).  

15.15 Two-way channel  
(a) Show the achievable rate region using independent codes.  
(b) Prove the converse for the two-way channel.  

15.16 Multiple-access channel  
(a) Find the capacity region for Y = X1 + sgn(X2).  
(b) Describe a coding scheme achieving the capacity region.



PROBLEMS

---

### 15.17 Slepian–Wolf

Let \((X, Y)\) have the joint probability mass function \(p(x, y)\):

\[
\begin{array}{c|ccc}
p(x, y) & 1 & 2 & 3 \\
\hline
1 & \alpha & \beta & \beta \\
2 & \beta & \alpha & \beta \\
3 & \beta & \beta & \alpha \\
\end{array}
\]
where \(\beta = 1 - \alpha\). (Note: This is a joint, not a conditional, probability mass function.)

(a) Find the Slepian–Wolf rate region for this source.

(b) What is \(\Pr\{X = Y\}\) in terms of \(\alpha\)?

(c) What is the rate region if \(\alpha = \frac{1}{3}\)?

(d) What is the rate region if \(\alpha = \frac{1}{9}\)?

---

### 15.18 Square Channel

What is the capacity of the following multiple-access channel?

\[
X_1 \in \{-1, 0, 1\}, \quad X_2 \in \{-1, 0, 1\}, \quad Y = X_1 + X_2^2.
\]

(a) Find the capacity region.

(b) Describe \(p^*(x_1)\), \(p^*(x_2)\) achieving a point on the boundary of the capacity region.

---

### 15.19 Slepian–Wolf

Two senders know random variables \(U_1\) and \(U_2\), respectively. Let the random variables \((U_1, U_2)\) have the following joint distribution:

\[
\begin{array}{c|cccc}
U_1 \backslash U_2 & 0 & 1 & 2 & \cdots & m-1 \\
\hline
0 & \alpha & \beta & \beta & \cdots & \beta \\
1 & \gamma & \beta & \beta & \cdots & \beta \\
2 & 0 & \gamma & \beta & \cdots & \beta \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
m-1 & \beta & \beta & \beta & \cdots & \alpha \\
\end{array}
\]
where \(\alpha + \beta + \gamma = 1\). Find the region of rates \((R_1, R_2)\) that would allow a common receiver to decode both random variables reliably.

---

### 15.20 Multiple Access

(a) Find the capacity region for the multiple-access channel \(Y = X_1 + X_2\), where \(X_1 \in \{2, 4\}\), \(X_2 \in \{1, 2\}\).

(b) Suppose that the range of \(X_1\) is \(\{1, 2\}\). Is the capacity region decreased? Why or why not?

---

### 15.21 Broadcast Channel

Consider the following degraded broadcast channel:

\[
\begin{array}{cccccc}
& & 1 - a_1 & & a_1 & \\
& \swarrow & & \searrow & \\
X & & Y_1 & & Y_2 \\
& & 1 - a_2 & & a_2 \\
& \swarrow & & \searrow & \\
& E & & E & \\
\end{array}
\]

(a) What is the capacity of the channel from \(X\) to \(Y_1\)?

(b) What is the channel capacity from \(X\) to \(Y_2\)?

(c) What is the capacity region of all \((R_1, R_2)\) achievable for this broadcast channel? Simplify and sketch.

---

### 15.22 Stereo

The sum and the difference of the right and left ear signals are to be individually compressed for a common receiver. Let \(Z_1\) be Bernoulli\((p_1)\) and \(Z_2\) be Bernoulli\((p_2)\) and suppose that \(Z_1\) and \(Z_2\) are independent. Let \(X = Z_1 + Z_2\), and \(Y = Z_1 - Z_2\).

(a) What is the Slepian–Wolf rate region of achievable \((R_X, R_Y)\)?

(b) Is this larger or smaller than the rate region of \((R_{Z_1}, R_{Z_2})\)? Why?

---

### 15.23 Multiplicative Multiple-Access Channel

Find and sketch the capacity region of the following multiplicative multiple-access channel:

\[
X_1 \in \{0, 1\}, \quad X_2 \in \{1, 2, 3\}, \quad Y = X_1 X_2.
\]

---

### 15.24 Distributed Data Compression

Let \(Z_1, Z_2, Z_3\) be independent Bernoulli\((p)\). Find the Slepian–Wolf rate region for the description of \((X_1, X_2, X_3)\), where:

\[
X_1 = Z_1, \quad X_2 = Z_1 + Z_2, \quad X_3 = Z_1 + Z_2 + Z_3.
\]

---

### 15.25 Noiseless Multiple-Access Channel

Consider the following multiple-access channel with two binary inputs \(X_1, X_2 \in \{0, 1\}\) and output \(Y = (X_1, X_2)\).

(a) Find the capacity region. Note that each sender can send at capacity.

(b) Now consider the cooperative capacity region, \(R_1 \geq 0\), \(R_2 \geq 0\), \(R_1 + R_2 \leq \max_{p(x_1, x_2)} I(X_1, X_2; Y)\). Argue that the throughput \(R_1 + R_2\) does not increase but the capacity region increases.

---

### 15.26 Infinite Bandwidth Multiple-Access Channel

Find the capacity region for the Gaussian multiple-access channel with infinite bandwidth. Argue that all senders can send at their individual capacities (i.e., infinite bandwidth eliminates interference).

---

### 15.27 Multiple-Access Identity

Let \(C(x) = \frac{1}{2} \log(1 + x)\) denote the channel capacity of a Gaussian channel with signal-to-noise ratio \(x\). Show that:

\[
C\left(\frac{P_1}{N}\right) + C\left(\frac{P_2}{P_1 + N}\right) = C\left(\frac{P_1 + P_2}{N}\right).
\]

This suggests that two independent users can send information as well as if they had pooled their power.

---

### 15.28 Frequency-Division Multiple Access (FDMA)

Maximize the throughput \(R_1 + R_2 = W_1 \log\left(1 + \frac{P_1}{N W_1}\right) + (W - W_1) \log\left(1 + \frac{P_2}{N (W - W_1)}\right)\) over \(W_1\) to show that bandwidth should be proportional to transmitted power for FDMA.

---

### 15.29 Trilingual-Speaker Broadcast Channel

A speaker of Dutch, Spanish, and French wishes to communicate simultaneously to three people: D, S, and F. D knows only Dutch but can distinguish when a Spanish word is being spoken as distinguished from a French word; similarly for the other two, who know only Spanish and French, respectively, but can distinguish when a foreign word is spoken and which language is being spoken. Suppose that each language, Dutch, Spanish, and French, has \(M\) words: \(M\) words of Dutch, \(M\) words of French, and \(M\) words of Spanish.

(a) What is the maximum rate at which the trilingual speaker can speak to D?

(b) If he speaks to D at the maximum rate, what is the maximum rate at which he can speak simultaneously to S?

(c) If he is speaking to D and S at the joint rate in part (b), can he also speak to F at some positive rate? If so, what is it? If not, why not?

---

### 15.30 Parallel Gaussian Channels from a Mobile Telephone

Assume that a sender \(X\) is sending to two fixed base stations. Assume that the sender sends a signal \(X\) that is constrained to have average power \(P\). Assume that the two base stations receive signals \(Y_1\) and \(Y_2\), where:

\[
Y_1 = \alpha_1 X + Z_1, \quad Y_2 = \alpha_2 X + Z_2,
\]

where \(Z_i \sim \mathcal{N}(0, N_1)\), \(Z_2 \sim \mathcal{N}(0, N_2)\), and \(Z_1\) and \(Z_2\) are independent. We will assume the \(\alpha\)'s are constant over a transmitted block.

(a) Assuming that both signals \(Y_1\) and \(Y_2\) are available at a common decoder \(Y = (Y_1, Y_2)\), what is the capacity of the channel from the sender to the common receiver?

(b) If, instead, the two receivers \(Y_1\) and \(Y_2\) each decode their signals independently, this becomes a broadcast channel. Let \(R_1\) be the rate to base station 1 and \(R_2\) be the rate to base station 2. Find the capacity region of this channel.

---

### 15.31 Gaussian Multiple Access

A group of \(m\) users, each with power \(P\), is using a Gaussian multiple-access channel at capacity, so that:

\[
\sum_{i=1}^m R_i = C\left(\frac{mP}{N}\right),
\]

where \(C(x) = \frac{1}{2} \log(1 + x)\) and \(N\) is the receiver noise power. A new user of power \(P_0\) wishes to join in.

(a) At what rate can he send without disturbing the other users?

(b) What should his power \(P_0\) be so that the new user's rate is equal to the combined communication rate \(C\left(\frac{mP}{N}\right)\) of all the other users?

---

### 15.32 Converse for Deterministic Broadcast Channel

A deterministic broadcast channel is defined by an input \(X\) and two outputs, \(Y_1\) and \(Y_2\), which are functions of the input \(X\). Thus, \(Y_1 = f_1(X)\) and \(Y_2 = f_2(X)\). Let \(R_1\) and \(R_2\) be the rates at which information can be sent to the two receivers. Prove that:

\[
R_1 \leq H(Y_1),
\]
\[
R_2 \leq H(Y_2),
\]
\[
R_1 + R_2 \leq H(Y_1, Y_2).
\]

---

### 15.33 Multiple-Access Channel

Consider the multiple-access channel \(Y = X_1 + X_2 \pmod{4}\), where \(X_1 \in \{0, 1, 2, 3\}\), \(X_2 \in \{0, 1\}\).

(a) Find the capacity region \((R_1, R_2)\).

(b) What is the maximum throughput \(R_1 + R_2\)?

---

### 15.34 Distributed Source Compression

Let \(Z_1\) and \(Z_2\) be independent Bernoulli trials with parameters \(p\) and \(q\), respectively. Let \(U = Z_1 Z_2\), \(V = Z_1 + Z_2\). Assume that \(Z_1\) and \(Z_2\) are independent. This induces a joint distribution on \((U, V)\). Let \((U_i, V_i)\) be i.i.d. according to this distribution. Sender 1 describes \(U^n\) at rate \(R_1\), and sender 2 describes \(V^n\) at rate \(R_2\).

(a) Find the Slepian–Wolf rate region for recovering \((U^n, V^n)\) at the receiver.

(b) What is the residual uncertainty (conditional entropy) that the receiver has about \((X^n, Y^n)\)?

---

### 15.35 Multiple-Access Channel Capacity with Costs

The cost of using symbol \(x\) is \(r(x)\). The cost of a codeword \(x^n\) is \(r(x^n) = \frac{1}{n} \sum_{i=1}^n r(x_i)\). A \((2^n R, n)\) codebook satisfies cost constraint \(r\) if \(\frac{1}{n} \sum_{i=1}^n r(x_i(w)) \leq r\) for all \(w \in 2^{nR}\).

(a) Find an expression for the capacity \(C(r)\) of a discrete memoryless channel with cost constraint \(r\).

(b) Find an expression for the multiple-access channel capacity region for \((X_1 \times X_2, p(y|x_1, x_2), Y)\) if sender \(X_1\) has cost constraint \(r_1\) and sender \(X_2\) has cost constraint \(r_2\).

(c) Prove the converse for part (b).

---

### 15.36 Slepian–Wolf

Three cards from a three-card deck are dealt, one to sender \(X_1\), one to sender \(X_2\), and one to sender \(X_3\). At what rates do \(X_1\), \(X_2\), and \(X_3\) need to communicate to some receiver so that their card information can be recovered?

Assume that \((X_{1i}, X_{2i}, X_{3i})\) are drawn i.i.d. from a uniform distribution over the permutations of \(\{1, 2, 3\}\).

---

HISTORICAL NOTES

This chapter is based on the review in El Gamal and Cover [186]. The two-way channel was studied by Shannon [486] in 1961. He derived inner and outer bounds on the capacity region. Dueck [175] and Schalkwijk [464, 465] suggested coding schemes for two-way channels that achieve rates exceeding Shannon’s inner bound; outer bounds for this channel were derived by Zhang et al. [596] and Willems and Hekstra [558].

The multiple-access channel capacity region was found by Ahlswede [7] and Liao [355] and was extended to the case of the multiple-access channel with common information by Slepian and Wolf [501]. Gaarder and Wolf [220] were the first to show that feedback increases the capacity of a discrete memoryless multiple-access channel. Cover and Leung [133] proposed an achievable region for the multiple-access channel with feedback, which was shown to be optimal for a class of multiple-access channels by Willems [557]. Ozarow [410] has determined the capacity region for a two-user Gaussian multiple-access channel with feedback. Cover et al. [129] and Ahlswede and Han [12] have considered the problem of transmission of a correlated source over a multiple-access channel.

The Slepian–Wolf theorem was proved by Slepian and Wolf [502] and was extended to jointly ergodic sources by a binning argument in Cover [122]. Superposition coding for broadcast channels was suggested by Cover in 1972 [119]. The capacity region for the degraded broadcast channel was determined by Bergmans [55] and Gallager [225]. The superposition codes for the degraded broadcast channel are also optimal for the less noisy broadcast channel (Körner and Marton [324]), the more capable broadcast channel (El Gamal [185]), and the broadcast channel with degraded message sets (Körner and Marton [325]). Van der Meulen [526] and Cover [121] proposed achievable regions for the general broadcast channel. The capacity of a deterministic broadcast channel was found by Gelfand and Pinsker [242, 243, 423] and Marton [377]. The best known achievable region for the broadcast channel is due to Marton [377]; a simpler proof of Marton’s region was given by El Gamal and Van der Meulen [188]. El Gamal [184] showed that feedback does not increase the capacity of a physically degraded broadcast channel. Dueck [176] introduced an example to illustrate that feedback can increase the capacity of a memoryless broadcast channel; Ozarow and Leung [411] described a coding procedure for the Gaussian broadcast channel with feedback that increased the capacity region.

The relay channel was introduced by Van der Meulen [528]; the capacity region for the degraded relay channel was determined by Cover and El Gamal [127]. Carleial [85] introduced the Gaussian interference channel with power constraints and showed that very strong interference is equivalent to no interference at all. Sato and Tanabe [459] extended the work of Carleial to discrete interference channels with strong interference. Sato [457] and Benzel [51] dealt with degraded interference channels. The best known achievable region for the general interference channel is due to Han and Kobayashi [274]. This region gives the capacity for Gaussian interference channels with interference parameters greater than 1, as was shown in Han and Kobayashi [274] and Sato [458]. Carleial [84] proved new bounds on the capacity region for interference channels.

The problem of coding with side information was introduced by Wyner and Ziv [573] and Wyner [570]; the achievable region for this problem was described in Ahlswede and Körner [13], Gray and Wyner [261], and Wyner [571, 572]. The problem of finding the rate distortion function with side information was solved by Wyner and Ziv [574]. The channel capacity counterpart of rate distortion with side information was solved by Gelfand and Pinsker [243]; the duality between the two results is explored in Cover and Chiang [113]. The problem of multiple descriptions is treated in El Gamal and Cover [187].

The special problem of encoding a function of two random variables was discussed by Körner and Marton [326], who described a simple method to encode the modulo 2 sum of two binary random variables. A general framework for the description of source networks may be found in Csiszár and Körner [148, 149]. A common model that includes Slepian–Wolf encoding, coding with side information, and rate distortion with side information as special cases was described by Berger and Yeung [54].

In 1989, Ahlswede and Dueck [17] introduced the problem of identification via communication channels, which can be viewed as a problem where the sender sends information to the receivers but each receiver only needs to know whether or not a single message was sent. In this case, the set of possible messages that can be sent reliably is doubly exponential in the block length.



HISTORICAL NOTES

611  
The block length, and the key result of this paper was to show that \(2^{nC}\) messages could be identified for any noisy channel with capacity \(C\). This problem spawned a set of papers [16, 18, 269, 434], including extensions to channels with feedback and multiuser channels.  

Another active area of work has been the analysis of MIMO (multiple-input multiple-output) systems or space-time coding, which use multiple antennas at the transmitter and receiver to take advantage of the diversity gains from multipath for wireless systems. The analysis of these multiple antenna systems by Foschini [217], Teletar [512], and Rayleigh and Cioffi [246] show that the capacity gains from the diversity obtained using multiple antennas in fading environments can be substantial relative to the single-user capacity achieved by traditional equalization and interleaving techniques. A special issue of the IEEE Transactions in Information Theory [70] has a number of papers covering different aspects of this technology.  

Comprehensive surveys of network information theory may be found in El Gamal and Cover [186], Van der Meulen [526–528], Berger [53], Csiszár and Körner [149], Verdu [538], Cover [111], and Ephremides and Hajek [197].  

CHAPTER 16  
INFORMATION THEORY AND PORTFOLIO THEORY  

The duality between the growth rate of wealth in the stock market and the entropy rate of the market is striking. In particular, we shall find the competitively optimal and growth rate optimal portfolio strategies. They are the same, just as the Shannon code is optimal both competitively and in the expected description rate. We also find the asymptotic growth rate of wealth for an ergodic stock market process. We end with a discussion of universal portfolios that enable one to achieve the same asymptotic growth rate as the best constant rebalanced portfolio in hindsight.  

In Section 16.8 we provide a “sandwich” proof of the asymptotic equipartition property for general ergodic processes that is motivated by the notion of optimal portfolios for stationary ergodic stock markets.  

16.1  
THE STOCK MARKET: SOME DEFINITIONS  

A stock market is represented as a vector of stocks \(X = (X_1, X_2, \dots, X_m)\), \(X_i \geq 0\), \(i = 1, 2, \dots, m\), where \(m\) is the number of stocks and the price relative \(X_i\) is the ratio of the price at the end of the day to the price at the beginning of the day. So typically, \(X_i\) is near 1. For example, \(X_i = 1.03\) means that the \(i\)-th stock went up 3 percent that day.  

Let \(X \sim F(x)\), where \(F(x)\) is the joint distribution of the vector of price relatives. A portfolio \(b = (b_1, b_2, \dots, b_m)\), \(b_i \geq 0\), \(\sum_{i=1}^m b_i = 1\), is an allocation of wealth across the stocks. Here \(b_i\) is the fraction of one’s wealth invested in stock \(i\). If one uses a portfolio \(b\) and the stock vector is \(X\), the wealth relative (ratio of the wealth at the end of the day to the wealth at the beginning of the day) is \(S = b^t X = \sum_{i=1}^m b_i X_i\).  

We wish to maximize \(S\) in some sense. But \(S\) is a random variable, the distribution of which depends on portfolio \(b\), so there is controversy over the choice of the best distribution for \(S\). The standard theory of stock market investment is based on consideration of the first and second moments of \(S\). The objective is to maximize the expected value of \(S\) subject to a constraint on the variance. Since it is easy to calculate these moments, the theory is simpler than the theory that deals with the entire distribution of \(S\).  

The mean–variance approach is the basis of the Sharpe–Markowitz theory of investment in the stock market and is used by business analysts and others. It is illustrated in Figure 16.1. The figure illustrates the set of achievable mean–variance pairs using various portfolios. The set of portfolios on the boundary of this region corresponds to the undominated portfolios: These are the portfolios that have the highest mean for a given variance. This boundary is called the efficient frontier, and if one is interested only in mean and variance, one should operate along this boundary.  

Normally, the theory is simplified with the introduction of a risk-free asset (e.g., cash or Treasury bonds, which provide a fixed interest rate with zero variance). This stock corresponds to a point on the Y axis in the figure. By combining the risk-free asset with various stocks, one obtains all points below the tangent from the risk-free asset to the efficient frontier. This line now becomes part of the efficient frontier.  

The concept of the efficient frontier also implies that there is a true price for a stock corresponding to its risk. This theory of stock prices, called the capital asset pricing model (CAPM), is used to decide whether the market price for a stock is too high or too low. Looking at the mean of a random variable gives information about the long-term behavior of the sum of i.i.d. versions of the random variable. But in the stock market, one normally reinvests every day, so that the wealth at the end of \(n\) days is the product of factors, one for each day of the market. The behavior of the product is determined not by the expected value but by the expected logarithm. This leads us to define the growth rate as follows:  

**Definition**  
The growth rate of a stock market portfolio \(b\) with respect to a stock distribution \(F(x)\) is defined as  
\[
W(b, F) = \int \log(b^t x) \, dF(x) = \mathbb{E}[\log(b^t X)].
\]  
(16.1)  
If the logarithm is to base 2, the growth rate is also called the doubling rate.  

**Definition**  
The optimal growth rate \(W^*(F)\) is defined as  
\[
W^*(F) = \max_b W(b, F),
\]  
(16.2)  
where the maximum is over all possible portfolios \(b_i \geq 0\), \(\sum_{i=1}^m b_i = 1\).  

**Definition**  
A portfolio \(b^*\) that achieves the maximum of \(W(b, F)\) is called a log-optimal portfolio or growth optimal portfolio.  

The definition of growth rate is justified by the following theorem, which shows that wealth grows as \(2^{nW^*}\).  

**Theorem 16.1.1**  
Let \(X_1, X_2, \dots, X_n\) be i.i.d. according to \(F(x)\). Let  
\[
S_n^* = \prod_{i=1}^n b^{*t} X_i
\]  
(16.3)  
be the wealth after \(n\) days using the constant rebalanced portfolio \(b^*\). Then  
\[
\frac{1}{n} \log S_n^* \to W^*
\]  
(16.4)  
with probability 1.  

**Proof:**  
By the strong law of large numbers,  
\[
\frac{1}{n} \log S_n^* = \frac{1}{n} \sum_{i=1}^n \log(b^{*t} X_i) \to W^*
\]  
(16.5)  
with probability 1.  
(16.6)  
Hence, \(S_n^* \approx 2^{nW^*}\).  

We now consider some of the properties of the growth rate.  

**Lemma 16.1.1**  
\(W(b, F)\) is concave in \(b\) and linear in \(F\). \(W^*(F)\) is convex in \(F\).  

**Proof:**  
The growth rate is  
\[
W(b, F) = \int \log(b^t x) \, dF(x).
\]  
(16.7)  
Since the integral is linear in \(F\), so is \(W(b, F)\). Since  
\[
\log(\lambda b_1 + (1 - \lambda) b_2)^t X \geq \lambda \log(b_1^t X) + (1 - \lambda) \log(b_2^t X),
\]  
(16.8)  
by the concavity of the logarithm, it follows, by taking expectations, that \(W(b, F)\) is concave in \(b\). Finally, to prove the convexity of \(W^*(F)\) as a function of \(F\), let \(F_1\) and \(F_2\) be two distributions on the stock market and let the corresponding optimal portfolios be \(b^*(F_1)\) and \(b^*(F_2)\), respectively. Let the log-optimal portfolio corresponding to \(\lambda F_1 + (1 - \lambda) F_2\) be \(b^*(\lambda F_1 + (1 - \lambda) F_2)\). Then by linearity of \(W(b, F)\) with respect to \(F\), we have  
\[
W^*(\lambda F_1 + (1 - \lambda) F_2) = W(b^*(\lambda F_1 + (1 - \lambda) F_2), \lambda F_1 + (1 - \lambda) F_2)
\]  
(16.9)  
\[
= \lambda W(b^*(\lambda F_1 + (1 - \lambda) F_2), F_1) + (1 - \lambda) W(b^*(\lambda F_1 + (1 - \lambda) F_2), F_2)
\]  
\[
\leq \lambda W(b^*(F_1), F_1) + (1 - \lambda) W(b^*(F_2), F_2),
\]  
(16.10)  
since \(b^*(F_1)\) maximizes \(W(b, F_1)\) and \(b^*(F_2)\) maximizes \(W(b, F_2)\).  

**Lemma 16.1.2**  
The set of log-optimal portfolios with respect to a given distribution is convex.  

**Proof:**  
Suppose that \(b_1\) and \(b_2\) are log-optimal (i.e., \(W(b_1, F) = W(b_2, F) = W^*(F)\)). By the concavity of \(W(b, F)\) in \(b\), we have  
\[
W(\lambda b_1 + (1 - \lambda) b_2, F) \geq \lambda W(b_1, F) + (1 - \lambda) W(b_2, F) = W^*(F).
\]  
(16.11)  
Thus, \(\lambda b_1 + (1 - \lambda) b_2\) is also log-optimal.  

In the next section we use these properties to characterize the log-optimal portfolio.  

16.2  
KUHN–TUCKER CHARACTERIZATION OF THE LOG-OPTIMAL PORTFOLIO  

Let \(B = \{b \in \mathbb{R}^m : b_i \geq 0, \sum_{i=1}^m b_i = 1\}\) denote the set of allowed portfolios. The determination of \(b^*\) that achieves \(W^*(F)\) is a problem of maximization of a concave function \(W(b, F)\) over a convex set \(B\). The maximum may lie on the boundary. We can use the standard Kuhn–Tucker conditions to characterize the maximum. Instead, we derive these conditions from first principles.  

**Theorem 16.2.1**  
The log-optimal portfolio \(b^*\) for a stock market \(X \sim F\) (i.e., the portfolio that maximizes the growth rate \(W(b, F)\)) satisfies the following necessary and sufficient conditions:  
\[
\mathbb{E}\left[\frac{X_i}{b^{*t} X}\right] = 1 \quad \text{if } b_i^* > 0, \quad \leq 1 \quad \text{if } b_i^* = 0.
\]  
(16.12)  

**Proof:**  
The growth rate \(W(b) = \mathbb{E}[\log(b^t X)]\) is concave in \(b\), where \(b\) ranges over the simplex of portfolios. It follows that \(b^*\) is log-optimum if and only if the directional derivative of \(W(\cdot)\) in the direction from \(b^*\) to any alternative portfolio \(b\) is nonpositive. Thus, letting \(b_\lambda = (1 - \lambda)b^* + \lambda b\) for \(0 \leq \lambda \leq 1\), we have  
\[
\frac{d}{d\lambda} W(b_\lambda) \bigg|_{\lambda = 0+} \leq 0, \quad b \in B.
\]  
(16.13)  
These conditions reduce to (16.12) since the one-sided derivative at \(\lambda = 0+\) of \(W(b_\lambda)\) is  
\[
\lim_{\lambda \downarrow 0} \frac{1}{\lambda} \mathbb{E}\left[\log\left((1 - \lambda)b^{*t} X + \lambda b^t X\right) - \log(b^{*t} X)\right]
\]  
(16.14)  
\[
= \mathbb{E}\left[\frac{b^t X}{b^{*t} X}\right] - 1,
\]  
(16.15)  
where the interchange of limit and expectation can be justified using the dominated convergence theorem [39]. Thus, (16.13) reduces to  
\[
\mathbb{E}\left[\frac{b^t X}{b^{*t} X}\right] - 1 \leq 0
\]  
(16.16)  
for all \(b \in B\). If the line segment from \(b\) to \(b^*\) can be extended beyond \(b^*\) in the simplex, the two-sided derivative at \(\lambda = 0\) of \(W(b_\lambda)\) vanishes and (16.17) holds with equality. If the line segment from \(b\) to \(b^*\) cannot be extended because of the inequality constraint on \(b\), we have an inequality in (16.17).  

The Kuhn–Tucker conditions will hold for all portfolios \(b \in B\) if they hold for all extreme points of the simplex \(B\) since \(\mathbb{E}\left[\frac{b^t X}{b^{*t} X}\right]\) is linear in \(b\). Furthermore, the line segment from the \(j\)-th extreme point ( \(b : b_j = 1\), \(b_i = 0\), \(i \neq j\) ) to \(b^*\) can be extended beyond \(b^*\) in the simplex iff \(b_j^* > 0\). Thus, the Kuhn–Tucker conditions that characterize the log-optimum \(b^*\) are equivalent to the following necessary and sufficient conditions:  
\[
\mathbb{E}\left[\frac{X_i}{b^{*t} X}\right] = 1 \quad \text{if } b_i^* > 0, \quad \leq 1 \quad \text{if } b_i^* = 0.
\]  
(16.18)  

This theorem has a few immediate consequences. One useful equivalence is expressed in the following theorem.  

**Theorem 16.2.2**  
Let \(S^* = b^{*t} X\) be the random wealth resulting from the log-optimal portfolio \(b^*\). Let \(S = b^t X\) be the wealth resulting from any other portfolio \(b\). Then  
\[
\mathbb{E}\left[\frac{\log S}{S^*}\right] \leq 0 \quad \text{for all } S \quad \Longleftrightarrow \quad \mathbb{E}\left[\frac{S}{S^*}\right] \leq 1 \quad \text{for all } S.
\]  
(16.19)  

**Proof:**  
From Theorem 16.2.1 it follows that for a log-optimal portfolio \(b^*\),  
\[
\mathbb{E}\left[\frac{X_i}{b^{*t} X}\right] \leq 1 \quad \text{for all } i.
\]  
(16.20)  
Multiplying this equation by \(b_i\) and summing over \(i\), we have  
\[
\sum_{i=1}^m b_i \mathbb{E}\left[\frac{X_i}{b^{*t} X}\right] \leq \sum_{i=1}^m b_i = 1,
\]  
(16.21)  
which is equivalent to  
\[
\mathbb{E}\left[\frac{b^t X}{b^{*t} X}\right] \leq 1.
\]  
(16.22)  
The converse follows from Jensen’s inequality, since  
\[
\mathbb{E}\left[\frac{\log S}{S^*}\right] \leq \log \mathbb{E}\left[\frac{S}{S^*}\right] \leq \log 1 = 0.
\]  
(16.23)



### ASYMPTOTIC OPTIMALITY OF THE LOG-OPTIMAL PORTFOLIO

Maximizing the expected logarithm was motivated by the asymptotic growth rate. But we have just shown that the log-optimal portfolio, in addition to maximizing the asymptotic growth rate, also "maximizes" the expected wealth relative \( E(S/S^*) \) for one day. We shall say more about the short-term optimality of the log-optimal portfolio when we consider the game-theoretic optimality of this portfolio.

Another consequence of the Kuhn–Tucker characterization of the log-optimal portfolio is the fact that the expected proportion of wealth in each stock under the log-optimal portfolio is unchanged from day to day. Consider the stocks at the end of the first day. The initial allocation of wealth is \( b^* \). The proportion of the wealth in stock \( i \) at the end of the day is

\[
\frac{b^*_i X_i}{b^*_t X},
\]

and the expected value of this proportion is

\[
E\left[\frac{b^*_i X_i}{b^*_t X}\right] = b^*_i E\left[\frac{X_i}{b^*_t X}\right] = b^*_i.
\]

Hence, the proportion of wealth in stock \( i \) expected at the end of the day is the same as the proportion invested in stock \( i \) at the beginning of the day. This is a counterpart to Kelly proportional gambling, where one invests in proportions that remain unchanged in expected value after the investment period.

### ASYMPTOTIC OPTIMALITY OF THE LOG-OPTIMAL PORTFOLIO

In Section 16.2, we introduced the log-optimal portfolio and explained its motivation in terms of the long-term behavior of a sequence of investments in a repeated independent version of the stock market. In this section, we expand on this idea and prove that with probability 1, the conditionally log-optimal investor will not do any worse than any other investor who uses a causal investment strategy.

We first consider an i.i.d. stock market (i.e., \( X_1, X_2, \ldots, X_n \) are i.i.d. according to \( F(x) \)). Let

\[
S_n = \sum_{i=1}^n b^*_i X_i
\]

be the wealth after \( n \) days for an investor who uses portfolio \( b_i \) on day \( i \). Let

\[
W^* = \max_b W(b, F) = \max_b E[\log b^*_t X]
\]

be the maximal growth rate, and let \( b^* \) be a portfolio that achieves the maximum growth rate. We only allow alternative portfolios \( b_i \) that depend causally on the past and are independent of the future values of the stock market.

**Definition**  
A nonanticipating or causal portfolio strategy is a sequence of mappings \( b_i : \mathbb{R}^{m(i-1)} \to \mathcal{B} \), with the interpretation that portfolio \( b_i(x_1, \ldots, x_{i-1}) \) is used on day \( i \).

From the definition of \( W^* \), it follows immediately that the log-optimal portfolio maximizes the expected log of the final wealth. This is stated in the following lemma.

**Lemma 16.3.1**  
Let \( S^*_n \) be the wealth after \( n \) days using the log-optimal strategy \( b^* \), and let \( S_n \) be the wealth using a causal portfolio strategy \( b_i \). Then

\[
E[\log S^*_n] = nW^* \geq E[\log S_n].
\]

**Proof**  
\[
\max_{b_1, b_2, \ldots, b_n} E[\log S_n] = \max_{b_1, b_2, \ldots, b_n} E\left[\sum_{i=1}^n \log b_i^t X_i\right] = \sum_{i=1}^n \max_{b_i(X_1, X_2, \ldots, X_{i-1})} E[\log b_i^t X_i].
\]

The maximum is achieved by a constant portfolio strategy \( b^* \). Thus,

\[
E[\log S^*_n] = nW^*,
\]

and the maximum is achieved by a constant portfolio strategy \( b^* \).

So far, we have proved two simple consequences of the definition of log-optimal portfolios: that \( b^* \) (satisfying \( (16.12) \)) maximizes the expected log wealth, and that the resulting wealth \( S^*_n \) is equal to \( 2^n W^* \) to first order in the exponent, with high probability.

Now we prove a much stronger result, which shows that \( S^*_n \) exceeds the wealth (to first order in the exponent) of any other investor for almost every sequence of outcomes from the stock market.

**Theorem 16.3.1**  
(Asymptotic optimality of the log-optimal portfolio)  
Let \( X_1, X_2, \ldots, X_n \) be a sequence of i.i.d. stock vectors drawn according to \( F(x) \). Let \( S^*_n = \sum_{i=1}^n b^*_i X_i \), where \( b^* \) is the log-optimal portfolio, and let \( S_n = \sum_{i=1}^n b_i X_i \) be the wealth resulting from any other causal portfolio. Then

\[
\limsup_{n \to \infty} \frac{1}{n} \log \frac{S_n}{S^*_n} \leq 0
\]

with probability 1.

**Proof**  
From the Kuhn–Tucker conditions and the log optimality of \( S^*_n \), we have

\[
E\left[\frac{S_n}{S^*_n}\right] \leq 1.
\]

Hence, by Markov’s inequality, we have

\[
\Pr\left(\frac{S_n}{S^*_n} > t_n\right) \leq \frac{1}{t_n}.
\]

Thus,

\[
\Pr\left(\frac{1}{n} \log \frac{S_n}{S^*_n} > \frac{1}{n} \log t_n\right) \leq \frac{1}{t_n}.
\]

Setting \( t_n = n^2 \) and summing over \( n \), we have

\[
\sum_{n=1}^\infty \Pr\left(\frac{1}{n} \log \frac{S_n}{S^*_n} > \frac{2 \log n}{n}\right) \leq \sum_{n=1}^\infty \frac{1}{n^2} = \frac{\pi^2}{6}.
\]

Then, by the Borel–Cantelli lemma,

\[
\Pr\left(\frac{1}{n} \log \frac{S_n}{S^*_n} > \frac{2 \log n}{n} \text{ infinitely often}\right) = 0.
\]

This implies that for almost every sequence from the stock market, there exists an \( N \) such that for all \( n > N \),

\[
\frac{1}{n} \log \frac{S_n}{S^*_n} < \frac{2 \log n}{n}.
\]

Thus,

\[
\limsup_{n \to \infty} \frac{1}{n} \log \frac{S_n}{S^*_n} \leq 0
\]

with probability 1.

The theorem proves that the log-optimal portfolio will perform as well as or better than any other portfolio to first order in the exponent.

### SIDE INFORMATION AND THE GROWTH RATE

We showed in Chapter 6 that side information \( Y \) for the horse race \( X \) can be used to increase the growth rate by the mutual information \( I(X; Y) \).

We now extend this result to the stock market. Here, \( I(X; Y) \) is an upper bound on the increase in the growth rate, with equality if \( X \) is a horse race. We first consider the decrease in growth rate incurred by believing in the wrong distribution.

**Theorem 16.4.1**  
Let \( X \sim f(x) \). Let \( b^*_f \) be a log-optimal portfolio corresponding to \( f(x) \), and let \( b^*_g \) be a log-optimal portfolio corresponding to some other density \( g(x) \). Then the increase in growth rate \( \Delta W \) by using \( b^*_f \) instead of \( b^*_g \) is bounded by

\[
\Delta W = W(b^*_f, F) - W(b^*_g, F) \leq D(f \| g).
\]

**Proof**  
We have

\[
\Delta W = \int f(x) \log \frac{b^*_f(x)}{b^*_g(x)} dx.
\]

By Jensen’s inequality and the Kuhn–Tucker conditions, this is bounded by

\[
\Delta W \leq D(f \| g).
\]

**Theorem 16.4.2**  
The increase \( \Delta W \) in growth rate due to side information \( Y \) is bounded by

\[
\Delta W \leq I(X; Y).
\]

**Proof**  
Let \( (X, Y) \sim f(x, y) \), where \( X \) is the market vector and \( Y \) is the related side information. Given side information \( Y = y \), the log-optimal investor uses the conditional log-optimal portfolio for the conditional distribution \( f(x|Y = y) \). Hence, conditional on \( Y = y \), we have, from Theorem 16.4.1,

\[
\Delta W|_{Y=y} \leq D(f(x|Y=y) \| f(x)).
\]

Averaging this over possible values of \( Y \), we have

\[
\Delta W \leq I(X; Y).
\]

### INVESTMENT IN STATIONARY MARKETS

We now extend some of the results of Section 16.4 from i.i.d. markets to time-dependent market processes. Let \( X_1, X_2, \ldots, X_n, \ldots \) be a vector-valued stochastic process with \( X_i \geq 0 \). We consider investment strategies that depend on the past values of the market in a causal fashion (i.e., \( b_i \) may depend on \( X_1, X_2, \ldots, X_{i-1} \)). Let

\[
S_n = \sum_{i=1}^n b_i(X_1, X_2, \ldots, X_{i-1}) X_i.
\]

Our objective is to maximize \( E[\log S_n] \) over all such causal portfolio strategies \( \{b_i(\cdot)\} \). Now

\[
\max_{b_1, b_2, \ldots, b_n} E[\log S_n] = \sum_{i=1}^n \max_{b_i(X_1, X_2, \ldots, X_{i-1})} E[\log b_i^t X_i].
\]

The maximum is achieved by the conditional log-optimal portfolio \( b^*_i \). Thus,

\[
E[\log S^*_n] = \sum_{i=1}^n E[\log b^*_i(X_1, X_2, \ldots, X_{i-1})^t X_i].
\]

Taking the expectation over the past, we write

\[
W^*(X_1, X_2, \ldots, X_n) = \sum_{i=1}^n W^*(X_i | X_1, X_2, \ldots, X_{i-1}),
\]

where \( W^*(X_i | X_1, X_2, \ldots, X_{i-1}) \) is the conditional optimal growth rate. The growth rate \( W^*_\infty \) is defined as

\[
W^*_\infty = \lim_{n \to \infty} \frac{1}{n} W^*(X_1, X_2, \ldots, X_n),
\]

if the limit exists.

**Theorem 16.5.1**  
For a stationary market, the growth rate exists and is equal to

\[
W^*_\infty = \lim_{n \to \infty} W^*(X_n | X_1, X_2, \ldots, X_{n-1}).
\]

**Proof**  
By stationarity, \( W^*(X_n | X_1, X_2, \ldots, X_{n-1}) \) is non-decreasing in \( n \). Hence, it must have a limit, possibly infinity. Since

\[
\frac{1}{n} W^*(X_1, X_2, \ldots, X_n) = \frac{1}{n} \sum_{i=1}^n W^*(X_i | X_1, X_2, \ldots, X_{i-1}),
\]

it follows by the theorem of the Cesàro mean that the left-hand side has the same limit as the limit of the terms on the right-hand side. Hence, \( W^*_\infty \) exists and

\[
W^*_\infty = \lim_{n \to \infty} \frac{1}{n} W^*(X_1, X_2, \ldots, X_n) = \lim_{n \to \infty} W^*(X_n | X_1, X_2, \ldots, X_{n-1}).
\]

We can now extend the asymptotic optimality property to stationary markets. We have the following theorem.

**Theorem 16.5.2**  
Consider an arbitrary stochastic process \( \{X_i\} \), \( X_i \in \mathbb{R}^m_+ \), conditionally log-optimal portfolios \( b^*_i(X_{i-1}) \), and wealth \( S^*_n \). Let \( S_n \) be the wealth generated by any other causal portfolio strategy \( b_i(X_{i-1}) \). Then \( S_n / S^*_n \) is a positive supermartingale with respect to the sequence of \( \sigma \)-fields generated by the past \( X_1, X_2, \ldots, X_n \). Consequently, there exists a random variable \( V \) such that

\[
\frac{S_n}{S^*_n} \to V
\]

with probability 1,

\[
E[V] \leq 1,
\]

and

\[
\Pr\left(\sup_n \frac{S_n}{S^*_n} \geq t\right) \leq \frac{1}{t}.
\]

**Proof**  
\( S_n / S^*_n \) is a positive supermartingale because

\[
E\left[\frac{S_{n+1}(X_{n+1})}{S^*_{n+1}(X_{n+1})} \bigg| X_n\right] = E\left[\frac{b_n(X_n) X_{n+1}}{b^*_n(X_n) X_{n+1}} \bigg| X_n\right] \leq 1.
\]

Thus, by the martingale convergence theorem, \( S_n / S^*_n \) converges to a random variable \( V \) with \( E[V] \leq 1 \).



INFORMATION THEORY AND PORTFOLIO THEORY  
by the Kuhn–Tucker condition on the conditionally log-optimal portfolio. Thus, by the martingale convergence theorem, \( S_n/S^*_n \) has a limit, call it \( V \), and \( EV \leq E(S_0/S^*_0) = 1 \). Finally, the result for \( \sup(S_n/S^*_n) \) follows from Kolmogorov’s inequality for positive martingales.  

We remark that (16.70) shows how strong the competitive optimality of \( S^*_n \) is. Apparently, the probability is less than 1/10 that \( S_n(X_n) \) will ever be 10 times as large as \( S^*_n(X_n) \). For a stationary ergodic market, we can extend the asymptotic equipartition property to prove the following theorem.  

**Theorem 16.5.3 (AEP for the stock market)**  
Let \( X_1, X_2, \ldots, X_n \) be a stationary ergodic vector-valued stochastic process. Let \( S^*_n \) be the wealth at time \( n \) for the conditionally log-optimal strategy, where  
\[
S^*_n = \prod_{i=1}^n b^*_i(X_1, X_2, \ldots, X_{i-1})X_i.  
\]  
Then  
\[
\frac{1}{n} \log S^*_n \to W^*_\infty \quad \text{with probability 1}.  
\]  

**Proof:**  
The proof involves a generalization of the sandwich argument [20] used to prove the AEP in Section 16.8. The details of the proof (in Algoet and Cover [21]) are omitted.  

Finally, we consider the example of the horse race once again. The horse race is a special case of the stock market in which there are \( m \) stocks corresponding to the \( m \) horses in the race. At the end of the race, the value of the stock for horse \( i \) is either 0 or \( o_i \), the value of the odds for horse \( i \). Thus, \( X \) is nonzero only in the component corresponding to the winning horse.  

In this case, the log-optimal portfolio is proportional betting, known as Kelly gambling (i.e., \( b^*_i = p_i \)), and in the case of uniform fair odds (i.e., \( o_i = m \), for all \( i \)),  
\[
W^* = \log m - H(X).  
\]  
When we have a sequence of correlated horse races, the optimal portfolio is conditional proportional betting, and the asymptotic growth rate is  
\[
W^*_\infty = \log m - H(X),  
\]  
where \( H(X) = \lim_{n \to \infty} \frac{1}{n} H(X_1, X_2, \ldots, X_n) \) if the limit exists. Then Theorem 16.5.3 asserts that  
\[
S^*_n \approx 2^{nW^*},  
\]  
in agreement with the results in Chapter 6.  

---

### 16.6 Competitive Optimality of the Log-Optimal Portfolio  

We now ask whether the log-optimal portfolio outperforms alternative portfolios at a given finite time \( n \). As a direct consequence of the Kuhn–Tucker conditions, we have  
\[
E\left[\frac{S_n}{S^*_n}\right] \leq 1,  
\]  
and hence, by Markov’s inequality,  
\[
\Pr(S_n > tS^*_n) \leq \frac{1}{t}.  
\]  
This result is similar to the result derived in Chapter 5 for the competitive optimality of Shannon codes.  

By considering examples, it can be seen that it is not possible to get a better bound on the probability that \( S_n > S^*_n \). Consider a stock market with two stocks and two possible outcomes,  
\[
(X_1, X_2) = \begin{cases} 
(1, 1) & \text{with probability } 1 - \epsilon, \\
(1 - \epsilon, 0) & \text{with probability } \epsilon.  
\end{cases}  
\]  
In this market, the log-optimal portfolio invests all the wealth in the first stock. [It is easy to verify that \( b = (1, 0) \) satisfies the Kuhn–Tucker conditions.] However, an investor who puts all his wealth in the second stock earns more money with probability \( 1 - \epsilon \). Hence, it is not true that with high probability the log-optimal investor will do better than any other investor.  

The problem with trying to prove that the log-optimal investor does best with a probability of at least \( \frac{1}{2} \) is that there exist examples like the one above, where it is possible to beat the log-optimal investor by a small amount most of the time. We can get around this by allowing each investor an additional fair randomization, which has the effect of reducing the effect of small differences in the wealth.  

**Theorem 16.6.1 (Competitive optimality)**  
Let \( S^* \) be the wealth at the end of one period of investment in a stock market \( X \) with the log-optimal portfolio, and let \( S \) be the wealth induced by any other portfolio. Let \( U^* \) be a random variable independent of \( X \) uniformly distributed on \([0, 2]\), and let \( V \) be any other random variable independent of \( X \) and \( U^* \) with \( V \geq 0 \) and \( EV = 1 \). Then  
\[
\Pr(VS \geq U^*S^*) \leq \frac{1}{2}.  
\]  

**Remark:**  
Here \( U^* \) and \( V \) correspond to initial “fair” randomizations of the initial wealth. This exchange of initial wealth \( S_0 = 1 \) for “fair” wealth \( U^* \) can be achieved in practice by placing a fair bet. The effect of the fair randomization is to randomize small differences, so that only the significant deviations of the ratio \( S/S^* \) affect the probability of winning.  

**Proof:**  
We have  
\[
\Pr(VS \geq U^*S^*) = \Pr\left(\frac{VS}{S^*} \geq U^*\right).  
\]  
Let \( W = \frac{VS}{S^*} \), where \( W \) is a non-negative-valued random variable with mean  
\[
E[W] = E[V]E\left[\frac{S}{S^*}\right] \leq 1,  
\]  
by the independence of \( V \) from \( X \) and the Kuhn–Tucker conditions. Let \( F \) be the distribution function of \( W \). Then, since \( U^* \) is uniform on \([0, 2]\),  
\[
\Pr(W \geq U^*) = \int_0^2 \Pr(W > w)f_{U^*}(w) \, dw = \int_0^2 \Pr(W > w) \frac{1}{2} \, dw.  
\]  
This simplifies to  
\[
\int_0^2 (1 - F(w)) \frac{1}{2} \, dw \leq \int_0^\infty (1 - F(w)) \frac{1}{2} \, dw = \frac{1}{2}E[W] \leq \frac{1}{2},  
\]  
using the easily proved fact (by integrating by parts) that  
\[
E[W] = \int_0^\infty (1 - F(w)) \, dw  
\]  
for a positive random variable \( W \). Hence, we have  
\[
\Pr(VS \geq U^*S^*) \leq \frac{1}{2}.  
\]  

Theorem 16.6.1 provides a short-term justification for the use of the log-optimal portfolio. If the investor’s only objective is to be ahead of his opponent at the end of the day in the stock market, and if fair randomization is allowed, Theorem 16.6.1 says that the investor should exchange his wealth for a uniform \([0, 2]\) wealth and then invest using the log-optimal portfolio. This is the game-theoretic solution to the problem of gambling competitively in the stock market.  

---

### 16.7 Universal Portfolios  

The development of the log-optimal portfolio strategy in Section 16.1 relies on the assumption that we know the distribution of the stock vectors and can therefore calculate the optimal portfolio \( b^* \). In practice, though, we often do not know the distribution. In this section, we describe a causal portfolio that performs well on individual sequences. Thus, we make no statistical assumptions about the market sequence. We assume that the stock market can be represented by a sequence of vectors \( x_1, x_2, \ldots \in \mathbb{R}^m_+ \), where \( x_{ij} \) is the price relative for stock \( j \) on day \( i \), and \( x_i \) is the vector of price relatives for all stocks on day \( i \). We begin with a finite-horizon problem, where we have \( n \) vectors \( x_1, \ldots, x_n \). We later extend the results to the infinite-horizon case.  

Given this sequence of stock market outcomes, what is the best we can do? A realistic target is the growth achieved by the best constant rebalanced portfolio strategy in hindsight (i.e., the best constant rebalanced portfolio on the known sequence of stock market vectors). Note that constant rebalanced portfolios are optimal against i.i.d. stock market sequences with known distribution, so that this set of portfolios is reasonably natural.  

Let us assume that we have a number of mutual funds, each of which follows a constant rebalanced portfolio strategy chosen in advance. Our objective is to perform as well as the best of these funds. In this section, we show that we can do almost as well as the best constant rebalanced portfolio without advance knowledge of the distribution of the stock market vectors.  

One approach is to distribute the wealth among a continuum of fund managers, each of which follows a different constantly rebalanced portfolio strategy. Since one of the managers will do exponentially better than the others, the total wealth after \( n \) days will be dominated by the largest term. We will show that we can achieve a performance of the best fund manager within a factor of \( n^{m-1/2} \). This is the essence of the argument for the infinite-horizon universal portfolio strategy.  

A second approach to this problem is as a game against a malicious opponent or nature who is allowed to choose the sequence of stock market vectors. We define a causal (nonanticipating) portfolio strategy \( \hat{b}_i(x_{i-1}, \ldots, x_1) \) that depends only on the past values of the stock market sequence. Then, nature, with knowledge of the strategy \( \hat{b}_i(x_{i-1}) \), chooses a sequence of vectors \( x_i \) to make the strategy perform as poorly as possible relative to the best constantly rebalanced portfolio for that stock sequence. Let \( b^*(x^n) \) be the best constantly rebalanced portfolio for a stock market sequence \( x^n \). Note that \( b^*(x^n) \) depends only on the empirical distribution of the sequence, not on the order in which the vectors occur. At the end of \( n \) days, a constantly rebalanced portfolio \( b \) achieves wealth:  
\[
S_n(b, x^n) = \prod_{i=1}^n b^T x_i,  
\]  
and the best constant portfolio \( b^*(x^n) \) achieves a wealth  
\[
S_n^*(x^n) = \max_b \prod_{i=1}^n b^T x_i,  
\]  
whereas the nonanticipating portfolio \( \hat{b}_i(x_{i-1}) \) strategy achieves  
\[
\hat{S}_n(x^n) = \prod_{i=1}^n \hat{b}_i^T(x_{i-1})x_i.  
\]  

Our objective is to find a nonanticipating portfolio strategy \( \hat{b}(\cdot) = (\hat{b}_1, \hat{b}_2(x_1), \ldots, \hat{b}_i(x_{i-1})) \) that does well in the worst case in terms of the ratio of \( \hat{S}_n \) to \( S_n^* \). We will find the optimal universal strategy and show that this strategy for each stock sequence achieves wealth \( \hat{S}_n \) that is within a factor \( V_n \approx n^{m-1/2} \) of the wealth \( S_n^* \) achieved by the best constantly rebalanced portfolio on that sequence. This strategy depends on \( n \), the horizon of the game. Later, we describe some horizon-free results that have the same worst-case asymptotic performance as that of the finite-horizon game.  

#### 16.7.1 Finite-Horizon Universal Portfolios  

We begin by analyzing a stock market of \( n \) periods, where \( n \) is known in advance, and attempt to find a portfolio strategy that does well against all possible sequences of \( n \) stock market vectors. The main result can be stated in the following theorem.  

**Theorem 16.7.1**  
For a stock market sequence \( x^n = x_1, \ldots, x_n \), \( x_i \in \mathbb{R}^m_+ \), of length \( n \) with \( m \) assets, let \( S_n^*(x^n) \) be the wealth achieved by the optimal constantly rebalanced portfolio on \( x^n \), and let \( \hat{S}_n(x^n) \) be the wealth achieved by any causal portfolio strategy \( \hat{b}_i(\cdot) \) on \( x^n \); then  
\[
\max_{\hat{b}_i(\cdot)} \min_{x_1, \ldots, x_n} \frac{\hat{S}_n(x^n)}{S_n^*(x^n)} = V_n,  
\]  
where  
\[
V_n = \left[ \sum_{n_1+\cdots+n_m=n} \binom{n}{n_1, \ldots, n_m} \right]^{-1}.  
\]  

Using Stirling’s approximation, we can show that \( V_n \) is on the order of \( n^{m-1/2} \), and therefore the growth rate for the universal portfolio on the worst sequence differs from the growth rate of the best constantly rebalanced portfolio on that sequence by at most a polynomial factor. The logarithm of the ratio of growth of wealth of the universal portfolio \( \hat{b} \) to the growth of wealth of the best constant portfolio behaves like the redundancy of a universal source code. (See Shtarkov [496], where \( \log V_n \) appears as the minimax individual sequence redundancy in data compression.)  

We first illustrate the main results by means of an example for \( n = 1 \). Consider the case of two stocks and a single day. Let the stock vector for the day be \( x = (x_1, x_2) \). If \( x_1 > x_2 \), the best portfolio is one that puts all its money on stock 1, and if \( x_2 > x_1 \), the best portfolio puts all its money on stock 2. (If \( x_1 = x_2 \), all portfolios are equivalent.)  

Now assume that we must choose a portfolio in advance and our opponent can choose the stock market sequence after we have chosen our portfolio to make us do as badly as possible relative to the best portfolio. Given our portfolio, the opponent can ensure that we do as badly as possible by making the stock on which we have put more weight equal to 0 and the other stock equal to 1. Our best strategy is therefore to put equal weight on both stocks.  

(Continued...)



INFORMATION THEORY AND PORTFOLIO THEORY

weight on both stocks, and with this, we will achieve a growth factor at least equal to half the growth factor of the best stock, and hence we will achieve at least half the gain of the best constantly rebalanced portfolio. It is not hard to calculate that \( V_n = 2 \) when \( n = 1 \) and \( m = 2 \) in equation (16.94). However, this result seems misleading, since it appears to suggest that for \( n \) days, we would use a constant uniform portfolio, putting half our money on each stock every day. If our opponent then chose the stock sequence so that only the first stock was 1 (and the other was 0) every day, this uniform strategy would achieve a wealth of \( 1/2^n \), and we would achieve a wealth only within a factor of \( 2n \) of the best constant portfolio, which puts all the money on the first stock for all time. The result of the theorem shows that we can do significantly better.

The main part of the argument is to reduce a sequence of stock vectors to the extreme cases where only one of the stocks is nonzero for each day. If we can ensure that we do well on such sequences, we can guarantee that we do well on any sequence of stock vectors, and achieve the bounds of the theorem.

Before we prove the theorem, we need the following lemma.

**Lemma 16.7.1**  
For \( p_1, p_2, \ldots, p_m \geq 0 \) and \( q_1, q_2, \ldots, q_m \geq 0 \),  
\[
\sum_{i=1}^m p_i \sum_{i=1}^m q_i \geq \min_i p_i q_i.
\]  
*Proof:*  
Let \( I \) denote the index \( i \) that minimizes the right-hand side in (16.96). Assume that \( p_I > 0 \) (if \( p_I = 0 \), the lemma is trivially true). Also, if \( q_I = 0 \), both sides of (16.96) are infinite (all the other \( q_i \)'s must also be zero), and again the inequality holds. Therefore, we can also assume that \( q_I > 0 \). Then  
\[
\sum_{i=1}^m p_i \sum_{i=1}^m q_i = p_I q_I \left(1 + \sum_{i \neq I} \frac{p_i}{p_I}\right) \left(1 + \sum_{i \neq I} \frac{q_i}{q_I}\right) \geq p_I q_I
\]  
because  
\[
\frac{p_i}{p_I} \geq \frac{q_i}{q_I}
\]  
for all \( i \).  

First consider the case when \( n = 1 \). The wealth at the end of the first day is  
\[
\hat{S}_1(x) = \hat{b}^t x,
\]  
\[
S_1(x) = b^t x,
\]  

and  
\[
\frac{\hat{S}_1(x)}{S_1(x)} \geq \min_i \frac{\hat{b}_i}{b_i}.
\]  
We wish to find \( \max_{\hat{b}} \min_{b,x} \frac{\hat{b}^t x}{b^t x} \). Nature should choose \( x = e_i \), where \( e_i \) is the \( i \)-th basis vector with 1 in the component \( i \) that minimizes \( \frac{\hat{b}_i}{b_i} \), and the investor should choose \( \hat{b} \) to maximize this minimum. This is achieved by choosing \( \hat{b} = \left( \frac{1}{m}, \frac{1}{m}, \ldots, \frac{1}{m} \right) \).

The important point to realize is that  
\[
\frac{\hat{S}_n(x_n)}{S_n(x_n)} = \frac{\sum_{i=1}^n \hat{b}_i^t x_i}{\sum_{i=1}^n b_i^t x_i}
\]  
can also be rewritten in the form of a ratio of terms  
\[
\frac{\hat{S}_n(x_n)}{S_n(x_n)} = \frac{\hat{b}^t x'}{b^t x'},
\]  
where \( \hat{b}, b, x' \in \mathbb{R}^{mn}_+ \). Here the \( mn \) components of the constantly rebalanced portfolios \( b \) are all of the product form \( b_{n1}^1 b_{n2}^2 \cdots b_{nm}^m \). One wishes to find a universal \( \hat{b} \) that is uniformly close to the \( b \)'s corresponding to constantly rebalanced portfolios.

We can now prove the main theorem (Theorem 16.7.1).  

*Proof of Theorem 16.7.1:*  
We will prove the theorem for \( m = 2 \). The proof extends in a straightforward fashion to the case \( m > 2 \). Denote the stocks by 1 and 2. The key idea is to express the wealth at time \( n \),  
\[
S_n(x_n) = \prod_{i=1}^n b_i^t x_i,
\]  
which is a product of sums, into a sum of products. Each term in the sum corresponds to a sequence of stock price relatives for stock 1 or stock 2 times the proportion \( b_{i1} \) or \( b_{i2} \) that the strategy places on stock 1 or stock 2 at time \( i \). We can therefore view the wealth \( S_n \) as a sum over all \( 2^n \) possible \( n \)-sequences of 1’s and 2’s of the product of the portfolio proportions times the stock price relatives:  
\[
S_n(x_n) = \sum_{j_n \in \{1,2\}^n} \left( \prod_{i=1}^n b_{ij_i} \right) \left( \prod_{i=1}^n x_{ij_i} \right).
\]  

If we let \( w(j_n) \) denote the product \( \prod_{i=1}^n b_{ij_i} \), the total fraction of wealth invested in the sequence \( j_n \), and let  
\[
x(j_n) = \prod_{i=1}^n x_{ij_i}
\]  
be the corresponding return for this sequence, we can write  
\[
S_n(x_n) = \sum_{j_n \in \{1,2\}^n} w(j_n) x(j_n).
\]  

Similar expressions apply to both the best constantly rebalanced portfolio and the universal portfolio strategy. Thus, we have  
\[
\frac{\hat{S}_n(x_n)}{S_n(x_n)} = \frac{\sum_{j_n \in \{1,2\}^n} \hat{w}(j_n) x(j_n)}{\sum_{j_n \in \{1,2\}^n} w^*(j_n) x(j_n)},
\]  
where \( \hat{w}(j_n) \) is the amount of wealth placed on the sequence \( j_n \) by the universal nonanticipating strategy, and \( w^*(j_n) \) is the amount placed by the best constant rebalanced portfolio strategy. Now applying Lemma 16.7.1, we have  
\[
\frac{\hat{S}_n(x_n)}{S_n(x_n)} \geq \min_{j_n} \frac{\hat{w}(j_n)}{w^*(j_n)}.
\]  

Thus, the problem of maximizing the performance ratio \( \frac{\hat{S}_n}{S_n} \) is reduced to ensuring that the proportion of money bet on a sequence of stocks by the universal portfolio is uniformly close to the proportion bet by \( b^* \). As might be obvious by now, this formulation of \( S_n \) reduces the \( n \)-period stock market to a special case of a single-period stock market—there are \( 2n \) stocks, one invests \( w(j_n) \) in stock \( j_n \) and receives a return \( x(j_n) \) for stock \( j_n \), and the total wealth \( S_n \) is \( \sum_{j_n} w(j_n) x(j_n) \).

We first calculate the weight \( w^*(j_n) \) associated with the best constant rebalanced portfolio \( b^* \). We observe that a constantly rebalanced portfolio \( b \) results in  
\[
w(j_n) = \prod_{i=1}^n b_{ij_i} = b^k (1 - b)^{n - k},
\]  
where \( k \) is the number of times 1 appears in the sequence \( j_n \). Thus, \( w(j_n) \) depends only on \( k \), the number of 1’s in \( j_n \). Fixing attention on \( j_n \), we find by differentiating with respect to \( b \) that the maximum value  
\[
w^*(j_n) = \max_{0 \leq b \leq 1} b^k (1 - b)^{n - k} = \binom{n}{k} \frac{k^k (n - k)^{n - k}}{n^n},
\]  
which is achieved by  
\[
b^* = \frac{k}{n}.
\]  

Note that \( \sum w^*(j_n) > 1 \), reflecting the fact that the amount “bet” on \( j_n \) is chosen in hindsight, thus relieving the hindsight investor of the responsibility of allocating his investments \( w^*(j_n) \) to sum to 1. The causal investor has no such luxury. How can the causal investor choose initial investments \( \hat{w}(j_n) \), \( \sum \hat{w}(j_n) = 1 \), to protect himself from all possible \( j_n \) and hindsight-determined \( w^*(j_n) \)? The answer will be to choose \( \hat{w}(j_n) \) proportional to \( w^*(j_n) \). Then the worst-case ratio of \( \frac{\hat{w}(j_n)}{w^*(j_n)} \) will be maximized. To proceed, we define \( V_n \) by  
\[
\frac{1}{V_n} = \sum_{j_n} \binom{n}{k(j_n)} \frac{k(j_n)^{k(j_n)} (n - k(j_n))^{n - k(j_n)}}{n^n},
\]  
and let  
\[
\hat{w}(j_n) = V_n \binom{n}{k(j_n)} \frac{k(j_n)^{k(j_n)} (n - k(j_n))^{n - k(j_n)}}{n^n}.
\]  

It is clear that \( \hat{w}(j_n) \) is a legitimate distribution of wealth over the \( 2n \) stock sequences (i.e., \( \hat{w}(j_n) \geq 0 \) and \( \sum_{j_n} \hat{w}(j_n) = 1 \)). Here \( V_n \) is the normalization factor that makes \( \hat{w}(j_n) \) a probability mass function. Also, from (16.109) and (16.113), for all sequences \( x_n \),  
\[
\frac{\hat{S}_n(x_n)}{S_n(x_n)} \geq \min_{j_n} \frac{\hat{w}(j_n)}{w^*(j_n)} = \min_k V_n \frac{k^k (n - k)^{n - k}}{n^n} \geq V_n,
\]  
where the last inequality follows from (16.112). Consequently, we have  
\[
\max_{\hat{b}} \min_{x_n} \frac{\hat{S}_n(x_n)}{S_n(x_n)} \geq V_n.
\]  

We have thus demonstrated a portfolio on the \( 2n \) possible sequences of length \( n \) that achieves wealth \( \hat{S}_n(x_n) \) within a factor \( V_n \) of the wealth \( S_n^* \) achieved by the best constant rebalanced portfolio in hindsight. To complete the proof of the theorem, we show that this is the best possible, that is, that any nonanticipating portfolio strategy \( b_i(x_{i-1}) \) cannot do better than a factor \( V_n \) in the worst case (i.e., for the worst choice of \( x_n \)). To prove this, we construct a set of extremal stock market sequences and show that the performance of any nonanticipating portfolio strategy is bounded by \( V_n \) for at least one of these sequences, proving the worst-case bound.

For each \( j_n \in \{1, 2\}^n \), we define the corresponding extremal stock market vector \( x_n(j_n) \) as  
\[
x_i(j_i) = \begin{cases} 
(1, 0)^t & \text{if } j_i = 1, \\
(0, 1)^t & \text{if } j_i = 2.
\end{cases}
\]  
Let \( e_1 = (1, 0)^t \), \( e_2 = (0, 1)^t \) be standard basis vectors. Let  
\[
K = \{ x(j_n) : j_n \in \{1, 2\}^n, x_{j_i} = e_{j_i} \}
\]  
be the set of extremal sequences. There are \( 2n \) such extremal sequences, and for each sequence at each time, there is only one stock that yields a nonzero return. The wealth invested in the other stock is lost. Therefore, the wealth at the end of \( n \) periods for extremal sequence \( x_n(j_n) \) is the product of the amounts invested in the stocks \( j_1, j_2, \ldots, j_n \), i.e.,  
\[
S_n(x_n(j_n)) = \prod_{i=1}^n b_{ij_i} = w(j_n).
\]  

For any extremal sequence \( x_n(j_n) \in K \), the best constant rebalanced portfolio is  
\[
b^*(x_n(j_n)) = \left( \frac{n_1(j_n)}{n}, \frac{n_2(j_n)}{n} \right)^t,
\]  
where \( n_1(j_n) \) is the number of occurrences of 1 in the sequence \( j_n \). The corresponding wealth at the end of \( n \) periods is  
\[
S_n^*(x_n(j_n)) = \binom{n}{n_1(j_n)} \frac{n_1(j_n)^{n_1(j_n)} n_2(j_n)^{n_2(j_n)}}{n^n} = \hat{w}(j_n) V_n,
\]  
from (16.116) and it therefore follows that  
\[
\sum_{x_n \in K} S_n^*(x_n) = \frac{1}{V_n} \sum_{j_n} \hat{w}(j_n) = \frac{1}{V_n}.
\]  

We then have the following inequality for any portfolio sequence \( \{b_i\}_{i=1}^n \), with \( S_n(x_n) \) defined as in (16.104):  
\[
\min_{x_n \in K} \frac{S_n(x_n)}{S_n^*(x_n)} \leq \frac{\sum_{\tilde{x}_n \in K} S_n^*(\tilde{x}_n)}{\sum_{x_n \in K} S_n^*(x_n) \sum_{\tilde{x}_n \in K} \frac{S_n(\tilde{x}_n)}{S_n^*(\tilde{x}_n)}} = \frac{1}{V_n},
\]  
where the inequality follows from the fact that the minimum is less than the average. Thus,  
\[
\max_b \min_{x_n \in K} \frac{S_n(x_n)}{S_n^*(x_n)} \leq V_n.
\]  

The strategy described in the theorem puts mass on all sequences of length \( n \) and is clearly dependent on \( n \). We can recast the strategy in incremental terms (i.e., in terms of the amount bet on stock 1 and stock 2 at time 1), then, conditional on the outcome at time 1, the amount bet on each of the two stocks at time 2, and so on. Consider the weight \( \hat{b}_i,1 \) assigned by the algorithm to stock 1 at time \( i \) given the previous sequence of stock vectors \( x_{i-1} \). We can calculate this by summing over all sequences \( j_n \) that have a 1 in position \( i \), giving  
\[
\hat{b}_i,1(x_{i-1}) = \frac{\sum_{j_{i-1} \in M_{i-1}} \hat{w}(j_i) x(j_{i-1})}{\sum_{j_i \in M_i} \hat{w}(j_i) x(j_{i-1})}.
\]



INFORMATION THEORY AND PORTFOLIO THEORY

Where  
\hat{w}(j^n) = \sum_{j^n: j^{n-1} \subseteq j^n} w(j^n)  
is the weight put on all sequences j^n that start with j^{i-1}, and  
x(j^{i-1}) = \sum_{k=1}^{i-1} x_k j_k  
is the return on those sequences as defined in (16.106).  

Investigation of the asymptotics of V_n reveals [401, 496] that  
V_n \sim \frac{2^n m-1 \hat{W}(m/2)}{\sqrt{\pi}}  
for m assets. In particular, for m = 2 assets,  
V_n \sim \frac{2^n}{\pi}  
and  
\frac{1}{2} \sqrt{n + 1} \leq V_n \leq 2 \sqrt{n + 1}  
for all n [400]. Consequently, for m = 2 stocks, the causal portfolio strategy \hat{b}(x^{i-1}) given in (16.132) achieves wealth \hat{S}_n(x^n) such that  
\frac{\hat{S}_n(x^n)}{S^*_n(x^n)} \geq V_n \geq \frac{1}{2} \sqrt{n + 1}  
for all market sequences x^n.  

#### 16.7.2 Horizon-Free Universal Portfolios  

We describe the horizon-free strategy in terms of a weighting of different portfolio strategies. As described earlier, each constantly rebalanced portfolio b can be viewed as corresponding to a mutual fund that rebalances the m assets according to b. Initially, we distribute the wealth among these funds according to a distribution \mu(b), where d\mu(b) is the amount of wealth invested in portfolios in the neighborhood db of the constantly rebalanced portfolio b.  

Let  
S_n(b, x^n) = \prod_{i=1}^n b^T x_i  
be the wealth generated by a constant rebalanced portfolio b on the stock sequence x^n. Recall that  
S^*_n(x^n) = \max_{b \in B} S_n(b, x^n)  
is the wealth of the best constant rebalanced portfolio in hindsight.  

We investigate the causal portfolio defined by  
\hat{b}_{i+1}(x^i) = \frac{\int_B b S_i(b, x^i) d\mu(b)}{\int_B S_i(b, x^i) d\mu(b)}.  

We note that  
\hat{b}^T_{i+1} x^{i+1} = \frac{\int_B b^T x^{i+1} S_i(b, x^i) d\mu(b)}{\int_B S_i(b, x^i) d\mu(b)} = \frac{\int_B S_{i+1}(b, x^{i+1}) d\mu(b)}{\int_B S_i(b, x^i) d\mu(b)}.  

Thus, the product \hat{b}^T_i x_i telescopes, and we see that the wealth \hat{S}_n(x^n) resulting from this portfolio is given by  
\hat{S}_n(x^n) = \prod_{i=1}^n \hat{b}^T_i x_i = \int_B S_n(b, x^n) d\mu(b).  

There is another way to interpret (16.145). The amount given to portfolio manager b is d\mu(b), the resulting growth factor for the manager rebalancing to b is S(b, x^n), and the total wealth of this batch of investments is  
\hat{S}_n(x^n) = \int_B S_n(b, x^n) d\mu(b).  

Then \hat{b}_{i+1}, defined in (16.141), is the performance-weighted total "buy order" of the individual portfolio manager b.  

So far, we have not specified what distribution \mu(b) we use to apportion the initial wealth. We now use a distribution \mu that puts mass on all possible portfolios, so that we approximate the performance of the best portfolio for the actual distribution of stock price vectors.  

In the next lemma, we bound \hat{S}_n / S^*_n as a function of the initial wealth distribution \mu(b).  

**Lemma 16.7.2**  
Let S^*_n(x^n) in (16.140) be the wealth achieved by the best constant rebalanced portfolio and let \hat{S}_n(x^n) in (16.144) be the wealth achieved by the universal mixed portfolio \hat{b}(\cdot), given by  
\hat{b}_{i+1}(x^i) = \frac{\int_B b S_i(b, x^i) d\mu(b)}{\int_B S_i(b, x^i) d\mu(b)}.  

Then  
\frac{\hat{S}_n(x^n)}{S^*_n(x^n)} \geq \min_{j^n} \frac{\int_B \prod_{i=1}^n b_j d\mu(b)}{\prod_{i=1}^n b^*_j}.  

**Proof:**  
As before, we can write  
S^*_n(x^n) = \sum_{j^n} w^*(j^n) x(j^n),  
where w^*(j^n) = \prod_{i=1}^n b^*_j is the amount invested on the sequence j^n and x(j^n) = \prod_{i=1}^n x_j is the corresponding return. Similarly, we can write  
\hat{S}_n(x^n) = \int_B \prod_{i=1}^n b^T x_i d\mu(b) = \sum_{j^n} \hat{w}(j^n) x(j^n),  
where \hat{w}(j^n) = \int_B \prod_{i=1}^n b_j d\mu(b). Now applying Lemma 16.7.1, we have  
\frac{\hat{S}_n(x^n)}{S^*_n(x^n)} = \frac{\sum_{j^n} \hat{w}(j^n) x(j^n)}{\sum_{j^n} w^*(j^n) x(j^n)} \geq \min_{j^n} \frac{\hat{w}(j^n)}{w^*(j^n)}.  

We now apply this lemma when \mu(b) is the Dirichlet(1/2) distribution.  

**Theorem 16.7.2**  
For the causal universal portfolio \hat{b}(\cdot), i = 1, 2, \ldots, given in (16.141), with m = 2 stocks and d\mu(b) the Dirichlet(1/2, 1/2) distribution, we have  
\frac{\hat{S}_n(x^n)}{S^*_n(x^n)} \geq \frac{1}{2} \sqrt{n + 1},  
for all n and all stock sequences x^n.  

**Proof:**  
As in the discussion preceding (16.112), we can show that the weight put by the best constant portfolio b^* on the sequence j^n is  
\prod_{i=1}^n b^*_j = \binom{n}{k} 2^{-n},  
where k is the number of indices where j_i = 1. We can also explicitly calculate the integral in the numerator of (16.148) in Lemma 16.7.2 for the Dirichlet(1/2) density, defined for m variables as  
d\mu(b) = \frac{\hat{W}(m/2)}{\hat{W}(1/2)^m} \prod_{j=1}^m b_j^{-1/2} db,  
where \hat{W}(x) = \int_0^\infty e^{-xt} t^{x-1} dt denotes the gamma function. For simplicity, we consider the case of two stocks, in which case  
d\mu(b) = \frac{1}{\pi} \frac{1}{\sqrt{b(1 - b)}} db, \quad 0 \leq b \leq 1,  
where b is the fraction of wealth invested in stock 1. Now consider any sequence j^n \in \{1, 2\}^n, and consider the amount invested in that sequence,  
b(j^n) = \prod_{i=1}^n b_j = b^l (1 - b)^{n - l},  
where l is the number of indices where j_i = 1. Then  
\int_B b(j^n) d\mu(b) = \int_0^1 b^l (1 - b)^{n - l} \frac{1}{\pi} \frac{1}{\sqrt{b(1 - b)}} db = \frac{1}{\pi} \int_0^1 b^{l - 1/2} (1 - b)^{n - l - 1/2} db \triangleq \frac{1}{\pi} B(l + 1/2, n - l + 1/2),  
where B(\lambda_1, \lambda_2) is the beta function, defined as  
B(\lambda_1, \lambda_2) = \int_0^1 x^{\lambda_1 - 1} (1 - x)^{\lambda_2 - 1} dx = \frac{\hat{W}(\lambda_1) \hat{W}(\lambda_2)}{\hat{W}(\lambda_1 + \lambda_2)}.  

Note that for any integer n, \hat{W}(n + 1) = n! and \hat{W}(n + 1/2) = \frac{1 \cdot 3 \cdot 5 \cdots (2n - 1)}{2^n} \sqrt{\pi}. We can calculate B(l + 1/2, n - l + 1/2) by means of simple recursion using integration by parts. Alternatively, using (16.164), we obtain  
B(l + 1/2, n - l + 1/2) = \frac{\pi}{2^{2n}} \binom{2n}{2l} \binom{2l}{l} \binom{2n - 2l}{n - l}.  

Combining all the results with Lemma 16.7.2, we have  
\frac{\hat{S}_n(x^n)}{S^*_n(x^n)} \geq \min_{j^n} \frac{\int_B \prod_{i=1}^n b_j d\mu(b)}{\prod_{i=1}^n b^*_j} \geq \min_l \frac{1}{\pi} B(l + 1/2, n - l + 1/2) 2^{-n} \geq \frac{1}{2} \sqrt{n + 1},  
using the results in [135, Theorem 2].  

It follows for m = 2 stocks that  
\frac{\hat{S}_n}{S^*_n} \geq \frac{1}{\sqrt{2\pi}} V_n  
for all n and all market sequences x_1, x_2, \ldots, x_n. Thus, good minimax performance for all n costs at most an extra factor \sqrt{2\pi} over the fixed horizon minimax portfolio. The cost of universality is V_n, which is asymptotically negligible in the growth rate in the sense that  
\frac{1}{n} \ln \hat{S}_n(x^n) - \frac{1}{n} \ln S^*_n(x^n) \geq \frac{1}{n} \ln V_n - \frac{1}{n} \ln \sqrt{2\pi} \to 0.  

Thus, the universal causal portfolio achieves the same asymptotic growth rate of wealth as the best hindsight portfolio.  

Let’s now consider how this portfolio algorithm performs on two real stocks. We consider a 14-year period (ending in 2004) and two stocks, Hewlett-Packard and Altria (formerly, Phillip Morris), which are both components of the Dow Jones Index. Over these 14 years, HP went up by a factor of 11.8, while Altria went up by a factor of 11.5. The performance of the different constantly rebalanced portfolios that contain HP and Altria are shown in Figure 16.2. The best constantly rebalanced portfolio (which can be computed only in hindsight) achieves a growth of a factor of 18.7 using a mixture of about 51% HP and 49% Altria. The universal portfolio strategy described in this section achieves a growth factor of 15.7 without foreknowledge.  

FIGURE 16.2. Performance of different constant rebalanced portfolios b for HP and Altria.  

16.8  
SHANNON–MCMILLAN–BREIMAN THEOREM (GENERAL AEP)  

The AEP for ergodic processes has come to be known as the Shannon–McMillan–Breiman theorem. In Chapter 3 we proved the AEP for i.i.d. processes. In this section we offer a proof of the theorem for a general ergodic process. We prove the convergence of  
\frac{1}{n} \ln p(X^n)  
by sandwiching it between two ergodic sequences.  

In a sense, an ergodic process is the most general dependent process for which the strong law of large numbers holds. For finite alphabet processes, ergodicity is equivalent to the convergence of the kth-order empirical distributions to their marginals for all k.  

The technical definition requires some ideas from probability theory. To be precise, an ergodic source is defined on a probability space (\Omega, B, P), where B is a \sigma-algebra of subsets of \Omega and P is a probability measure. A random variable X is defined as a function X(\omega), \omega \in \Omega, on the probability space. We also have a transformation T: \Omega \to \Omega, which plays the role of a time shift. We will say that the transformation is stationary if P(T^{-1} A) = P(A) for all A \in B. The transformation is called ergodic if every set A such that T^{-1} A = A, a.e., satisfies P(A) = 0 or 1. If T is stationary and ergodic, we say that the process defined by X_n(\omega) = X(T^n \omega) is stationary and ergodic. For a stationary ergodic source, Birkhoff’s ergodic theorem states that  
\frac{1}{n} \sum_{i=1}^n X_i(\omega) \to E[X] = \int X dP  
with probability 1.  

Thus, the law of large numbers holds for ergodic processes.  

We wish to use the ergodic theorem to conclude that  
-\frac{1}{n} \ln p(X_0, X_1, \ldots, X_{n-1}) = -\frac{1}{n} \sum_{i=0}^{n-1} \ln p(X_i | X_{i-1}^0) \to \lim_{n \to \infty} E[-\ln p(X_n | X_{n-1}^0)].  

But the stochastic sequence p(X_i | X_{i-1}^0) is not ergodic. However, the closely related quantities p(X_i | X_{i-k}^{i-1}) and p(X_i | X_{-\infty}^{i-1}) are ergodic and have expectations easily identified as entropy rates. We plan to sandwich p(X_i | X_{i-1}^0) between these two more tractable processes.



---  
**Page 71**  
**16.8 Shannon–McMillan–Breiman Theorem (General AEP)**  

We define the kth-order entropy \( H_k \) as  
\[
H_k = E\{-\log p(X_k|X_{k-1}, X_{k-2}, \ldots, X_0)\}  
\]  
\[
= E\{-\log p(X_0|X_{-1}, X_{-2}, \ldots, X_{-k})\},  
\]  
where the last equation follows from stationarity. Recall that the entropy rate is given by  
\[
H = \lim_{k \to \infty} H_k  
\]  
\[
= \lim_{n \to \infty} \frac{1}{n} \sum_{k=0}^{n-1} H_k.  
\]  
Of course, \( H_k \downarrow H \) by stationarity and the fact that conditioning does not increase entropy. It will be crucial that \( H_k \downarrow H = H_\infty \), where  
\[
H_\infty = E\{-\log p(X_0|X_{-1}, X_{-2}, \ldots)\}.  
\]  
The proof that \( H_\infty = H \) involves exchanging expectation and limit.  

The main idea in the proof goes back to the idea of (conditional) proportional gambling. A gambler receiving uniform odds with the knowledge of the \( k \) past will have a growth rate of wealth \( \log |X| - H_k \), while a gambler with knowledge of the infinite past will have a growth rate of wealth of \( \log |X| - H_\infty \). We don’t know the wealth growth rate of a gambler with growing knowledge of the past \( X_n^0 \), but it is certainly sandwiched between \( \log |X| - H_k \) and \( \log |X| - H_\infty \). But \( H_k \downarrow H = H_\infty \). Thus, the sandwich closes and the growth rate must be \( \log |X| - H \).  

We will prove the theorem based on lemmas that will follow the proof.  

**Theorem 16.8.1 (AEP: Shannon–McMillan–Breiman Theorem)**  
If \( H \) is the entropy rate of a finite-valued stationary ergodic process \( \{X_n\} \), then  
\[
-\frac{1}{n} \log p(X_0, \ldots, X_{n-1}) \to H  
\]  
with probability 1.  

**Proof:**  
We prove this for finite alphabet \( X \); this proof and the proof for countable alphabets and densities is given in Algoet and Cover [20]. We argue that the sequence of random variables \( -\frac{1}{n} \log p(X_n^0) \) is asymptotically sandwiched between the upper bound \( H_k \) and the lower bound \( H_\infty \) for all \( k \geq 0 \). The AEP will follow since \( H_k \to H_\infty \) and \( H_\infty = H \). The kth-order Markov approximation to the probability is defined for \( n \geq k \) as  
\[
p_k(X_n^0) = p(X_k^0) \prod_{i=k}^{n-1} p(X_i|X_{i-1}, X_{i-k}).  
\]  

---  
**Page 72**  
From Lemma 16.8.3 we have  
\[
\limsup_{n \to \infty} \frac{1}{n} \log \frac{p_k(X_n^0)}{p(X_n^0)} \leq 0,  
\]  
which we rewrite, taking the existence of the limit \( \frac{1}{n} \log p_k(X_n^0) \) into account (Lemma 16.8.1), as  
\[
\limsup_{n \to \infty} \frac{1}{n} \log \frac{1}{p(X_n^0)} \leq \lim_{n \to \infty} \frac{1}{n} \log \frac{1}{p_k(X_n^0)} = H_k  
\]  
for \( k = 1, 2, \ldots \). Also, from Lemma 16.8.3, we have  
\[
\limsup_{n \to \infty} \frac{1}{n} \log \frac{p(X_n^0)}{p(X_n^0|X_{-1}, X_{-2}, \ldots)} \leq 0,  
\]  
which we rewrite as  
\[
\liminf \frac{1}{n} \log \frac{1}{p(X_n^0)} \geq \lim \frac{1}{n} \log \frac{1}{p(X_n^0|X_{-1}, X_{-2}, \ldots)} = H_\infty  
\]  
from the definition of \( H_\infty \) in Lemma 16.8.1.  

Putting together (16.182) and (16.184), we have  
\[
H_\infty \leq \liminf -\frac{1}{n} \log p(X_n^0) \leq \limsup -\frac{1}{n} \log p(X_n^0) \leq H_k  
\]  
for all \( k \). But by Lemma 16.8.2, \( H_k \to H_\infty = H \). Consequently,  
\[
\lim -\frac{1}{n} \log p(X_n^0) = H.  
\]  
□  

We now prove the lemmas that were used in the main proof.  

**Lemma 16.8.1 (Markov approximations)**  
For a stationary ergodic stochastic process \( \{X_n\} \),  
\[
-\frac{1}{n} \log p_k(X_n^0) \to H_k \quad \text{with probability 1},  
\]  
\[
-\frac{1}{n} \log p(X_n^0|X_{-1}, X_{-2}, \ldots) \to H_\infty \quad \text{with probability 1}.  
\]  

**Proof:**  
Functions \( Y_n = f(X_n^0) \) of ergodic processes \( \{X_i\} \) are ergodic processes. Thus, \( \log p(X_n|X_{n-1}, X_{n-2}, \ldots, X_{n-k}) \) and \( \log p(X_n|X_{n-1}, X_{n-2}, \ldots, X_{-\infty}) \) are also ergodic processes, and  
\[
-\frac{1}{n} \log p_k(X_n^0) = -\frac{1}{n} \log p(X_k^0) - \frac{1}{n} \sum_{i=k}^{n-1} \log p(X_i|X_{i-1}, X_{i-2}, \ldots, X_{i-k})  
\]  
\[
\to 0 + H_k \quad \text{with probability 1},  
\]  
by the ergodic theorem. Similarly, by the ergodic theorem,  
\[
-\frac{1}{n} \log p(X_n^0|X_{-1}, X_{-2}, \ldots) = -\frac{1}{n} \sum_{i=0}^{n-1} \log p(X_i|X_{i-1}, X_{i-2}, \ldots)  
\]  
\[
\to H_\infty \quad \text{with probability 1}.  
\]  
□  

**Lemma 16.8.2 (No gap)**  
\( H_k \downarrow H_\infty \) and \( H = H_\infty \).  

**Proof:**  
We know that for stationary processes, \( H_k \downarrow H \), so it remains to show that \( H_k \downarrow H_\infty \), thus yielding \( H = H_\infty \). Levy’s martingale convergence theorem for conditional probabilities asserts that  
\[
p(x_0|X_{-1}, X_{-2}, \ldots, X_{-k}) \to p(x_0|X_{-1}, X_{-2}, \ldots, X_{-\infty}) \quad \text{with probability 1}  
\]  
for all \( x_0 \in X \). Since \( X \) is finite and \( p \log p \) is bounded and continuous in \( p \) for all \( 0 \leq p \leq 1 \), the bounded convergence theorem allows interchange of expectation and limit, yielding  
\[
\lim_{k \to \infty} H_k = \lim_{k \to \infty} E\left[-\sum_{x_0 \in X} p(x_0|X_{-1}, X_{-2}, \ldots, X_{-k}) \log p(x_0|X_{-1}, X_{-2}, \ldots, X_{-k})\right]  
\]  
\[
= E\left[-\sum_{x_0 \in X} p(x_0|X_{-1}, X_{-2}, \ldots, X_{-\infty}) \log p(x_0|X_{-1}, X_{-2}, \ldots, X_{-\infty})\right]  
\]  
\[
= H_\infty.  
\]  
Thus, \( H_k \downarrow H = H_\infty \).  
□  

**Lemma 16.8.3 (Sandwich)**  
\[
\limsup_{n \to \infty} \frac{1}{n} \log \frac{p_k(X_n^0)}{p(X_n^0)} \leq 0,  
\]  
\[
\limsup_{n \to \infty} \frac{1}{n} \log \frac{p(X_n^0)}{p(X_n^0|X_{-1}, X_{-2}, \ldots)} \leq 0.  
\]  

**Proof:**  
Let \( A \) be the support set of \( p(X_n^0) \). Then  
\[
E\left[\frac{p_k(X_n^0)}{p(X_n^0)}\right] = \sum_{x_n^0 \in A} p(x_n^0) \frac{p_k(x_n^0)}{p(x_n^0)} = \sum_{x_n^0 \in A} p_k(x_n^0) \leq 1.  
\]  
Similarly, let \( B(X_{-1}, X_{-2}, \ldots) \) denote the support set of \( p(\cdot|X_{-1}, X_{-2}, \ldots) \). Then we have  
\[
E\left[\frac{p(X_n^0)}{p(X_n^0|X_{-1}, X_{-2}, \ldots)}\right] = E\left[E\left[\frac{p(X_n^0)}{p(X_n^0|X_{-1}, X_{-2}, \ldots)} \bigg| X_{-1}, X_{-2}, \ldots\right]\right]  
\]  
\[
= E\left[\sum_{x_n \in B(X_{-1}, X_{-2}, \ldots)} \frac{p(x_n)}{p(x_n|X_{-1}, X_{-2}, \ldots)} p(x_n|X_{-1}, X_{-2}, \ldots)}\right]  
\]  
\[
= E\left[\sum_{x_n \in B(X_{-1}, X_{-2}, \ldots)} p(x_n)\right] \leq 1.  
\]  
By Markov’s inequality and (16.202), we have  
\[
\Pr\left(\frac{p_k(X_n^0)}{p(X_n^0)} \geq t_n\right) \leq \frac{1}{t_n},  
\]  
or  
\[
\Pr\left(\frac{1}{n} \log \frac{p_k(X_n^0)}{p(X_n^0)} \geq \frac{1}{n} \log t_n\right) \leq \frac{1}{t_n}.  
\]  
Letting \( t_n = n^2 \) and noting that \( \sum_{n=1}^\infty \frac{1}{n^2} < \infty \), we see by the Borel–Cantelli lemma that the event  
\[
\left\{\frac{1}{n} \log \frac{p_k(X_n^0)}{p(X_n^0)} \geq \frac{1}{n} \log t_n\right\}  
\]  
occurs only finitely often with probability 1. Thus,  
\[
\limsup_{n \to \infty} \frac{1}{n} \log \frac{p_k(X_n^0)}{p(X_n^0)} \leq 0 \quad \text{with probability 1}.  
\]  
Applying the same arguments using Markov’s inequality to (16.206), we obtain  
\[
\limsup_{n \to \infty} \frac{1}{n} \log \frac{p(X_n^0)}{p(X_n^0|X_{-1}, X_{-2}, \ldots)} \leq 0 \quad \text{with probability 1},  
\]  
proving the lemma.  
□  

The arguments used in the proof can be extended to prove the AEP for the stock market (Theorem 16.5.3).  

---  
**Summary**  

**Growth rate.** The growth rate of a stock market portfolio \( b \) with respect to a distribution \( F(x) \) is defined as  
\[
W(b, F) = \int \log b_t(x) dF(x) = E[\log b_t(x)].  
\]  
**Log-optimal portfolio.** The optimal growth rate with respect to a distribution \( F(x) \) is  
\[
W^*(F) = \max_b W(b, F).  
\]  
The portfolio \( b^* \) that achieves the maximum of \( W(b, F) \) is called the log-optimal portfolio.  

**Concavity.** \( W(b, F) \) is concave in \( b \) and linear in \( F \). \( W^*(F) \) is convex in \( F \).  

**Optimality conditions.** The portfolio \( b^* \) is log-optimal if and only if  
\[
E\left[\frac{X_i}{b^*_t X}\right] = 1 \quad \text{if } b^*_i > 0, \leq 1 \quad \text{if } b^*_i = 0.  
\]  

**Expected ratio optimality.** If \( S_n^* = \prod_{i=1}^n b^*_t X_i \), \( S_n = \prod_{i=1}^n b_t X_i \), then  
\[
E\left[\frac{S_n}{S_n^*}\right] \leq 1 \quad \text{if and only if} \quad E\left[\frac{\log S_n}{S_n^*}\right] \leq 0.  
\]  

**Growth rate (AEP)**  
\[
\frac{1}{n} \log S_n^* \to W^*(F) \quad \text{with probability 1}.  
\]  

**Asymptotic optimality**  
\[
\limsup_{n \to \infty} \frac{1}{n} \log \frac{S_n}{S_n^*} \leq 0 \quad \text{with probability 1}.  
\]  

**Wrong information.** Believing \( g \) when \( f \) is true loses  
\[
\Delta W = W(b^*(f), F) - W(b^*(g), F) \leq D(f \| g).  
\]  

**Side information \( Y \)**  
\[
\Delta W \leq I(X; Y).  
\]  

**Chain rule**  
\[
W^*(X_i | X_1, X_2, \ldots, X_{i-1}) = \max_{b_i(x_1, x_2, \ldots, x_{i-1})} E[\log b_i(x_1, x_2, \ldots, x_{i-1}) X_i].  
\]  
\[
W^*(X_1, X_2, \ldots, X_n) = \sum_{i=1}^n W^*(X_i | X_1, X_2, \ldots, X_{i-1}).  
\]  

---  
**Page 77**  
**Summary**  

**Growth rate for a stationary market.**  
\[
W_\infty^* = \lim_{n \to \infty} W^*(X_1, X_2, \ldots, X_n)  
\]  
\[
\frac{1}{n} \log S_n^* \to W_\infty^*.  
\]  

**Competitive optimality of log-optimal portfolios.**  
\[
\Pr(V S \geq U S^*) \leq \frac{1}{2}.  
\]  

**Universal portfolio.**  
\[
\max_{\hat{b}_i(\cdot)} \min_{x_i, b} \frac{1}{n} \sum_{i=1}^n \hat{b}_t(x^{i-1}) x_i}{\frac{1}{n} \sum_{i=1}^n b_t x_i} = V_n,  
\]  
where  
\[
V_n = \left[\sum_{n_1 + \cdots + n_m = n} \frac{n}{n_1, n_2, \ldots, n_m} 2^{-nH(n_1/n, \ldots, n_m/n)}\right]^{-1}.  
\]  
For \( m = 2 \),  
\[
V_n \sim \frac{2}{\pi n}.  
\]  

The causal universal portfolio  
\[
\hat{b}_{i+1}(x^i) = \frac{\int_{S} S(b, x^i) d\mu(b)}{\int_{S} S(b, x^i) d\mu(b)}  
\]  
achieves  
\[
\hat{S}_n(x^n) S_n^*(x^n) \geq \frac{1}{2} \sqrt{n + 1}  
\]  
for all \( n \) and all \( x^n \).  

**AEP.** If \( \{X_i\} \) is stationary ergodic, then  
\[
-\frac{1}{n} \log p(X_1, X_2, \ldots, X_n) \to H(X) \quad \text{with probability 1}.  
\]  

---  
**Page 78**  
**Information Theory and Portfolio Theory**  

**Problems**  

**16.1 Growth rate.**  
Let \( X = \begin{cases} (1, a) & \text{with probability } \frac{1}{2}, \\ (1, 1/a) & \text{with probability } \frac{1}{2}. \end{cases} \)  
where \( a > 1 \). This vector \( X \) represents a stock market vector of cash vs. a hot stock. Let  
\[
W(b, F) = E[\log b_t X]  
\]  
and  
\[
W^* = \max_b W(b, F)  
\]  
be the growth rate.  

(a) Find the log optimal portfolio \( b^* \).  

(b) Find the growth rate \( W^* \).  

(c) Find the asymptotic behavior of \( S_n = \prod_{i=1}^n b_t X_i \) for all \( b \).  

**16.2 Side information.**  
Suppose, in Problem 16.1, that  
\[
Y = \begin{cases} 1 & \text{if } (X_1, X_2) \geq (1, 1), \\ 0 & \text{if } (X_1, X_2) \leq (1, 1). \end{cases}  
\]  
Let the portfolio \( b \) depend on \( Y \). Find the new growth rate \( W^{**} \) and verify that \( \Delta W = W^{**} - W^* \) satisfies  
\[
\Delta W \leq I(X; Y).  
\]  

**16.3 Stock dominance.**  
Consider a stock market vector \( X = (X_1, X_2) \). Suppose that \( X_1 = 2 \) with probability 1. Thus, an investment in the first stock is doubled at the end of the day.  

Perform the following actions:  
1. Remove unnecessary formatting, extra spaces, or line breaks.  
2. Fix broken sentences or paragraphs.  
3. Ensure consistent punctuation and capitalization.  
4. Remove any non-standard characters or encoding artifacts.  
5. Preserve the original meaning and structure of the text.  
6. Make the text more readable and well-formatted.  

Return only the cleaned text without additional explanations.



PROBLEMS  
653  
(a) Find necessary and sufficient conditions on the distribution of stock X2 such that the log-optimal portfolio b∗ invests all the wealth in stock X2 [i.e., b∗ = (0, 1)].  
(b) Argue for any distribution on X2 that the growth rate satisfies W∗ ≥ 1.  

16.4 Including experts and mutual funds.  
Let X ∼ F(x), x ∈ Rm+, be the vector of price relatives for a stock market. Suppose that an “expert” suggests a portfolio b. This would result in a wealth factor btX. We add this to the stock alternatives to form ˜X = (X1, X2, . . . , Xm, btX). Show that the new growth rate,  
˜W∗ = max  
b1,...,bm,bm+1  
∫ ln(bt˜x) dF(˜x),  
(16.231)  
is equal to the old growth rate,  
W∗ = max  
b1,...,bm  
∫ ln(btx) dF(x).  
(16.232)  

16.5 Growth rate for symmetric distribution.  
Consider a stock vector X ∼ F(x), X ∈ Rm, X ≥ 0, where the component stocks are exchangeable. Thus, F(x1, x2, . . . , xm) = F(xσ(1), xσ(2), . . . , xσ(m)) for all permutations σ.  
(a) Find the portfolio b∗ optimizing the growth rate and establish its optimality. Now assume that X has been normalized so that 1/m ∑m i=1 Xi = 1, and F is symmetric as before.  
(b) Again assuming X to be normalized, show that all symmetric distributions F have the same growth rate against b∗.  
(c) Find this growth rate.  

16.6 Convexity.  
We are interested in the set of stock market densities that yield the same optimal portfolio. Let Pb0 be the set of all probability densities on Rm+ for which b0 is optimal. Thus, Pb0 = {p(x) : ∫ ln(btx)p(x) dx is maximized by b = b0}. Show that Pb0 is a convex set. It may be helpful to use Theorem 16.2.2.  

16.7 Short selling.  
Let X =  
(1, 2), p,  
(1, 1/2), 1 − p.  
Let B = {(b1, b2) : b1 + b2 = 1}. Thus, this set of portfolios B does not include the constraint bi ≥ 0. (This allows short selling.)  

--- Page 80 ---  
654  
INFORMATION THEORY AND PORTFOLIO THEORY  
(a) Find the log optimal portfolio b∗(p).  
(b) Relate the growth rate W∗(p) to the entropy rate H(p).  

16.8 Normalizing x.  
Suppose that we define the log-optimal portfolio b∗ to be the portfolio maximizing the relative growth rate  
∫ ln(btx) (1/m ∑m i=1 xi) dF(x1, . . . , xm).  
The virtue of the normalization 1/m ∑ Xi, which can be viewed as the wealth associated with a uniform portfolio, is that the relative growth rate is finite even when the growth rate ∫ ln(btx) dF(x) is not. This matters, for example, if X has a St. Petersburg-like distribution. Thus, the log-optimal portfolio b∗ is defined for all distributions F, even those with infinite growth rates W∗(F).  
(a) Show that if b maximizes ∫ ln(btx) dF(x), it also maximizes ∫ ln(btx) utx dF(x), where u = (1/m, 1/m, . . . , 1/m).  
(b) Find the log optimal portfolio b∗ for  
X =  
(22k+1, 22k), 2−(k+1),  
(22k, 22k+1), 2−(k+1),  
where k = 1, 2, . . . .  
(c) Find EX and W∗.  
(d) Argue that b∗ is competitively better than any portfolio b in the sense that Pr{btX > cb∗tX} ≤ 1/c.  

16.9 Universal portfolio.  
We examine the first n = 2 steps of the implementation of the universal portfolio in (16.7.2) for µ(b) uniform for m = 2 stocks. Let the stock vectors for days 1 and 2 be x1 = (1, 1/2), and x2 = (1, 2). Let b = (b, 1 − b) denote a portfolio.  
(a) Graph S2(b) = ∑2 i=1 btxi, 0 ≤ b ≤ 1.  
(b) Calculate S∗2 = maxb S2(b).  
(c) Argue that log S2(b) is concave in b.  
(d) Calculate the (universal) wealth ˆS2 = ∫1 0 S2(b) db.  
(e) Calculate the universal portfolio at times n = 1 and n = 2:  
ˆb1 = ∫1 0 b db,  
ˆb2(x1) = ∫1 0 b S1(b) db / ∫1 0 S1(b) db.  
(f) Which of S2(b), S∗2, ˆS2, ˆb2 are unchanged if we permute the order of appearance of the stock vector outcomes [i.e., if the sequence is now (1, 2), (1, 1/2)]?  

16.10 Growth optimal.  
Let X1, X2 ≥ 0, be price relatives of two independent stocks. Suppose that EX1 > EX2. Do you always want some of X1 in a growth rate optimal portfolio S(b) = bX1 + bX2? Prove or provide a counterexample.  

16.11 Cost of universality.  
In the discussion of finite-horizon universal portfolios, it was shown that the loss factor due to universality is  
Vn = ∑k=0 n (n k) (n −k) n (n−k) .  
(16.233)  
Evaluate Vn for n = 1, 2, 3.  

16.12 Convex families.  
This problem generalizes Theorem 16.2.2. We say that S is a convex family of random variables if S1, S2 ∈ S implies that λS1 + (1 − λ)S2 ∈ S. Let S be a closed convex family of random variables. Show that there is a random variable S∗ ∈ S such that  
E[ln(S/S∗)] ≤ 0  
(16.234)  
for all S ∈ S if and only if  
E[S/S∗] ≤ 1  
(16.235)  
for all S ∈ S.  

HISTORICAL NOTES  
There is an extensive literature on the mean–variance approach to investment in the stock market. A good introduction is the book by Sharpe [491]. Log-optimal portfolios were introduced by Kelly [308] and Latané [346], and generalized by Breiman [75]. The bound on the increase in the growth rate in terms of the mutual information is due to Barron and Cover [31]. See Samuelson [453, 454] for a criticism of log-optimal investment.  

The proof of the competitive optimality of the log-optimal portfolio is due to Bell and Cover [39, 40]. Breiman [75] investigated asymptotic optimality for random market processes.  

The AEP was introduced by Shannon. The AEP for the stock market and the asymptotic optimality of log-optimal investment are given in Algoet and Cover [21]. The relatively simple sandwich proof for the AEP is due to Algoet and Cover [20]. The AEP for real-valued ergodic processes was proved in full generality by Barron [34] and Orey [402].  

The universal portfolio was defined in Cover [110] and the proof of universality was given in Cover [110] and more exactly in Cover and Ordentlich [135]. The fixed-horizon exact calculation of the cost of universality Vn is given in Ordentlich and Cover [401]. The quantity Vn also appears in data compression in the work of Shtarkov [496].  

--- Page 83 ---  
CHAPTER 17  
INEQUALITIES IN INFORMATION THEORY  
This chapter summarizes and reorganizes the inequalities found throughout this book. A number of new inequalities on the entropy rates of subsets and the relationship of entropy and Lp norms are also developed. The intimate relationship between Fisher information and entropy is explored, culminating in a common proof of the entropy power inequality and the Brunn–Minkowski inequality. We also explore the parallels between the inequalities in information theory and inequalities in other branches of mathematics, such as matrix theory and probability theory.  

17.1 BASIC INEQUALITIES OF INFORMATION THEORY  
Many of the basic inequalities of information theory follow directly from convexity.  

Definition  
A function f is said to be convex if  
f(λx1 + (1 − λ)x2) ≤ λf(x1) + (1 − λ)f(x2)  
(17.1)  
for all 0 ≤ λ ≤ 1 and all x1 and x2.  

Theorem 17.1.1 (Theorem 2.6.2: Jensen’s inequality)  
If f is convex, then  
f(EX) ≤ Ef(X).  
(17.2)  

Lemma 17.1.1  
The function log x is concave and x log x is convex, for 0 < x < ∞.  

Theorem 17.1.2 (Theorem 2.7.1: Log sum inequality)  
For positive numbers a1, a2, . . . , an and b1, b2, . . . , bn,  
∑i=1 n ai log(ai/bi) ≥  
(∑i=1 n ai) log(∑i=1 n ai / ∑i=1 n bi)  
(17.3)  
with equality if and only if ai/bi = constant.  

We recall the following properties of entropy from Section 2.1.  

Definition  
The entropy H(X) of a discrete random variable X is defined by  
H(X) = − ∑x∈X p(x) log p(x).  
(17.4)  

Theorem 17.1.3 (Lemma 2.1.1, Theorem 2.6.4: Entropy bound)  
0 ≤ H(X) ≤ log |X|.  
(17.5)  

Theorem 17.1.4 (Theorem 2.6.5: Conditioning reduces entropy)  
For any two random variables X and Y,  
H(X|Y) ≤ H(X),  
(17.6)  
with equality if and only if X and Y are independent.  

Theorem 17.1.5 (Theorem 2.5.1 with Theorem 2.6.6: Chain rule)  
H(X1, X2, . . . , Xn) = ∑i=1 n H(Xi|Xi−1, . . . , X1) ≤ ∑i=1 n H(Xi),  
(17.7)  
with equality if and only if X1, X2, . . . , Xn are independent.  

Theorem 17.1.6 (Theorem 2.7.3)  
H(p) is a concave function of p.  

We now state some properties of relative entropy and mutual information (Section 2.3).  

Definition  
The relative entropy or Kullback–Leibler distance between two probability mass functions p(x) and q(x) is defined by  
D(p||q) = ∑x∈X p(x) log(p(x)/q(x)).  
(17.8)  

Definition  
The mutual information between two random variables X and Y is defined by  
I(X; Y) = ∑x∈X ∑y∈Y p(x, y) log(p(x, y)/p(x)p(y)) = D(p(x, y)||p(x)p(y)).  
(17.9)  

The following basic information inequality can be used to prove many of the other inequalities in this chapter.  

Theorem 17.1.7 (Theorem 2.6.3: Information inequality)  
For any two probability mass functions p and q,  
D(p||q) ≥ 0  
(17.10)  
with equality if and only if p(x) = q(x) for all x ∈ X.  

Corollary  
For any two random variables X and Y,  
I(X; Y) = D(p(x, y)||p(x)p(y)) ≥ 0  
(17.11)  
with equality if and only if p(x, y) = p(x)p(y) (i.e., X and Y are independent).  

Theorem 17.1.8 (Theorem 2.7.2: Convexity of relative entropy)  
D(p||q) is convex in the pair (p, q).  

Theorem 17.1.9 (Theorem 2.4.1)  
I(X; Y) = H(X) − H(X|Y).  
(17.12)  
I(X; Y) = H(Y) − H(Y|X).  
(17.13)  
I(X; Y) = H(X) + H(Y) − H(X, Y).  
(17.14)  
I(X; X) = H(X).  
(17.15)  

Theorem 17.1.10 (Section 4.4)  
For a Markov chain:  
1. Relative entropy D(µn||µ'n) decreases with time.  
2. Relative entropy D(µn||µ) between a distribution and the stationary distribution decreases with time.  
3. Entropy H(Xn) increases if the stationary distribution is uniform.  
4. The conditional entropy H(Xn|X1) increases with time for a stationary Markov chain.  

Theorem 17.1.11  
Let X1, X2, . . . , Xn be i.i.d. ∼ p(x). Let ˆpn be the empirical probability mass function of X1, X2, . . . , Xn. Then  
ED(ˆpn||p) ≤ ED(ˆpn−1||p).  
(17.16)  

17.2 DIFFERENTIAL ENTROPY  
We now review some of the basic properties of differential entropy (Section 8.1).  

Definition  
The differential entropy h(X1, X2, . . . , Xn), sometimes written h(f), is defined by  
h(X1, X2, . . . , Xn) = − ∫ f(x) log f(x) dx.  
(17.17)  

The differential entropy for many common densities is given in Table 17.1.  

Definition  
The relative entropy between probability densities f and g is  
D(f||g) = ∫ f(x) log(f(x)/g(x)) dx.  
(17.18)  

The properties of the continuous version of relative entropy are identical to the discrete version. Differential entropy, on the other hand, has some properties that differ from those of discrete entropy. For example, differential entropy may be negative.  

We now restate some of the theorems that continue to hold for differential entropy.  

Theorem 17.2.1 (Theorem 8.6.1: Conditioning reduces entropy)  
h(X|Y) ≤ h(X), with equality if and only if X and Y are independent.  

Theorem 17.2.2 (Theorem 8.6.2: Chain rule)  
h(X1, X2, . . . , Xn) = ∑i=1 n h(Xi|Xi−1, Xi−2, . . . , X1) ≤ ∑i=1 n h(Xi)  
(17.19)  
with equality if and only if X1, X2, . . . , Xn are independent.  

Lemma 17.2.1  
If X and Y are independent, then h(X + Y) ≥ h(X).  
Proof:  
h(X + Y) ≥ h(X + Y|Y) = h(X|Y) = h(X).  
□  

Elements of Information Theory, Second Edition,  
By Thomas M. Cover and Joy A. Thomas  
Copyright  2006 John Wiley & Sons, Inc.  
657  

--- Page 84 ---  
658  
INEQUALITIES IN INFORMATION THEORY  
Theorem 17.1.1 (Theorem 2.6.2: Jensen’s inequality)  
If f is convex, then  
f(EX) ≤ Ef(X).  
(17.2)  

Lemma 17.1.1  
The function log x is concave and x log x is convex, for 0 < x < ∞.  

Theorem 17.1.2 (Theorem 2.7.1: Log sum inequality)  
For positive numbers a1, a2, . . . , an and b1, b2, . . . , bn,  
∑i=1 n ai log(ai/bi) ≥  
(∑i=1 n ai) log(∑i=1 n ai / ∑i=1 n bi)  
(17.3)  
with equality if and only if ai/bi = constant.  

We recall the following properties of entropy from Section 2.1.  

Definition  
The entropy H(X) of a discrete random variable X is defined by  
H(X) = − ∑x∈X p(x) log p(x).  
(17.4)  

Theorem 17.1.3 (Lemma 2.1.1, Theorem 2.6.4: Entropy bound)  
0 ≤ H(X) ≤ log |X|.  
(17.5)  

Theorem 17.1.4 (Theorem 2.6.5: Conditioning reduces entropy)  
For any two random variables X and Y,  
H(X|Y) ≤ H(X),  
(17.6)  
with equality if and only if X and Y are independent.  

Theorem 17.1.5 (Theorem 2.5.1 with Theorem 2.6.6: Chain rule)  
H(X1, X2, . . . , Xn) = ∑i=1 n H(Xi|Xi−1, . . . , X1) ≤ ∑i=1 n H(Xi),  
(17.7)  
with equality if and only if X1, X2, . . . , Xn are independent.  

Theorem 17.1.6 (Theorem 2.7.3)  
H(p) is a concave function of p.  

We now state some properties of relative entropy and mutual information (Section 2.3).  

Definition  
The relative entropy or Kullback–Leibler distance between two probability mass functions p(x) and q(x) is defined by  
D(p||q) = ∑x∈X p(x) log(p(x)/q(x)).  
(17.8)  

Definition  
The mutual information between two random variables X and Y is defined by  
I(X; Y) = ∑x∈X ∑y∈Y p(x, y) log(p(x, y)/p(x)p(y)) = D(p(x, y)||p(x)p(y)).  
(17.9)  

The following basic information inequality can be used to prove many of the other inequalities in this chapter.  

Theorem 17.1.7 (Theorem 2.6.3: Information inequality)  
For any two probability mass functions p and q,  
D(p||q) ≥ 0  
(17.10)  
with equality if and only if p(x) = q(x) for all x ∈ X.  

Corollary  
For any two random variables X and Y,  
I(X; Y) = D(p(x, y)||p(x)p(y)) ≥ 0  
(17.11)  
with equality if and only if p(x, y) = p(x)p(y) (i.e., X and Y are independent).  

Theorem 17.1.8 (Theorem 2.7.2: Convexity of relative entropy)  
D(p||q) is convex in the pair (p, q).  

Theorem 17.1.9 (Theorem 2.4.1)  
I(X; Y) = H(X) − H(X|Y).  
(17.12)  
I(X; Y) = H(Y) − H(Y|X).  
(17.13)  
I(X; Y) = H(X) + H(Y) − H(X, Y).  
(17.14)  
I(X; X) = H(X).  
(17.15)  

Theorem 17.1.10 (Section 4.4)  
For a Markov chain:  
1. Relative entropy D(µn||µ'n) decreases with time.  
2. Relative entropy D(µn||µ) between a distribution and the stationary distribution decreases with time.  
3. Entropy H(Xn) increases if the stationary distribution is uniform.  
4. The conditional entropy H(Xn|X1) increases with time for a stationary Markov chain.  

Theorem 17.1.11  
Let X1, X2, . . . , Xn be i.i.d. ∼ p(x). Let ˆpn be the empirical probability mass function of X1, X2, . . . , Xn. Then  
ED(ˆpn||p) ≤ ED(ˆpn−1||p).  
(17.16)  

17.2 DIFFERENTIAL ENTROPY  
We now review some of the basic properties of differential entropy (Section 8.1).  

Definition  
The differential entropy h(X1, X2, . . . , Xn), sometimes written h(f), is defined by  
h(X1, X2, . . . , Xn) = − ∫ f(x) log f(x) dx.  
(17.17)  

The differential entropy for many common densities is given in Table 17.1.  

Definition  
The relative entropy between probability densities f and g is  
D(f||g) = ∫ f(x) log(f(x)/g(x)) dx.  
(17.18)  

The properties of the continuous version of relative entropy are identical to the discrete version. Differential entropy, on the other hand, has some properties that differ from those of discrete entropy. For example, differential entropy may be negative.  

We now restate some of the theorems that continue to hold for differential entropy.  

Theorem 17.2.1 (Theorem 8.6.1: Conditioning reduces entropy)  
h(X|Y) ≤ h(X), with equality if and only if X and Y are independent.  

Theorem 17.2.2 (Theorem 8.6.2: Chain rule)  
h(X1, X2, . . . , Xn) = ∑i=1 n h(Xi|Xi−1, Xi−2, . . . , X1) ≤ ∑i=1 n h(Xi)  
(17.19)  
with equality if and only if X1, X2, . . . , Xn are independent.  

Lemma 17.2.1  
If X and Y are independent, then h(X + Y) ≥ h(X).  
Proof:  
h(X + Y) ≥ h(X + Y|Y) = h(X|Y) = h(X).  
□



--- Page 87 ---
17.2
DIFFERENTIAL ENTROPY
661
TABLE 17.1
Differential Entropiesa
Distribution
Name
Density
Entropy (nats)
f(x) = x^{p-1}(1 - x)^{q-1}
B(p, q)
,
ln B(p, q) - (p - 1)[ψ(p) - ψ(p + q)]
0 ≤ x ≤ 1, p, q > 0
- (q - 1)[ψ(q) - ψ(p + q)]
f(x) = λ π / (λ² + x²)
Cauchy
ln(4πλ)
-∞ < x < ∞, λ > 0
f(x) = (2^n/2σ^n√W(n/2)x^{n-1}e^{-x²/2σ²})
Chi
ln σ√W(n/2) - n/2 - 1/2 ψ(n/2) + n/2
x > 0, n > 0
f(x) = (2^n/2σ^n√W(n/2)x^{n/2 - 1}e^{-x²/2σ²})
Chi-squared
ln 2σ²√W(n/2) - (1 - n/2)ψ(n/2) + n/2
x > 0, n > 0
f(x) = β^n/(n - 1)! x^{n - 1}e^{-βx}
Erlang
(1 - n)ψ(n) + ln β + n
x, β > 0, n > 0
Exponential
f(x) = 1/λ e^{-x/λ}
λ, x, λ > 0
1 + ln λ
f(x) = (n_1/2B(n_1/2, n_2/2)) x^{(n_1/2) - 1} / (n_1 + n_2/2)
F
ln B(n_1/2, n_2/2) + (1 - n_1/2)ψ(n_1/2) - (1 - n_2/2)ψ(n_2/2) + n_1 + n_2/2 ψ((n_1 + n_2)/2)
x > 0, n_1, n_2 > 0
Gamma
f(x) = x^{α - 1}e^{-x/β} / βαW(α)
ln(βW(α)) + (1 - α)ψ(α) + α
x, α, β > 0
f(x) = 1/(2λ) e^{-|x - θ|/λ}
Laplace
1 + ln 2λ
-∞ < x, θ < ∞, λ > 0
f(x) = e^{-x} / (1 + e^{-x})²
Logistic
2
-∞ < x < ∞

--- Page 88 ---
662
INEQUALITIES IN INFORMATION THEORY
TABLE 17.1 (continued)
Distribution
Name
Density
Entropy (nats)
f(x) = 1/(σ√(2π)) e^{-(ln(x - m))² / 2σ²}
Lognormal
m + 1/2 ln(2πeσ²)
x > 0, -∞ < m < ∞, σ > 0
Maxwell–
f(x) = (4π)^{-1/2} β^{3/2} x² e^{-βx²}
Boltzmann
1/2 ln π β + γ - 1/2
x, β > 0
f(x) = 1/(√(2πσ²)) e^{-(x - µ)² / 2σ²}
Normal
1/2 ln(2πeσ²)
-∞ < x, µ < ∞, σ > 0
Generalized
f(x) = 2β^{α/2} W(α/2) x^{α - 1} e^{-βx²}
normal
ln W(α/2) - α/2 ψ(α/2) + β/2
x, α, β > 0
Pareto
f(x) = a k^a / x^{a + 1}
ln k + 1 + 1/a
x ≥ k > 0, a > 0
Rayleigh
f(x) = x / b² e^{-x² / 2b²}
1 + ln β + γ/2
x, b > 0
f(x) = (1 + x²/n)^{-(n + 1)/2} / √n B(1/2, n/2)
Student’s t
(n + 1)/2 ψ((n + 1)/2) - 1/2 ψ(n/2) + ln √n B(1/2, n/2)
-∞ < x < ∞, n > 0
Triangular
f(x) = 2x/a, 0 ≤ x ≤ a; 2(1 - x)/(1 - a), a ≤ x ≤ 1
1/2 - ln 2
Uniform
f(x) = 1/(β - α), α ≤ x ≤ β
ln(β - α)
Weibull
f(x) = c α x^{c - 1} e^{-x^c / α}
(c - 1)γ + ln α + 1/c
x, c, α > 0
a All entropies are in nats; W(z) = ∫₀^∞ e^{-tt} t^{z - 1} dt; ψ(z) = d/dz ln W(z); γ = Euler’s constant = 0.57721566.... Source: Lazo and Rathie [543].

--- Page 89 ---
17.3
BOUNDS ON ENTROPY AND RELATIVE ENTROPY
663
Theorem 17.2.3 (Theorem 8.6.5)
Let the random vector X ∈ R^n have zero mean and covariance K = EX^t (i.e., K_{ij} = EX_iX_j, 1 ≤ i, j ≤ n). Then
h(X) ≤ 1/2 log(2πe)^n |K|
(17.20)
with equality if and only if X ∼ N(0, K).

17.3
BOUNDS ON ENTROPY AND RELATIVE ENTROPY
In this section, we revisit some of the bounds on the entropy function. The most useful is Fano’s inequality, which is used to bound away from zero the probability of error of the best decoder for a communication channel at rates above capacity.

Theorem 17.3.1 (Theorem 2.10.1: Fano’s inequality)
Given two random variables X and Y, let ˆX = g(Y) be any estimator of X given Y, and let P_e = Pr(X ≠ ˆX) be the probability of error. Then
H(P_e) + P_e log |X| ≥ H(X|ˆX) ≥ H(X|Y).
(17.21)
Consequently, if H(X|Y) > 0, then P_e > 0.

A similar result is given in the following lemma.

Lemma 17.3.1 (Lemma 2.10.1)
If X and X' are i.i.d. with entropy H(X), then
Pr(X = X') ≥ 2^{-H(X)}
(17.22)
with equality if and only if X has a uniform distribution.

The continuous analog of Fano’s inequality bounds the mean-squared error of an estimator.

Theorem 17.3.2 (Theorem 8.6.6)
Let X be a random variable with differential entropy h(X). Let ˆX be an estimate of X, and let E(X - ˆX)^2 be the expected prediction error. Then
E(X - ˆX)^2 ≥ 1/(2πe) e^{2h(X)}.
(17.23)
Given side information Y and estimator ˆX(Y),
E(X - ˆX(Y))² ≥ 1/(2πe) e^{2h(X|Y)}.
(17.24)

--- Page 90 ---
664
INEQUALITIES IN INFORMATION THEORY
Theorem 17.3.3 (L1 bound on entropy)
Let p and q be two probability mass functions on X such that
||p - q||_1 = ∑_{x∈X} |p(x) - q(x)| ≤ 1/2.
(17.25)
Then
|H(p) - H(q)| ≤ -||p - q||_1 log ||p - q||_1 |X|.
(17.26)
Proof:
Consider the function f(t) = -t log t shown in Figure 17.1. It can be verified by differentiation that the function f(·) is concave. Also, f(0) = f(1) = 0. Hence, the function is positive between 0 and 1. Consider the chord of the function from t to t + ν (where ν ≤ 1/2). The maximum absolute slope of the chord is at either end (when t = 0 or 1 - ν). Hence, for 0 ≤ t ≤ 1 - ν, we have
|f(t) - f(t + ν)| ≤ max{f(ν), f(1 - ν)} = -ν log ν.
(17.27)
Let r(x) = |p(x) - q(x)|. Then
|H(p) - H(q)| = ∑_{x∈X} (-p(x) log p(x) + q(x) log q(x))
≤ ∑_{x∈X} |(-p(x) log p(x) + q(x) log q(x))|
≤ ∑_{x∈X} -r(x) log r(x)
= ||p - q||_1 ∑_{x∈X} -r(x)/||p - q||_1 log r(x)/||p - q||_1 ||p - q||_1
= -||p - q||_1 log ||p - q||_1 + ||p - q||_1 ∑_{x∈X} -r(x)/||p - q||_1 log r(x)/||p - q||_1
≤ -||p - q||_1 log ||p - q||_1 + ||p - q||_1 log |X|,
(17.33)
where (17.30) follows from (17.27).
□

Finally, relative entropy is stronger than the L1 norm in the following sense:

Lemma 17.3.2 (Lemma 11.6.1)
D(p || p2) ≥ (1/2 ln 2)||p - q||_1.
(17.34)

The relative entropy between two probability mass functions P(x) and Q(x) is zero when P = Q. Around this point, the relative entropy has a quadratic behavior, and the first term in the Taylor series expansion of the relative entropy D(P || Q) around the point P = Q is the chi-squared distance between the distributions P and Q. Let
χ²(P, Q) = ∑_{x} (P(x) - Q(x))² / Q(x).
(17.35)

Lemma 17.3.3
For P near Q,
D(P || Q) = 1/2 χ² + · · · .
(17.36)
Proof:
See Problem 11.2.
□

--- Page 91 ---
17.4
INEQUALITIES FOR TYPES
665
The method of types is a powerful tool for proving results in large deviation theory and error exponents. We repeat the basic theorems.

Theorem 17.4.1 (Theorem 11.1.1)
The number of types with denominator n is bounded by
|P_n| ≤ (n + 1)|X|.
(17.37)

Theorem 17.4.2 (Theorem 11.1.2)
If X₁, X₂, ..., Xₙ are drawn i.i.d. according to Q(x), the probability of xⁿ depends only on its type and is given by
Qⁿ(xⁿ) = 2^{-n(H(P_{xⁿ}) + D(P_{xⁿ} || Q))}.
(17.38)

Theorem 17.4.3 (Theorem 11.1.3: Size of a type class T(P))
For any type P ∈ Pₙ,
1 / ((n + 1)|X|) 2^{-nH(P)} ≤ |T(P)| ≤ 2^{-nH(P)}.
(17.39)

Theorem 17.4.4 (Theorem 11.1.4)
For any P ∈ Pₙ and any distribution Q, the probability of the type class T(P) under Qⁿ is 2^{-nD(P || Q)} to first order in the exponent. More precisely,
1 / ((n + 1)|X|) 2^{-nD(P || Q)} ≤ Qⁿ(T(P)) ≤ 2^{-nD(P || Q)}.
(17.40)

--- Page 92 ---
666
INEQUALITIES IN INFORMATION THEORY
17.5
COMBINATORIAL BOUNDS ON ENTROPY
We give tight bounds on the size of (∑_{k=0}^n C(n, k)) when k is not 0 or n using the result of Wozencraft and Reiffen [568]:

Lemma 17.5.1
For 0 < p < 1, q = 1 - p, such that np is an integer,
1 / √(8npq) ≤ C(n, np) 2^{-nH(p)} ≤ 1 / √(πnpq).
(17.41)
Proof:
We begin with a strong form of Stirling’s approximation [208], which states that
√(2πn) (n/e)^n ≤ n! ≤ √(2πn) (n/e)^n e^{1/12n}.
(17.42)
Applying this to find an upper bound, we obtain
C(n, np) ≤ √(2πn(n/e)^{np} (np/e)^{np} (nq/e)^{nq}) e^{1/12n} / √(2πnpq).
(17.43)
= 1 / √(2πnpq) 1 / p^{np} q^{nq} e^{1/12n}
< 1 / √(πnpq) 2^{-nH(p)},
(17.44)
since e^{1/12} < √2, hence proving the upper bound.

The lower bound is obtained similarly. Using Stirling’s formula, we obtain
C(n, np) ≥ 1 / √(2πnpq) 2^{-nH(p)}.
(17.45)

--- Page 93 ---
17.6
ENTROPY RATES OF SUBSETS
667
Finally, I'll ensure that the entire text is well-formatted, with consistent punctuation, capitalization, and spacing. The cleaned text is now ready for use.



n, np,  
≥,  
√,  
2πn(n  
e)ne  
−  
1  
12np +  
1  
12nq  
√2πnp(np  
e)np√2πnq(nq  
e)nq  
(17.46)  
=  
1  
√2πnpq  
1  
pnpqnq e  
−  
1  
12np +  
1  
12nq  
(17.47)  
=  
1  
√2πnpq 2nH(p) e  
−  
1  
12np +  
1  
12nq  
(17.48)  

If np ≥ 1, and nq ≥ 3, then  
e  
−  
1  
12np +  
1  
12nq  
≥ e−1  
9 = 0.8948 >  
√π  
2  
= 0.8862,  
(17.49)  
and the lower bound follows directly from substituting this into the equation. The exceptions to this condition are the cases where np = 1, nq = 1 or 2, and np = 2, nq = 2 (the case when np ≥ 3, nq = 1 or 2 can be handled by flipping the roles of p and q). In each of these cases:  

- np = 1, nq = 1 → n = 2, p = 1/2,  
  n  
  np  
  = 2, bound = 2.  
- np = 1, nq = 2 → n = 3, p = 1/3,  
  n  
  np  
  = 3, bound = 2.92.  
- np = 2, nq = 2 → n = 4, p = 1/2,  
  n  
  np  
  = 6, bound = 5.66.  

Thus, even in these special cases, the bound is valid, and hence the lower bound is valid for all p ≠ 0, 1. Note that the lower bound blows up when p = 0 or p = 1, and is therefore not valid.  

□  

17.6 ENTROPY RATES OF SUBSETS  

We now generalize the chain rule for differential entropy. The chain rule provides a bound on the entropy rate of a collection of random variables in terms of the entropy of each random variable:  
h(X₁, X₂, ..., Xₙ) ≤  
n  
∑  
i=1  
h(Xᵢ).  
(17.50)  

--- Page 94 ---  

We extend this to show that the entropy per element of a subset of a set of random variables decreases as the size of the subset increases. This is not true for each subset but is true on the average over subsets, as expressed in Theorem 17.6.1.  

**Definition**  
Let (X₁, X₂, ..., Xₙ) have a density, and for every S ⊆ {1, 2, ..., n}, denote by X(S) the subset {Xᵢ : i ∈ S). Let  
h(n)  
k  
= 1  
n  
k  
∑  
S: |S|=k  
h(X(S))  
k  
.  
(17.51)  
Here, h(n)  
k  
is the average entropy in bits per symbol of a randomly drawn k-element subset of {X₁, X₂, ..., Xₙ}.  

The following theorem by Han [270] says that the average entropy decreases monotonically in the size of the subset.  

**Theorem 17.6.1**  
h(n)  
1  
≥ h(n)  
2  
≥ · · · ≥ h(n)  
n  
.  
(17.52)  

**Proof:**  
We first prove the last inequality, h(n)  
n  
≤ h(n)  
n−1. We write  
h(X₁, X₂, ..., Xₙ) = h(X₁, X₂, ..., Xₙ−1) + h(Xₙ | X₁, X₂, ..., Xₙ−1),  
h(X₁, X₂, ..., Xₙ) = h(X₁, X₂, ..., Xₙ−2, Xₙ) + h(Xₙ−1 | X₁, X₂, ..., Xₙ−2, Xₙ),  
...  
h(X₁, X₂, ..., Xₙ) ≤ h(X₂, X₃, ..., Xₙ) + h(X₁).  

Adding these n inequalities and using the chain rule, we obtain  
n h(X₁, X₂, ..., Xₙ) ≤  
n  
∑  
i=1  
h(X₁, X₂, ..., Xᵢ−1, Xᵢ+1, ..., Xₙ) + h(X₁, X₂, ..., Xₙ)  
(17.53)  
or  
1  
n h(X₁, X₂, ..., Xₙ) ≤ 1  
n  
n  
∑  
i=1  
h(X₁, X₂, ..., Xᵢ−1, Xᵢ+1, ..., Xₙ)  
n−1  
,  
(17.54)  

--- Page 95 ---  

which is the desired result h(n)  
n  
≤ h(n)  
n−1. We now prove that h(n)  
k  
≤ h(n)  
k−1 for all k ≤ n by first conditioning on a k-element subset, and then taking a uniform choice over its (k−1)-element subsets. For each k-element subset, h(k)  
k  
≤ h(k)  
k−1, and hence the inequality remains true after taking the expectation over all k-element subsets chosen uniformly from the n elements.  

□  

**Theorem 17.6.2**  
Let r > 0, and define  
t(n)  
k  
= 1  
n  
k  
∑  
S: |S|=k  
e  
r h(X(S))  
k  
.  
(17.55)  
Then  
t(n)  
1  
≥ t(n)  
2  
≥ · · · ≥ t(n)  
n  
.  
(17.56)  

**Proof:**  
Starting from (17.54), we multiply both sides by r, exponentiate, and then apply the arithmetic mean geometric mean inequality, to obtain  
e  
1  
n r h(X₁, X₂, ..., Xₙ)  
≤ e  
1  
n  
n  
∑  
i=1  
r h(X₁, X₂, ..., Xᵢ−1, Xᵢ+1, ..., Xₙ)  
(n−1)  
(17.57)  
≤ 1  
n  
n  
∑  
i=1  
e  
r h(X₁, X₂, ..., Xᵢ−1, Xᵢ+1, ..., Xₙ)  
(n−1)  
for all r ≥ 0,  
(17.58)  
which is equivalent to t(n)  
n  
≤ t(n)  
n−1. Now we use the same arguments as in Theorem 17.6.1, taking an average over all subsets to prove the result that for all k ≤ n, t(n)  
k  
≤ t(n)  
k−1.  

□  

**Definition**  
The average conditional entropy rate per element for all subsets of size k is the average of the above quantities for k-element subsets of {1, 2, ..., n}:  
g(n)  
k  
= 1  
n  
k  
∑  
S:|S|=k  
h(X(S) | X(Sᶜ))  
k  
.  
(17.59)  
Here, g_k(S) is the entropy per element of the set S conditional on the elements of the set Sᶜ. When the size of the set S increases, one can expect a greater dependence among the elements of the set S, which explains Theorem 17.6.1.  

--- Page 96 ---  

In the case of the conditional entropy per element, as k increases, the size of the conditioning set Sᶜ decreases and the entropy of the set S increases. The increase in entropy per element due to the decrease in conditioning dominates the decrease due to additional dependence among the elements, as can be seen from the following theorem due to Han [270]. Note that the conditional entropy ordering in the following theorem is the reverse of the unconditional entropy ordering in Theorem 17.6.1.  

**Theorem 17.6.3**  
g(n)  
1  
≤ g(n)  
2  
≤ · · · ≤ g(n)  
n  
.  
(17.60)  

**Proof:**  
The proof proceeds on lines very similar to the proof of the theorem for the unconditional entropy per element for a random subset. We first prove that g(n)  
n  
≥ g(n)  
n−1 and then use this to prove the rest of the inequalities. By the chain rule, the entropy of a collection of random variables is less than the sum of the entropies:  
h(X₁, X₂, ..., Xₙ) ≤  
n  
∑  
i=1  
h(Xᵢ).  
(17.61)  

Subtracting both sides of this inequality from n h(X₁, X₂, ..., Xₙ), we have  
(n − 1) h(X₁, X₂, ..., Xₙ) ≥  
n  
∑  
i=1  
(h(X₁, X₂, ..., Xₙ) − h(Xᵢ))  
(17.62)  
=  
n  
∑  
i=1  
h(X₁, ..., Xᵢ−1, Xᵢ+1, ..., Xₙ | Xᵢ).  
(17.63)  

Dividing this by n(n − 1), we obtain  
h(X₁, X₂, ..., Xₙ)  
n  
≥ 1  
n  
n  
∑  
i=1  
h(X₁, X₂, ..., Xᵢ−1, Xᵢ+1, ..., Xₙ | Xᵢ)  
n−1  
,  
(17.64)  
which is equivalent to g(n)  
n  
≥ g(n)  
n−1. We now prove that g(n)  
k  
≥ g(n)  
k−1 for all k ≤ n by first conditioning on a k-element subset and then taking a uniform choice over its (k−1)-element subsets. For each k-element subset, g(k)  
k  
≥ g(k)  
k−1, and hence the inequality remains true after taking the expectation over all k-element subsets chosen uniformly from the n elements.  

□  

--- Page 97 ---  

**Theorem 17.6.4**  
Let  
f(n)  
k  
= 1  
n  
k  
∑  
S:|S|=k  
I(X(S); X(Sᶜ))  
k  
.  
(17.65)  
Then  
f(n)  
1  
≥ f(n)  
2  
≥ · · · ≥ f(n)  
n  
.  
(17.66)  

**Proof:**  
The theorem follows from the identity I(X(S); X(Sᶜ)) = h(X(S)) − h(X(S) | X(Sᶜ)) and Theorems 17.6.1 and 17.6.3.  

□  

17.7 ENTROPY AND FISHER INFORMATION  

The differential entropy of a random variable is a measure of its descriptive complexity. The Fisher information is a measure of the minimum error in estimating a parameter of a distribution. In this section, we derive a relationship between these two fundamental quantities and use this to derive the entropy power inequality.  

Let X be any random variable with density f(x). We introduce a location parameter θ and write the density in a parametric form as f(x − θ). The Fisher information (Section 11.10) with respect to θ is given by  
J(θ) = ∫  
−∞  
∞  
f(x − θ)  
(∂/∂θ ln f(x − θ))² dx.  
(17.67)  

In this case, differentiation with respect to x is equivalent to differentiation with respect to θ. So we can write the Fisher information as  
J(X) = ∫  
−∞  
∞  
f(x − θ)  
(∂/∂x ln f(x − θ))² dx  
= ∫  
−∞  
∞  
f(x)  
(∂/∂x ln f(x))² dx,  
(17.68)  
which we can rewrite as  
J(X) = ∫  
−∞  
∞  
f(x)  
(∂/∂x f(x)/f(x))² dx.  
(17.69)  

We will call this the Fisher information of the distribution of X. Notice that, like entropy, it is a function of the density.  

The importance of Fisher information is illustrated in the following theorem.  

**Theorem 17.7.1**  
(Theorem 11.10.1: Cramér–Rao inequality) The mean-squared error of any unbiased estimator T(X) of the parameter θ is lower bounded by the reciprocal of the Fisher information:  
var(T) ≥ 1/J(θ).  
(17.70)  

We now prove a fundamental relationship between the differential entropy and the Fisher information:  

**Theorem 17.7.2**  
(de Bruijn’s identity: entropy and Fisher information)  
Let X be any random variable with a finite variance with a density f(x). Let Z be an independent normally distributed random variable with zero mean and unit variance. Then  
∂/∂t h_e(X + √tZ) = 1/2 J(X + √tZ),  
(17.71)  
where h_e is the differential entropy to base e. In particular, if the limit exists as t → 0,  
∂/∂t h_e(X + √tZ)  
|  
t=0  
= 1/2 J(X).  
(17.72)  

**Proof:**  
Let Yₜ = X + √tZ. Then the density of Yₜ is  
gₜ(y) = ∫  
−∞  
∞  
f(x) 1/√(2πt) e^{−(y−x)²/(2t)} dx.  
(17.73)  

Then  
∂/∂t gₜ(y) = ∫  
−∞  
∞  
f(x) ∂/∂t [1/√(2πt) e^{−(y−x)²/(2t)}] dx  
(17.74)  
= ∫  
−∞  
∞  
f(x) [−1/(2t) 1/√(2πt) e^{−(y−x)²/(2t)} + (y − x)²/(2t²) 1/√(2πt) e^{−(y−x)²/(2t)}] dx.  

(17.75)  

We also calculate  
∂/∂y gₜ(y) = ∫  
−∞  
∞  
f(x) 1/√(2πt) ∂/∂y e^{−(y−x)²/(2t)} dx  
(17.76)  
= ∫  
−∞  
∞  
f(x) 1/√(2πt) [−(y − x)/t e^{−(y−x)²/(2t)}] dx  
(17.77)  
= ∫  
−∞  
∞  
f(x) [−(y − x)/t] 1/√(2πt) e^{−(y−x)²/(2t)} dx.  

(17.77)  

[The proof continues with further derivations and conclusions.]  

--- Page 98 ---



ENTROPY AND FISHER INFORMATION  

We will use this relationship to calculate the derivative of the entropy of \( Y_t \), where the entropy is given by  
\[
h_e(Y_t) = -\int_{-\infty}^{\infty} g_t(y) \ln g_t(y) \, dy. \quad (17.81)
\]  
Differentiating, we obtain  
\[
\frac{\partial}{\partial t} h_e(Y_t) = -\int_{-\infty}^{\infty} \frac{\partial}{\partial t} g_t(y) \, dy - \int_{-\infty}^{\infty} \frac{\partial}{\partial t} g_t(y) \ln g_t(y) \, dy. \quad (17.82)
\]  
This simplifies to  
\[
\frac{\partial}{\partial t} h_e(Y_t) = -\frac{\partial}{\partial t} \int_{-\infty}^{\infty} g_t(y) \, dy - \frac{1}{2} \int_{-\infty}^{\infty} \frac{\partial^2}{\partial y^2} g_t(y) \ln g_t(y) \, dy. \quad (17.83)
\]  
The first term is zero since \( \int_{-\infty}^{\infty} g_t(y) \, dy = 1 \). The second term can be integrated by parts to obtain  
\[
\frac{\partial}{\partial t} h_e(Y_t) = -\frac{1}{2} \left[ \frac{\partial g_t(y)}{\partial y} \ln g_t(y) \right]_{-\infty}^{\infty} + \frac{1}{2} \int_{-\infty}^{\infty} \left( \frac{\partial g_t(y)}{\partial y} \right)^2 \frac{1}{g_t(y)} \, dy. \quad (17.84)
\]  
The second term in (17.84) is \( \frac{1}{2} J(Y_t) \). So the proof will be complete if we show that the first term in (17.84) is zero. We can rewrite the first term as  
\[
\frac{\partial g_t(y)}{\partial y} \ln g_t(y) = \left( \frac{\partial g_t(y)}{\partial y} \sqrt{g_t(y)} \right) \cdot \frac{\ln g_t(y)}{\sqrt{g_t(y)}}. \quad (17.85)
\]  
The square of the first factor integrates to the Fisher information and hence must be bounded as \( y \to \pm\infty \). The second factor goes to zero since \( x \ln x \to 0 \) as \( x \to 0 \) and \( g_t(y) \to 0 \) as \( y \to \pm\infty \). Hence, the first term in (17.84) goes to 0 at both limits, and the theorem is proved. In the proof, we have exchanged integration and differentiation in (17.74), (17.76), (17.78), and (17.82). Strict justification of these exchanges requires the application of the bounded convergence and mean value theorems; the details may be found in Barron [30].  

This theorem can be used to prove the entropy power inequality, which gives a lower bound on the entropy of a sum of independent random variables.  

**Theorem 17.7.3**  
(Entropy power inequality) If \( X \) and \( Y \) are independent random \( n \)-vectors with densities, then  
\[
2^{2n} h(X + Y) \geq 2^{2n} h(X) + 2^{2n} h(Y). \quad (17.86)
\]  
We outline the basic steps in the proof due to Stam [505] and Blachman [61]. A different proof is given in Section 17.8.  

Stam’s proof of the entropy power inequality is based on a perturbation argument. Let \( n = 1 \). Let \( X_t = X + \sqrt{f(t)} Z_1 \), \( Y_t = Y + \sqrt{g(t)} Z_2 \), where \( Z_1 \) and \( Z_2 \) are independent \( N(0, 1) \) random variables. Then the entropy power inequality for \( n = 1 \) reduces to showing that \( s(0) \leq 1 \), where we define  
\[
s(t) = \frac{2^{2} h(X_t) + 2^{2} h(Y_t)}{2^{2} h(X_t + Y_t)}. \quad (17.87)
\]  
If \( f(t) \to \infty \) and \( g(t) \to \infty \) as \( t \to \infty \), it is easy to show that \( s(\infty) = 1 \). If, in addition, \( s'(t) \geq 0 \) for \( t \geq 0 \), this implies that \( s(0) \leq 1 \). The proof of the fact that \( s'(t) \geq 0 \) involves a clever choice of the functions \( f(t) \) and \( g(t) \), an application of Theorem 17.7.2, and the use of a convolution inequality for Fisher information,  
\[
\frac{1}{J(X + Y)} \geq \frac{1}{J(X)} + \frac{1}{J(Y)}. \quad (17.88)
\]  
The entropy power inequality can be extended to the vector case by induction. The details may be found in the papers by Stam [505] and Blachman [61].  

ENTROPY POWER INEQUALITY AND BRUNN–MINKOWSKI INEQUALITY  

The entropy power inequality provides a lower bound on the differential entropy of a sum of two independent random vectors in terms of their individual differential entropies. In this section, we restate and outline an alternative proof of the entropy power inequality.



ENTROPY POWER INEQUALITY AND BRUNN–MINKOWSKI INEQUALITY

17.8  
ENTROPY POWER INEQUALITY AND BRUNN–MINKOWSKI INEQUALITY  
675  
An alternative proof of the entropy power inequality is presented. We also demonstrate how the entropy power inequality and the Brunn–Minkowski inequality are related through a common proof.  

The entropy power inequality for dimension \( n = 1 \) can be rewritten to emphasize its relationship to the normal distribution. Let \( X \) and \( Y \) be two independent random variables with densities, and let \( X' \) and \( Y' \) be independent normals with the same entropy as \( X \) and \( Y \), respectively. Then,  
\[
2h(X) = 2h(X') = (2\pi e)\sigma_X^2  
\]  
and similarly,  
\[
2h(Y) = (2\pi e)\sigma_Y^2.  
\]  
Hence, the entropy power inequality can be rewritten as  
\[
2h(X+Y) \geq (2\pi e)(\sigma_X^2 + \sigma_Y^2) = 2h(X' + Y'),  
\]  
since \( X' \) and \( Y' \) are independent. Thus, we have a new statement of the entropy power inequality.  

**Theorem 17.8.1 (Restatement of the entropy power inequality)**  
For two independent random variables \( X \) and \( Y \),  
\[
h(X + Y) \geq h(X' + Y'),  
\]  
where \( X' \) and \( Y' \) are independent normal random variables with \( h(X') = h(X) \) and \( h(Y') = h(Y) \).  

This form of the entropy power inequality bears a striking resemblance to the Brunn–Minkowski inequality, which bounds the volume of set sums.  

**Definition**  
The set sum \( A + B \) of two sets \( A, B \subset \mathbb{R}^n \) is defined as the set \( \{x + y : x \in A, y \in B\} \).  

**Example 17.8.1**  
The set sum of two spheres of radius 1 is a sphere of radius 2.  

**Theorem 17.8.2 (Brunn–Minkowski inequality)**  
The volume of the set sum of two sets \( A \) and \( B \) is greater than the volume of the set sum of two spheres \( A' \) and \( B' \) with the same volume as \( A \) and \( B \), respectively:  
\[
V(A + B) \geq V(A' + B'),  
\]  
where \( A' \) and \( B' \) are spheres with \( V(A') = V(A) \) and \( V(B') = V(B) \).  

The similarity between the two theorems was pointed out in [104]. A common proof was found by Dembo [162] and Lieb, starting from a strengthened version of Young’s inequality. The same proof can be used to prove a range of inequalities that includes the entropy power inequality and the Brunn–Minkowski inequality as special cases. We begin with a few definitions.  

**Definition**  
Let \( f \) and \( g \) be two densities over \( \mathbb{R}^n \), and let \( f * g \) denote the convolution of the two densities. Let the \( L^r \) norm of the density be defined by  
\[
\|f\|_r = \left( \int f^r(x) dx \right)^{1/r}.  
\]  

**Lemma 17.8.1 (Strengthened Young’s inequality)**  
For any two densities \( f \) and \( g \) over \( \mathbb{R}^n \),  
\[
\|f * g\|_r \leq C_p C_q C_r^{n/2} \|f\|_p \|g\|_q,  
\]  
where  
\[
\frac{1}{r} = \frac{1}{p} + \frac{1}{q} - 1,  
\]  
and  
\[
C_p = \left( \frac{p}{p'} \right)^{1/p} \left( \frac{1}{p'} \right)^{1/p'}, \quad \frac{1}{p} + \frac{1}{p'} = 1.  
\]  

**Proof:**  
The proof of this inequality may be found in [38] and [73].  

We define a generalization of the entropy.  

**Definition**  
The Renyi entropy \( h_r(X) \) of order \( r \) is defined as  
\[
h_r(X) = \frac{1}{1 - r} \log \left( \int f^r(x) dx \right)  
\]  
for \( 0 < r < \infty, r \neq 1 \). If we take the limit as \( r \to 1 \), we obtain the Shannon entropy function,  
\[
h(X) = h_1(X) = -\int f(x) \log f(x) dx.  
\]  
If we take the limit as \( r \to 0 \), we obtain the logarithm of the volume of the support set,  
\[
h_0(X) = \log \mu(\{x : f(x) > 0\}).  

Thus, the zeroth-order Renyi entropy gives the logarithm of the measure of the support set of the density \( f \), and the Shannon entropy \( h_1 \) gives the logarithm of the size of the “effective” support set (Theorem 8.2.2). We now define the equivalent of the entropy power for Renyi entropies.  

**Definition**  
The Renyi entropy power \( V_r(X) \) of order \( r \) is defined as  
\[
V_r(X) = \begin{cases}  
\left( \int f^r(x) dx \right)^{-2/n(r - 1)}, & 0 < r \leq \infty, r \neq 1, \\  
\exp \left( 2 n h(X) \right), & r = 1, \\  
\mu(\{x : f(x) > 0\})^{2/n}, & r = 0.  
\end{cases}  

**Theorem 17.8.3**  
For two independent random variables \( X \) and \( Y \) and any \( 0 \leq r < \infty \) and any \( 0 \leq \lambda \leq 1 \), we have  
\[
\log V_r(X + Y) \geq \lambda \log V_p(X) + (1 - \lambda) \log V_q(Y) + H(\lambda)  
+ \frac{1 + r}{1 - r} \left( H\left( \frac{r + \lambda(1 - r)}{1 + r} \right) - H\left( \frac{r}{1 + r} \right) \right),  
\]  
where \( p = \frac{r}{r + \lambda(1 - r)} \), \( q = \frac{r}{r + (1 - \lambda)(1 - r)} \), and \( H(\lambda) = -\lambda \log \lambda - (1 - \lambda) \log(1 - \lambda) \).  

**Proof:**  
If we take the logarithm of Young’s inequality (17.93), we obtain  
\[
\frac{1}{r'} \log V_r(X + Y) \geq \frac{1}{p'} \log V_p(X) + \frac{1}{q'} \log V_q(Y) + \log C_r - \log C_p - \log C_q.  
\]  
Setting \( \lambda = \frac{r'}{p'} \) and using (17.94), we have \( 1 - \lambda = \frac{r'}{q'} \), \( p = \frac{r}{r + \lambda(1 - r)} \), and \( q = \frac{r}{r + (1 - \lambda)(1 - r)} \). Thus, (17.101) becomes  
\[
\log V_r(X + Y) \geq \lambda \log V_p(X) + (1 - \lambda) \log V_q(Y) + \frac{r'}{r} \log r - \log r'  
- \frac{r'}{p} \log p + \lambda \log p' - \frac{r'}{q} \log q + (1 - \lambda) \log q'.  
\]  
Simplifying further, we obtain  
\[
\log V_r(X + Y) \geq \lambda \log V_p(X) + (1 - \lambda) \log V_q(Y) + H(\lambda)  
+ \frac{1 + r}{1 - r} \left( H\left( \frac{r + \lambda(1 - r)}{1 + r} \right) - H\left( \frac{r}{1 + r} \right) \right).  
\]  

The Brunn–Minkowski inequality and the entropy power inequality can then be obtained as special cases of this theorem.  

- **The entropy power inequality:** Taking the limit of (17.100) as \( r \to 1 \) and setting  
\[
\lambda = \frac{V_1(X)}{V_1(X) + V_1(Y)},  
\]  
we obtain  
\[
V_1(X + Y) \geq V_1(X) + V_1(Y),  
\]  
which is the entropy power inequality.  

- **The Brunn–Minkowski inequality:** Similarly, letting \( r \to 0 \) and choosing  
\[
\lambda = \frac{\sqrt{V_0(X)}}{\sqrt{V_0(X)} + \sqrt{V_0(Y)}},  
\]  
we obtain  
\[
\sqrt{V_0(X + Y)} \geq \sqrt{V_0(X)} + \sqrt{V_0(Y)}.  
\]  
Now let \( A \) be the support set of \( X \) and \( B \) be the support set of \( Y \). Then \( A + B \) is the support set of \( X + Y \), and (17.109) reduces to  
\[
\mu(A + B)^{1/n} \geq \mu(A)^{1/n} + \mu(B)^{1/n},  
\]  
which is the Brunn–Minkowski inequality.  

The general theorem unifies the entropy power inequality and the Brunn–Minkowski inequality and introduces a continuum of new inequalities that lie between the entropy power inequality and the Brunn–Minkowski inequality. This further strengthens the analogy between entropy power and volume.  

17.9  
INEQUALITIES FOR DETERMINANTS  

Throughout the remainder of this chapter, we assume that \( K \) is a non-negative definite symmetric \( n \times n \) matrix. Let \( |K| \) denote the determinant of \( K \).  

We first give an information-theoretic proof of a result due to Ky Fan [199].  

**Theorem 17.9.1**  
\( \log |K| \) is concave.  

**Proof:**  
Let \( X_1 \) and \( X_2 \) be normally distributed \( n \)-vectors, \( X_i \sim N(0, K_i) \), \( i = 1, 2 \). Let the random variable \( \theta \) have the distribution  
\[
\Pr\{\theta = 1\} = \lambda, \quad \Pr\{\theta = 2\} = 1 - \lambda  
\]  
for some \( 0 \leq \lambda \leq 1 \). Let \( \theta \), \( X_1 \), and \( X_2 \) be independent, and let \( Z = X_\theta \). Then \( Z \) has covariance \( K_Z = \lambda K_1 + (1 - \lambda)K_2 \). However, \( Z \) will not be multivariate normal. By first using Theorem 17.2.3, followed by Theorem 17.2.1, we have  
\[
\frac{1}{2} \log(2\pi e)^n | \lambda K_1 + (1 - \lambda)K_2 | \geq h(Z)  
\]  
\[
\geq h(Z | \theta)  
\]  
\[
= \lambda \frac{1}{2} \log(2\pi e)^n |K_1| + (1 - \lambda) \frac{1}{2} \log(2\pi e)^n |K_2|.  
\]  
Thus,  
\[
| \lambda K_1 + (1 - \lambda)K_2 | \geq |K_1|^{\lambda} |K_2|^{1 - \lambda},  
\]  
as desired.  

We now give Hadamard’s inequality using an information-theoretic proof [128].  

**Theorem 17.9.2 (Hadamard)**  
\[
|K| \leq \prod_{i=1}^n K_{ii},  
\]  
with equality if and only if \( K_{ij} = 0 \) for \( i \neq j \).  

**Proof:**  
Let \( X \sim N(0, K) \). Then  
\[
\frac{1}{2} \log(2\pi e)^n |K| = h(X_1, X_2, \ldots, X_n) \leq \sum_{i=1}^n h(X_i)  
\]  
\[
= \sum_{i=1}^n \frac{1}{2} \log 2\pi e K_{ii},  
\]  
with equality if and only if \( X_1, X_2, \ldots, X_n \) are independent (i.e., \( K_{ij} = 0 \), \( i \neq j \)).  

We now prove a generalization of Hadamard’s inequality due to Szasz [391]. Let \( K(i_1, i_2, \ldots, i_k) \) be the \( k \times k \) principal submatrix of \( K \) formed by the rows and columns with indices \( i_1, i_2, \ldots, i_k \).  

**Theorem 17.9.3 (Szasz)**  
If \( K \) is a positive definite \( n \times n \) matrix and  
\[
P_k = \prod_{1 \leq i_1 < i_2 < \cdots < i_k \leq n} |K(i_1, i_2, \ldots, i_k)|,  
\]  
then  
\[
P_1 \geq P_2^{1/(n-1)} \geq P_3^{1/(n-2)} \geq \cdots \geq P_n.  

**Proof:**  
Let \( X \sim N(0, K) \). Then the theorem follows directly from Theorem 17.6.1, with the identification  
\[
h(n)_k = \frac{1}{2n(n - k)} \log P_k + \frac{1}{2} \log 2\pi e.  

We can also prove a related theorem.  

**Theorem 17.9.4**  
Let \( K \) be a positive definite \( n \times n \) matrix and let  
\[
S(n)_k = \left( \frac{1}{\binom{n}{k}} \sum_{1 \leq i_1 < i_2 < \cdots < i_k \leq n} |K(i_1, i_2, \ldots, i_k)| \right)^{1/k}.  
\]  
Then  
\[
\left( \frac{1}{n} \operatorname{tr}(K) \right) = S(n)_1 \geq S(n)_2 \geq \cdots \geq S(n)_n = |K|^{1/n}.  

**Proof:**  
This follows directly from the corollary to Theorem 17.6.1, with the identification \( t(n)_k = (2\pi e) S(n)_k \) and \( r = 2 \).  

**Theorem 17.9.5**  
Let  
\[
Q_k = \left( \frac{1}{\binom{n}{k}} \sum_{\substack{S : |S| = k}} \frac{|K|}{|K(S^c)|} \right)^{1/k(n - k)}.  
\]  
Then  
\[
\left( \frac{1}{n} \sum_{i=1}^n \sigma_i^2 \right)^{1/n} \leq Q_1 \leq Q_2 \leq \cdots \leq Q_{n-1} \leq Q_n = |K|^{1/n}.  

**Proof:**  
The theorem follows immediately from Theorem 17.6.3 and the identification  
\[
h(X(S) | X(S^c)) = \frac{1}{2} \log(2\pi e)^k \frac{|K|}{|K(S^c)|}.  

The outermost inequality \( Q_1 \leq Q_n \) can be rewritten as  
\[
|K| \geq \prod_{i=1}^n \sigma_i^2,  
\]  
where  
\[
\sigma_i^2 = \frac{|K|}{|K(1, 2, \ldots, i - 1, i + 1, \ldots, n)|}  
\]  
is the minimum mean-squared error in the linear prediction of \( X_i \) from the remaining \( X \)'s. Thus, \( \sigma_i^2 \) is the conditional variance of \( X_i \) given the remaining \( X \)'s if \( X_1, X_2, \ldots, X_n \) are jointly normal. Combining this with Hadamard’s inequality gives upper and lower bounds on the determinant of a positive definite matrix.  

**Corollary**  
\[
\prod_{i=1}^n K_{ii} \geq |K| \geq \prod_{i=1}^n \sigma_i^2.  

Hence, the determinant of a covariance matrix lies between the product of the unconditional variances \( K_{ii} \) of the random variables \( X_i \) and the product of the conditional variances \( \sigma_i^2 \).  

We now prove a property of Toeplitz matrices, which are important as the covariance matrices of stationary random processes. A Toeplitz matrix \( K \) is characterized by the property that \( K_{ij} = K_{rs} \) if \( |i - j| = |r - s| \). Let \( K_k \) denote the principal minor \( K(1, 2, \ldots, k) \). For such a matrix, the following property can be proved easily from the properties of the entropy function.



INEQUALITIES IN INFORMATION THEORY

Theorem 17.9.6  
If the positive definite n × n matrix K is Toeplitz, then  
|K₁| ≥ |K₂|₁² ≥ · · · ≥ |Kₙ₋₁|₁² ≥ |Kₙ|₁ⁿ,  
and |Kₖ|/|Kₖ₋₁| is decreasing in k, and  
limₙ→∞ |Kₙ|₁ⁿ = limₙ→∞ |Kₙ| / |Kₙ₋₁|.  

Proof:  
Let (X₁, X₂, ..., Xₙ) ∼ N(0, Kₙ). We observe that  
h(Xₖ|Xₖ₋₁, ..., X₁) = h(Xₖ) - h(Xₖ₋₁)  
= ½ log(2πe) |Kₖ| / |Kₖ₋₁|.  

Thus, the monotonicity of |Kₖ| / |Kₖ₋₁| follows from the monotonicity of  
h(Xₖ|Xₖ₋₁, ..., X₁), which follows from  
h(Xₖ|Xₖ₋₁, ..., X₁) = h(Xₖ₊₁|Xₖ, ..., X₂)  
≥ h(Xₖ₊₁|Xₖ, ..., X₂, X₁),  

where the equality follows from the Toeplitz assumption and the inequality  
from the fact that conditioning reduces entropy. Since h(Xₖ|Xₖ₋₁, ..., X₁) is decreasing, it follows that the running averages  
(1/k) h(X₁, ..., Xₖ) = (1/k) Σᵢ=₁ʰ h(Xᵢ|Xᵢ₋₁, ..., X₁)  

are decreasing in k. Then (17.127) follows from h(X₁, X₂, ..., Xₖ) = ½ log(2πe)ᵏ |Kₖ|.  

Finally, since h(Xₙ|Xₙ₋₁, ..., X₁) is a decreasing sequence, it has a limit. Hence, by the theorem of the Cesàro mean,  
limₙ→∞ (1/n) h(X₁, X₂, ..., Xₙ) = limₙ→∞ (1/n) Σᵢ=₁ⁿ h(Xᵢ|Xᵢ₋₁, ..., X₁)  
= limₙ→∞ h(Xₙ|Xₙ₋₁, ..., X₁).  

INEQUALITIES FOR RATIOS OF DETERMINANTS  

Translating this to determinants, one obtains  
limₙ→∞ |Kₙ|₁ⁿ = limₙ→∞ |Kₙ| / |Kₙ₋₁|.  

Theorem 17.9.7 (Minkowski inequality [390])  
|K₁ + K₂|₁ⁿ ≥ |K₁|₁ⁿ + |K₂|₁ⁿ.  

Proof:  
Let X₁, X₂ be independent with Xᵢ ∼ N(0, Kᵢ). Noting that X₁ + X₂ ∼ N(0, K₁ + K₂) and using the entropy power inequality (Theorem 17.7.3) yields  
(2πe)|K₁ + K₂|₁ⁿ = ²₂ⁿ h(X₁ + X₂)  
≥ ²₂ⁿ h(X₁) + ²₂ⁿ h(X₂)  
= (2πe)|K₁|₁ⁿ + (2πe)|K₂|₁ⁿ.  

INEQUALITIES FOR RATIOS OF DETERMINANTS  

We now prove similar inequalities for ratios of determinants. Before developing the next theorem, we make an observation about minimum mean-squared-error linear prediction. If (X₁, X₂, ..., Xₙ) ∼ N(0, Kₙ), we know that the conditional density of Xₙ given (X₁, X₂, ..., Xₙ₋₁) is univariate normal with mean linear in X₁, X₂, ..., Xₙ₋₁ and conditional variance σₙ². Here, σₙ² is the minimum mean squared error E[(Xₙ - Xₙ)²] over all linear estimators Xₙ based on X₁, X₂, ..., Xₙ₋₁.  

Lemma 17.10.1  
σₙ² = |Kₙ| / |Kₙ₋₁|.  

Proof:  
Using the conditional normality of Xₙ, we have  
½ log 2πe σₙ² = h(Xₙ|X₁, X₂, ..., Xₙ₋₁)  
= h(X₁, X₂, ..., Xₙ) - h(X₁, X₂, ..., Xₙ₋₁)  
= ½ log(2πe)ⁿ |Kₙ| - ½ log(2πe)ⁿ⁻¹ |Kₙ₋₁|  
= ½ log 2πe |Kₙ| / |Kₙ₋₁|.  

Minimization of σₙ² over a set of allowed covariance matrices {Kₙ} is aided by the following theorem. Such problems arise in maximum entropy spectral density estimation.  

Theorem 17.10.1 (Bergstrøm [42])  
log(|Kₙ| / |Kₙ₋ᵖ|) is concave in Kₙ.  

Proof:  
We remark that Theorem 17.9.1 cannot be used because log(|Kₙ| / |Kₙ₋ᵖ|) is the difference of two concave functions. Let Z = Xθ, where X₁ ∼ N(0, Sₙ), X₂ ∼ N(0, Tₙ), Pr{θ = 1} = λ = 1 - Pr{θ = 2}, and let X₁, X₂, θ be independent. The covariance matrix Kₙ of Z is given by  
Kₙ = λ Sₙ + (1 - λ) Tₙ.  

The following chain of inequalities proves the theorem:  
λ ½ log(2πe)ᵖ |Sₙ| / |Sₙ₋ᵖ| + (1 - λ) ½ log(2πe)ᵖ |Tₙ| / |Tₙ₋ᵖ|  
= λ h(X₁ₙ, X₁ₙ₋₁, ..., X₁ₙ₋ᵖ⁺¹ | X₁₁, ..., X₁ₙ₋ᵖ) + (1 - λ) h(X₂ₙ, X₂ₙ₋₁, ..., X₂ₙ₋ᵖ⁺¹ | X₂₁, ..., X₂ₙ₋ᵖ)  
= h(Zₙ, Zₙ₋₁, ..., Zₙ₋ᵖ⁺¹ | Z₁, ..., Zₙ₋ᵖ, θ)  
≤ h(Zₙ, Zₙ₋₁, ..., Zₙ₋ᵖ⁺¹ | Z₁, ..., Zₙ₋ᵖ)  
≤ ½ log(2πe)ᵖ |Kₙ| / |Kₙ₋ᵖ|,  

where (a) follows from h(Xₙ, Xₙ₋₁, ..., Xₙ₋ᵖ⁺¹ | X₁, ..., Xₙ₋ᵖ) = h(X₁, ..., Xₙ) - h(X₁, ..., Xₙ₋ᵖ), (b) follows from the conditioning lemma, and (c) follows from a conditional version of Theorem 17.2.3.  

Theorem 17.10.2 (Bergstrøm [42])  
|Kₙ| / |Kₙ₋₁| is concave in Kₙ.  

Proof:  
Again, we use the properties of Gaussian random variables. Let us assume that we have two independent Gaussian random n-vectors, X ∼ N(0, Aₙ) and Y ∼ N(0, Bₙ). Let Z = X + Y. Then  
½ log 2πe |Aₙ + Bₙ| / |Aₙ₋₁ + Bₙ₋₁|  
= h(Zₙ | Zₙ₋₁, Zₙ₋₂, ..., Z₁)  

≥ h(Zₙ | Zₙ₋₁, Zₙ₋₂, ..., Z₁, Xₙ₋₁, Xₙ₋₂, ..., X₁, Yₙ₋₁, Yₙ₋₂, ..., Y₁)  
= h(Xₙ + Yₙ | Xₙ₋₁, Xₙ₋₂, ..., X₁, Yₙ₋₁, Yₙ₋₂, ..., Y₁)  
= E[½ log(2πe Var(Xₙ + Yₙ | Xₙ₋₁, Xₙ₋₂, ..., X₁, Yₙ₋₁, Yₙ₋₂, ..., Y₁))]  
= E[½ log(2πe (Var(Xₙ | Xₙ₋₁, Xₙ₋₂, ..., X₁) + Var(Yₙ | Yₙ₋₁, Yₙ₋₂, ..., Y₁)))]  
= E[½ log(2πe) (|Aₙ| / |Aₙ₋₁| + |Bₙ| / |Bₙ₋₁|)]  
= ½ log(2πe) (|Aₙ| / |Aₙ₋₁| + |Bₙ| / |Bₙ₋₁|).  

Setting A = λ S and B = λ T, we obtain  
|λ Sₙ + λ Tₙ| / |λ Sₙ₋₁ + λ Tₙ₋₁| ≥ λ |Sₙ| / |Sₙ₋₁| + λ |Tₙ| / |Tₙ₋₁|,  

(i.e., |Kₙ| / |Kₙ₋₁| is concave). Simple examples show that |Kₙ| / |Kₙ₋ᵖ| is not necessarily concave for p ≥ 2.  

A number of other determinant inequalities can be proved by these techniques. A few of them are given as problems.  

OVERALL SUMMARY  
Entropy. H(X) = -Σ p(x) log p(x).  
Relative entropy. D(p||q) = Σ p(x) log p(x) / q(x).  
Mutual information. I(X; Y) = Σ p(x, y) log p(x, y) / p(x) p(y).  
Information inequality. D(p||q) ≥ 0.  
Asymptotic equipartition property. -1/n log p(X₁, X₂, ..., Xₙ) → H(X).  
Data compression. H(X) ≤ L* < H(X) + 1.  
Kolmogorov complexity. K(x) = min l(p) : U(p) = x.  
Universal probability. log 1 / P_U(x) ≈ K(x).  
Channel capacity. C = max p(x) I(X; Y).  
Data transmission  
• R < C: Asymptotically error-free communication possible  
• R > C: Asymptotically error-free communication not possible  
Gaussian channel capacity. C = ½ log(1 + P / N).  
Rate distortion. R(D) = min I(X; X̂) over all p(x̂|x) such that E p(x) p(x̂|x) d(X, X̂) ≤ D.  
Growth rate for investment. W* = max b* E log b t X.  

PROBLEMS  

17.1 Sum of positive definite matrices.  
For any two positive definite matrices, K₁ and K₂, show that |K₁ + K₂| ≥ |K₁|.  

HISTORICAL NOTES  
The entropy power inequality was stated by Shannon [472]; the first formal proofs are due to Stam [505] and Blachman [61]. The unified proof of the entropy power and Brunn–Minkowski inequalities is in Dembo et al. [164].  
Most of the matrix inequalities in this chapter were derived using information-theoretic methods by Cover and Thomas [118]. Some of the subset inequalities for entropy rates may be found in Han [270].



BIBLIOGRAPHY  
[1] J. Abrahams. Code and parse trees for lossless source encoding. Proc. Compression and Complexity of Sequences 1997, pages 145–171, 1998.  
[2] N. Abramson. The ALOHA system—another alternative for computer communications. AFIPS Conf. Proc., pages 281–285, 1970.  
[3] N. M. Abramson. Information Theory and Coding. McGraw-Hill, New York, 1963.  
[4] Y. S. Abu-Mostafa. Information theory. Complexity, pages 25–28, Nov. 1989.  
[5] R. L. Adler, D. Coppersmith, and M. Hassner. Algorithms for sliding block codes: an application of symbolic dynamics to information theory. IEEE Trans. Inf. Theory, IT-29(1):5–22, 1983.  
[6] R. Ahlswede. The capacity of a channel with arbitrary varying Gaussian channel probability functions. Trans. 6th Prague Conf. Inf. Theory, pages 13–21, Sept. 1971.  
[7] R. Ahlswede. Multi-way communication channels. In Proc. 2nd Int. Symp. Inf. Theory (Tsahkadsor, Armenian S.S.R.), pages 23–52. Hungarian Academy of Sciences, Budapest, 1971.  
[8] R. Ahlswede. The capacity region of a channel with two senders and two receivers. Ann. Prob., 2:805–814, 1974.  
[9] R. Ahlswede. Elimination of correlation in random codes for arbitrarily varying channels. Z. Wahrscheinlichkeitstheorie und verwandte Gebiete, 33:159–175, 1978.  
[10] R. Ahlswede. Coloring hypergraphs: A new approach to multiuser source coding. J. Comb. Inf. Syst. Sci., pages 220–268, 1979.  
[11] R. Ahlswede. A method of coding and an application to arbitrarily varying channels. J. Comb. Inf. Syst. Sci., pages 10–35, 1980.  
[12] R. Ahlswede and T. S. Han. On source coding with side information via a multiple access channel and related problems in multi-user information theory. IEEE Trans. Inf. Theory, IT-29:396–412, 1983.  
[13] R. Ahlswede and J. Körner. Source coding with side information and a converse for the degraded broadcast channel. IEEE Trans. Inf. Theory, IT-21:629–637, 1975.  
[14] R. F. Ahlswede. Arbitrarily varying channels with states sequence known to the sender. IEEE Trans. Inf. Theory, pages 621–629, Sept. 1986.  
[15] R. F. Ahlswede. The maximal error capacity of arbitrarily varying channels for constant list sizes (corresp.). IEEE Trans. Inf. Theory, pages 1416–1417, July 1993.  
[16] R. F. Ahlswede and G. Dueck. Identification in the presence of feedback: a discovery of new capacity formulas. IEEE Trans. Inf. Theory, pages 30–36, Jan. 1989.  
[17] R. F. Ahlswede and G. Dueck. Identification via channels. IEEE Trans. Inf. Theory, pages 15–29, Jan. 1989.  
[18] R. F. Ahlswede, E. H. Yang, and Z. Zhang. Identification via compressed data. IEEE Trans. Inf. Theory, pages 48–70, Jan. 1997.  
[19] H. Akaike. Information theory and an extension of the maximum likelihood principle. Proc. 2nd Int. Symp. Inf. Theory, pages 267–281, 1973.  
[20] P. Algoet and T. M. Cover. A sandwich proof of the Shannon–McMillan–Breiman theorem. Ann. Prob., 16(2):899–909, 1988.  
[21] P. Algoet and T. M. Cover. Asymptotic optimality and asymptotic equipartition property of log-optimal investment. Ann. Prob., 16(2):876–898, 1988.  
[22] S. Amari. Differential-Geometrical Methods in Statistics. Springer-Verlag, New York, 1985.  
[23] S. I. Amari and H. Nagaoka. Methods of Information Geometry. Oxford University Press, Oxford, 1999.  
[24] V. Anantharam and S. Verdu. Bits through queues. IEEE Trans. Inf. Theory, pages 4–18, Jan. 1996.  
[25] S. Arimoto. An algorithm for calculating the capacity of an arbitrary discrete memoryless channel. IEEE Trans. Inf. Theory, IT-18:14–20, 1972.  
[26] S. Arimoto. On the converse to the coding theorem for discrete memoryless channels. IEEE Trans. Inf. Theory, IT-19:357–359, 1973.  
[27] R. B. Ash. Information Theory. Interscience, New York, 1965.  
[28] J. Aczél and Z. Daróczy. On Measures of Information and Their Characterization. Academic Press, New York, 1975.  
[29] L. R. Bahl, J. Cocke, F. Jelinek, and J. Raviv. Optimal decoding of linear codes for minimizing symbol error rate (corresp.). IEEE Trans. Inf. Theory, pages 284–287, March 1974.  
[30] A. Barron. Entropy and the central limit theorem. Ann. Prob., 14(1):336–342, 1986.  
[31] A. Barron and T. M. Cover. A bound on the financial value of information. IEEE Trans. Inf. Theory, IT-34:1097–1100, 1988.  
[32] A. Barron and T. M. Cover. Minimum complexity density estimation. IEEE Trans. Inf. Theory, 37(4):1034–1054, July 1991.  
[33] A. R. Barron. Logically smooth density estimation. Ph.D. thesis, Department of Electrical Engineering, Stanford University, Stanford, CA, 1985.  
[34] A. R. Barron. The strong ergodic theorem for densities: generalized Shannon–McMillan–Breiman theorem. Ann. Prob., 13:1292–1303, 1985.  
[35] A. R. Barron. Are Bayes’ rules consistent in information? Prob. Commun. Computation, pages 85–91, 1987.  
[36] A. R. Barron, J. Rissanen, and Bin Yu. The minimum description length principle in coding and modeling. IEEE Trans. Inf. Theory, pages 2743–2760, Oct. 1998.  
[37] E. B. Baum. Neural net algorithms that learn in polynomial time from examples and queries. IEEE Trans. Neural Networks, pages 5–19, 1991.  
[38] W. Beckner. Inequalities in Fourier analysis. Ann. Math., 102:159–182, 1975.  
[39] R. Bell and T. M. Cover. Competitive optimality of logarithmic investment. Math. Oper. Res., 5(2):161–166, May 1980.  
[40] R. Bell and T. M. Cover. Game-theoretic optimal portfolios. Manage. Sci., 34(6):724–733, 1988.  
[41] T. C. Bell, J. G. Cleary, and I. H. Witten. Text Compression. Prentice-Hall, Englewood Cliffs, NJ, 1990.  
[42] R. Bellman. Notes on matrix theory. IV: An inequality due to Bergstrøm. Am. Math. Monthly, 62:172–173, 1955.  
[43] C. H. Bennett and G. Brassard. Quantum cryptography: public key distribution and coin tossing. Proc. IEEE Int. Conf. Comput., pages 175–179, 1984.  
[44] C. H. Bennett, D. P. DiVincenzo, J. Smolin, and W. K. Wootters. Mixed state entanglement and quantum error correction. Phys. Rev. A, pages 3824–3851, 1996.  
[45] C. H. Bennett, D. P. DiVincenzo, and J. A. Smolin. Capacities of quantum erasure channels. Phys. Rev. Lett., pages 3217–3220, 1997.  
[46] C. H. Bennett and S. J. Wiesner. Communication via one- and two-particle operators on Einstein–Podolsky–Rosen states. Phys. Rev. Lett., pages 2881–2884, 1992.  
[47] C. H. Bennett. Demons, engines and the second law. Sci. Am., 259(5):108–116, Nov. 1987.  
[48] C. H. Bennett and R. Landauer. The fundamental physical limits of computation. Sci. Am., 255(1):48–56, July 1985.  
[49] C. H. Bennett and P. W. Shor. Quantum information theory. IEEE Trans. Inf. Theory, IT-44:2724–2742, Oct. 1998.  
[50] J. Bentley, D. Sleator, R. Tarjan, and V. Wei. Locally adaptive data compression scheme. Commun. ACM, pages 320–330, 1986.  
[51] R. Benzel. The capacity region of a class of discrete additive degraded interference channels. IEEE Trans. Inf. Theory, IT-25:228–231, 1979.  
[52] T. Berger. Rate Distortion Theory: A Mathematical Basis for Data Compression. Prentice-Hall, Englewood Cliffs, NJ, 1971.  
[53] T. Berger. Multiterminal source coding. In G. Longo (Ed.), The Information Theory Approach to Communications. Springer-Verlag, New York, 1977.  
[54] T. Berger and R. W. Yeung. Multiterminal source encoding with one distortion criterion. IEEE Trans. Inf. Theory, IT-35:228–236, 1989.  
[55] P. Bergmans. Random coding theorem for broadcast channels with degraded components. IEEE Trans. Inf. Theory, IT-19:197–207, 1973.  
[56] E. R. Berlekamp. Block Coding with Noiseless Feedback. Ph.D. thesis, MIT, Cambridge, MA, 1964.  
[57] C. Berrou, A. Glavieux, and P. Thitimajshima. Near Shannon limit error-correcting coding and decoding: Turbo codes. Proc. 1993 Int. Conf. Commun., pages 1064–1070, May 1993.  
[58] D. Bertsekas and R. Gallager. Data Networks, 2nd ed. Prentice-Hall, Englewood Cliffs, NJ, 1992.  
[59] M. Bierbaum and H. M. Wallmeier. A note on the capacity region of the multiple access channel. IEEE Trans. Inf. Theory, IT-25:484, 1979.  
[60] E. Biglieri, J. Proakis, and S. Shamai. Fading channels: information-theoretic and communications aspects. IEEE Trans. Inf. Theory, pages 2619–2692, October 1998.  
[61] N. Blachman. The convolution inequality for entropy powers. IEEE Trans. Inf. Theory, IT-11:267–271, Apr. 1965.  
[62] D. Blackwell, L. Breiman, and A. J. Thomasian. Proof of Shannon’s transmission theorem for finite-state indecomposable channels. Ann. Math. Stat., pages 1209–1220, 1958.  
[63] D. Blackwell, L. Breiman, and A. J. Thomasian. The capacity of a class of channels. Ann. Math. Stat., 30:1229–1241, 1959.  
[64] D. Blackwell, L. Breiman, and A. J. Thomasian. The capacities of certain channel classes under random coding. Ann. Math. Stat., 31:558–567, 1960.  
[65] R. Blahut. Computation of channel capacity and rate distortion functions. IEEE Trans. Inf. Theory, IT-18:460–473, 1972.  
[66] R. E. Blahut. Information bounds of the Fano–Kullback type. IEEE Trans. Inf. Theory, IT-22:410–421, 1976.  
[67] R. E. Blahut. Principles and Practice of Information Theory. Addison-Wesley, Reading, MA, 1987.  
[68] R. E. Blahut. Hypothesis testing and information theory. IEEE Trans. Inf. Theory, IT-20:405–417, 1974.  
[69] R. E. Blahut. Theory and Practice of Error Control Codes. Addison-Wesley, Reading, MA, 1983.  
[70] B. M. Hochwald, G. Caire, B. Hassibi, and T. L. Marzetta (Eds.). IEEE Trans. Inf. Theory, Special Issue on Space-Time Transmission, Reception, Coding and Signal-Processing, Vol. 49, Oct. 2003.  
[71] L. Boltzmann. Beziehung zwischen dem zweiten Hauptsatze der mechanischen Wärmertheorie und der Wahrscheinlichkeitsrechnung respektive den Sätzen über das Wärmegleichgewicht. Wien. Ber., pages 373–435, 1877.  
[72] R. C. Bose and D. K. Ray-Chaudhuri. On a class of error correcting binary group codes. Inf. Control, 3:68–79, Mar. 1960.  
[73] H. J. Brascamp and E. J. Lieb. Best constants in Young’s inequality, its converse and its generalization to more than three functions. Adv. Math., 20:151–173, 1976.  
[74] L. Breiman. The individual ergodic theorems of information theory. Ann. Math. Stat., 28:809–811, 1957. With correction made in 31:809-810.  
[75] L. Breiman. Optimal gambling systems for favourable games. In Fourth Berkeley Symposium on Mathematical Statistics and Probability, Vol. 1, pages 65–78. University of California Press, Berkeley, CA, 1961.  
[76] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classification and Regression Trees. Wadsworth & Brooks, Pacific Grove, CA, 1984.  
[77] L. Brillouin. Science and Information Theory. Academic Press, New York, 1962.  
[78] J. A. Bucklew. The source coding theorem via Sanov’s theorem. IEEE Trans. Inf. Theory, pages 907–909, Nov. 1987.  
[79] J. A. Bucklew. Large Deviation Techniques in Decision, Simulation, and Estimation. Wiley, New York, 1990.  
[80] J. P. Burg. Maximum entropy spectral analysis. Ph.D. thesis, Department of Geophysics, Stanford University, Stanford, CA, 1975.  
[81] M. Burrows and D. J. Wheeler. A Block-Sorting Lossless Data Compression Algorithm (Tech. Rept. 124). Digital Systems Research Center, Palo Alto, CA, May 1994.  
[82] A. R. Calderbank. The art of signaling: fifty years of coding theory. IEEE Trans. Inf. Theory, pages 2561–2595, Oct. 1998.  
[83] A. R. Calderbank and P. W. Shor. Good quantum error-correcting codes exist. Phys. Rev. A, pages 1098–1106, 1995.  
[84] A. Carleial. Outer bounds on the capacity of the interference channel. IEEE Trans. Inf. Theory, IT-29:602–606, 1983.  
[85] A. B. Carleial. A case where interference does not reduce capacity. IEEE Trans. Inf. Theory, IT-21:569–570, 1975.  
[86] G. Chaitin. Information-Theoretic Incompleteness. World Scientific, Singapore, 1992.  
[87] G. J. Chaitin. On the length of programs for computing binary sequences. J. ACM, pages 547–569, 1966.



BIBLIOGRAPHY  
[88] G. J. Chaitin. The limits of mathematics. J. Universal Comput. Sci., 2(5):270–305, 1996.  
[89] G. J. Chaitin. On the length of programs for computing binary sequences. J. ACM, 13:547–569, 1966.  
[90] G. J. Chaitin. Information theoretical limitations of formal systems. J. ACM, 21:403–424, 1974.  
[91] G. J. Chaitin. Randomness and mathematical proof. Sci. Am., 232(5):47–52, May 1975.  
[92] G. J. Chaitin. Algorithmic information theory. IBM J. Res. Dev., 21:350–359, 1977.  
[93] G. J. Chaitin. Algorithmic Information Theory. Cambridge University Press, Cambridge, 1987.  
[94] C. S. Chang and J. A. Thomas. Huffman algebras for independent random variables. Discrete Event Dynam. Syst., 4:23–40, 1994.  
[95] C. S. Chang and J. A. Thomas. Effective bandwidth in high-speed digital networks. IEEE J. Select. Areas Commun., 13:1091–1114, Aug. 1995.  
[96] R. Chellappa. Markov Random Fields: Theory and Applications. Academic Press, San Diego, CA, 1993.  
[97] H. Chernoff. A measure of the asymptotic efficiency of tests of a hypothesis based on a sum of observations. Ann. Math. Stat., 23:493–507, 1952.  
[98] B. S. Choi and T. M. Cover. An information-theoretic proof of Burg’s maximum entropy spectrum. Proc. IEEE, 72:1094–1095, 1984.  
[99] N. Chomsky. Three models for the description of language. IEEE Trans. Inf. Theory, pages 113–124, Sept. 1956.  
[100] P. A. Chou, M. Effros, and R. M. Gray. A vector quantization approach to universal noiseless coding and quantization. IEEE Trans. Inf. Theory, pages 1109–1138, July 1996.  
[101] K. L. Chung. A note on the ergodic theorem of information theory. Ann. Math. Stat., 32:612–614, 1961.  
[102] B. S. Clarke and A. R. Barron. Information-theoretic asymptotics of Bayes’ methods. IEEE Trans. Inf. Theory, pages 453–471, May 1990.  
[103] B. S. Clarke and A. R. Barron. Jeffreys’ prior is asymptotically least favorable under entropy risk. J. Stat. Planning Inf., pages 37–60, Aug. 1994.  
[104] M. Costa and T. M. Cover. On the similarity of the entropy power inequality and the Brunn–Minkowski inequality. IEEE Trans. Inf. Theory, IT-30:837–839, 1984.  
[105] M. H. M. Costa. On the Gaussian interference channel. IEEE Trans. Inf. Theory, pages 607–615, Sept. 1985.  
[106] M. H. M. Costa and A. A. El Gamal. The capacity region of the discrete memoryless interference channel with strong interference. IEEE Trans. Inf. Theory, pages 710–711, Sept. 1987.  
[107] T. M. Cover. Geometrical and statistical properties of systems of linear inequalities with applications to pattern recognition. IEEE Trans. Electron. Computation, pages 326–334, 1965.  
[108] T. M. Cover. Universal Gambling Schemes and the Complexity Measures of Kolmogorov and Chaitin (Tech. Rept. 12). Department of Statistics, Stanford University, Stanford, CA, Oct. 1974.  
[109] T. M. Cover. Open problems in information theory. Proc. Moscow Inf. Theory Workshop, pages 35–36, 1975.  
[110] T. M. Cover. Universal portfolios. Math. Finance, pages 1–29, Jan. 1991.  
[111] T. M. Cover. Comments on broadcast channels. IEEE Trans. Inf. Theory, pages 2524–2530, Oct. 1998.  
[112] T. M. Cover. Shannon and investment. IEEE Inf. Theory Newslett (Special Golden Jubilee Issue), pp. 10–11, June 1998.  
[113] T. M. Cover and M. S. Chiang. Duality between channel capacity and rate distortion with two-sided state information. IEEE Trans. Inf. Theory, IT-48(6):1629–1638, June 2002.  
[114] T. M. Cover, P. Gács, and R. M. Gray. Kolmogorov’s contributions to information theory and algorithmic complexity. Ann. Prob., pages 840–865, July 1989.  
[115] T. M. Cover, A. A. El Gamal, and M. Salehi. Multiple access channels with arbitrarily correlated sources. IEEE Trans. Inf. Theory, pages 648–657, Nov. 1980.  
[116] T. M. Cover and P. E. Hart. Nearest neighbor pattern classification. IEEE Trans. Inf. Theory, pages 21–27, Jan. 1967.  
[117] T. M. Cover and S. Pombra. Gaussian feedback capacity. IEEE Trans. Inf. Theory, pages 37–43, January 1989.  
[118] T. M. Cover and J. A. Thomas. Determinant inequalities via information theory. SIAM J. Matrix Anal. and Its Applications, 9(3):384–392, July 1988.  
[119] T. M. Cover. Broadcast channels. IEEE Trans. Inf. Theory, IT-18:2–14, 1972.  
[120] T. M. Cover. Enumerative source encoding. IEEE Trans. Inf. Theory, IT-19(1):73–77, Jan. 1973.  
[121] T. M. Cover. An achievable rate region for the broadcast channel. IEEE Trans. Inf. Theory, IT-21:399–404, 1975.  
[122] T. M. Cover. A proof of the data compression theorem of Slepian and Wolf for ergodic sources. IEEE Trans. Inf. Theory, IT-22:226–228, 1975.  
[123] T. M. Cover. An algorithm for maximizing expected log investment return. IEEE Trans. Inf. Theory, IT-30(2):369–373, 1984.  
[124] T. M. Cover. Kolmogorov complexity, data compression, and inference. In J. Skwirzynski (Ed.), The Impact of Processing Techniques on Communications, Vol. 91 of Applied Sciences. Martinus-Nijhoff, Dordrecht, The Netherlands, 1985.  
[125] T. M. Cover. On the competitive optimality of Huffman codes. IEEE Trans. Inf. Theory, 37(1):172–174, Jan. 1991.  
[126] T. M. Cover. Universal portfolios. Math. Finance, pages 1–29, Jan. 1991.  
[127] T. M. Cover and A. El Gamal. Capacity theorems for the relay channel. IEEE Trans. Inf. Theory, IT-25:572–584, 1979.  
[128] T. M. Cover and A. El Gamal. An information-theoretic proof of Hadamard’s inequality. IEEE Trans. Inf. Theory, IT-29(6):930–931, Nov. 1983.  
[129] T. M. Cover, A. El Gamal, and M. Salehi. Multiple access channels with arbitrarily correlated sources. IEEE Trans. Inf. Theory, IT-26:648–657, 1980.  
[130] T. M. Cover. Pick the largest number, Open Problems in Communication and Computation. Ed. by T. M. Cover and B. Gopinath, page 152, New York, 1987.  
[131] T. M. Cover and R. King. A convergent gambling estimate of the entropy of English. IEEE Trans. Inf. Theory, IT-24:413–421, 1978.  
[132] T. M. Cover and C. S. K. Leung. Some equivalences between Shannon entropy and Kolmogorov complexity. IEEE Trans. Inf. Theory, IT-24:331–338, 1978.  
[133] T. M. Cover and C. S. K. Leung. An achievable rate region for the multiple access channel with feedback. IEEE Trans. Inf. Theory, IT-27:292–298, 1981.  
[134] T. M. Cover, R. J. McEliece, and E. Posner. Asynchronous multiple access channel capacity. IEEE Trans. Inf. Theory, IT-27:409–413, 1981.  
[135] T. M. Cover and E. Ordentlich. Universal portfolios with side information. IEEE Trans. Inf. Theory, IT-42:348–363, Mar. 1996.  
[136] T. M. Cover and S. Pombra. Gaussian feedback capacity. IEEE Trans. Inf. Theory, IT-35:37–43, 1989.  
[137] H. Cramer. Mathematical Methods of Statistics. Princeton University Press, Princeton, NJ, 1946.  
[138] I. Csiszár. Information type measures of difference of probability distributions and indirect observations. Stud. Sci. Math. Hung., 2:299–318, 1967.  
[139] I. Csiszár. On the computation of rate distortion functions. IEEE Trans. Inf. Theory, IT-20:122–124, 1974.  
[140] I. Csiszár. I-divergence geometry of probability distributions and minimization problems. Ann. Prob., pages 146–158, Feb. 1975.  
[141] I. Csiszár. Sanov property, generalized I-projection, and a conditional limit theorem. Ann. Prob., 12:768–793, 1984.  
[142] I. Csiszár. Information theory and ergodic theory. Probl. Contr. Inf. Theory, pages 3–27, 1987.  
[143] I. Csiszár. A geometric interpretation of Darroch and Ratcliff’s generalized iterative scaling. Ann. Stat., pages 1409–1413, 1989.  
[144] I. Csiszár. Why least squares and maximum entropy? An axiomatic approach to inference for linear inverse problems. Ann. Stat., pages 2032–2066, Dec. 1991.  
[145] I. Csiszár. Arbitrarily varying channels with general alphabets and states. IEEE Trans. Inf. Theory, pages 1725–1742, Nov. 1992.  
[146] I. Csiszár. The method of types. IEEE Trans. Inf. Theory, pages 2505–2523, October 1998.  
[147] I. Csiszár, T. M. Cover, and B. S. Choi. Conditional limit theorems under Markov conditioning. IEEE Trans. Inf. Theory, IT-33:788–801, 1987.  
[148] I. Csiszár and J. Körner. Towards a general theory of source networks. IEEE Trans. Inf. Theory, IT-26:155–165, 1980.  
[149] I. Csiszár and J. Körner. Information Theory: Coding Theorems for Discrete Memoryless Systems. Academic Press, New York, 1981.  
[150] I. Csiszár and J. Körner. Feedback does not affect the reliability function of a DMC at rates above capacity (corresp.). IEEE Trans. Inf. Theory, pages 92–93, Jan. 1982.  
[151] I. Csiszár and J. Körner. Broadcast channels with confidential messages. IEEE Trans. Inf. Theory, pages 339–348, May 1978.  
[152] I. Csiszár and J. Körner. Graph decomposition: a new key to coding theorems. IEEE Trans. Inf. Theory, pages 5–12, Jan. 1981.  
[153] I. Csiszár and G. Longo. On the Error Exponent for Source Coding and for Testing Simple Statistical Hypotheses. Hungarian Academy of Sciences, Budapest, 1971.  
[154] I. Csiszár and P. Narayan. Capacity of the Gaussian arbitrarily varying channel. IEEE Trans. Inf. Theory, pages 18–26, Jan. 1991.  
[155] I. Csiszár and G. Tusnády. Information geometry and alternating minimization procedures. Statistics and Decisions, Supplement Issue 1:205–237, 1984.  
[156] G. B. Dantzig and D. R. Fulkerson. On the max-flow min-cut theorem of networks. In H. W. Kuhn and A. W. Tucker (Eds.), Linear Inequalities and Related Systems (Vol. 38 of Annals of Mathematics Study), pages 215–221. Princeton University Press, Princeton, NJ, 1956.  
[157] J. N. Darroch and D. Ratcliff. Generalized iterative scaling for log-linear models. Ann. Math. Stat., pages 1470–1480, 1972.  
[158] I. Daubechies. Ten Lectures on Wavelets. SIAM, Philadelphia, 1992.  
[159] L. D. Davisson. Universal noiseless coding. IEEE Trans. Inf. Theory, IT-19:783–795, 1973.  
[160] L. D. Davisson. Minimax noiseless universal coding for Markov sources. IEEE Trans. Inf. Theory, pages 211–215, Mar. 1983.



BIBLIOGRAPHY

[162] A. Dembo. Information Inequalities and Uncertainty Principles (Technical Report), Department of Statistics, Stanford University, Stanford, CA, 1990.

[163] A. Dembo. Information inequalities and concentration of measure. Ann. Prob., pp. 927–939, 1997.

[164] A. Dembo, T. M. Cover, and J. A. Thomas. Information theoretic inequalities. IEEE Trans. Inf. Theory, 37(6):1501–1518, Nov. 1991.

[165] A. Dembo and O. Zeitouni. Large Deviations Techniques and Applications. Jones & Bartlett, Boston, 1993.

[166] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. J. Roy. Stat. Soc. B, 39(1):1–38, 1977.

[167] L. Devroye and L. Györfi. Nonparametric Density Estimation: The L1 View. Wiley, New York, 1985.

[168] L. Devroye, L. Györfi, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer-Verlag, New York, 1996.

[169] D. P. DiVincenzo, P. W. Shor, and J. A. Smolin. Quantum-channel capacity of very noisy channels. Phys. Rev. A, pp. 830–839, 1998.

[170] R. L. Dobrushin. General formulation of Shannon’s main theorem of information theory. Usp. Math. Nauk, 14:3–104, 1959. Translated in Am. Math. Soc. Trans., 33:323–438.

[171] R. L. Dobrushin. Survey of Soviet research in information theory. IEEE Trans. Inf. Theory, pp. 703–724, Nov. 1972.

[172] D. L. Donoho. De-noising by soft-thresholding. IEEE Trans. Inf. Theory, pp. 613–627, May 1995.

[173] R. O. Duda and P. E. Hart. Pattern Classification and Scene Analysis. Wiley, New York, 1973.

[174] G. Dueck. Maximal error capacity regions are smaller than average error capacity regions for multi-user channels. Probl. Contr. Inf. Theory, pp. 11–19, 1978.

[175] G. Dueck. The capacity region of the two-way channel can exceed the inner bound. Inf. Control, 40:258–266, 1979.

[176] G. Dueck. Partial feedback for two-way and broadcast channels. Inf. Control, 46:1–15, 1980.

[177] G. Dueck and J. Körner. Reliability function of a discrete memoryless channel at rates above capacity. IEEE Trans. Inf. Theory, IT-25:82–85, 1979.

[178] P. M. Ebert. The capacity of the Gaussian channel with feedback. Bell Syst. Tech. J., 49:1705–1712, Oct. 1970.

[180] K. Eckschlager. Information Theory in Analytical Chemistry. Wiley, New York, 1994.

[181] M. Effros, K. Visweswariah, S. R. Kulkarni, and S. Verdu. Universal lossless source coding with the Burrows–Wheeler transform. IEEE Trans. Inf. Theory, IT-48:1061–1081, May 2002.

[182] B. Efron and R. Tibshirani. An Introduction to the Bootstrap. Chapman & Hall, London, 1993.

[183] H. G. Eggleston. Convexity (Cambridge Tracts in Mathematics and Mathematical Physics, No. 47). Cambridge University Press, Cambridge, 1969.

[184] A. El Gamal. The feedback capacity of degraded broadcast channels. IEEE Trans. Inf. Theory, IT-24:379–381, 1978.

[185] A. El Gamal. The capacity region of a class of broadcast channels. IEEE Trans. Inf. Theory, IT-25:166–169, 1979.

[186] A. El Gamal and T. M. Cover. Multiple user information theory. Proc. IEEE, 68:1466–1483, 1980.

[187] A. El Gamal and T. M. Cover. Achievable rates for multiple descriptions. IEEE Trans. Inf. Theory, IT-28:851–857, 1982.

[188] A. El Gamal and E. C. Van der Meulen. A proof of Marton’s coding theorem for the discrete memoryless broadcast channel. IEEE Trans. Inf. Theory, IT-27:120–122, 1981.

[189] P. Elias. Error-free coding. IRE Trans. Inf. Theory, IT-4:29–37, 1954.

[190] P. Elias. Coding for noisy channels. IRE Conv. Rec., Pt. 4, pp. 37–46, 1955.

[191] P. Elias. Networks of Gaussian channels with applications to feedback systems. IEEE Trans. Inf. Theory, pp. 493–501, July 1967.

[192] P. Elias. The efficient construction of an unbiased random sequence. Ann. Math. Stat., pp. 865–870, 1972.

[193] P. Elias. Universal codeword sets and representations of the integers. IEEE Trans. Inf. Theory, pp. 194–203, Mar. 1975.

[194] P. Elias. Interval and recency rank source coding: two on-line adaptive variable-length schemes. IEEE Trans. Inf. Theory, pp. 3–10, Jan. 1987.

[195] P. Elias, A. Feinstein, and C. E. Shannon. A note on the maximum flow through a network. IEEE Trans. Inf. Theory, pp. 117–119, December 1956.

[196] R. S. Ellis. Entropy, Large Deviations, and Statistical Mechanics. Springer-Verlag, New York, 1985.

[197] A. Ephremides and B. Hajek. Information theory and communication networks: an unconsummated union. IEEE Trans. Inf. Theory, pp. 2416–2434, Oct. 1998.

[198] W. H. R. Equitz and T. M. Cover. Successive refinement of information. IEEE Trans. Inf. Theory, pp. 269–275, Mar. 1991.

[199] Ky Fan. On a theorem of Weyl concerning the eigenvalues of linear transformations II. Proc. Nat. Acad. Sci. USA, 36:31–35, 1950.

[200] Ky Fan. Some inequalities concerning positive-definite matrices. Proc. Cambridge Philos. Soc., 51:414–421, 1955.

[201] R. M. Fano. Class notes for Transmission of Information, course 6.574 (Technical Report). MIT, Cambridge, MA, 1952.

[202] R. M. Fano. Transmission of Information: A Statistical Theory of Communication. Wiley, New York, 1961.

[203] M. Feder. A note on the competitive optimality of Huffman codes. IEEE Trans. Inf. Theory, 38(2):436–439, Mar. 1992.

[204] M. Feder, N. Merhav, and M. Gutman. Universal prediction of individual sequences. IEEE Trans. Inf. Theory, pp. 1258–1270, July 1992.

[205] A. Feinstein. A new basic theorem of information theory. IRE Trans. Inf. Theory, IT-4:2–22, 1954.

[206] A. Feinstein. Foundations of Information Theory. McGraw-Hill, New York, 1958.

[207] A. Feinstein. On the coding theorem and its converse for finite-memory channels. Inf. Control, 2:25–44, 1959.

[208] W. Feller. An Introduction to Probability Theory and Its Applications, 2nd ed., Vol. 1. Wiley, New York, 1957.

[209] R. A. Fisher. On the mathematical foundations of theoretical statistics. Philos. Trans. Roy. Soc., London A, 222:309–368, 1922.

[210] R. A. Fisher. Theory of statistical estimation. Proc. Cambridge Philos. Soc., 22:700–725, 1925.

[211] B. M. Fitingof. Optimal encoding with unknown and variable message statistics. Probl. Inf. Transm. (USSR), pp. 3–11, 1966.

[212] B. M. Fitingof. The compression of discrete information. Probl. Inf. Transm. (USSR), pp. 28–36, 1967.

[213] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Can. J. Math., pp. 399–404, 1956.

[214] L. R. Ford and D. R. Fulkerson. Flows in Networks. Princeton University Press, Princeton, NJ, 1962.

[215] G. D. Forney. Exponential error bounds for erasure, list and decision feedback schemes. IEEE Trans. Inf. Theory, IT-14:549–557, 1968.

[216] G. D. Forney. Information Theory: unpublished course notes. Stanford University, Stanford, CA, 1972.

[217] G. J. Foschini. Layered space-time architecture for wireless communication in a fading environment when using multi-element antennas. Bell Syst. Tech. J., 1(2):41–59, 1996.

[218] P. Franaszek, P. Tsoucas, and J. Thomas. Context allocation for multiple dictionary data compression. In Proc. IEEE Int. Symp. Inf. Theory, Trondheim, Norway, p. 12, 1994.

[219] P. A. Franaszek. On synchronous variable length coding for discrete noiseless channels. Inf. Control, 15:155–164, 1969.

[220] T. Gaarder and J. K. Wolf. The capacity region of a multiple-access discrete memoryless channel can increase with feedback. IEEE Trans. Inf. Theory, IT-21:100–102, 1975.

[221] D. Gabor. Theory of communication. J. Inst. Elec. Engg., pp. 429–457, Sept. 1946.

[222] P. Gacs and J. Körner. Common information is much less than mutual information. Probl. Contr. Inf. Theory, pp. 149–162, 1973.

[223] R. G. Gallager. Source coding with side information and universal coding. Unpublished manuscript, also presented at the Int. Symp. Inf. Theory, Oct. 1974.

[224] R. G. Gallager. A simple derivation of the coding theorem and some applications. IEEE Trans. Inf. Theory, IT-11:3–18, 1965.

[225] R. G. Gallager. Capacity and coding for degraded broadcast channels. Probl. Peredachi Inf., 10(3):3–14, 1974.

[226] R. G. Gallager. Basic limits on protocol information in data communication networks. IEEE Trans. Inf. Theory, pp. 385–398, July 1976.

[227] R. G. Gallager. A minimum delay routing algorithm using distributed computation. IEEE Trans. Commun., pp. 73–85, Jan. 1977.

[228] R. G. Gallager. Variations on a theme by Huffman. IEEE Trans. Inf. Theory, pp. 668–674, Nov. 1978.

[229] R. G. Gallager. Source Coding with Side Information and Universal Coding (Tech. Rept. LIDS-P-937). Laboratory for Information Decision Systems, MIT, Cambridge, MA, 1979.

[230] R. G. Gallager. A perspective on multiaccess channels. IEEE Trans. Inf. Theory, pp. 124–142, Mar. 1985.

[231] R. G. Gallager. Low density parity check codes. IRE Trans. Inf. Theory, IT-8:21–28, Jan. 1962.

[232] R. G. Gallager. Low Density Parity Check Codes. MIT Press, Cambridge, MA, 1963.

[233] R. G. Gallager. Information Theory and Reliable Communication. Wiley, New York, 1968.

[234] A. A. El Gamal and T. M. Cover. Achievable rates for multiple descriptions. IEEE Trans. Inf. Theory, pp. 851–857, November 1982.

[235] A. El Gamal. Broadcast channels with and without feedback. 11th Ann. Asilomar Conf. Circuits, pp. 180–183, Nov. 1977.

[236] A. El Gamal. Capacity of the product and sum of two unmatched broadcast channels. Probl. Peredachi Inf., pp. 3–23, Jan.–Mar. 1980.

[237] A. A. El Gamal. The feedback capacity of degraded broadcast channels (corresp.). IEEE Trans. Inf. Theory, pp. 379–381, May 1978.

[238] A. A. El Gamal. The capacity of a class of broadcast channels. IEEE Trans. Inf. Theory, pp. 166–169, Mar. 1979.

[239] A. A. El Gamal. The capacity of the physically degraded Gaussian broadcast channel with feedback (corresp.). IEEE Trans. Inf. Theory, pp. 508–511, July 1981.

[240] A. A. El Gamal and E. C. van der Meulen. A proof of Marton’s coding theorem for the discrete memoryless broadcast channel. IEEE Trans. Inf. Theory, pp. 120–122, Jan. 1981.

[241] I. M. Gelfand, A. N. Kolmogorov, and A. M. Yaglom. On the general definition of mutual information. Rept. Acad. Sci. USSR, pp. 745–748, 1956.

[242] S. I. Gelfand. Capacity of one broadcast channel. Probl. Peredachi Inf., pp. 106–108, July–Sept. 1977.

[243] S. I. Gelfand and M. S. Pinsker. Capacity of a broadcast channel with one deterministic component. Probl. Peredachi Inf., pp. 24–34, Jan.–Mar. 1980.

[244] S. I. Gelfand and M. S. Pinsker. Coding for channel with random parameters. Probl. Contr. Inf. Theory, pp. 19–31, 1980.

[245] A. Gersho and R. M. Gray. Vector Quantization and Signal Compression. Kluwer, Boston, 1992.

[246] G. G. Raleigh and J. M. Cioffi. Spatio-temporal coding for wireless communication. IEEE Trans. Commun., 46:357–366, 1998.

[247] J. D. Gibson and J. L. Melsa. Introduction to Nonparametric Detection with Applications. IEEE Press, New York, 1996.

[248] E. N. Gilbert. Codes based on inaccurate source probabilities. IEEE Trans. Inf. Theory, pp. 304–314, May 1971.

[249] E. N. Gilbert and E. F. Moore. Variable length binary encodings. Bell Syst. Tech. J., 38:933–967, 1959.

[250] S. Goldman. Some fundamental considerations concerning noise reduction and range in radar and communication. Proc. Inst. Elec. Engg., pp. 584–594, 1948.

[251] S. Goldman. Information Theory. Prentice-Hall, Englewood Cliffs, NJ, 1953.

[252] A. Goldsmith and M. Effros. The capacity region of Gaussian broadcast channels with intersymbol interference and colored Gaussian noise. IEEE Trans. Inf. Theory, 47:2–8, Jan. 2001.

[253] S. W. Golomb. Run-length encodings. IEEE Trans. Inf. Theory, pp. 399–401, July 1966.

[254] S. W. Golomb, R. E. Peile, and R. A. Scholtz. Basic Concepts in Information Theory and Coding: The Adventures of Secret Agent 00111 (Applications of Communications Theory). Plenum Publishing, New York, 1994.

[255] A. J. Grant, B. Rimoldi, R. L. Urbanke, and P. A. Whiting. Rate-splitting multiple access for discrete memoryless channels. IEEE Trans. Inf. Theory, pp. 873–890, Mar. 2001.



BIBLIOGRAPHY  
[256] R. M. Gray. Source Coding Theory. Kluwer, Boston, 1990.  
[257] R. M. Gray and L. D. Davisson, (Eds.). Ergodic and Information Theory. Dowden, Hutchinson & Ross, Stroudsburg, PA, 1977.  
[258] R. M. Gray and Lee D. Davisson. Source coding theorems without the ergodic assumption. IEEE Trans. Inf. Theory, pages 502–516, July 1974.  
[259] R. M. Gray. Sliding block source coding. IEEE Trans. Inf. Theory, IT-21:357–368, 1975.  
[260] R. M. Gray. Entropy and Information Theory. Springer-Verlag, New York, 1990.  
[261] R. M. Gray and A. Wyner. Source coding for a simple network. Bell Syst. Tech. J., 58:1681–1721, 1974.  
[262] U. Grenander and G. Szego. Toeplitz Forms and Their Applications. University of California Press, Berkeley, CA, 1958.  
[263] B. Grünbaum. Convex Polytopes. Interscience, New York, 1967.  
[264] S. Guiasu. Information Theory with Applications. McGraw-Hill, New York, 1976.  
[265] B. E. Hajek and M. B. Pursley. Evaluation of an achievable rate region for the broadcast channel. IEEE Trans. Inf. Theory, pages 36–46, Jan. 1979.  
[266] R. V. Hamming. Error detecting and error correcting codes. Bell Syst. Tech. J., 29:147–160, 1950.  
[267] T. S. Han. The capacity region for the deterministic broadcast channel with a common message (corresp.). IEEE Trans. Inf. Theory, pages 122–125, Jan. 1981.  
[268] T. S. Han and S. I. Amari. Statistical inference under multiterminal data compression. IEEE Trans. Inf. Theory, pages 2300–2324, Oct. 1998.  
[269] T. S. Han and S. Verdu. New results in the theory of identification via channels. IEEE Trans. Inf. Theory, pages 14–25, Jan. 1992.  
[270] T. S. Han. Nonnegative entropy measures of multivariate symmetric correlations. Inf. Control, 36(2):133–156, 1978.  
[271] T. S. Han. The capacity region of a general multiple access channel with certain correlated sources. Inf. Control, 40:37–60, 1979.  
[272] T. S. Han. Information-Spectrum Methods in Information Theory. Springer-Verlag, New York, 2002.  
[273] T. S. Han and M. H. M. Costa. Broadcast channels with arbitrarily correlated sources. IEEE Trans. Inf. Theory, IT-33:641–650, 1987.  
[274] T. S. Han and K. Kobayashi. A new achievable rate region for the interference channel. IEEE Trans. Inf. Theory, IT-27:49–60, 1981.  
[275] R. V. Hartley. Transmission of information. Bell Syst. Tech. J., 7:535, 1928.  
[276] C. W. Helstrom. Elements of Signal Detection and Estimation. Prentice-Hall, Englewood Cliffs, NJ, 1995.  
[277] Y. Hershkovits and J. Ziv. On sliding-window universal data compression with limited memory. IEEE Trans. Inf. Theory, pages 66–78, Jan. 1998.  
[278] P. A. Hocquenghem. Codes correcteurs d’erreurs. Chiffres, 2:147–156, 1959.  
[279] J. L. Holsinger. Digital Communication over Fixed Time-Continuous Channels with Memory, with Special Application to Telephone Channels (Technical Report). MIT, Cambridge, MA, 1964.  
[280] M. L. Honig, U. Madhow, and S. Verdu. Blind adaptive multiuser detection. IEEE Trans. Inf. Theory, pages 944–960, July 1995.  
[281] J. E. Hopcroft and J. D. Ullman. Introduction to Automata Theory, Formal Languages and Computation. Addison-Wesley, Reading, MA, 1979.  
[282] Y. Horibe. An improved bound for weight-balanced tree. Inf. Control, 34:148–151, 1977.  
[283] D. A. Huffman. A method for the construction of minimum redundancy codes. Proc. IRE, 40:1098–1101, 1952.  
[284] J. Y. Hui. Switching and Traffic Theory for Integrated Broadband Networks. Kluwer, Boston, 1990.  
[285] J. Y. N. Hui and P. A. Humblet. The capacity region of the totally asynchronous multiple-access channel. IEEE Trans. Inf. Theory, pages 207–216, Mar. 1985.  
[286] S. Ihara. On the capacity of channels with additive non-Gaussian noise. Inf. Control, pages 34–39, 1978.  
[287] S. Ihara. Information Theory for Continuous Systems. World Scientific, Singapore, 1993.  
[288] K. A. Schouhamer Immink, Paul H. Siegel, and Jack K. Wolf. Codes for digital recorders. IEEE Trans. Inf. Theory, pages 2260–2299, Oct. 1998.  
[289] N. S. Jayant (Ed.). Waveform Quantization and Coding. IEEE Press, New York, 1976.  
[290] N. S. Jayant and P. Noll. Digital Coding of Waveforms. Prentice-Hall, Englewood Cliffs, NJ, 1984.  
[291] E. T. Jaynes. Information theory and statistical mechanics. Phys. Rev., 106:620, 1957.  
[292] E. T. Jaynes. Information theory and statistical mechanics II. Phys. Rev., 108:171, 1957.  
[293] E. T. Jaynes. On the rationale of maximum entropy methods. Proc. IEEE, 70:939–952, 1982.  
[294] E. T. Jaynes. Papers on Probability, Statistics and Statistical Physics. Reidel, Dordrecht, The Netherlands, 1982.  
[295] F. Jelinek. Buffer overflow in variable length encoding of fixed rate sources. IEEE Trans. Inf. Theory, IT-14:490–501, 1968.  
[296] F. Jelinek. Evaluation of expurgated error bounds. IEEE Trans. Inf. Theory, IT-14:501–505, 1968.  
[297] F. Jelinek. Probabilistic Information Theory. McGraw-Hill, New York, 1968.  
[298] F. Jelinek. Statistical Methods for Speech Recognition. MIT Press, Cambridge, MA, 1998.  
[299] R. Jozsa and B. Schumacher. A new proof of the quantum noiseless coding theorem. J. Mod. Opt., pages 2343–2350, 1994.  
[300] G. G. Langdon, Jr. A note on the Ziv–Lempel model for compressing individual sequences. IEEE Trans. Inf. Theory, pages 284–287, Mar. 1983.  
[301] J. Justesen. A class of constructive asymptotically good algebraic codes. IEEE Trans. Inf. Theory, IT-18:652–656, 1972.  
[302] M. Kac. On the notion of recurrence in discrete stochastic processes. Bull. Am. Math. Soc., pages 1002–1010, Oct. 1947.  
[303] T. Kailath and J. P. M. Schalkwijk. A coding scheme for additive noise channels with feedback. Part I: No bandwidth constraints. IEEE Trans. Inf. Theory, IT-12:172–182, 1966.  
[304] T. Kailath and H. V. Poor. Detection of stochastic processes. IEEE Trans. Inf. Theory, pages 2230–2259, Oct. 1998.  
[305] S. Karlin. Mathematical Methods and Theory in Games, Programming and Economics, Vol. 2. Addison-Wesley, Reading, MA, 1959.  
[306] J. Karush. A simple proof of an inequality of McMillan. IRE Trans. Inf. Theory, IT-7:118, 1961.  
[307] F. P. Kelly. Notes on effective bandwidth. Stochastic Networks Theory and Applications, pages 141–168, 1996.  
[308] J. Kelly. A new interpretation of information rate. Bell Syst. Tech. J., 35:917–926, July 1956.  
[309] J. H. B. Kemperman. On the Optimum Rate of Transmitting Information (Lecture Notes in Mathematics), pages 126–169. Springer Verlag, New York, 1967.  
[310] M. Kendall and A. Stuart. The Advanced Theory of Statistics. Macmillan, New York, 1977.  
[311] A. Y. Khinchin. Mathematical Foundations of Information Theory. Dover, New York, 1957.  
[312] J. C. Kieffer. A simple proof of the Moy–Perez generalization of the Shannon–McMillan theorem. Pac. J. Math., 51:203–206, 1974.  
[313] J. C. Kieffer. A survey of the theory of source coding with a fidelity criterion. IEEE Trans. Inf. Theory, pages 1473–1490, Sept. 1993.  
[314] Y. H. Kim. Feedback capacity of first-order moving average Gaussian channel. Proc. IEEE Int. Symp. Information Theory, Adelaide, pages 416–420, Sept. 2005.  
[315] D. E. Knuth. Dynamic Huffman coding. J. Algorithms, pages 163–180, 1985.  
[316] D. E. Knuth. Art of Computer Programming.  
[317] D. E. Knuth and A. C. Yao. The complexity of random number generation. In J. F. Traub (Ed.), Algorithms and Complexity: Recent Results and New Directions (Proceedings of the Symposium on New Directions and Recent Results in Algorithms and Complexity, Carnegie-Mellon University, 1976), pages 357–428. Academic Press, New York, 1976.  
[318] A. N. Kolmogorov. A new metric invariant of transitive dynamical systems and automorphism in Lebesgue spaces. Dokl. Akad. Nauk SSSR, pages 861–864, 1958.  
[319] A. N. Kolmogorov. On the Shannon theory of information transmission in the case of continuous signals. IRE Trans. Inf. Theory, IT-2:102–108, Sept. 1956.  
[320] A. N. Kolmogorov. A new invariant for transitive dynamical systems. Dokl. Acad. Nauks SSR, 119:861–864, 1958.  
[321] A. N. Kolmogorov. Three approaches to the quantitative definition of information. Probl. Inf. Transm. (USSR), 1:4–7, 1965.  
[322] A. N. Kolmogorov. Logical basis for information theory and probability theory. IEEE Trans. Inf. Theory, IT-14:662–664, 1968.  
[323] A. N. Kolmogorov. The theory of transmission of information. In Selected Works of A. N. Kolmogorov, Vol. III: Information Theory and the Theory of Algorithms, Session on scientific problems of automation in industry, Vol. 1, Plenary talks, Izd. Akad. Nauk SSSR, Moscow, 1957, pages 66–99. Kluwer, Dordrecht, The Netherlands, 1993.  
[324] J. Körner and K. Marton. The comparison of two noisy channels. In I. Csiszár and P. Elias (Ed.), Topics in Information Theory (Coll. Math. Soc. J. Bolyai, No. 16), pages 411–423. North-Holland, Amsterdam, 1977.  
[325] J. Körner and K. Marton. General broadcast channels with degraded message sets. IEEE Trans. Inf. Theory, IT-23:60–64, 1977.  
[326] J. Körner and K. Marton. How to encode the modulo 2 sum of two binary sources. IEEE Trans. Inf. Theory, IT-25:219–221, 1979.  
[327] J. Körner and A. Orlitsky. Zero error information theory. IEEE Trans. Inf. Theory, IT-44:2207–2229, Oct. 1998.  
[328] V. A. Kotel’nikov. On the transmission capacity of “ether” and wire in electrocommunications. Izd. Red. Upr. Svyazi RKKA, 44, 1933.  
[329] V. A. Kotel’nikov. The Theory of Optimum Noise Immunity. McGraw-Hill, New York, 1959.  
[330] L. G. Kraft. A device for quantizing, grouping and coding amplitude modulated pulses. Master’s thesis, Department of Electrical Engineering, MIT, Cambridge, MA, 1949.  
[331] R. E. Krichevsky. Laplace’s law of succession and universal encoding. IEEE Trans. Inf. Theory, pages 296–303, Jan. 1998.  
[332] R. E. Krichevsky. Universal Compression and Retrieval. Kluwer, Dordrecht, The Netherlands, 1994.



--- Page 37 ---  
BIBLIOGRAPHY  
711  
[410] L. H. Ozarow. The capacity of the white Gaussian multiple access channel with feedback. IEEE Trans. Inf. Theory, IT-30:623–629, 1984.  
[411] L. H. Ozarow and C. S. K. Leung. An achievable region and an outer bound for the Gaussian broadcast channel with feedback. IEEE Trans. Inf. Theory, IT-30:667–671, 1984.  
[412] H. Pagels. The Dreams of Reason: the Computer and the Rise of the Sciences of Complexity. Simon and Schuster, New York, 1988.  
[413] C. Papadimitriou. Information theory and computational complexity: The expanding interface. IEEE Inf. Theory Newslett. (Special Golden Jubilee Issue), pages 12–13, June 1998.  
[414] R. Pasco. Source coding algorithms for fast data compression. Ph.D. thesis, Stanford University, Stanford, CA, 1976.  
[415] A. J. Paulraj and C. B. Papadias. Space-time processing for wireless communications. IEEE Signal Processing Mag., pages 49–83, Nov. 1997.  
[416] W. B. Pennebaker and J. L. Mitchell. JPEG Still Image Data Compression Standard. Van Nostrand Reinhold, New York, 1988.  
[417] A. Perez. Extensions of Shannon–McMillan’s limit theorem to more general stochastic processes. In Trans. Third Prague Conference on Information Theory, Statistical Decision Functions and Random Processes, pages 545–574, Czechoslovak Academy of Sciences, Prague, 1964.  
[418] J. R. Pierce. The early days of information theory. IEEE Trans. Inf. Theory, pages 3–8, Jan. 1973.  
[419] J. R. Pierce. An Introduction to Information Theory: Symbols, Signals and Noise, 2nd ed. Dover Publications, New York, 1980.  
[420] J. T. Pinkston. An application of rate-distortion theory to a converse to the coding theorem. IEEE Trans. Inf. Theory, IT-15:66–71, 1969.  
[421] M. S. Pinsker. Talk at Soviet Information Theory meeting, 1969. No abstract published.  
[422] M. S. Pinsker. Information and Information Stability of Random Variables and Processes. Holden-Day, San Francisco, CA, 1964. (Originally published in Russian in 1960.)  
[423] M. S. Pinsker. The capacity region of noiseless broadcast channels. Probl. Inf. Transm. (USSR), 14(2):97–102, 1978.  
[424] M. S. Pinsker and R. L. Dobrushin. Memory increases capacity. Probl. Inf. Transm. (USSR), pages 94–95, Jan. 1969.  
[425] M. S. Pinsker. Information and Stability of Random Variables and Processes. Izd. Akad. Nauk, 1960. Translated by A. Feinstein, 1964.  
[426] E. Plotnik, M. Weinberger, and J. Ziv. Upper bounds on the probability of sequences emitted by finite-state sources and on the redundancy of the Lempel–Ziv algorithm. IEEE Trans. Inf. Theory, IT-38(1):66–72, Jan. 1992.  
[427] D. Pollard. Convergence of Stochastic Processes. Springer-Verlag, New York, 1984.  

--- Page 38 ---  
712  
BIBLIOGRAPHY  
[428] G. S. Poltyrev. Carrying capacity for parallel broadcast channels with degraded components. Probl. Peredachi Inf., pages 23–35, Apr.–June 1977.  
[429] S. Pombra and T. M. Cover. Nonwhite Gaussian multiple access channels with feedback. IEEE Trans. Inf. Theory, pages 885–892, May 1994.  
[430] H. V. Poor. An Introduction to Signal Detection and Estimation, 2nd ed. Springer-Verlag, New York, 1994.  
[431] F. Pratt. Secret and Urgent. Blue Ribbon Books, Garden City, NY, 1939.  
[432] L. R. Rabiner. A tutorial on hidden Markov models and selected applications in speech recognition. Proc. IEEE, pages 257–286, Feb. 1989.  
[433] L. R. Rabiner and R. W. Schafer. Digital Processing of Speech Signals. Prentice-Hall, Englewood Cliffs, NJ, 1978.  
[434] R. Ahlswede and Z. Zhang. New directions in the theory of identification via channels. IEEE Trans. Inf. Theory, 41:1040–1050, 1995.  
[435] C. R. Rao. Information and accuracy obtainable in the estimation of statistical parameters. Bull. Calcutta Math. Soc., 37:81–91, 1945.  
[436] I. S. Reed. 1982 Claude Shannon lecture: Application of transforms to coding and related topics. IEEE Inf. Theory Newslett., pages 4–7, Dec. 1982.  
[437] F. M. Reza. An Introduction to Information Theory. McGraw-Hill, New York, 1961.  
[438] S. O. Rice. Mathematical analysis of random noise. Bell Syst. Tech. J., pages 282–332, Jan. 1945.  
[439] S. O. Rice. Communication in the presence of noise: probability of error for two encoding schemes. Bell Syst. Tech. J., 29:60–93, 1950.  
[440] B. E. Rimoldi and R. Urbanke. A rate-splitting approach to the Gaussian multiple-access channel. IEEE Trans. Inf. Theory, pages 364–375, Mar. 1996.  
[441] J. Rissanen. Generalized Kraft inequality and arithmetic coding. IBM J. Res. Dev., 20:198, 1976.  
[442] J. Rissanen. Modelling by shortest data description. Automatica, 14:465–471, 1978.  
[443] J. Rissanen. A universal prior for integers and estimation by minimum description length. Ann. Stat., 11:416–431, 1983.  
[444] J. Rissanen. Universal coding, information, prediction and estimation. IEEE Trans. Inf. Theory, IT-30:629–636, 1984.  
[445] J. Rissanen. Stochastic complexity and modelling. Ann. Stat., 14:1080–1100, 1986.  
[446] J. Rissanen. Stochastic complexity (with discussions). J. Roy. Stat. Soc., 49:223–239, 252–265, 1987.  
[447] J. Rissanen. Stochastic complexity in Statistical Inquiry. World Scientific, Singapore, 1989.  
[448] J. J. Rissanen. Complexity of strings in the class of Markov sources. IEEE Trans. Inf. Theory, pages 526–532, July 1986.  

--- Page 39 ---  
713  
BIBLIOGRAPHY  
[449] J. J. Rissanen and G. G. Langdon, Jr. Universal modeling and coding. IEEE Trans. Inf. Theory, pages 12–23, Jan. 1981.  
[450] B. Y. Ryabko. Encoding a source with unknown but ordered probabilities. Probl. Inf. Transm., pages 134–139, Oct. 1979.  
[451] B. Y. Ryabko. A fast on-line adaptive code. IEEE Trans. Inf. Theory, pages 1400–1404, July 1992.  
[452] P. A. Samuelson. Lifetime portfolio selection by dynamic stochastic programming. Rev. Econ. Stat., pages 236–239, 1969.  
[453] P. A. Samuelson. The “fallacy” of maximizing the geometric mean in long sequences of investing or gambling. Proc. Natl. Acad. Sci. USA, 68:214–224, Oct. 1971.  
[454] P. A. Samuelson. Why we should not make mean log of wealth big though years to act are long. J. Banking and Finance, 3:305–307, 1979.  
[455] I. N. Sanov. On the probability of large deviations of random variables. Mat. Sbornik, 42:11–44, 1957. English translation in Sel. Transl. Math. Stat. Prob., Vol. 1, pp. 213-244, 1961.  
[456] A. A. Sardinas and G. W. Patterson. A necessary and sufficient condition for the unique decomposition of coded messages. IRE Conv. Rec., Pt. 8, pages 104–108, 1953.  
[457] H. Sato. On the capacity region of a discrete two-user channel for strong interference. IEEE Trans. Inf. Theory, IT-24:377–379, 1978.  
[458] H. Sato. The capacity of the Gaussian interference channel under strong interference. IEEE Trans. Inf. Theory, IT-27:786–788, 1981.  
[459] H. Sato and M. Tanabe. A discrete two-user channel with strong interference. Trans. IECE Jap., 61:880–884, 1978.  
[460] S. A. Savari. Redundancy of the Lempel–Ziv incremental parsing rule. IEEE Trans. Inf. Theory, pages 9–21, January 1997.  
[461] S. A. Savari and R. G. Gallager. Generalized Tunstall codes for sources with memory. IEEE Trans. Inf. Theory, pages 658–668, Mar. 1997.  
[462] K. Sayood. Introduction to Data Compression. Morgan Kaufmann, San Francisco, CA, 1996.  
[463] J. P. M. Schalkwijk. A coding scheme for additive noise channels with feedback. II: Bandlimited signals. IEEE Trans. Inf. Theory, pages 183–189, Apr. 1966.  
[464] J. P. M. Schalkwijk. The binary multiplying channel: a coding scheme that operates beyond Shannon’s inner bound. IEEE Trans. Inf. Theory, IT-28:107–110, 1982.  
[465] J. P. M. Schalkwijk. On an extension of an achievable rate region for the binary multiplying channel. IEEE Trans. Inf. Theory, IT-29:445–448, 1983.  
[466] C. P. Schnorr. A unified approach to the definition of random sequences. Math. Syst. Theory, 5:246–258, 1971.  

--- Page 40 ---  
714  
BIBLIOGRAPHY  
[467] C. P. Schnorr. Process, complexity and effective random tests. J. Comput. Syst. Sci., 7:376–388, 1973.  
[468] C. P. Schnorr. A survey on the theory of random sequences. In R. Butts and J. Hinitikka (Eds.), Logic, Methodology and Philosophy of Science. Reidel, Dordrecht, The Netherlands, 1977.  
[469] G. Schwarz. Estimating the dimension of a model. Ann. Stat., 6:461–464, 1978.  
[470] S. Shamai and S. Verdu. The empirical distribution of good codes. IEEE Trans. Inf. Theory, pages 836–846, May 1997.  
[471] C. E. Shannon. A Mathematical Theory of Cryptography (Tech. Rept. MM 45-110-02). Bell Lab. Tech. Memo., Sept. 1, 1945.  
[472] C. E. Shannon. A mathematical theory of communication. Bell Syst. Tech. J., 27:379–423, 623–656, 1948.  
[473] C. E. Shannon. Some geometrical results in channel capacity. Verh. Dtsch. Elektrotechnik. Fachber., pages 13–15, 1956.  
[474] C. E. Shannon. The zero-error capacity of a noisy channel. IRE Trans. Inf. Theory, IT-2:8–19, 1956.  
[475] C. E. Shannon. Channels with side information at the transmitter. IBM J. Res. Dev., pages 289–293, 1958.  
[476] C. E. Shannon. Probability of error for optimal codes in a Gaussian channel. Bell Syst. Tech. J., pages 611–656, May 1959.  
[477] C. E. Shannon. Two-way communication channels. Proc. 4th Berkeley Symp. Mathematical Statistics and Probability (June 20–July 30, 1960), pages 611–644, 1961.  
[478] C. E. Shannon. The wonderful world of feedback. IEEE Int. Symp. Infor. Theory, ser. First Shannon Lecture, Ashkelon, Israel, 1973.  
[479] C. E. Shannon. The mind reading machine. In Shannon’s Collected Papers, pages 688–689, 1993.  
[480] C. E. Shannon. Communication in the presence of noise. Proc. IRE, 37:10–21, January 1949.  
[481] C. E. Shannon. Communication theory of secrecy systems. Bell Syst. Tech. J., 28:656–715, 1949.  
[482] C. E. Shannon. Prediction and entropy of printed English. Bell Syst. Tech. J., 30:50–64, January 1951.  
[483] C. E. Shannon. Certain results in coding theory for noisy channels. Infor. Control, 1:6–25, 1957.  
[484] C. E. Shannon. Channels with side information at the transmitter. IBM J. Res. Dev., 2:289–293, 1958.  
[485] C. E. Shannon. Coding theorems for a discrete source with a fidelity criterion. IRE Nat. Conv. Rec., Pt. 4, pages 142–163, 1959.



BIBLIOGRAPHY

[486] C. E. Shannon. Two-way communication channels. In Proc. 4th Berkeley Symp. Math. Stat. Prob., Vol. 1, pages 611–644. University of California Press, Berkeley, CA, 1961.

[487] C. E. Shannon, R. G. Gallager, and E. R. Berlekamp. Lower bounds to error probability for coding in discrete memoryless channels. I. Inf. Control, 10:65–103, 1967.

[488] C. E. Shannon, R. G. Gallager, and E. R. Berlekamp. Lower bounds to error probability for coding in discrete memoryless channels. II. Inf. Control, 10:522–552, 1967.

[489] C. E. Shannon and W. W. Weaver. The Mathematical Theory of Communication. University of Illinois Press, Urbana, IL, 1949.

[490] C. E. Shannon. General treatment of the problem of coding. IEEE Trans. Inf. Theory, pages 102–104, February 1953.

[491] W. F. Sharpe. Investments, 3rd ed. Prentice-Hall, Englewood Cliffs, NJ, 1985.

[492] P. C. Shields. Universal redundancy rates do not exist. IEEE Trans. Inf. Theory, pages 520–524, Mar. 1993.

[493] P. C. Shields. The interactions between ergodic theory and information theory. IEEE Trans. Inf. Theory, pages 2079–2093, Oct. 1998.

[494] P. C. Shields and B. Weiss. Universal redundancy rates for the class of B-processes do not exist. IEEE Trans. Inf. Theory, pages 508–512, Mar. 1995.

[495] J. E. Shore and R. W. Johnson. Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy. IEEE Trans. Inf. Theory, IT-26:26–37, 1980.

[496] Y. M. Shtarkov. Universal sequential coding of single messages. Probl. Inf. Transm. (USSR), 23(3):3–17, July–Sept. 1987.

[497] A. Shwartz and A. Weiss. Large Deviations for Performance Analysis, Queues, Communication and Computing. Chapman & Hall, London, 1995.

[498] D. Slepian. Key Papers in the Development of Information Theory. IEEE Press, New York, 1974.

[499] D. Slepian. On bandwidth. Proc. IEEE, pages 292–300, Mar. 1976.

[500] D. Slepian and H. O. Pollak. Prolate spheroidal wave functions, Fourier analysis and uncertainty: Part I. Bell Syst. Tech. J., 40:43–64, 1961.

[501] D. Slepian and J. K. Wolf. A coding theorem for multiple access channels with correlated sources. Bell Syst. Tech. J., 52:1037–1076, 1973.

[502] D. Slepian and J. K. Wolf. Noiseless coding of correlated information sources. IEEE Trans. Inf. Theory, IT-19:471–480, 1973.

[503] D. S. Slepian. Information theory in the ﬁfties. IEEE Trans. Inf. Theory, pages 145–148, Mar. 1973.

[504] R. J. Solomonoff. A formal theory of inductive inference. Inf. Control, 7:1–22,224–254, 1964.

[505] A. Stam. Some inequalities satisﬁed by the quantities of information of Fisher and Shannon. Inf. Control, 2:101–112, June 1959.

[506] A. Steane. Quantum computing. Rept. Progr. Phys., pages 117–173, Feb. 1998.

[507] J. A. Storer and T. G. Szymanski. Data compression via textual substitution. J. ACM, 29(4):928–951, 1982.

[508] W. Szpankowski. Asymptotic properties of data compression and sufﬁx trees. IEEE Trans. Inf. Theory, pages 1647–1659, Sept. 1993.

[509] W. Szpankowski. Average Case Analysis of Algorithms on Sequences. Wiley-Interscience, New York, 2001.

[510] D. L. Tang and L. R. Bahl. Block codes for a class of constrained noiseless channels. Inf. Control, 17:436–461, 1970.

[511] I. E. Teletar and R. G. Gallager. Combining queueing theory with information theory for multiaccess. IEEE J. Sel. Areas Commun., pages 963–969, Aug. 1995.

[512] E. Teletar. Capacity of multiple antenna Gaussian channels. Eur. Trans. Telecommun., 10(6):585–595, 1999.

[513] J. A. Thomas. Feedback can at most double Gaussian multiple access channel capacity. IEEE Trans. Inf. Theory, pages 711–716, Sept. 1987.

[514] T. J. Tjalkens and F. M. J. Willems. A universal variable-to-ﬁxed length source code based on Lawrence’s algorithm. IEEE Trans. Inf. Theory, pages 247–253, Mar. 1992.

[515] T. J. Tjalkens and F. M. J. Willems. Variable- to ﬁxed-length codes for Markov sources. IEEE Trans. Inf. Theory, pages 246–257, Mar. 1987.

[516] S. C. Tornay. Ockham: Studies and Selections (chapter “Commentarium in Sententias,” I, 27). Open Court Publishers, La Salle, IL, 1938.

[517] H. L. Van Trees. Detection, Estimation, and Modulation Theory, Part I. Wiley, New York, 1968.

[518] B. S. Tsybakov. Capacity of a discrete-time Gaussian channel with a ﬁlter. Probl. Inf. Transm., pages 253–256, July–Sept. 1970.

[519] B. P. Tunstall. Synthesis of noiseless compression codes. Ph.D. dissertation, Georgia Institute of Technology, Atlanta, GA, Sept. 1967.

[520] G. Ungerboeck. Channel coding with multilevel/phase signals. IEEE Trans. Inf. Theory, pages 55–67, January 1982.

[521] G. Ungerboeck. Trellis-coded modulation with redundant signal sets part I: Introduction. IEEE Commun. Mag., pages 5–11, Feb. 1987.

[522] G. Ungerboeck. Trellis-coded modulation with redundant signal sets part II: State of the art. IEEE Commun. Mag., pages 12–21, Feb. 1987.

[523] I. Vajda. Theory of Statistical Inference and Information. Kluwer, Dordrecht, The Netherlands, 1989.

[524] L. G. Valiant. A theory of the learnable. Commun. ACM, pages 1134–1142, 1984.

[525] J. M. Van Campenhout and T. M. Cover. Maximum entropy and conditional probability. IEEE Trans. Inf. Theory, IT-27:483–489, 1981.

[526] E. Van der Meulen. Random coding theorems for the general discrete memoryless broadcast channel. IEEE Trans. Inf. Theory, IT-21:180–190, 1975.

[527] E. C. van der Meulen. Some reﬂections on the interference channel. In R. E. Blahut, D. J. Costello, U. Maurer, and T. Mittelholzer, (Eds.), Communications and Cryptography: Two Sides of One Tapestry. Kluwer, Boston, 1994.

[528] E. C. Van der Meulen. A survey of multi-way channels in information theory. IEEE Trans. Inf. Theory, IT-23:1–37, 1977.

[529] E. C. Van der Meulen. Recent coding theorems for multi-way channels. Part I: The broadcast channel (1976-1980). In J. K. Skwyrzinsky (Ed.), New Concepts in Multi-user Communication (NATO Advanced Study Institute Series), pages 15–51. Sijthoff & Noordhoff, Amsterdam, 1981.

[530] E. C. Van der Meulen. Recent coding theorems and converses for multi-way channels. Part II: The multiple access channel (1976–1985) (Technical Report). Department Wiskunde, Katholieke Universiteit Leuven, 1985.

[531] V. N. Vapnik. Estimation of Dependencies Based on Empirical Data. Springer-Verlag, New York, 1982.

[532] V. N. Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag, New York, 1991.

[533] V. N. Vapnik and A. Y. Chervonenkis. On the uniform convergence of relative frequencies to their probabilities. Theory Prob. Appl., pages 264–280, 1971.

[534] V. N. Vapnik and A. Y. Chervonenkis. Necessary and sufﬁcient conditions for the uniform convergence of means to their expectations. Theory Prob. Appl., pages 532–553, 1981.

[535] S. Verdu. The capacity region of the symbol-asynchronous Gaussian multiple-access channel. IEEE Trans. Inf. Theory, pages 733–751, July 1989.

[536] S. Verdu. Recent Progress in Multiuser Detection (Advances in Communication and Signal Processing), Springer-Verlag, Berlin, 1989. [Reprinted in N. Abramson (Ed.), Multiple Access Communications, IEEE Press, New York, 1993.]

[537] S. Verdu. The exponential distribution in information theory. Probl. Inf. Transm. (USSR), pages 86–95, Jan.–Mar. 1996.

[538] S. Verdu. Fifty years of Shannon theory. IEEE Trans. Inf. Theory, pages 2057–2078, Oct. 1998.

[539] S. Verdu. Multiuser Detection. Cambridge University Press, New York, 1998.

[540] S. Verdu and T. S. Han. A general formula for channel capacity. IEEE Trans. Inf. Theory, pages 1147–1157, July 1994.

[541] S. Verdu and T. S. Han. The role of the asymptotic equipartition property in noiseless source coding. IEEE Trans. Inf. Theory, pages 847–857, May 1997.

[542] S. Verdu and S. W. McLaughlin (Eds.). Information Theory: 50 Years of Discovery. Wiley–IEEE Press, New York, 1999.

[543] S. Verdu and V. K. W. Wei. Explicit construction of optimal constant-weight codes for identiﬁcation via channels. IEEE Trans. Inf. Theory, pages 30–36, Jan. 1993.

[544] A. C. G. Verdugo Lazo and P. N. Rathie. On the entropy of continuous probability distributions. IEEE Trans. Inf. Theory, IT-24:120–122, 1978.

[545] M. Vidyasagar. A Theory of Learning and Generalization. Springer-Verlag, New York, 1997.

[546] K. Visweswariah, S. R. Kulkarni, and S. Verdu. Source codes as random number generators. IEEE Trans. Inf. Theory, pages 462–471, Mar. 1998.

[547] A. J. Viterbi and J. K. Omura. Principles of Digital Communication and Coding. McGraw-Hill, New York, 1979.

[548] J. S. Vitter. Dynamic Huffman coding. ACM Trans. Math. Software, pages 158–167, June 1989.

[549] V. V. V’yugin. On the defect of randomness of a ﬁnite object with respect to measures with given complexity bounds. Theory Prob. Appl., 32(3):508–512, 1987.

[550] A. Wald. Sequential Analysis. Wiley, New York, 1947.

[551] A. Wald. Note on the consistency of the maximum likelihood estimate. Ann. Math. Stat., pages 595–601, 1949.

[552] M. J. Weinberger, N. Merhav, and M. Feder. Optimal sequential probability assignment for individual sequences. IEEE Trans. Inf. Theory, pages 384–396, Mar. 1994.

[553] N. Weiner. Cybernetics. MIT Press, Cambridge, MA, and Wiley, New York, 1948.

[554] T. A. Welch. A technique for high-performance data compression. Computer, 17(1):8–19, Jan. 1984.

[555] N. Wiener. Extrapolation, Interpolation and Smoothing of Stationary Time Series. MIT Press, Cambridge, MA, and Wiley, New York, 1949.

[556] H. J. Wilcox and D. L. Myers. An Introduction to Lebesgue Integration and Fourier Series. R.E. Krieger, Huntington, NY, 1978.

[557] F. M. J. Willems. The feedback capacity of a class of discrete memoryless multiple access channels. IEEE Trans. Inf. Theory, IT-28:93–95, 1982.

[558] F. M. J. Willems and A. P. Hekstra. Dependence balance bounds for single-output two-way channels. IEEE Trans. Inf. Theory, IT-35:44–53, 1989.

[559] F. M. J. Willems. Universal data compression and repetition times. IEEE Trans. Inf. Theory, pages 54–58, Jan. 1989.



BIBLIOGRAPHY  
[560] F. M. J. Willems, Y. M. Shtarkov, and T. J. Tjalkens. The context-tree weighting method: basic properties. IEEE Trans. Inf. Theory, pages 653–664, May 1995.  
[561] F. M. J. Willems, Y. M. Shtarkov, and T. J. Tjalkens. Context weighting for general finite-context sources. IEEE Trans. Inf. Theory, pages 1514–1520, Sept. 1996.  
[562] H. S. Witsenhausen. The zero-error side information problem and chromatic numbers. IEEE Trans. Inf. Theory, pages 592–593, Sept. 1976.  
[563] H. S. Witsenhausen. Some aspects of convexity useful in information theory. IEEE Trans. Inf. Theory, pages 265–271, May 1980.  
[564] I. H. Witten, R. M. Neal, and J. G. Cleary. Arithmetic coding for data compression. Commun. ACM, 30(6):520–540, June 1987.  
[565] J. Wolfowitz. The coding of messages subject to chance errors. Ill. J. Math., 1:591–606, 1957.  
[566] J. Wolfowitz. Coding Theorems of Information Theory. Springer-Verlag, Berlin, and Prentice-Hall, Englewood Cliffs, NJ, 1978.  
[567] P. M. Woodward. Probability and Information Theory with Applications to Radar. McGraw-Hill, New York, 1953.  
[568] J. Wozencraft and B. Reiffen. Sequential Decoding. MIT Press, Cambridge, MA, 1961.  
[569] J. M. Wozencraft and I. M. Jacobs. Principles of Communication Engineering. Wiley, New York, 1965.  
[570] A. Wyner. A theorem on the entropy of certain binary sequences and applications II. IEEE Trans. Inf. Theory, IT-19:772–777, 1973.  
[571] A. Wyner. The common information of two dependent random variables. IEEE Trans. Inf. Theory, IT-21:163–179, 1975.  
[572] A. Wyner. On source coding with side information at the decoder. IEEE Trans. Inf. Theory, IT-21:294–300, 1975.  
[573] A. Wyner and J. Ziv. A theorem on the entropy of certain binary sequences and applications I. IEEE Trans. Inf. Theory, IT-19:769–771, 1973.  
[574] A. Wyner and J. Ziv. The rate distortion function for source coding with side information at the receiver. IEEE Trans. Inf. Theory, IT-22:1–11, 1976.  
[575] A. Wyner and J. Ziv. On entropy and data compression. IEEE Trans. Inf. Theory, 1991.  
[576] A. D. Wyner. Capacity of the band-limited Gaussian channel. Bell Syst. Tech. J., 45:359–395, Mar. 1966.  
[577] A. D. Wyner. Communication of analog data from a Gaussian source over a noisy channel. Bell Syst. Tech. J., pages 801–812, May–June 1968.  
[578] A. D. Wyner. Recent results in the Shannon theory. IEEE Trans. Inf. Theory, pages 2–10, Jan. 1974.  
[579] A. D. Wyner. The wiretap channel. Bell Syst. Tech. J., pages 1355–1387, 1975.  
[580] A. D. Wyner. The rate-distortion function for source coding with side information at the decoder. II: General sources. Inf. Control, pages 60–80, 1978.  
[581] A. D. Wyner. Shannon-theoretic approach to a Gaussian cellular multiple-access channel. IEEE Trans. Inf. Theory, pages 1713–1727, Nov. 1994.  
[582] A. D. Wyner and A. J. Wyner. Improved redundancy of a version of the Lempel–Ziv algorithm. IEEE Trans. Inf. Theory, pages 723–731, May 1995.  
[583] A. D. Wyner and J. Ziv. Bounds on the rate-distortion function for stationary sources with memory. IEEE Trans. Inf. Theory, pages 508–513, Sept. 1971.  
[584] A. D. Wyner and J. Ziv. The rate-distortion function for source coding with side information at the decoder. IEEE Trans. Inf. Theory, pages 1–10, Jan. 1976.  
[585] A. D. Wyner and J. Ziv. Some asymptotic properties of the entropy of a stationary ergodic data source with applications to data compression. IEEE Trans. Inf. Theory, pages 1250–1258, Nov. 1989.  
[586] A. D. Wyner and J. Ziv. Classification with finite memory. IEEE Trans. Inf. Theory, pages 337–347, Mar. 1996.  
[587] A. D. Wyner, J. Ziv, and A. J. Wyner. On the role of pattern matching in information theory. IEEE Trans. Inf. Theory, pages 2045–2056, Oct. 1998.  
[588] A. J. Wyner. The redundancy and distribution of the phrase lengths of the fixed-database Lempel–Ziv algorithm. IEEE Trans. Inf. Theory, pages 1452–1464, Sept. 1997.  
[589] A. D. Wyner. The capacity of the band-limited Gaussian channel. Bell Syst. Tech. J., 45:359–371, 1965.  
[590] A. D. Wyner and N. J. A. Sloane (Eds.) Claude E. Shannon: Collected Papers. Wiley–IEEE Press, New York, 1993.  
[591] A. D. Wyner and J. Ziv. The sliding window Lempel–Ziv algorithm is asymptotically optimal. Proc. IEEE, 82(6):872–877, 1994.  
[592] E.-H. Yang and J. C. Kieffer. On the performance of data compression algorithms based upon string matching. IEEE Trans. Inf. Theory, pages 47–65, Jan. 1998.  
[593] R. Yeung. A First Course in Information Theory. Kluwer Academic, Boston, 2002.  
[594] H. P. Yockey. Information Theory and Molecular Biology. Cambridge University Press, New York, 1992.  
[595] Z. Zhang, T. Berger, and J. P. M. Schalkwijk. New outer bounds to capacity regions of two-way channels. IEEE Trans. Inf. Theory, pages 383–386, May 1986.  
[596] Z. Zhang, T. Berger, and J. P. M. Schalkwijk. New outer bounds to capacity regions of two-way channels. IEEE Trans. Inf. Theory, IT-32:383–386, 1986.  
[597] J. Ziv. Coding of sources with unknown statistics. II: Distortion relative to a fidelity criterion. IEEE Trans. Inf. Theory, IT-18:389–394, 1972.  
[598] J. Ziv. Coding of sources with unknown statistics. II: Distortion relative to a fidelity criterion. IEEE Trans. Inf. Theory, pages 389–394, May 1972.  
[599] J. Ziv. Coding theorems for individual sequences. IEEE Trans. Inf. Theory, pages 405–412, July 1978.  
[600] J. Ziv. Distortion-rate theory for individual sequences. IEEE Trans. Inf. Theory, pages 137–143, March 1980.  
[601] J. Ziv. Universal decoding for finite-state channels. IEEE Trans. Inf. Theory, pages 453–460, July 1985.  
[602] J. Ziv. Variable-to-fixed length codes are better than fixed-to-variable length codes for Markov sources. IEEE Trans. Inf. Theory, pages 861–863, July 1990.  
[603] J. Ziv and A. Lempel. A universal algorithm for sequential data compression. IEEE Trans. Inf. Theory, IT-23:337–343, 1977.  
[604] J. Ziv and A. Lempel. Compression of individual sequences by variable rate coding. IEEE Trans. Inf. Theory, IT-24:530–536, 1978.  
[605] J. Ziv and N. Merhav. A measure of relative entropy between individual sequences with application to universal classification. IEEE Trans. Inf. Theory, pages 1270–1279, July 1993.  
[606] W. H. Zurek. Algorithmic randomness and physical entropy. Phys. Rev. A, 40:4731–4751, Oct. 15, 1989.  
[607] W. H. Zurek. Thermodynamic cost of computation, algorithmic complexity, and the information metric. Nature, 341(6238):119–124, Sept. 1989.  
[608] W. H. Zurek (Ed.) Complexity, Entropy and the Physics of Information (Proceedings of the 1988 Workshop on the Complexity, Entropy and the Physics of Information). Addison-Wesley, Reading, MA, 1990.  

LIST OF SYMBOLS  
X, 14  
p(x), 14  
p_X(x), 14  
X, 14  
H(X), 14  
H(p), 14  
H_b(X), 14  
E_pg(X), 14  
E_g(X), 14  
def, 15  
==, 15  
p(x, y), 17  
H(X, Y), 17  
H(Y|X), 17  
D(p||q), 20  
I(X; Y), 21  
I(X; Y|Z), 24  
D(p(y|x)||q(y|x)), 25  
|X|, 30  
X → Y → Z, 35  
N(µ, σ²), 37  
T(X), 38  
f_θ(x), 38  
P_e, 39  
H(p), 45  
{0, 1}*, 55  
2^{-n(H±ε)}, 58  
A(n, ε), 59  
ε, 59  
x, 60  
X^n, 60  
Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas Copyright © 2006 John Wiley & Sons, Inc.  
x^n, 61  
B(n, ε), 62  
δ, 62  
.=, 63  
Z^n, 65  
H(X), 71  
P_ij, 72  
H'(X), 75  
C(x), 103  
l(x), 103  
C, 103  
L(C), 104  
D, 104  
C*, 105  
l_i, 107  
l_max, 108  
L, 110  
L*, 111  
H_D(X), 111  
⌈x⌉, 113  
F(x), 127  
F(x), 128  
sgn(t), 132  
p(j), 138  
i, 138  
p_i, 159  
o_i, 159  
b_i, 160  
b(i), 160  
S^n, 160  
S(X), 160  
W(b, p), 160  
b, 160  
p, 160  
W*(p), 161  
W*(X|Y), 165  
C, 184  
ˆW, 193  
(X^n, p(y|x), Y^n), 193  
λ_i, 194  
λ(n), 194  
P^(n, ε), 194  
R, 195  
(2^{nR}, n), 195  
C, 200  
E, 202  
E_i, 203  
∪, 203  
C*, 204  
CFB, 216  
⊕, 224  
F(x), 243  
f(x), 243  
h(X), 243  
S, 243  
h(f), 243  
φ(x), 244  
Vol(A), 245  
X^, 247  
h(X|Y), 249  
N^n(µ, K), 249  
|K|, 249  
D(f||g), 250  
X, 253  
P, 261  
W, 270  
F(ω), 271  
sinc(t), 271  
N_0, 272  
x^+, 276  
K_X, 278  
K_Z, 278  
tr(K_X), 278  
B, 282  
K_V, 282  
C_n,FB, 283  
{ˆX(w)}, 303  
ˆX, 304  
R^+, 304  
d_max, 304  
D, 306  
ˆX^n(w), 306  
R(D), 306  
D(R), 306  
R(I)(D), 307  
A(n, d, ε), 319  
D, 321  
K(x^n, ˆx^n), 322  
N(a|x^n), 326  
A*(n, ε), 326  
N(a, b|x^n, y^n), 326  
φ(D), 337  
V_{y|x}(b|a), 342  
TV(x^n), 342  
A*(n, ε)(Y|x^n), 343  
x, 347  
X, 347  
P_x, 348  
P_{x^n}, 348  
P_n, 348  
T(P), 348  
Q_n(x^n), 349  
T_ε(Q), 356  
Q_n(E), 361  
P*, 362  
L1, 369  
|| ||_1, 369  
S_t, 372  
D*, 372  
α, 375  
β, 376  
a*, 376  
b*, 376  
A_n, 376  
B_n, 376  
φ_A(), 376  
P_λ, 380  
λ*, 380  
C(P1, P2), 386  
ψ(s), 392  
T(X1, X2, ..., Xn), 393  
V, 394  
J(θ), 394  
b_T(θ), 396  
J_ij(θ), 397  
R(k), 415  
S(λ), 415  
ˆR(k), 415  
K(n), 416  
σ²_∞, 417  
K_p, 420  
f(u), 422  
p_θ, 428  
R(p_θ, q), 429  
R*, 429  
q_π, 430  
A(n, k), 434  
q^{1/2}(x^n), 436  
F_U(u), 437  
R_n(X0, X1, ..., Xn−1), 444  
Q_u(i), 445  
A_jk, 445  
c(n), 450  
n_k, 450  
cls, 452  
{X_i}^∞_{−∞}, 455  
AD, 460  
U, 466  
U(p), 466  
p, 466  
K_U(x), 466  
K_U(x|l(x)), 467  
K(x|l(x)), 468  
log* n, 469  
H0(p), 470  
K(n), 471  
P_U(x), 481  
, 484  
,n, 484  
K_k(x^n|n), 496  
k*, 497  
p*, 497  
S*, 497  
S**, 497  
p**, 497  
C(P/N), 514  
S, 520  
X(S), 520  
A(n, ε)(S), 521  
a_n = 2n(b ± ε), 521  
A(n, ε)(S1|s2), 523  
E_ij, 531  
Q, 534  
CI, 535  
R(S), 543  
S_c, 543  
β*_{p1}, 569  
b, 613  
S, 613  
W(b, F), 615  
W*(F), 615  
b*, 615  
S*  
n, 615  
W*, 615  
B, 617  
W*, 622  
W*  
∞, 624  
U*, 628  
S*  
n(x^n), 630  
ˆS_n(x^n), 630  
V_n, 631  
j_n, 634  
w(j_n), 634  
K, 636



--- Page 52 ---
726
LIST OF SYMBOLS
Ŵ(m), 638  
B(λ1, λ2), 642  
(, B, P), 644  
X(ω), 644  
Hk, 645  
H∞, 645  
ˆpn, 660  
Ŵ(z), 662  
ψ(z), 662  
γ, 662  
X(S), 668  
h(n)k, 668  
t(n)k, 669  
g(n)k, 669  
f(n)k, 671  
J(X), 671  
gt(y), 672  
V(A), 675  
f∗g, 676  
Lr, 676  
||f||r, 676  
Cp, 676  
hr(X), 676  
Vr(X), 677  
Pk, 680  
S(n)k, 680  
K(i1, i2, ..., ik), 680  
Qk, 681  
σ2i, 681  
Kk, 681  

--- Page 53 ---
INDEX  
Abrahams, J., 689  
Abramson, N.M., xxiii, 689  
Abu-Mostafa, Y.S., 689  
acceptance region, 376, 383  
achievable rate, 195, 268, 306, 538, 546, 550, 598  
achievable rate distortion pair, 306  
achievable rate region, 514, 550  
Aczél, J., 690  
Adams, K., xxiii  
adaptive dictionary compression algorithms, 441  
additive channel, 229  
additive noise channel, 280, 287, 296  
additive white Gaussian noise (AWGN), 289  
Adler, R.L., 158, 689  
AEP, xix, xx, 6, 12, 57, 58, 64, 69, 77, 101, 168, 219, 220, 222, 223, 347, 356, 381, 382, 409, 460, 531, 554, 560, 566, 644, 645, 649, 656, 686, see also Shannon-McMillan-Breiman theorem  
continuous random variables, 245  
discrete random variables, 58–62  
distortion typical, 319  
growth rate, 650  
joint, 196, 223  
products, 66  
relative entropy, 380  
sandwich proof, 644–649  
stationary ergodic processes, 644–649  
Ahlswede, R., 11, 609, 610, 689, 690, 712  
Akaike, H., 690  
Algoet, P., xxiii, 69, 626, 645, 656, 690  
algorithm, 442  
alternating minimization, 332  
arithmetic coding, see arithmetic coding  
Blahut-Arimoto, see Blahut-Arimoto algorithm  
Durbin, 419  
Frank-Wolfe, 191  
gradient search, 191  
greedy, 127  
Huffman coding, 127, 154, see Huffman coding  
iterative, 332  
Lempel-Ziv, see Lempel-Ziv coding  
Levinson, 419  
online, 427  
algorithmic complexity, 1, 3, 463, 464, 466, 507, 508, see Kolmogorov complexity  
algorithmically random, 477, 486, 502, 507  
almost sure, 58, 69  
alphabet, 13, 347  
discrete, 13  
effective size, 46, 256  
effective support set, 256  
input, 183  
output, 183  
alphabetic codes, 121  
Altria, 643  
Amari, S.I., 55, 690, 703  
Anantharam, V., 690  
antenna, 292, 611  
Arimoto, S., 191, 240, 335, 346, 690, see also Blahut-Arimoto algorithm  
arbitrarily varying channel, 689, 690, 697  
arithmetic coding, 130, 158, 171, 427, 428, 435–440, 461  
finite precision arithmetic, 439  
arithmetic mean geometric mean inequality, 669  
ASCII, 466  
Ash, R.B., 690  
asymmetric distortion, 337  
asymptotic equipartition property, see AEP  
asymptotic optimality, log-optimal portfolio, 619  
ATM networks, 218  
atmosphere, 412  
atom, 137–140, 257, 404  
autocorrelation, 415, 420  
autoregressive process, 416  
auxiliary random variable, 565, 569  
average codeword length, 124, 129, 148  
average description length, 103  
average distortion, 318, 324, 325, 329, 344  
average power, 261, 295, 296, 547  
average probability of error, 199, 201, 202, 204, 207, 231, 297, 532, 554  
AWGN (Additive white Gaussian noise), 289  
axiomatic definition of entropy, 14, 54  
Baer, M., xxi  
Bahl, L.R., xxiii, 690, 716  
bandlimited, 272  
bandpass filter, 270  
bandwidth, 272–274, 289, 515, 547, 606  
Barron, A.R., xxi, xxiii, 69, 420, 508, 656, 674, 691, 694  
base of logarithm, 14  
baseball, 389  
Baum, E.B., 691  
Bayesian error exponent, 388  
Bayesian hypothesis testing, 384  
Bayesian posterior probability, 435  
Bayesian probability of error, 385, 399  
BCH (Bose-Chaudhuri-Hocquenghem) codes, 214  
Beckenbach, E.F., 484  
Beckner, W., 691  
Bell, R., 182, 656, 691  
Bell, T.C., 439, 691  
Bell’s inequality, 56  
Bellman, R., 691  
Bennett, C.H., 241, 691  
Bentley, J., 691  
Benzel, R., 692  
Berger, T., xxiii, 325, 345, 610, 611, 692, 720  
Bergmans, P., 609, 692  
Bergstrøm’s inequality, 684  
Berlekamp, E.R., 692, 715  
Bernoulli, J., 182  
Bernoulli distribution, 434  
Bernoulli process, 237, 361, 437, 476, 484, 488  
Bernoulli random variable, 63  
Bernoulli source, 307, 338  
rate distortion, 307  
Berrou, C., 692  
Berry’s paradox, 483  
Bertsekas, D., 692  
beta distribution, 436, 661  
beta function, 642  
betting, 162, 164, 167, 173, 174, 178, 487, 626  
horse race, 160  
proportional, 162, 626  
bias, 393, 402  
Bierbaum, M., 692  
Biglieri, E., 692  
binary entropy function, 15, 49  
graph, 15, 16  
binary erasure channel, 188, 189, 218, 227, 232, 457  
binary multiplier channel, 234, 602  
binary rate distortion function, 307  
binary source, 308, 337  
binary symmetric channel (BSC), 8, 187–189, 210, 215, 222, 224, 225, 227–229, 231, 232, 237, 238, 262, 308, 568, 601  
capacity, 187  
binning, 609  
random, 551, 585  
bioinformatics, xv  
bird, 97  
Birkhoff’s ergodic theorem, 644  
bishop, 80  
bit, 14  
Blachman, N., 674, 687, 692  
Blackwell, D., 692  

--- Page 54 ---
728
INDEX
arithmetic coding, 130, 158, 171, 427, 428, 435–440, 461  
finite precision arithmetic, 439  
arithmetic mean geometric mean inequality, 669  
ASCII, 466  
Ash, R.B., 690  
asymmetric distortion, 337  
asymptotic equipartition property, see AEP  
asymptotic optimality, log-optimal portfolio, 619  
ATM networks, 218  
atmosphere, 412  
atom, 137–140, 257, 404  
autocorrelation, 415, 420  
autoregressive process, 416  
auxiliary random variable, 565, 569  
average codeword length, 124, 129, 148  
average description length, 103  
average distortion, 318, 324, 325, 329, 344  
average power, 261, 295, 296, 547  
average probability of error, 199, 201, 202, 204, 207, 231, 297, 532, 554  
AWGN (Additive white Gaussian noise), 289  
axiomatic definition of entropy, 14, 54  
Baer, M., xxi  
Bahl, L.R., xxiii, 690, 716  
bandlimited, 272  
bandpass filter, 270  
bandwidth, 272–274, 289, 515, 547, 606  
Barron, A.R., xxi, xxiii, 69, 420, 508, 656, 674, 691, 694  
base of logarithm, 14  
baseball, 389  
Baum, E.B., 691  
Bayesian error exponent, 388  
Bayesian hypothesis testing, 384  
Bayesian posterior probability, 435  
Bayesian probability of error, 385, 399  
BCH (Bose-Chaudhuri-Hocquenghem) codes, 214  
Beckenbach, E.F., 484  
Beckner, W., 691  
Bell, R., 182, 656, 691  
Bell, T.C., 439, 691  
Bell’s inequality, 56  
Bellman, R., 691  
Bennett, C.H., 241, 691  
Bentley, J., 691  
Benzel, R., 692  
Berger, T., xxiii, 325, 345, 610, 611, 692, 720  
Bergmans, P., 609, 692  
Bergstrøm’s inequality, 684  
Berlekamp, E.R., 692, 715  
Bernoulli, J., 182  
Bernoulli distribution, 434  
Bernoulli process, 237, 361, 437, 476, 484, 488  
Bernoulli random variable, 63  
Bernoulli source, 307, 338  
rate distortion, 307  
Berrou, C., 692  
Berry’s paradox, 483  
Bertsekas, D., 692  
beta distribution, 436, 661  
beta function, 642  
betting, 162, 164, 167, 173, 174, 178, 487, 626  
horse race, 160  
proportional, 162, 626  
bias, 393, 402  
Bierbaum, M., 692  
Biglieri, E., 692  
binary entropy function, 15, 49  
graph, 15, 16  
binary erasure channel, 188, 189, 218, 227, 232, 457  
binary multiplier channel, 234, 602  
binary rate distortion function, 307  
binary source, 308, 337  
binary symmetric channel (BSC), 8, 187–189, 210, 215, 222, 224, 225, 227–229, 231, 232, 237, 238, 262, 308, 568, 601  
capacity, 187  
binning, 609  
random, 551, 585  
bioinformatics, xv  
bird, 97  
Birkhoff’s ergodic theorem, 644  
bishop, 80  
bit, 14  
Blachman, N., 674, 687, 692  
Blackwell, D., 692  

--- Page 55 ---
729
INDEX
Blahut, R.E., 191, 240, 335, 346, 692, see also Blahut-Arimoto algorithm  
Blahut-Arimoto algorithm, 191, 334  
block code, 226  
block length, 195, 204  
block Markov encoding, 573  
Boltzmann, L., 11, 55, 693, see also Maxwell-Boltzmann distribution  
bone, 93  
bookie, 163  
Borel-Cantelli lemma, 357, 621, 649  
Bose, R.C., 214, 693  
bottleneck, 47, 48  
bounded convergence theorem, 396, 647  
bounded distortion, 307, 321  
brain, 465  
Brascamp, H.J., 693  
Brassard, G., 691  
Breiman, L., 69, 655, 656, 692, 693, see also Shannon-McMillan-Breiman theorem  
Brillouin, L., 56, 693  
broadcast channel, 11, 100, 515, 518, 533, 563, 560–571, 593, 595, 599, 601, 604, 607, 609, 610  
capacity region, 564  
convexity, 598  
common information, 563  
converse, 599  
degraded, 599, 609  
achievability, 565  
converse, 599  
with feedback, 610  
Gaussian, 515, 610  
physically degraded, 564  
stochastically degraded, 564  
Brunn-Minkowski inequality, xx, 657, 674–676, 678, 679, 687  
BSC, see binary symmetric channel (BSC)  
Bucklew, J.A., 693  
Burg, J.P., 416, 425, 693  
Burg’s algorithm, 416  
Burg’s maximum entropy theorem, 417  
Burrows, M., 462, 693  
burst error correcting code, 215  
Buzo, A., 708  
Caire, G., 693  
cake, 65  
calculus, 103, 111, 410  
Calderbank, A.R., 693  
Canada, 82  
capacity, 223, 428  
channel, see channel capacity  
capacity region, 11, 509–610  
degraded broadcast channel, 565  
multiple access channel, 526  
capacity theorem, xviii, 215  
CAPM (Capital Asset Pricing Model), 614  
Carathéodory’s theorem, 538  
cardinality, 245, 538, 542, 565, 569  
cards, 55, 84, 167, 608  
Carleial, A.B., 610, 693  
cascade, 225  
cascade of channels, 568  
Castelli, V., xxiii  
Cauchy distribution, 661  
Cauchy-Schwarz inequality, 393, 395  
causal, 516, 623  
causal investment strategy, 619, 623, 635  
causal portfolio, 620, 621, 624, 629–631, 635, 639, 641, 643  
universal, 651  
CCITT, 462  
CDMA (Code Division Multiple Access), 548  
central limit theorem, xviii, 261, 361  
centroid, 303, 312  
Cesàro mean, 76, 625, 682  
chain rule, xvii, 35, 43, 49, 287, 418, 540, 541, 555, 578, 584, 590, 624, 667  
differential entropy, 253  
entropy, 23, 31, 43, 76  
growth rate, 650  
mutual information, 24, 25, 43  
relative entropy, 44, 81  
Chaitin, G.J., 3, 4, 484, 507, 508, 693, 694  
Chang, C.S., xxi, 694  
channel, 225, 568  
binary erasure, 188, 222  
binary symmetric, see binary symmetric channel (BSC)  
broadcast, see broadcast channel  
cascade, 225, 568  
discrete memoryless, see discrete memoryless channel  
exponential noise, 291  
extension, 193  
feedback, 216  
Gaussian, see Gaussian channel  
interference, see interference channel  
multiple access, see multiple access channel  
noiseless binary, 7, 184  
parallel, 224, 238  
relay, see relay channel  
symmetric, 189, 190, 222  
time-varying, 226, 294  
two-way, see two-way channel  
union, 236  
weakly symmetric, 190  
with memory, 224, 240  
Z-channel, 225  
channel capacity, xv, xviii, xix, 1, 3, 7–9, 11, 38, 183–241, 291, 297, 298, 307, 333, 427, 432, 458, 461, 544–546, 548, 592, 604, 608, 686  
achievability, 200  
computation, 332  
feedback, 223  
information, 184, 263  
operational definition, 184  
properties, 222  
zero-error, 226  
channel capacity theorem, 38  
channel code, 193, 220  
channel coding, 207, 219, 220, 318, 324, 325, 344  
achievability, 195  
channel coding theorem, 9, 199, 207, 210, 223, 230, 321, 324, 347  
achievability, 200  
converse, 207  
channel transition matrix, 190, 234, 428, 433, 509  
channels with memory, 224, 277  
characteristic function, 422  
Chellappa, R., 694  
Cheng, J.F., 709  
Chernoff, H., 694  
Chernoff bound, 392  
Chernoff information, 380, 384, 386, 388, 399  
Chernoff-Stein lemma, 347, 376, 380, 383, 399  
Chervonenkis, A.Y., 717  
chessboard, 80, 97  
χ2 distance, 665  
χ2 statistic, 400  
Chi-squared distribution, 661  
Chiang, M.S., xxi, 695  
chicken, 301  
Choi, B.S., 425, 694, 697  
Chomsky, N., 694  
Chou, P.A., 694  
Chuang, I., 241, 710  
Chung, K.L., 69, 694  
Church’s thesis, 465  
Cioffi, J.M., 611, 702  
cipher, 170  
substitution, 170  
Clarke, B.S., 694  
classroom, 561  
Cleary, J.G., 691, 719  
closed system, 11  
closure, 363  
Cocke, J., 690  
cocktail party, 515  
code, 10, 20, 61, 62, 98, 104–158, 172, 194–232, 264–281, 302–339, 357, 360, 428, 429, 434, 436, 438, 440, 443, 444, 456, 460–463, 492, 525–602  
arithmetic, see arithmetic coding  
block, see block code  
channel, see channel code  
distributed source, see distributed source coding  
error correcting, see error correcting code  
extension, 105, 126  
Gaussian channel, 264  
Hamming, see Hamming code  
Huffman, see Huffman code  
instantaneous, 103, 106, 107, 110, 118, 124, 142, 143, 146, 152, see also code, prefix  
minimum distance, 212  
minimum weight, 212  
Morse, see Morse code  
nonsingular, 105, 152  
optimal, 124, 144  
prefix, 106  
random, 199, 201  
self-punctuating, 105, 106  
Shannon, see Shannon code



INDEX  
731  
source, see source code  
uniquely decodable, 105, 110, 116–118, 127, 131, 141–143, 147, 148, 150, 152, 157, 158  
zero-error, 205  
codebook, 204, 206, 212, 266, 268, 321, 322, 327, 328, 513–519, 530, 534, 565, 574, 580  
codelength, 114, 115, 122  
code points, 302  
codeword, 109, 122, 131  
optimal length, 113  
Cohn, M., xxi  
coin flips, 44, 103, 155  
coin tosses, 37, 134, 137, 139, 225, 375  
coin weighing, 45  
coins,  
bent, 48  
biased, 96, 375, 407  
fair, 134  
large deviations, 365  
colored noise, 288  
coloring,  
random, 558  
comma, 105, 468  
common information, 563, 564, 567, 568  
communication channel, 1, 7, 187, 223, 261, 560, 663  
communication networks, 509  
communication system, 8, 184, 192  
communication theory, 1  
compact disc, 3, 215  
compact set, 432, 538  
company mergers, 149  
competitive optimality,  
log-optimal portfolio, 627  
Shannon code, 130  
competitively optimal, 103, 613  
composition class, 348  
compression, see data compression  
computable, 466, 479, 482, 484, 491  
computable probability distribution, 482  
computable statistical tests, 479  
computation, 4, 5, 12, 68, 190, 438, 463, 466, 594  
channel capacity, 191, 332  
halting, see halting computation  
models of, 464  
physical limits, 56  
rate distortion, 332  
computer science, xvii, 1, 463, 483  
computer source code, 360  
computers, 4, 442, 464, 504  
concatenation, 105, 116  
concavity, 27, 31, 33, 222, 453, 474, 616, see also convexity  
conditional entropy, 17, 38–40, 75, 77, 88, 89, 206, 417, 669, 670  
conditional limit theorem, 366, 371, 375, 389, 398  
conditional mutual information, 24, 589  
conditional rate distortion,  
convexity, 583, 585  
conditional rate distortion function, 584, 585  
conditional relative entropy, 25  
conditional type, 342, 366  
conditionally typical set, 327, 342  
conditioning reduces entropy, 33, 39, 85, 311, 313, 318, 418, 578, 584, 682  
constant rebalanced portfolio, 615, 630  
constrained sequences, 94, 101  
continuous alphabet, 261, 305  
continuous channel, 263  
continuous random variable, 21, 37, 243, 245, 248, 256, 301, 304, 338, 500, see also differential entropy, rate distortion theory  
quantization, see quantization  
continuous time, 270  
convergence of random variables,  
convergence in mean square, 58  
convergence in probability, 58  
convergence with probability, 1, 58  
converse,  
broadcast channel, 599  
discrete memoryless channel, 206  
with feedback, 216  
general multiterminal network, 589  
multiple access channel, 538  
rate distortion, 315  
Slepian-Wolf coding, 555  

convex closure, 538  
convex families, 655  
convex hull, 526, 530, 532, 534–536, 538, 543, 544, 565, 591, 594, 595  
convex set, 330, 534  
convexiﬁcation, 598  
convexity, 26, 28, 32, 42, 432, 616, 657, see also concavity  
capacity region,  
broadcast channel, 598  
multiple access channel, 534  
rate distortion function, 316  
strict, 26  
convolution, 270, 674  
convolutional code, 215  
interleaved, 215  
cookie-cutting, 240  
copper, 274  
Coppersmith, D.J., 689  
correlated random variables,  
coding, see Slepian-Wolf coding  
correlation, 46, 252, 258, 294, 295, 593  
Costa, M.H.M., 593, 694, 703  
Costello, D.J., 708  
counterfeit, 45  
covariance matrix, 249, 277, 279, 280, 284, 292, 397, 416, 417, 681  
Cover, T.M., xxi, 69, 158, 182, 240, 299, 425, 508, 575, 593, 609–611, 626, 656, 687, 690, 691, 694–699, 701, 712, 717  
CPU, 465  
Cram´er, H., 696  
Cram´er-Rao inequality, 392, 395, 396, 399  
with bias, 402  
crosstalk, 273  
cryptography, 171  
Csisz´ar, I., 55, 325, 332, 334, 335, 346, 347, 358, 408, 461, 610, 696, 697  
Csisz´ar-Tusn´ady algorithm, 335  
cumulative distribution function, 127, 128, 130, 243, 437, 439  
cut set, 512, 589, 597  
D-adic distribution, 112  
D-ary alphabet, 103, 109  
Dantzig, G.B., 697  
Darroch, J.N., 697  
Dar´oczy, Z., 690  
data compression, xv, xvii, xix, 1, 3, 5, 11, 103, 156, 163, 172, 173, 184, 218, 221, 301, 427, 442, 457, 549, 656, 686  
data processing inequality, 35, 39, 44, 47, 371, 687  
data transmission, 5, 686  
Daubechies, I., 697  
Davisson, L.D., 461, 697, 702, 703  
de Bruijn’s identity, 672  
decision function, 375  
decision theory, see hypothesis testing  
decoder, 194  
decoding, 194  
robust, 296  
decoding delay, 148  
decoding function, 194, 219, 264, 552, 571, 583  
degraded, 515, 516, 533, 565–570, 595, 599, 601, 604, 609  
broadcast channel, see broadcast channel, degraded  
physically degraded, 564  
relay channel, see relay channel, degraded  
stochastically degraded, 564  
Dembo, A., xxiii, 687, 697, 698  
demodulation, 3  
Dempster, A.P., 698  
density, 243  
descriptive complexity, 463  
determinant, 249, 279, 679, 681  
determinant inequalities, xviii, 679–685  
deterministic, 53, 173, 204, 511, 607, 609  
deterministic decoding rule, 194  
deterministic function, 339, 456  
Devroye, L., 698  
dice, 364, 375, 411, 412  
dictionary, 442  
differential entropy, xix, 243–674  
bound, 258  
conditional, 249  
Gaussian distribution, 244  
relationship to discrete entropy, 247  
table of, 661  
Diggavi, S., xxi  
digital, 273, 274, 465  
dimension, 211, 675  
Dirichlet distribution, 436, 641  
Dirichlet partition, 303  

discrete, 184  
discrete channel, 263, 264, 268  
discrete entropy, 244, 259, 660  
discrete memoryless channel, 183, 184–241, 280, 318, 344, 536  
discrete memoryless source, 92  
discrete random variable, 14, 49, 54, 134, 243, 245, 249, 251, 252, 258, 347, 371, 658  
discrete time, 261  
discrimination, 55  
distance, 20, 301, 325, 332, 369, 432  
between probability distributions, 13  
Euclidean, 297, 332, 367  
relative entropy, 356, 366  
variational, 370  
distinguishable inputs, 10, 222  
distinguishable signals, 183  
distortion, 10, 301–341, 580–582, 586, 587, see also rate distortion theory  
asymmetric, 337  
Hamming, see Hamming distortion  
squared error, see squared error distortion  
distortion function, 304, 307, 309, 312, 336, 340, 344, 345  
distortion measure, 301, 302, 304, 305, 307, 314, 319, 321  
bounded, 304  
Hamming distortion, 304,  
Itakura-Saito, 305  
squared error distortion, 305  
distortion rate function, 306, 341  
distortion typical, 319, 321, 322, 328  
distributed source coding, 509, 511, 550, 556, 586, see also Slepian-Wolf coding  
distribution, 427, 428, 456  
two mass point, 28  
divergence, 55  
DiVincenzo, D.P., 691, 698  
DMC, see discrete memoryless channel  
Dobrushin, R.L., 698, 711  
dog, 93  
Donoho, D.L., 698  
doubling rate, xvii, 9, 11, 12, 160, 163–167, 175, 179, 180  
doubly stochastic matrix, 88  
DSL, 274  
duality, 610  
data compression and data transmission, 184  
data compression and gambling, 173  
growth rate and entropy rate, 159, 613  
multiple access channel and Slepian-Wolf coding, 558  
rate distortion and channel capacity, 311  
source coding and generation of random variables, 134  
Duda, R.O., 698  
Dueck, G., 609, 610, 690, 698  
Durbin algorithm, 419  
Dutch, 561, 562, 606  
Dutch book, 164, 180  
DVD, 3  
dyadic, 129, 130, 132, 137–139  
dyadic distribution, 151  
ear, 219  
Ebert, P.M., 299, 698  
echoes, 273  
Eckschlager, K., 698  
economics, 4  
edge process, 98  
effectively computable, 465  
efficient estimator, 396  
efficient frontier, 614  
Effros, M., 462, 694, 698, 702  
Efron, B., 699  
Eggleston, H.G., 538, 699  
eigenvalue, 95, 279, 282, 315, 336  
Einstein, A., xvii  
Ekroot, L., xxiii  
El Gamal, A., xxiii, 575, 609–611, 694–696, 699, 701, 702, 710  
elephant, 301  
Elias, P., 158, 699  
Ellis, R.S., 699  
EM algorithm, 335  
empirical, 68, 381, 409, 470, 474, 542, 660  
empirical distribution, 68, 168, 209, 347, 356, 366, 630  
convergence, 68  
empirical entropy, 195  
empirical frequency, 474  
encoder, 173, 231, 304, 321, 357, 360, 443, 458, 459, 549, 553, 557, 574, 585, 588  
encoding function, 193, 264, 305, 359, 571, 583, 599  
encrypted text, 506  
energy, 261, 265, 272, 273, 294, 424  
England, 82  
English, 104, 168–171, 174, 175, 182, 360, 470, 506  
entropy rate, 159, 182  
models of, 168  
entanglement, 56  
entropy, xvii, 3, 4, 13–56, 87, 659, 671, 686  
average, 49  
axiomatic definition, 14, 54  
base of logarithm, 14, 15  
bounds, 663  
chain rule, 23  
concavity, 33, 34  
conditional, 16, 51, see conditional entropy  
conditioning, 42  
cross entropy, 55  
differential, see differential entropy  
discrete, 14  
encoded bits, 156  
functions, 45  
grouping, 50  
independence bound, 31  
infinite, 49  
joint, 16, 47, see joint entropy  
mixing increase, 51  
mixture, 46  
and mutual information, 21  
properties of, 42  
relative, see relative entropy  
Renyi, 676  
sum, 47  
thermodynamics, 14  
entropy and relative entropy, 12, 28  
entropy power, xviii, 674, 675, 678, 679, 687  
entropy power inequality, xx, 298, 657, 674–676, 678, 679, 687  
entropy rate, 4, 74, 71–101, 114, 115, 134, 151, 156, 159, 163, 167, 168, 171, 175, 182, 221, 223, 259, 417, 419, 420, 423–425, 428–462, 613, 624, 645, 667, 669  
differential, 416  
English, 168, 170, 174, 175  
Gaussian process, 416  
Hidden Markov model, 86  
Markov chain, 77  
subsets, 667  
envelopes, 182  
Ephremides, A., 611, 699  
Epimenides liar paradox, 483  
equalization, 611  
Equitz, W., xxiii, 699  
erasure, 188, 226, 227, 232, 235, 527, 529, 594  
erasure channel, 219, 235, 433  
ergodic, 69, 96, 167, 168, 175, 297, 360, 443, 444, 455, 462, 557, 613, 626, 644, 646, 647, 651  
ergodic process, xx, 11, 77, 168, 444, 446, 451, 453, 644  
ergodic source, 428, 644  
ergodic theorem, 644  
ergodic theory, 11  
Erkip, E., xxi, xxiii  
Erlang distribution, 661  
error correcting code, 205  
error detecting code, 211  
error exponent, 4, 376, 380, 384, 385, 388, 399, 403  
estimation, xviii, 255, 347, 392, 425, 508  
spectrum, 415  
estimator, 39, 40, 52, 255, 392, 393, 395–397, 401, 402, 407, 417, 500, 663  
bias, 393  
biased, 401  
consistent in probability, 393  
domination, 393  
efficient, 396  
unbiased, 392, 393, 395–397, 399, 401, 402, 407  
Euclidean distance, 514  
Euclidean geometry, 378  
Euclidean space, 538  
Euler’s constant, 153, 662  
exchangeable stocks, 653  
expectation, 14, 167, 281, 306, 321, 328, 393, 447, 479, 617, 645, 647, 669, 670  
expected length, 104  
exponential distribution, 256, 661  
extension of channel, 193  
extension of code, 105



INDEX

F-distribution, 661  
face vase illusion, 505  
factorial, 351, 353  
Stirling’s approximation, 405  
fading, 611  
fading channel, 291  
Fahn, P., xxi  
fair odds, 159, 164, 487, 488  
fair randomization, 627, 629  
Fan, K., 679, 699  
Fano, R.M., 56, 158, 240, 699, 700, see also Shannon-Fano-Elias code  
Fano’s inequality, 13, 38, 39, 41, 44, 52, 56, 206, 208, 221, 255, 268, 283, 539–541, 555, 576, 578, 590, 663  
FAX, 130  
FDMA (Frequency Division Multiple Access), 547, 548, 606  
Feder, M., 158, 462, 700, 709, 718  
Feder, T., 461  
feedback, xix, 189, 193, 216, 218, 238, 280–284, 286–290, 509, 519, 593, 594, 610, 611  
discrete memoryless channel, 216  
Gaussian channel, xv, 280–289  
Feinstein, A., 240, 699, 700  
Feller, W., 182, 700  
Fermat’s last theorem, 486  
fingers, 143  
finite alphabet, 220, 318, 344, 473, 474, 645  
finitely often, 649  
finitely refutable, 486  
first order in the exponent, 63  
Fisher, R.A., 56, 700  
Fisher information, xviii, xx, 247, 347, 392, 394, 395, 397, 399, 401, 407, 657, 671, 673, 674  
examples, 401  
multiparameter, 397  
Fitingof, B.M., 461, 700  
fixed rate block code, 357  
flag, 61, 442, 460  
flow of information, 588, 589  
flow of time, 89  
flow of water, 511  
football, 390, 391  
Ford, L.R., 700  
Ford-Fulkerson theorem, 511, 512  
Forney, G.D., 240, 700  
Foschini, G.J., 611, 700  
Fourier transform, 271, 415  
fractal, 471  
Franaszek, P.A., xxi, xxiii, 158, 700  
Frank-Wolfe algorithm, 191  
French, 606  
frequency, 168–170, 270, 274, 315, 404, 547  
Friedman, J.H., 693  
Fulkerson, D.R., 697, 700  
function,  
concave, 26  
convex, 26  
functional, 161, 276, 313, 330  
future, 93  
Gaarder, T., 593, 609, 700  
Gabor, D., 701  
Gács, P., 695, 701  
Gadsby, 168  
Gallager, R.G., xxiii, 215, 240, 299, 430, 461, 609, 692, 701, 713, 715, 716  
Galois field theory, 214  
gambling, xviii, xx, 11, 13, 159, 171–173, 175, 178, 181, 182, 488, 507, 629  
universal, 487  
gambling and data compression, 171  
game, 181, 298, 391, 631  
20 questions, 6, 120, 121, 143, 145, 157, 237  
Hi-Lo, 147  
mutual information, 298  
red and black, 167, 177  
Shannon guessing, 174  
stock market, 630  
game theory, 132  
fundamental theorem, 432  
game-theoretic optimality, 132, 619  
γ (Euler’s constant), 153, 662  
Gamma distribution, 661  
gas, 34, 409, 411, 412  
Gauss’s law, 548  
Gauss-Markov process, 417–420  
Gaussian, 252, 255, 258, 378, 389, 684, 685  
Gaussian channel, xv, xix, 205, 261–299, 324, 513, 514, 519, 520, 544, 546, 686  
achievability, 266  
AWGN (additive white Gaussian noise), 289  
bandlimited, 270–274  
broadcast, see broadcast channel, Gaussian  
capacity, 264  
colored noise, 277  
converse, 268  
feedback, 280–289  
interference, see interference channel, Gaussian  
with memory, 277, 280  
multiple access, see multiple access channel, Gaussian  
parallel, 274–280, 292  
relay, see relay channel, Gaussian  
Gaussian distribution, see normal distribution  
Gaussian process, 272, 279, 417  
Gaussian source, 311, 336  
rate distortion function, 311  
Gaussian stochastic process, 315, 416, 417, 423  
Gelfand, I.M., 702  
Gelfand, S.I., 609, 610, 702  
Gemelos, G., xxi  
general multiterminal network, 587  
general theory of relativity, 490  
generalized Lloyd algorithm, 303  
generation of random variables, 134  
geodesic, 380  
geometric distribution, 405, 444  
geometry, 9, 301, 367  
Euclidean, 378  
geophysical applications, 415  
Gersho, A., 702  
Gibson, J.D., 702  
GIF, 443, 462  
Gilbert, E.N., 158, 702  
Gill, J., xxiii  
Glavieux, A., 692  
Gödel’s incompleteness theorem, 483  
Goldbach’s conjecture, 486  
Goldberg, M., xxiii  
Goldman, S., 702  
Goldsmith, A., 702  
Golomb, S.W., 702  
Goodell, K., xxiii  
Gopinath, R., xxi  
Gotham, 470, 550  
gradient search, 191  
grammar, 171  
Grant, A.J., 702  
graph, 73, 78, 79, 97  
graph coloring, 557  
gravestone, 55  
gravitation, 490  
Gray, R.M., 610, 694, 695, 702, 703, 708  
greetings telegrams, 441  
Grenander, U., 703  
grouping rule, 50  
growth rate, xix, 4, 159, 178, 180, 182, 615, 613–656, 686  
chain rule, 624, 650  
competitive optimality, 628  
convexity, 616, 650  
optimal, 615  
side information, 622, 650  
growth rate optimal, 162, 613  
Grünbaum, B., 538, 703  
Guiasu, S., 703  
Gupta, V., xxi  
Gutman, M., 462, 700  
Györfi, L., 698  
gzip, 442  
Hadamard’s inequality, 279, 680, 681  
Hajek, B., 611, 699, 703  
halting, 484  
halting computation, 466, 486  
halting problem, 483  
halting program, 473  
Hamming codes, 205, 212–214  
Hamming distortion, 307, 308, 336, 337  
Hamming, R.V., 210, 703  
Han, T.S., xxi, 593, 609, 610, 668, 670, 687, 689, 703, 717, 718  
handwriting, 87  
Hart, P.E., 695, 698  
Hartley, R.V., 55, 703  
Hassanpour, N., xxi  
Hassibi, B., 693  
Hassner, M., 689  
HDTV, 560  
Hekstra, A.P., 609, 718  
Helstrom, C.W., 703  
Hershkovits, Y., 703  
Hewlett-Packard, 643  
hidden Markov model (HMM), 87, 101



--- Page 64 ---
738
INDEX
irreducible Markov chain, see Markov chain, irreducible
Itakura-Saito distance, 305
iterative decoding, 215
Iyengar, G., xxi
Jacobs, I.M., 719
Jayant, N.S., 704
Jaynes, E.T., 56, 416, 425, 704
Jelinek, F., xxiii, 158, 690, 704, 705
Jensen’s inequality, 28, 32, 41, 42, 44, 49, 252, 253, 270, 318, 447, 453, 474, 585, 618, 622, 657
Johnson, R.W., 715
joint AEP, 202, 203, 267, 329, 520
joint density, 249
joint distribution, 16, 23, 34, 51, 52, 71, 228, 268, 307, 308, 323, 328, 343, 365, 402, 537, 539, 542, 550, 564, 565, 578, 586, 595, 600, 602, 608
joint entropy, 16
joint source channel coding theorem, 218
joint type, 499
joint typicality, 195, 222, 240
jointly typical, 198–203, 227–230, 240, 266, 267, 319, 327–329, 341, 343, 365, 366, 520, 553, 557, 559, 560, 575, 580
jointly typical sequences, 520
jointly typical set, 227, 228, 319, 327
Jozsa, R., 705
JPEG, 130
Julian, D., xxi
Justesen, J., 215, 705
Kac, M., 443, 705
Kac’s lemma, 444
Kailath, T., 705
Karlin, S., 705
Karush, J., 158, 705
Kaul, A., xxiii
Kawabata, B., xxiii
Keegel, J.C., 707
Kelly, J., 182, 655, 705
Kelly, F.P., 705
Kelly gambling, 182, 626
Kemperman, J.H.B., 408, 705
Kendall, M., 705
keyboard, 480, 482
Khairat, M.A., 707
Khinchin, A.Y., 705
Kieffer, J.C., 69, 705, 720
Kim, Y.H., xxi, 299, 705
Kimber, D., xxiii
kinetic energy, 409
King, R., 182, 696
Knuth, D.E., 153, 705
Kobayashi, K., 610, 703
Kolmogorov, A.N., 3, 345, 417, 463, 507, 702, 706
Kolmogorov complexity, xv, xviii, xix, 1, 3, 4, 10–12, 428, 466, 463–508, 686
conditional, 467
and entropy, 473, 502
of integers, 475
lower bound, 469, 502
universal probability, 490
upper bound, 501
Kolmogorov structure function, 496, 503, 507
Kolmogorov sufficient statistic, 496, 497, 508
Kolmogorov’s inequality, 626
Kontoyiannis, Y., xxi
Körner, J., 241, 325, 347, 358, 408, 609, 610, 690, 697, 698, 701, 706
Kotel’nikov, V.A., 706
Kraft, L.G., 158, 706
Kraft inequality, 103, 107–110, 112, 113, 116–118, 127, 138, 141, 143, 158, 473, 484, 494
Krichevsky, R.E., 706
Kuhn-Tucker conditions, 164, 177, 191, 314, 331, 617, 618, 621, 622
Kulkarni, S.R., 698, 707, 718
Kullback, J.H., 707
Kullback, S., xix, 55, 408, 707
Kullback Leibler distance, 20, 55, 251, see also relative entropy
L1 distance, 369
Lagrange multipliers, 110, 153, 161, 276, 313, 330, 334, 335, 421
Laird, N.M., 698
Lamping, J., xxi
Landau, H.J., 272, 299, 707
Landauer, R., 56, 691
Langdon, G.G., 705, 707, 713

--- Page 65 ---
INDEX
739
Lapidoth, A., xxi, 707
Laplace, P.S., 488, 489
Laplace distribution, 257, 661
Laplace estimate, 488
large deviation theory, 4, 12, 357, 360
Latané, H.A., 182, 655, 707
Lavenberg, S., xxiii
law of large numbers, 57, 199, 245, 267, 319, 326, 355–357, 361, 403, 477, 479, 520, 522, 615
incompressible sequences, 477, 502
method of types, 355
weak law, 57, 58, 65, 196, 245, 361, 380, 479
lecturer, 561
Lee, E.A., 707
Leech, J., 707
Lehmann, E.L., 56, 707
Leibler, R.A., 55, 707
Lempel, A., 428, 442, 462, 707, 721, see also Lempel-Ziv coding
Lempel-Ziv,
fixed database, 459
infinite dictionary, 458
sliding window, 443
tree structured, 448
Lempel-Ziv algorithm, xxiii, 441
Lempel-Ziv coding, 440–456
Lempel-Ziv compression, 360
Lempel-Ziv parsing, 427
letter, 105, 168–171, 174, 175, 209, 210, 224, 226, 233
Leung, C.S.K., 593, 609, 610, 696, 711
Levin, L.A., 507, 707
Levinson algorithm, 419
Levy’s martingale convergence theorem, 647
lexicographic order, 327, 472
Li, M., 508, 707
Liao, H., 10, 609, 708
liar paradox, 483
Lieb, E.J., 693
likelihood, 20, 365, 377, 404, 482, 508
likelihood ratio, 482
likelihood ratio test, 377, 378, 385, 389
Lin, S., 708
Lind, D., 708
Linde, Y., 708
Linder, T., 708
Lindley, D., 708
linear algebra, 211
linear code, 214
linear inequalities, 534
linear predictive coding, 416
list decoding, 517, 575
Liversidge, A., 708
Lloyd, S.P., 708
Lloyd algorithm, 303
local realism, 56
logarithm,
base of, 14
lognormal distribution, 662
log likelihood, 65, 67, 405
log-optimal portfolio, 616–624, 626–629, 649, 653, 654, 656
competitive optimality, 627, 651
log sum inequality, 31–33, 44
Longo, G., 697
Lotto, 178
Louchard, G., 708
Lovász, L., 226, 241, 708
low density parity check (LDPC) codes, 215
Lucky, R.W., 170, 171, 708
Lugosi, G., 698, 707, 708
LZ77, 441
LZ78, 441
MacKay, D.J.C., 215, 708, 709
macrostate, 55, 409, 411, 412
MacWilliams, F.J., 708
Madhow, U., 704
magnetic recording, 94, 101, 105, 158
Malone, D., 175
Mandelbrot set, 471
Marcus, B., 158, 708
margin, 181
marginal distribution, 297, 333
Markov approximation, 169, 646
Markov chain, 35, 36, 39, 40, 47, 52, 71–100, 144, 206, 258, 294, 295, 423, 458, 470, 497, 499, 578–580, 584, 687
aperiodic, 72, 78
functions of, 84
irreducible, 72, 78, 98
stationary distribution, 73
Markov fields, 35
Markov lemma, 586
Markov process, 87, 100, 144, 422, 428, see also Gauss-Markov process
Markov’s inequality, 49, 64, 157, 238, 392, 460, 621, 627, 648, 649
Markowitz, H., 614
Marks, R.J., 708
Marshall, A., 708, 709
Martian, 143
Martin-Löf, P., 507, 709
martingale, 647
martingale convergence theorem, 626
Marton, K., 609, 610, 706, 709
Marzetta, T.L., 693
Massey, J.L., 709
mathematics, xvi
Mathis, C., xxi
Mathys, P., 709
matrix, 88, 95, 99, 200, 212, 239, 337, 338, 340, 342, 397, 432, 458, 657, 681, 682, 687
channel transition, 190
doubly stochastic, 190
parity check, 211
permutation, 88
probability transition, 72
trace, 278
transition, 77, 88
matrix inequalities, 687
max-flow min-cut, 512
maximal probability of error, 204, 207, 264, 268
maximum a posteriori, 388
maximum entropy, xviii, 51, 92, 96, 255, 258, 263, 282, 289, 375, 409, 412–415, 417, 420–425, 451
conditional limit theorem, 371
prediction error, 423
spectral density, 419, 421
maximum entropy distribution, 30, 364, 375, 409, 410, 412–414
maximum entropy graph, 97
maximum entropy process, 419, 422
maximum likelihood, 201, 231, 500
maximum likelihood decoding, 231
maximum likelihood estimation, 404
Maxwell-Boltzmann distribution, 409, 662
Maxwell’s demon, 507
maze, 97
Mazo, J., xxiii
McDonald, R.A., 345, 709
McEliece, R.J., 696, 697, 709
McLaughlin, S.W., 718
McMillan, B., 69, 158, 709, see also Shannon-McMillan-Breiman theorem
McMillan’s inequality, 141
MDL (minimum description length), 501
mean value theorem, 247
mean-variance theory, 614
measure theory, xx
median, 257
medical testing, 375
Melsa, J.L., 702
memoryless, 184, 216, 280, 513, 563, 572, 588, 593, 610, see also channel, discrete memoryless
merges, 149
Merhav, N., 461, 462, 700, 709, 718, 721
Merton, R.C., 709
Messerschmitt, D.G., 707
method of types, xv, 347, 357, 361, 665
metric, 46
microprocessor, 468
microstate, 55, 409, 411
MIMO (multiple-input multiple-output), 611
minimal sufficient statistic, 38
minimax redundancy, 456
minimum description length, 3, 501, 508
minimum distance, 213, 325, 332
between convex sets, 332
relative entropy, 367
minimum variance, 396
minimum weight, 212
Minkowski, H., 710
Mirsky, L., 710
Mitchell, J.L., 711
mixed strategy, 391
mobile telephone, 607
models of computation, 464
modem, 273, 442
modulation, 3, 263
modulo 2 arithmetic, 211, 308, 596



INDEX

741  
molecules, 409  
moments, 255, 414, 614  
Mona Lisa, 471, 499  
money, 160, 164, 171, 172, 176–178, 487, 631, 634, see also wealth  
monkey, 480, 482, 504  
Moore, E.F., 158, 702  
Morgenstern, O., 710  
Morrell, M., xxiii  
Morse code, 103, 104  
Moy, S.C., 69, 710  
multipath, 292, 611  
multiple access channel, 10, 518, 524, 589, 594, 609  
    achievability, 530  
    binary erasure channel, 527  
    binary erasure multiple access channel, 594  
    binary multiplier channel, 527  
    capacity region, 526  
    convexity, 534  
    converse, 538  
    cooperative capacity, 596  
    correlated source, 593  
    duality with Slepian-Wolf coding, 558  
    erasure channel, 529  
    feedback, 594  
    Gaussian, 514, 598, 607  
    independent BSC’s, 526  
    multiplexing, 273, 515, 547  
    multi-user information theory, see network information theory  
    multivariate distributions, 411  
    multivariate normal, 249, 254, 287, 305, 315, 413, 417, 679  
music, 1, 428  
mutual fund, 653  
mutual information, xvii, 12, 20, 159, 252, 656, 686  
    chain rule, 24  
    conditional, 45, 49  
    continuous random variables, 251  
    non-negativity, 29  
    properties, 43  
Myers, D.L., 718  
Nagaoka, H., 690  
Nahamoo, D., xxiii  
Narayan, P., 697, 707  
nats, 14, 244, 255, 313  
Nayak, P.P., xxi  
Neal, R.M., 215, 708, 719  
nearest neighbor, 303  
nearest neighbor decoding, 3  
neighborhood, 361, 638  
Nelson, R., xxi  
network, 11, 270, 273, 274, 509–511, 519, 520, 587, 588, 592, 594  
network information theory, xv, xix, 3, 10, 11, 509–611  
    feedback, 593  
Neumann, J.von, 710  
Newton, I., xvii, 4  
Newtonian physics, 490  
Neyman, J., 710  
Neyman-Pearson lemma, 376, 398  
Nielsen, M., 241, 710  
Nobel, A., xxiii  
noise, xvii, xix, 1, 3, 11, 183, 224, 234, 237, 257, 261, 265, 272–274, 276–281, 289, 291–293, 297–299, 324, 509, 513–516, 519, 520, 533, 546, 548, 588  
    colored, 277  
noiseless channel, 8, 558  
noisy typewriter, 186  
Noll, P., 704  
nonnegative definite matrix, 284, 285  
nonnegativity,  
    entropy, 15  
    mutual information, 29  
    relative entropy, 20, 29  
nonsense, 464, 482, 504  
norm, 297  
    Euclidean, 297  
normal distribution, 38, 254, 269, 311, 411, 414, 662, 675, see also Gaussian channel, Gaussian source  
    generalized, 662  
    maximum entropy property, 254  
null space, 211  
Nyquist, H., 270, 272, 710  

Occam’s Razor, 1, 4, 463, 481, 488, 490, 500  
odds, 11, 67, 159, 162–164, 176–180, 626, 645  
    even, 159  
    fair, 159, 167, 176  
    subfair, 164, 176  
    superfair, 164  
    uniform, 172  
    uniform fair, 163  
Olkin, I., 708, 709  
Olshen, R.A., 693  
Omura, J.K., 718, 710  
onion-peeling, 546  
Oppenheim, A., 710  
optical channel, 101  
optimal code length, 148, 149  
optimal decoding, 231, 514  
optimal doubling rate, 162, 165, 166  
optimal portfolio, 613, 626, 629, 652  
portfolio strategy, 620, 629–631, 634, 636, 643  
portfolio theory, xv, 613  
positive definite matrix, 279, 686, 687  
Posner, E., 696  
power, 84, 116, 142, 273, 293, 295, 297, 298, 320, 324, 357, 415, 513–515, 517, 518, 546–548, 606, 607, 610, 674  
    power constraint, 261, 262–264, 266, 268, 270, 274, 277, 278, 281, 289, 291, 292, 296, 513, 547  
    power spectral density, 272, 289, 415  
Pratt, F., 712  

prediction, 11  
prediction error, 423  
prefix, 106, 109, 110, 118, 124, 149, 150, 443, 473  
prefix code, 109, 110, 118, 148, 150  
principal minor, 680, 681  
prior, 385, 388, 389, 435, 436  
    Bayesian, 384  
Proakis, J., 692  
probability density, 243, 250, 420, 425  
probability mass function, 5  
probability of error,  
    Bayesian, 385  
    maximal, 195, 204  
probability simplex, 348, 359, 362, 378–380, 385, 386, 391, 408  
probability theory, 1, 12  
probability transition matrix, 7, 72, 73, 226, 524  
process, 183  
program length, 3, 463  
prolate spheroidal functions, 272  
proportional betting, 487  
proportional gambling, 162–164, 173, 182, 619, 645  
punctuation, 168  
Pursley, M.B., 697, 703  
Pythagorean theorem, 367, 368  
quantization, 247, 248, 251, 263, 301–303, 312, 363  
quantum channel capacity, 56  
quantum data compression, 56  
quantum information theory, 11, 241  
quantum mechanics, 11, 56, 241  
queen, 80  
Rabiner, L.R., 712  
race, see horse race  
radio, 261, 270, 547, 560  
radium, 257  
random box size, 67  
random coding, 3, 201, 204, 230, 324, 565  
random number generation, 134  
random process,  
    Bernoulli, 98  
random questions, 53  
random variable, 5, 6, 13, 14, 103  
    Bernoulli, 53, 63  
    generation, 134, 155  
random walk, 78  
randomization, 627  
rank, 211, 393  
Rao, C.R., 712  
Ratcliff, D., 697  
rate,  
    achievable, see achievable rate  
    entropy, see entropy rate  
rate distortion, xv, 301–347, 582, 585, 586, 596, 610, 686  
    achievability, 306, 318  
    Bernoulli source, 307, 336  
    computation, 332  
    converse, 316  
    erasure distortion, 338  
    Gaussian source, 310, 311, 325, 336  
    infinite distortion, 336  
    multivariate Gaussian source, 336  
    operational definition, 307  
    parallel Gaussian source, 314  
    Shannon lower bound, 337  
    with side information, 580, 596  
    squared error distortion, 310, 338  
rate distortion code, 305, 316, 321, 324, 325, 329, 341, 583  
    optimal, 339  
rate distortion function, 306–308, 310, 311, 313–316, 321, 327, 333, 334, 337–340, 344, 596, 610  
    convexity, 316  
    information, 307  
rate distortion region, 306, 586  
rate distortion theorem, 307, 310, 324, 325, 336, 341, 583, 585  
rate distortion theory, 10, 301, 303, 307, 357  
rate region, 535, 536, 557, 569, 592, 593, 602–605, 608  
Rathie, P.N., 662, 718  
Raviv, J., 690  
Ray-Chaudhuri, D.K., 214, 693  
Rayleigh, G.G., 611, 702  
Rayleigh distribution, 662  
rebalanced portfolio, 613, 629–632, 634, 636, 638, 639, 643  
receiver, 183  
recurrence, 91, 457, 459, 460  
recurrence time, 444, 445  

recursion, 90, 95, 123, 469  
redistribution of wealth, 82  
redundancy, 148, 171, 184, 210, 429, 430, 435, 436, 456, 461, 462, 631  
    minimax, 429  
Reed, I.S., 214, 712  
Reed-Solomon codes, 214, 215  
Reiffen, B., 666, 719  
reinvest, 181, 615  
relative entropy, xvii, xix, 4, 9, 11, 12, 20, 25, 30, 43, 52, 68, 81, 87, 112, 115, 151, 252, 259, 305, 332, 333, 362, 366, 368, 369, 378–384, 401, 421, 427, 429, 545, 658–660, 665, 686  
    χ2 bound, 400  
    asymmetry, 52  
    bounds, 663  
    chain rule, 25  
    convexity, 33  
    and Fisher information, 401  
    L1 bound, 398  
    non-negativity, 29, 50  
    properties, 43  
relative entropy distance, 82, 356, 433  
relative entropy neighborhood, 361  
relay channel, 510, 516, 571, 572, 591, 595, 610  
    achievability, 573  
    capacity, 573  
    converse, 572  
    degraded, 571, 573, 591, 610  
    feedback, 591  
    Gaussian, 516  
    physically degraded, 571, 573, 591  
    reversely degraded, 575  
Renyi entropy, 676, 677  
Renyi entropy power, 677  
reproduction points, 302  
reverse water-filling, 315, 336, 345  
Reza, F.M., 712  
Rice, S.O., 712  
Riemann integrability, 248  
Rimoldi, B., 702, 712  
risk-free asset, 614  
Rissanen, J., 158, 420, 462, 508, 691, 707, 712, 713  
Roche, J., xxi, xxiii  
rook, 80  
Roy, B., xxi  
Rubin, D.B., 698  
run length coding, 49  
Ryabko, B.Ya., 430, 461, 713  
saddlepoint, 298  
Salehi, M., xxiii, 695, 696  
Salz, J., xxiii  
sample correlation, 415  
sampling theorem, 272  
Samuelson, P.A., 656, 709, 713  
sandwich argument, 69, 644, 648  
Sanov, I.N., 408, 713  
Sanov’s theorem, 362, 378, 386, 391, 398, 403  
Sardinas, A.A., 149, 713  
Sardinas-Patterson test, 149  
satellite, 215, 261, 509, 515, 565  
Sato, H., 610, 713  
Savari, S.A., 713  
Sayood, K., 713  
Schafer, R.W., 712  
Schalkwijk, J.P.M., 609, 713, 720  
Scheffé, H., 56, 707  
Schnorr, C.P., 507, 713, 714  
Scholtz, R.A., 702  
Schrödinger’s wave equation, xvii  
Schultheiss, P.M., 345, 709  
Schumacher, B., 705  
Schwalkwijk, J.P.M., 705  
Schwarz, G., 714  
score function, 393, 394  
second law of thermodynamics, xviii, 4, 11, 55, 81, 87, 507, see also statistical mechanics  
    concavity, 100  
self-information, 13, 22  
self-punctuating, 468  
self-reference, 483  
sequence length, 55  
sequential projection, 400  
set sum, 675  
sgn function, 132  
Shakespeare, 482  
Shamai, S., 692, 714  
Shannon code, 115, 122, 131, 132, 142, 145, 463, 470, 613  
    competitive optimality, 130, 132, 142, 158  
Shannon guessing game, 174



--- Page 71 ---  
INDEX  
745  
Shannon lower bound, 337  
Shannon’s first theorem (source coding theorem), 115  
Shannon’s second theorem (channel coding theorem), 189, 192  
Shannon’s third theorem (rate distortion theorem), 307  
Shannon, C.E., xv, xviii, 55, 69, 100, 157, 171, 174, 182, 205, 240, 270, 299, 345, 609, 656, 687, 699, 714, 715, see also Shannon code, Shannon-Fano-Elias code, Shannon-McMillan-Breiman theorem  
Shannon-Fano code, 158, 491, see also Shannon code  
Shannon-Fano-Elias code, 127, 130, 428  
Shannon-McMillan-Breiman theorem, 69, 644–649  
Shannon-Nyquist sampling theorem, 272  
Sharpe, W.F., 614, 715  
Sharpe-Markowitz theory, 614  
Shields, P.C., 462, 715  
Shimizu, M., xxiii  
Shor, P.W., 241, 691, 693, 698  
Shore, J.E., 715  
short selling, 181  
Shtarkov, Y.M., 631, 656, 719  
Shtarkov, Y.V., 715  
shuffle, 84, 89  
Shwartz, A., 715  
side information,  
and source coding, 575  
side information, xvii, 12, 159, 165, 166, 180, 255, 574, 576, 580–583, 596, 610, 623, 652  
and doubling rate, 165, 622  
Siegel, P.H., 704  
σ algebra, 644  
signal, 1, 171, 192, 199, 234, 258, 262–299, 513, 517, 519, 533, 544, 561, 607  
Sigurjonsson, S., xxi  
Silicon Dreams, 170  
silver iodide crystals, 293  
simplex, 348, 378, 380, 385, 386, 391, 408, 617, 618  
sinc function, 271  
Sleator, D., 691  
Slepian, D., 272, 299, 549, 609, 715  
Slepian-Wolf coding, 10, 549–560, 575, 581, 586, 592, 593, 595, 598, 603–605, 608–610  
achievability, 551  
converse, 555  
duality with multiple access channels, 558  
slice code, 122  
slice questions, 121  
sliding window Lempel-Ziv, 441  
Sloane, N.J.A., 707, 708, 720  
smallest probable set, 64  
Smolin, J., 691, 698  
SNR (Signal to Noise Ratio), 273, 514, 516  
Solomon, G., 214  
Solomonoff, R.J., 3, 4, 507, 716  
source, 103, 337  
binary, 307  
Gaussian, 310  
source channel coding theorem, 218, 223  
source channel separation, 218, 318, 344, 592, 593  
source code, 103, 123, 552, 631  
source coding, 60, 134, 447, 473, 511  
and channel capacity, 430  
with side information, 575, 595  
source coding theorem, 144, 158  
space-time coding, 611  
Spanish, 561, 562, 606  
spectral representation theorem, 315  
spectrum, 271, 279, 280, 315, 415, 417, 419, 421  
spectrum estimation, 415  
speech, 1, 87, 101, 171, 218, 305, 416  
sphere, 265, 297, 324, 675  
sphere covering, 324  
sphere packing, 10, 324, 325  
squared error, 302, 393, 423, 683  
squared error distortion, 336  
St. Petersburg paradox, 181, 182  
Stam, A., 674, 687, 716  
state diagram, 95  
state transition, 73, 465  
stationary, 4, 69, 114, 168, 220, 221, 279, 297, 415–417, 423, 428, 444, 446, 451, 453, 455, 458, 462, 613, 625, 626, 646, 647, 651, 659, 681  
stationary distribution, 73, 77–79, 96  
stationary ergodic processes, 69  

--- Page 72 ---  
746  
INDEX  
stationary ergodic source, 219  
stationary market, 624  
stationary process, 71, 142, 644  
statistic, 36, 38, 400  
Kolmogorov sufficient, 496  
minimal sufficient, 38  
sufficient, 38  
statistical mechanics, 4, 6, 11, 55, 56, 425  
statistics, xvi–xix, 1, 4, 12, 13, 20, 36–38, 169, 347, 375, 497, 499  
Steane, A., 716  
Stein’s lemma, 399  
stereo, 604  
Stirling’s approximation, 351, 353, 405, 411, 666  
stochastic process, 71, 72, 74, 75, 77, 78, 87, 88, 91, 93, 94, 97, 98, 100, 114, 166, 219, 220, 223, 279, 415, 417, 420, 423, 455, 625, 626, 646  
ergodic, see ergodic process  
function of, 93  
Gaussian, 315  
without entropy rate, 75  
stock, 9, 613–615, 619, 624, 626, 627, 629–634, 636, 637, 639–641, 652, 653  
stock market, xix, 4, 9, 159, 613, 614–617, 619–622, 627, 629–631, 634, 636, 649, 652, 653, 655, 656  
Stone, C.J., 693  
stopping time, 55  
Storer, J.A., 441, 459, 716  
strategy, 160, 163, 164, 166, 178, 391, 392, 487  
investment, 620  
strong converse, 208, 240  
strongly jointly typical, 327, 328  
strongly typical, 326, 327, 357, 579, 580  
strongly typical set, 342, 357  
Stuart, A., 705  
Student’s t distribution, 662  
subfair odds, 164  
submatrix, 680  
subset, xx, 8, 66, 71, 183, 192, 211, 222, 319, 347, 520, 644, 657, 668–670  
subset inequalities, 668–671  
subsets, 505  
entropy rate, 667  
subspace, 211  
sufficient statistic, 13, 36, 37, 38, 44, 56, 209, 497–499  
minimal, 38, 56  
suffix code, 145  
superfair odds, 164, 180  
supermartingale, 625  
superposition coding, 609  
support set, 29, 243, 244, 249, 251, 252, 256, 409, 676–678  
surface area, 247  
Sutivong, A., xxi  
Sweetkind-Singer, J., xxi  
symbol, 103  
symmetric channel, 187, 190  
synchronization, 94  
Szasz’s inequality, 680  
Szegő, G., 703  
Szpankowski, W., 716, 708  
Szymanski, T.G., 441, 459, 716  
Tanabe, M., 713  
Tang, D.L., 716  
Tarjan, R., 691  
TDMA (Time Division Multiple Access), 547, 548  
Telatar, I.E., 716  
telegraph, 441  
telephone, 261, 270, 273, 274  
channel capacity, 273  
Teletar, E., 611, 716  
temperature, 409, 411  
ternary, 119, 145, 157, 239, 439, 504, 527  
ternary alphabet, 349  
ternary channel, 239  
ternary code, 145, 152, 157  
text, 428  
thermodynamics, 1, 4, see also second law of thermodynamics  
Thitimajshima, P., 692  
Thomas, J.A., xxi, 687, 694, 695, 698, 700, 716  
Thomasian, A.J., 692  
Tibshirani, R., 699  
time symmetry, 100  
timesharing, 527, 532–534, 538, 562, 598, 600  
Tjalkens, T.J., 716, 719  
Toeplitz matrix, 416, 681  
Tornay, S.C., 716  

--- Page 73 ---  
INDEX  
747  
trace, 279, 547  
transition matrix, 77, 92, 98, 144, 190  
doubly stochastic, 83, 88, 190  
transmitter, 266, 294, 296, 299, 515, 517–519, 546, 573, 574, 588, 601, 611  
Treasury bonds, 614  
tree,  
code, 107  
Huffman, 124  
random, 89  
tree structured Lempel-Ziv, 441, 442  
triangle inequality, 20, 369  
triangular distribution, 662  
trigram model, 171  
Troﬁmov, V.K., 706  
Trott, M., xxiii  
Tseng, C.W., xxiii  
Tsoucas, P., 700  
Tsybakov, B.S., 716  
Tunstall, B.P., 716  
Tunstall coding, 460  
turbo codes, 3, 205, 215  
Turing, A., 465  
Turing machine, xix, 465, 466  
Tusn´ady, G., 332, 335, 346, 697  
Tuttle, D.L., 182, 707  
TV, 509, 560, 561  
twin, 171  
two envelope problem, 179  
two level signalling, 262  
two stage description, 496  
two-way channel, 510, 519, 594, 602, 609  
type, 342, 347, 348–350, 353–356, 358, 360, 361, 366, 367, 371, 373, 374, 378, 391, 408, 474, 490, 499, 570, 666  
type class, 348–351, 353–356, 666  
typewriter, 74, 192, 224, 235, 482  
typical sequence, 11, 12, 57, 63, 245, 381, 522  
typical set, 57, 59, 61, 62, 64, 68, 77, 196, 220, 227, 245, 247, 258, 319, 321, 356, 381, 382, 384, 524, 551  
conditionally typical, 341  
data compression, 60  
distortion typical, 319  
properties, 59, 64, 245  
strongly typical, 326  
volume, 245  
Ullman, J.D., 704  
uncertainty, 5, 6, 11, 13, 15, 20, 22, 24, 31, 53, 83, 89, 170, 517, 518, 593  
Ungerboeck, G., 716  
uniform distribution, 5, 30, 43, 83, 88, 148, 163, 190, 195, 202, 204, 209, 210, 228, 268, 338, 375, 408, 411, 412, 434, 436, 437, 553, 662, 663  
uniform fair odds, 163, 166, 176, 626  
uniquely decodable code, see code, uniquely decodable  
universal computer, 465, 501  
universal data compression, 333, 457  
universal gambling, 487, 488, 507  
universal portfolios, 629–643, 651  
finite horizon, 631  
horizon free, 638  
universal probability, 481, 487, 489–491, 502, 503, 507, 686  
universal probability mass function, 481  
universal source, 358  
universal source code, 357, 360, 461  
universal source coding, xv, 355, 427–462  
error exponent, 400  
universal Turing machine, 465, 480  
Unix, 443  
Urbanke, R., 702, 712  
V.90, 273  
V’yugin, V.V., 507, 718  
Vajda, I., 716  
Valiant, L.G., 717  
Van Campenhout, J.M., 717  
Van der Meulen, E., xxiii, 609–611, 699, 702, 717  
Van Trees, H.L., 716  
Vapnik, V.N., 717  
variable-to-fixed length coding, 460  
variance, xx, 36, 37, 64, 65, 255, 261, 265, 272, 292, 315, 325, 389, 393, 396, 614, 655, 681, 685, see also covariance matrix  
variational distance, 370  
vector quantization, 303, 306  
Venkata, R., xxi  
Venkatesh, S.S., 707  
Venn diagram, 23, 47, 50, 213  
Verdu, S., 690, 698, 703, 704, 714, 717, 718



INDEX  
Verdugo Lazo, A.C.G., 662, 718  
video, 218  
video compression, xv  
Vidyasagar, M., 718  
Visweswariah, K., 698, 718  
vitamins, xx  
Vitanyi, P., 508, 707  
Viterbi, A.J., 718  
Vitter, J.S., 718  
vocabulary, 561  
volume, 67, 149, 244–247, 249, 265, 324, 675, 676, 679  
Von Neumann, J., 11  
Voronoi partition, 303  
waiting time, 99  
Wald, A., 718  
Wallace, M.S., 697  
Wallmeier, H.M., 692  
Washington, 551  
Watanabe, Y., xxiii  
water-filling, 164, 177, 277, 279, 282, 289, 315  
waveform, 270, 305, 519  
weak, 342  
weakly typical, see typical  
wealth, 175  
wealth relative, 613, 619  
weather, 470, 471, 550, 551  
Weaver, W.W., 715  
web search, xv  
Wei, V.K.W., 691, 718  
Weibull distribution, 662  
Weinberger, M.J., 711, 718  
Weiner, N., 718  
Weiss, A., 715  
Weiss, B., 462, 710, 715  
Welch, T.A., 462, 718  
Wheeler, D.J., 462, 693  
white noise, 270, 272, 280, see also Gaussian channel  
Whiting, P.A., 702  
Wiener, N., 718  
Wiesner, S.J., 691  
Wilcox, H.J., 718  
Willems, F.M.J., xxi, 461, 594, 609, 716, 718, 719  
window, 442  
wine, 153  
wireless, xv, 215, 611  
Witsenhausen, H.S., 719  
Witten, I.H., 691, 719  
Wolf, J.K., 549, 593, 609, 700, 704, 715  
Wolfowitz, J., 240, 408, 719  
Woodward, P.M., 719  
Wootters, W.K., 691  
World Series, 48  
Wozencraft, J.M., 666, 719  
Wright, E.V., 168  
wrong distribution, 115  
Wyner, A.D., xxiii, 299, 462, 581, 586, 610, 703, 719, 720  
Wyner, A.J., 720  
Yaglom, A.M., 702  
Yamamoto, H., xxiii  
Yang, E.-H., 690, 720  
Yao, A.C., 705  
Yard, J., xxi  
Yeung, R.W., xxi, xxiii, 610, 692, 720  
Yockey, H.P., 720  
Young’s inequality, 676, 677  
Yu, Bin, 691  
Yule-Walker equations, 418, 419  
Zeevi, A., xxi  
Zeger, K., 708  
Zeitouni, O., 698  
zero-error, 205, 206, 210, 226, 301  
zero-error capacity, 210, 226  
zero-sum game, 131  
Zhang, Z., 609, 690, 712, 720  
Ziv, J., 428, 442, 462, 581, 586, 610, 703, 707, 711, 719–721, see also Lempel-Ziv coding  
Ziv’s inequality, 449, 450, 453, 455, 456  
Zurek, W.H., 507, 721  
Zvonkin, A.K., 507, 707